Authors,Researcher Ids,ORCIDs,title,Volume,Issue,DOI,Document Type,Publication Date,Abstract,ISSN,eISSN,ISBN,Pages,Publisher,Proceedings title,Keywords,text,label
"Lagutin, Evgeny; Gavrilov, Daniil; Kalaidin, Pavel",,"Lagutin, Evgeny/0000-0003-3078-6377",Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning,,, ,Proceedings Paper ,2021.0,"Likelihood training and maximization-based decoding result in dull and repetitive generated texts even when using powerful language models (Holtzman et al., 2019). Adding a loss function for regularization was shown to improve text generation output by helping avoid unwanted properties, such as contradiction or repetition (Li et al., 2020). In this work, we propose fine-tuning a language model by using policy gradient reinforcement learning, directly optimizing for better generation. We apply this approach to minimizing repetition in generated text, and show that, when combined with unlikelihood training (Welleck et al., 2020), our method further reduces repetition without impacting the language model quality. We also evaluate other methods for improving generation at training and decoding time, and compare them using various metrics aimed at control for better text generation output.",,,978-1-954085-02-2,1432-1441, , 16th Conference of the European-Chapter-of-the-Association-for-Computational-Linguistics (EACL)16th Conference of the European-Chapter-of-the-Association-for-Computational-Linguistics (EACL) ,,"Title:Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning

 Likelihood training and maximization-based decoding result in dull and repetitive generated texts even when using powerful language models (Holtzman et al., 2019). Adding a loss function for regularization was shown to improve text generation output by helping avoid unwanted properties, such as contradiction or repetition (Li et al., 2020). In this work, we propose fine-tuning a language model by using policy gradient reinforcement learning, directly optimizing for better generation. We apply this approach to minimizing repetition in generated text, and show that, when combined with unlikelihood training (Welleck et al., 2020), our method further reduces repetition without impacting the language model quality. We also evaluate other methods for improving generation at training and decoding time, and compare them using various metrics aimed at control for better text generation output.",
"Zhu, Linan; Xu, Yifei; Zhu, Zhechao; Bao, Yinwei; Kong, Xiangjie","Kong, Xiangjie/B-8809-2016","Kong, Xiangjie/0000-0003-2698-3319; Zhu, Linan/0000-0002-7451-4421",Fine-Grained Sentiment-Controlled Text Generation Approach Based on Pre-Trained Language Model,13,1,10.3390/app13010264 ,Article ,2023.0,"Sentiment-controlled text generation aims to generate texts according to the given sentiment. However, most of the existing studies focus only on the document- or sentence-level sentiment control, leaving a gap for finer-grained control over the content of generated results. Fine-grained control allows a generated review to express different opinions toward multiple aspects. Some previous works attempted to generate reviews conditioned on aspect-level sentiments, but they usually suffer from low adaptability and the lack of an annotated dataset. To alleviate these problems, we propose a novel pre-trained extended generative model that can dynamically refer to the prompt sentiment, together with an auxiliary classifier that extracts the fine-grained sentiments from the unannotated sentences, thus we conducted training on both annotated and unannotated datasets. We also propose a query-hint mechanism to further guide the generation process toward the aspect-level sentiments at every time step. Experimental results from real-world datasets demonstrated that our model has excellent adaptability in generating aspect-level sentiment-controllable review texts with high sentiment coverage and stable quality since, on both datasets, our model steadily outperforms other baseline models in the metrics of BLEU-4, METETOR, and ROUGE-L etc. The limitation of this work is that we only focus on fine-grained sentiments that are explicitly expressed. Moreover, the implicitly expressed fine-grained sentiment-controllable text generation will be an important puzzle for future work.",,2076-3417,,, ,  ,,"Title:Fine-Grained Sentiment-Controlled Text Generation Approach Based on Pre-Trained Language Model

 Sentiment-controlled text generation aims to generate texts according to the given sentiment. However, most of the existing studies focus only on the document- or sentence-level sentiment control, leaving a gap for finer-grained control over the content of generated results. Fine-grained control allows a generated review to express different opinions toward multiple aspects. Some previous works attempted to generate reviews conditioned on aspect-level sentiments, but they usually suffer from low adaptability and the lack of an annotated dataset. To alleviate these problems, we propose a novel pre-trained extended generative model that can dynamically refer to the prompt sentiment, together with an auxiliary classifier that extracts the fine-grained sentiments from the unannotated sentences, thus we conducted training on both annotated and unannotated datasets. We also propose a query-hint mechanism to further guide the generation process toward the aspect-level sentiments at every time step. Experimental results from real-world datasets demonstrated that our model has excellent adaptability in generating aspect-level sentiment-controllable review texts with high sentiment coverage and stable quality since, on both datasets, our model steadily outperforms other baseline models in the metrics of BLEU-4, METETOR, and ROUGE-L etc. The limitation of this work is that we only focus on fine-grained sentiments that are explicitly expressed. Moreover, the implicitly expressed fine-grained sentiment-controllable text generation will be an important puzzle for future work.",
"Ke, Pei; Zhou, Hao; Lin, Yankai; Li, Peng; Zhou, Jie; Zhu, Xiaoyan; Huang, Minlie","zhu, y x/IVU-7833-2023","Zhou, Hao/0000-0002-3587-5013",CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation,,, ,Proceedings Paper ,2022.0,"Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities(1).",,,978-1-955917-21-6,2306-2319, , 60th Annual Meeting of the Association-for-Computational-Linguistics (ACL)60th Annual Meeting of the Association-for-Computational-Linguistics (ACL) ,,"Title:CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation

 Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities(1).",
"Gu, Yuxuan; Feng, Xiaocheng; Ma, Sicheng; Wu, Jiaming; Gong, Heng; Qin, Bing",,,Improving Controllable Text Generation with Position-Aware Weighted Decoding,,, ,Proceedings Paper ,2022.0,"Weighted decoding methods composed of the pretrained language model (LM) and the controller have achieved promising results for controllable text generation. However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text. In this paper, we illustrate this trade-off is arisen by the controller imposing the target attribute on the LM at improper positions. And we propose a novel framework based on existing weighted decoding methods called CAT-PAW(1), which introduces a lightweight regulator to adjust bias signals from the controller at different decoding positions. Experiments on positive sentiment control, topic control, and language detoxification show the effectiveness of our CAT-PAW upon 4 SOTA models(2).",,,978-1-955917-25-4,3449-3467, , 60th Annual Meeting of the Association-for-Computational-Linguistics (ACL)60th Annual Meeting of the Association-for-Computational-Linguistics (ACL) ,,"Title:Improving Controllable Text Generation with Position-Aware Weighted Decoding

 Weighted decoding methods composed of the pretrained language model (LM) and the controller have achieved promising results for controllable text generation. However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text. In this paper, we illustrate this trade-off is arisen by the controller imposing the target attribute on the LM at improper positions. And we propose a novel framework based on existing weighted decoding methods called CAT-PAW(1), which introduces a lightweight regulator to adjust bias signals from the controller at different decoding positions. Experiments on positive sentiment control, topic control, and language detoxification show the effectiveness of our CAT-PAW upon 4 SOTA models(2).",
"Liu, Dayiheng; Xue, Yang; He, Feng; Chen, Yuanyuan; Lv, Jiancheng","Chen, Yuanyuan/GXG-2130-2022",,μ-Forcing: Training Variational Recurrent Autoencoders for Text Generation,19,1,10.1145/3341110 ,Article ,2020.0,"It has been previously observed that training Variational Recurrent Autoencoders (VRAE) for text generation suffers from serious uninformative latent variables problems. The model would collapse into a plain language model that totally ignores the latent variables and can only generate repeating and dull samples. In this article, we explore the reason behind this issue and propose an effective regularizer-based approach to address it. The proposed method directly injects extra constraints on the posteriors of latent variables into the learning process of VRAE, which can flexibly and stably control the tradeoff between the Kullback-Leibler (KL) term and the reconstruction term, making the model learn dense and meaningful latent representations. The experimental results show that the proposed method outperforms several strong baselines and can make the model learn interpretable latent variables and generate diverse meaningful sentences. Furthermore, the proposed method can perform well without using other strategies, such as KL annealing.",2375-4699,2375-4702,,, ,  ,,"Title:μ-Forcing: Training Variational Recurrent Autoencoders for Text Generation

 It has been previously observed that training Variational Recurrent Autoencoders (VRAE) for text generation suffers from serious uninformative latent variables problems. The model would collapse into a plain language model that totally ignores the latent variables and can only generate repeating and dull samples. In this article, we explore the reason behind this issue and propose an effective regularizer-based approach to address it. The proposed method directly injects extra constraints on the posteriors of latent variables into the learning process of VRAE, which can flexibly and stably control the tradeoff between the Kullback-Leibler (KL) term and the reconstruction term, making the model learn dense and meaningful latent representations. The experimental results show that the proposed method outperforms several strong baselines and can make the model learn interpretable latent variables and generate diverse meaningful sentences. Furthermore, the proposed method can perform well without using other strategies, such as KL annealing.",
"Lee, Jieh-Sheng",,"Lee, Jieh-Sheng/0000-0002-0990-6170",Measuring and Controlling Text Generation by Semantic Search,,,10.1145/3366424.3382086 ,Proceedings Paper ,2020.0,"Our motivation in this work is to measure patent text generation by semantic search, particularly by textual similarity in high dimensional space for neural network models. The objective is to control patent text generation by semantic search. Conceptually it is an attempt to integrate two subfields in NLP: text generation and semantic search. In our previous milestone of the PatentTransformer project, a prototype based on GPT-2 is capable of generating fluent patent title, abstract, independent claim, and dependent claim. However, beneath the surface form, the quality issue in the generated patent text was less explored. How to control text generation is also a hard problem in NLP field. We would like to address these issues in this work and experiment with different approaches. On the measurement side, this work will address the quality measurement issue from the perspective of textual similarity. Based on that, the approaches we propose include two embedding spaces, span-based textual similarity, and language model for patent claim spans. One the control side, we propose a knob-turning approach for controlling text generation based on measuring a range of textual similarity. In this way, we can search for a Goldilocks zone in which the similarity of generated patent text is close to but not too far from prior patents. We hypothesize that patent novelty may exist in such a zone.",,,978-1-4503-7024-0,269-273, , 29th World Wide Web Conference (WWW)29th World Wide Web Conference (WWW) ,,"Title:Measuring and Controlling Text Generation by Semantic Search

 Our motivation in this work is to measure patent text generation by semantic search, particularly by textual similarity in high dimensional space for neural network models. The objective is to control patent text generation by semantic search. Conceptually it is an attempt to integrate two subfields in NLP: text generation and semantic search. In our previous milestone of the PatentTransformer project, a prototype based on GPT-2 is capable of generating fluent patent title, abstract, independent claim, and dependent claim. However, beneath the surface form, the quality issue in the generated patent text was less explored. How to control text generation is also a hard problem in NLP field. We would like to address these issues in this work and experiment with different approaches. On the measurement side, this work will address the quality measurement issue from the perspective of textual similarity. Based on that, the approaches we propose include two embedding spaces, span-based textual similarity, and language model for patent claim spans. One the control side, we propose a knob-turning approach for controlling text generation based on measuring a range of textual similarity. In this way, we can search for a Goldilocks zone in which the similarity of generated patent text is close to but not too far from prior patents. We hypothesize that patent novelty may exist in such a zone.",
"Zou, Xu; Yin, Da; Zhong, Qingyang; Yang, Hongxia; Yang, Zhilin; Tang, Jie",,,Controllable Generation from Pre-trained Language Models via Inverse Prompting,,,10.1145/3447548.3467418 ,Proceedings Paper ,2021.0,"Large-scale pre-trained language models have demonstrated strong capabilities of generating realistic texts. However, it remains challenging to control the generation results. Previous approaches such as prompting are far from sufficient, and lack of controllability limits the usage of language models. To tackle this challenge, we propose an innovative method, inverse prompting, to better control text generation. The core idea of inverse prompting is to use generated text to inversely predict the prompt during beam search, which enhances the relevance between the prompt and the generated text and thus improves controllability. Empirically, we pre-train a large-scale Chinese language model to perform a systematic study using human evaluation on the tasks of open-domain poem generation and open-domain long-form question answering. Results demonstrate that our proposed method substantially outperforms the baselines and that our generation quality is close to human performance on some of the tasks.",,,978-1-4503-8332-5,2450-2460, , 27th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)27th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) ,,"Title:Controllable Generation from Pre-trained Language Models via Inverse Prompting

 Large-scale pre-trained language models have demonstrated strong capabilities of generating realistic texts. However, it remains challenging to control the generation results. Previous approaches such as prompting are far from sufficient, and lack of controllability limits the usage of language models. To tackle this challenge, we propose an innovative method, inverse prompting, to better control text generation. The core idea of inverse prompting is to use generated text to inversely predict the prompt during beam search, which enhances the relevance between the prompt and the generated text and thus improves controllability. Empirically, we pre-train a large-scale Chinese language model to perform a systematic study using human evaluation on the tasks of open-domain poem generation and open-domain long-form question answering. Results demonstrate that our proposed method substantially outperforms the baselines and that our generation quality is close to human performance on some of the tasks.",
"Mireshghallah, Fatemehsadat; Goya, Kartik; Berg-Kirkpatrick, Taylor",,,Mix and Match: Learning-free Controllable Text Generation using Energy Language Models,,, ,Proceedings Paper ,2022.0,"Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.",,,978-1-955917-21-6,401-415, , 60th Annual Meeting of the Association-for-Computational-Linguistics (ACL)60th Annual Meeting of the Association-for-Computational-Linguistics (ACL) ,,"Title:Mix and Match: Learning-free Controllable Text Generation using Energy Language Models

 Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.",
"Kumar, Sachin; Malmi, Eric; Severyn, Aliaksei; Tsvetkov, Yulia",,,Controlled Text Generation as Continuous Optimization with Multiple Constraints,34,, ,Proceedings Paper ,2021.0,"As large-scale language model pretraining pushes the state-of-the-art in text generation, recent work has turned to controlling attributes of the text such models generate. While modifying the pretrained models via fine-tuning remains the popular approach, it incurs a significant computational cost and can be infeasible due to lack of appropriate data. As an alternative, we propose MUCOCO-a flexible and modular algorithm for controllable inference from pretrained models. We formulate the decoding process as an optimization problem which allows for multiple attributes we aim to control to be easily incorporated as differentiable constraints to the optimization. By relaxing this discrete optimization to a continuous one, we make use of Lagrangian multipliers and gradient-descent based techniques to generate the desired text. We evaluate our approach on controllable machine translation and style transfer with multiple sentence-level attributes and observe significant improvements over baselines.(1)",1049-5258,,*****************,, , 35th Conference on Neural Information Processing Systems (NeurIPS)35th Conference on Neural Information Processing Systems (NeurIPS) ,,"Title:Controlled Text Generation as Continuous Optimization with Multiple Constraints

 As large-scale language model pretraining pushes the state-of-the-art in text generation, recent work has turned to controlling attributes of the text such models generate. While modifying the pretrained models via fine-tuning remains the popular approach, it incurs a significant computational cost and can be infeasible due to lack of appropriate data. As an alternative, we propose MUCOCO-a flexible and modular algorithm for controllable inference from pretrained models. We formulate the decoding process as an optimization problem which allows for multiple attributes we aim to control to be easily incorporated as differentiable constraints to the optimization. By relaxing this discrete optimization to a continuous one, we make use of Lagrangian multipliers and gradient-descent based techniques to generate the desired text. We evaluate our approach on controllable machine translation and style transfer with multiple sentence-level attributes and observe significant improvements over baselines.(1)",
"Gong, Heng; Feng, Xiaocheng; Qin, Bing",,,DiffuD2T: Empowering Data-to-Text Generation with Diffusion,12,9,10.3390/electronics12092136 ,Article ,2023.0,"Surrounded by structured data, such as medical data, financial data, knowledge bases, etc., data-to-text generation has become an important natural language processing task that can help people better understand the meaning of those data by providing them with user-friendly text. Existing methods for data-to-text generation show promising results in tackling two major challenges: content planning and surface realization, which transform structured data into fluent text. However, they lack an iterative refinement process for generating text, which can enable the model to perfect the text step-by-step while accepting control over the process. In this paper, we explore enhancing data-to-text generation with an iterative refinement process via diffusion. We have four main contributions: (1) we use the diffusion model to improve the prefix tuning for data-to-text generation; (2) we propose a look-ahead guiding loss to supervise the iterative refinement process for better text generation; (3) we extract content plans from reference text and propose a planning-then-writing pipeline to give the model content planning ability; and (4) we conducted experiments on three data-to-text generation datasets and both automatic evaluation criteria (BLEU, NIST, METEOR, ROUGE(L), CIDEr, TER, MoverScore, BLEURT, and BERTScore) and human evaluation criteria (Quality and Naturalness) show the effectiveness of our model. Our model can improve the competitive prefix tuning method by 2.19% in terms of a widely-used automatic evaluation criterion BLEU (BiLingual Evaluation Understudy) on WebNLG dataset with GPT-2 Large as the pretrained language model backbone. Human evaluation criteria also show that our model can improve the quality and naturalness of the generated text across all three datasets.",,2079-9292,,, ,  ,,"Title:DiffuD2T: Empowering Data-to-Text Generation with Diffusion

 Surrounded by structured data, such as medical data, financial data, knowledge bases, etc., data-to-text generation has become an important natural language processing task that can help people better understand the meaning of those data by providing them with user-friendly text. Existing methods for data-to-text generation show promising results in tackling two major challenges: content planning and surface realization, which transform structured data into fluent text. However, they lack an iterative refinement process for generating text, which can enable the model to perfect the text step-by-step while accepting control over the process. In this paper, we explore enhancing data-to-text generation with an iterative refinement process via diffusion. We have four main contributions: (1) we use the diffusion model to improve the prefix tuning for data-to-text generation; (2) we propose a look-ahead guiding loss to supervise the iterative refinement process for better text generation; (3) we extract content plans from reference text and propose a planning-then-writing pipeline to give the model content planning ability; and (4) we conducted experiments on three data-to-text generation datasets and both automatic evaluation criteria (BLEU, NIST, METEOR, ROUGE(L), CIDEr, TER, MoverScore, BLEURT, and BERTScore) and human evaluation criteria (Quality and Naturalness) show the effectiveness of our model. Our model can improve the competitive prefix tuning method by 2.19% in terms of a widely-used automatic evaluation criterion BLEU (BiLingual Evaluation Understudy) on WebNLG dataset with GPT-2 Large as the pretrained language model backbone. Human evaluation criteria also show that our model can improve the quality and naturalness of the generated text across all three datasets.",
"Oken, Barry S.; Orhan, Umut; Roark, Brian; Erdogmus, Deniz; Fowler, Andrew; Mooney, Aimee; Peters, Betts; Miller, Meghan; Fried-Oken, Melanie B.","Erdogmus, Deniz/A-8170-2009","Peters, Betts/0000-0002-7019-079X",Brain-Computer Interface With Language Model-Electroencephalography Fusion for Locked-In Syndrome,28,4,10.1177/1545968313516867 ,Article ,2014.0,"Background. Some noninvasive brain-computer interface (BCI) systems are currently available for locked-in syndrome (LIS) but none have incorporated a statistical language model during text generation. Objective. To begin to address the communication needs of individuals with LIS using a noninvasive BCI that involves rapid serial visual presentation (RSVP) of symbols and a unique classifier with electroencephalography (EEG) and language model fusion. Methods. The RSVP Keyboard was developed with several unique features. Individual letters are presented at 2.5 per second. Computer classification of letters as targets or nontargets based on EEG is performed using machine learning that incorporates a language model for letter prediction via Bayesian fusion enabling targets to be presented only 1 to 4 times. Nine participants with LIS and 9 healthy controls were enrolled. After screening, subjects first calibrated the system, and then completed a series of balanced word generation mastery tasks that were designed with 5 incremental levels of difficulty, which increased by selecting phrases for which the utility of the language model decreased naturally. Results. Six participants with LIS and 9 controls completed the experiment. All LIS participants successfully mastered spelling at level 1 and one subject achieved level 5. Six of 9 control participants achieved level 5. Conclusions. Individuals who have incomplete LIS may benefit from an EEG-based BCI system, which relies on EEG classification and a statistical language model. Steps to further improve the system are discussed.",1545-9683,1552-6844,,387-394, ,  ,,"Title:Brain-Computer Interface With Language Model-Electroencephalography Fusion for Locked-In Syndrome

 Background. Some noninvasive brain-computer interface (BCI) systems are currently available for locked-in syndrome (LIS) but none have incorporated a statistical language model during text generation. Objective. To begin to address the communication needs of individuals with LIS using a noninvasive BCI that involves rapid serial visual presentation (RSVP) of symbols and a unique classifier with electroencephalography (EEG) and language model fusion. Methods. The RSVP Keyboard was developed with several unique features. Individual letters are presented at 2.5 per second. Computer classification of letters as targets or nontargets based on EEG is performed using machine learning that incorporates a language model for letter prediction via Bayesian fusion enabling targets to be presented only 1 to 4 times. Nine participants with LIS and 9 healthy controls were enrolled. After screening, subjects first calibrated the system, and then completed a series of balanced word generation mastery tasks that were designed with 5 incremental levels of difficulty, which increased by selecting phrases for which the utility of the language model decreased naturally. Results. Six participants with LIS and 9 controls completed the experiment. All LIS participants successfully mastered spelling at level 1 and one subject achieved level 5. Six of 9 control participants achieved level 5. Conclusions. Individuals who have incomplete LIS may benefit from an EEG-based BCI system, which relies on EEG classification and a statistical language model. Steps to further improve the system are discussed.",
"Yu, Long; Lu, Yuliang; Yan, Xuehu; Wang, Xianhui",,"Yu, Long/0000-0001-7385-7644",Generative Text Steganography via Multiple Social Network Channels Based on Transformers,13551,,10.1007/978-3-031-17120-8_47 ,Proceedings Paper ,2022.0,"Generative text steganography uses the conditional probability to encode the candidate words when generating tokens by language model, and then selects the corresponding word to output according to the secret message to be embedded, so as to generate stego text. The complex and open characteristics of social network provide a good camouflage environment for the transmission of stego texts, but also bring challenges: transmitting stego text through a single channel is easy to cause the destruction and loss of secret message; the speech of each social account needs to be combined with its background knowledge, so it has different language features. The existing text steganography schemes cannot solve these problems well. This paper proposes a multichannel generative text steganography scheme in the context of social network, which hides secret message into multiple semantically natural texts, even if only a part of which can reconstruct secret message. Combined with the characteristics of social network, the bag-of-words models are used to control the topics of the stego texts in the process of text generation by language model. Two goal programming models are proposed to optimize the topic relevance and text quality of stego text. The experiment verifies the effectiveness of this scheme.",2945-9133,1611-3349,978-3-031-17120-8; 978-3-031-17119-2,606-617, , 11th CCF International Conference on Natural Language Processing and Chinese Computing (NLPCC)11th CCF International Conference on Natural Language Processing and Chinese Computing (NLPCC) ,,"Title:Generative Text Steganography via Multiple Social Network Channels Based on Transformers

 Generative text steganography uses the conditional probability to encode the candidate words when generating tokens by language model, and then selects the corresponding word to output according to the secret message to be embedded, so as to generate stego text. The complex and open characteristics of social network provide a good camouflage environment for the transmission of stego texts, but also bring challenges: transmitting stego text through a single channel is easy to cause the destruction and loss of secret message; the speech of each social account needs to be combined with its background knowledge, so it has different language features. The existing text steganography schemes cannot solve these problems well. This paper proposes a multichannel generative text steganography scheme in the context of social network, which hides secret message into multiple semantically natural texts, even if only a part of which can reconstruct secret message. Combined with the characteristics of social network, the bag-of-words models are used to control the topics of the stego texts in the process of text generation by language model. Two goal programming models are proposed to optimize the topic relevance and text quality of stego text. The experiment verifies the effectiveness of this scheme.",
"Chung, John Joon Young; Kim, Wooseok; Yoo, Kang Min; Lee, Hwaran; Adar, Eytan; Chang, Minsuk",,"Lee, Hwaran/0000-0002-3773-4871",TaleBrush: Sketching Stories with Generative Pretrained Language Models,,,10.1145/3491102.3501819 ,Proceedings Paper ,2022.0,"While advanced text generation algorithms (e.g., GPT-3) have enabled writers to co-create stories with an AI, guiding the narrative remains a challenge. Existing systems often leverage simple turn-taking between the writer and the AI in story development. However, writers remain unsupported in intuitively understanding the AI's actions or steering the iterative generation. We introduce TaleBrush, a generative story ideation tool that uses line sketching interactions with a GPT-based language model for control and sensemaking of a protagonist's fortune in co-created stories. Our empirical evaluation found our pipeline reliably controls story generation while maintaining the novelty of generated sentences. In a user study with 14 participants with diverse writing experiences, we found participants successfully leveraged sketching to iteratively explore and write stories according to their intentions about the character's fortune while taking inspiration from generated stories. We conclude with a reflection on how sketching interactions can facilitate the iterative human-AI co-creation process.",,,978-1-4503-9157-3,, , CHI Conference on Human Factors in Computing Systems (CHI)CHI Conference on Human Factors in Computing Systems (CHI) ,,"Title:TaleBrush: Sketching Stories with Generative Pretrained Language Models

 While advanced text generation algorithms (e.g., GPT-3) have enabled writers to co-create stories with an AI, guiding the narrative remains a challenge. Existing systems often leverage simple turn-taking between the writer and the AI in story development. However, writers remain unsupported in intuitively understanding the AI's actions or steering the iterative generation. We introduce TaleBrush, a generative story ideation tool that uses line sketching interactions with a GPT-based language model for control and sensemaking of a protagonist's fortune in co-created stories. Our empirical evaluation found our pipeline reliably controls story generation while maintaining the novelty of generated sentences. In a user study with 14 participants with diverse writing experiences, we found participants successfully leveraged sketching to iteratively explore and write stories according to their intentions about the character's fortune while taking inspiration from generated stories. We conclude with a reflection on how sketching interactions can facilitate the iterative human-AI co-creation process.",
"Li, Piji; Zhang, Haisong; Liu, Xiaojiang; Shi, Shuming",,,Rigid Formats Controlled Text Generation,,, ,Proceedings Paper ,2020.0,"Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.(1)",,,978-1-952148-25-5,742-751, , 58th Annual Meeting of the Association-for-Computational-Linguistics (ACL)58th Annual Meeting of the Association-for-Computational-Linguistics (ACL) ,,"Title:Rigid Formats Controlled Text Generation

 Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.(1)",
"Yu, Long; Lu, Yuliang; Yan, Xuehu; Jiang, Yue; Wang, Jiayu",,"Yan, Xuehu/0000-0001-6388-1720; Yu, Long/0000-0001-7385-7644; Jiang, Yue/0000-0001-9403-1913",Generative Text Secret Sharing with Topic-Controlled Shadows,2022,,10.1155/2022/7189130 ,Article ,2022.0,"Secret image sharing has been extensively and thoroughly researched. However, in the social network environment, shadow images are subject to compression or noise pollution during uploading and transmitting, which makes it challenging to recover secrets losslessly. Texts are more suited for transmission in social networks as shadows because of the broad variety of application scenarios and inherent robustness. Through a secret sharing technique of k,n threshold, a secret is encrypted as n shadows, where any k or more shadows can recover the secret, while less than k cannot obtain any information on the secret. In this article, we propose a generative text secret sharing scheme with topic-controlled shadows, which encrypts a secret message as a number of semantically natural shadow texts and controls the topics of shadow texts using bag-of-words models during text generation by the language model. This study also proposes two goal programming models to improve the shadow texts' topic relevance and fluency. The shadow texts of the proposed scheme satisfy loss tolerance, semantic comprehensibility, topic controllability, and robustness. An ablation study, comparative test, and anti-detection experiment verify the effectiveness of the proposed scheme.",1939-0114,1939-0122,,, ,  ,,"Title:Generative Text Secret Sharing with Topic-Controlled Shadows

 Secret image sharing has been extensively and thoroughly researched. However, in the social network environment, shadow images are subject to compression or noise pollution during uploading and transmitting, which makes it challenging to recover secrets losslessly. Texts are more suited for transmission in social networks as shadows because of the broad variety of application scenarios and inherent robustness. Through a secret sharing technique of k,n threshold, a secret is encrypted as n shadows, where any k or more shadows can recover the secret, while less than k cannot obtain any information on the secret. In this article, we propose a generative text secret sharing scheme with topic-controlled shadows, which encrypts a secret message as a number of semantically natural shadow texts and controls the topics of shadow texts using bag-of-words models during text generation by the language model. This study also proposes two goal programming models to improve the shadow texts' topic relevance and fluency. The shadow texts of the proposed scheme satisfy loss tolerance, semantic comprehensibility, topic controllability, and robustness. An ablation study, comparative test, and anti-detection experiment verify the effectiveness of the proposed scheme.",
"Lee, Helena H.; Shu, Ke; Achananuparp, Palakorn; Prasetyo, Philips Kokoh; Liu, Yue; Lim, Ee-Peng; Varshney, Lav R.","Achananuparp, Palakorn/GLS-6036-2022; LIM, Ee Peng/E-8562-2012","Achananuparp, Palakorn/0000-0002-7684-1725; LIM, Ee Peng/0000-0003-0065-8665",RecipeGPT: Generative Pre-training Based Cooking Recipe Generation and Evaluation System,,,10.1145/3366424.3383536 ,Proceedings Paper ,2020.0,"Interests in the automatic generation of cooking recipes have been growing steadily over the past few years thanks to a large amount of online cooking recipes. We present RecipeGPT, a novel online recipe generation and evaluation system. The system provides two modes of text generations: (1) instruction generation from given recipe title and ingredients; and (2) ingredient generation from recipe title and cooking instructions. Its back-end text generation module comprises a generative pre-trained language model GPT-2 fine-tuned on a large cooking recipe dataset. Moreover, the recipe evaluation module allows the users to conveniently inspect the quality of the generated recipe contents and store the results for future reference. RecipeGPT can be accessed online at https://recipegpt.org/",,,978-1-4503-7024-0,181-184, , 29th World Wide Web Conference (WWW)29th World Wide Web Conference (WWW) ,,"Title:RecipeGPT: Generative Pre-training Based Cooking Recipe Generation and Evaluation System

 Interests in the automatic generation of cooking recipes have been growing steadily over the past few years thanks to a large amount of online cooking recipes. We present RecipeGPT, a novel online recipe generation and evaluation system. The system provides two modes of text generations: (1) instruction generation from given recipe title and ingredients; and (2) ingredient generation from recipe title and cooking instructions. Its back-end text generation module comprises a generative pre-trained language model GPT-2 fine-tuned on a large cooking recipe dataset. Moreover, the recipe evaluation module allows the users to conveniently inspect the quality of the generated recipe contents and store the results for future reference. RecipeGPT can be accessed online at https://recipegpt.org/",
"Subramani, Nishant; Suresh, Nivedita; Peters, Matthew E.",,,Extracting Latent Steering Vectors from Pretrained Language Models,,, ,Proceedings Paper ,2022.0,"Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.(1)",,,978-1-955917-25-4,566-581, , 60th Annual Meeting of the Association-for-Computational-Linguistics (ACL)60th Annual Meeting of the Association-for-Computational-Linguistics (ACL) ,,"Title:Extracting Latent Steering Vectors from Pretrained Language Models

 Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.(1)",
"Tang, Yu-Siou; Wu, Chung-Hsien",,,Latent Attribute Control for Story Generation,,,10.1109/IALP54817.2021.9675173 ,Proceedings Paper ,2021.0,"Neural open-domain story generation aims to generate long and fluent text as human writing. Recent work attempts to generate stories in fine-grained controls such as plot-like structure and ending valence. Although those outputs comply with the rules of grammar, they generally have logical conflicts and a lack of long-range cohesion because of explicit controlling. In this study, we propose to capture challenging story representation using latent variable modeling for the storytelling model, and we align the encoder output with story latent embeddings. Our approach and baselines are all built on the pre-trained BART language model. Experimental results demonstrated that our model largely improved compared to strong baselines on human evaluation. Human evaluators favored our generated stories, and the results were more relevant to the prompt and more coherent than the baselines.",2159-1962,2159-1970,978-1-6654-8311-7,148-153, , 25th International Conference on Asian Language Processing (IALP)25th International Conference on Asian Language Processing (IALP) ,,"Title:Latent Attribute Control for Story Generation

 Neural open-domain story generation aims to generate long and fluent text as human writing. Recent work attempts to generate stories in fine-grained controls such as plot-like structure and ending valence. Although those outputs comply with the rules of grammar, they generally have logical conflicts and a lack of long-range cohesion because of explicit controlling. In this study, we propose to capture challenging story representation using latent variable modeling for the storytelling model, and we align the encoder output with story latent embeddings. Our approach and baselines are all built on the pre-trained BART language model. Experimental results demonstrated that our model largely improved compared to strong baselines on human evaluation. Human evaluators favored our generated stories, and the results were more relevant to the prompt and more coherent than the baselines.",
"Pan, Weijun; Jiang, Peiyuan; Li, Yukun; Wang, Zhuang; Huang, Junxiang","Pan, Weijun/JRY-9848-2023","Pan, Weijun/0000-0002-9402-135X",Research on automatic pilot repetition generation method based on deep reinforcement learning,17,,10.3389/fnbot.2023.1285831 ,Article ,2023.0,"Using computers to replace pilot seats in air traffic control (ATC) simulators is an effective way to improve controller training efficiency and reduce training costs. To achieve this, we propose a deep reinforcement learning model, RoBERTa-RL (RoBERTa with Reinforcement Learning), for generating pilot repetitions. RoBERTa-RL is based on the pre-trained language model RoBERTa and is optimized through transfer learning and reinforcement learning. Transfer learning is used to address the issue of scarce data in the ATC domain, while reinforcement learning algorithms are employed to optimize the RoBERTa model and overcome the limitations in model generalization caused by transfer learning. We selected a real-world area control dataset as the target task training and testing dataset, and a tower control dataset generated based on civil aviation radio land-air communication rules as the test dataset for evaluating model generalization. In terms of the ROUGE evaluation metrics, RoBERTa-RL achieved significant results on the area control dataset with ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.9962, 0.992, and 0.996, respectively. On the tower control dataset, the scores were 0.982, 0.954, and 0.982, respectively. To overcome the limitations of ROUGE in this field, we conducted a detailed evaluation of the proposed model architecture using keyword-based evaluation criteria for the generated repetition instructions. This evaluation criterion calculates various keyword-based metrics based on the segmented results of the repetition instruction text. In the keyword-based evaluation criteria, the constructed model achieved an overall accuracy of 98.8% on the area control dataset and 81.8% on the tower control dataset. In terms of generalization, RoBERTa-RL improved accuracy by 56% compared to the model before improvement and achieved a 47.5% improvement compared to various comparative models. These results indicate that employing reinforcement learning strategies to enhance deep learning algorithms can effectively mitigate the issue of poor generalization in text generation tasks, and this approach holds promise for future application in other related domains.",1662-5218,,,, ,  ,,"Title:Research on automatic pilot repetition generation method based on deep reinforcement learning

 Using computers to replace pilot seats in air traffic control (ATC) simulators is an effective way to improve controller training efficiency and reduce training costs. To achieve this, we propose a deep reinforcement learning model, RoBERTa-RL (RoBERTa with Reinforcement Learning), for generating pilot repetitions. RoBERTa-RL is based on the pre-trained language model RoBERTa and is optimized through transfer learning and reinforcement learning. Transfer learning is used to address the issue of scarce data in the ATC domain, while reinforcement learning algorithms are employed to optimize the RoBERTa model and overcome the limitations in model generalization caused by transfer learning. We selected a real-world area control dataset as the target task training and testing dataset, and a tower control dataset generated based on civil aviation radio land-air communication rules as the test dataset for evaluating model generalization. In terms of the ROUGE evaluation metrics, RoBERTa-RL achieved significant results on the area control dataset with ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.9962, 0.992, and 0.996, respectively. On the tower control dataset, the scores were 0.982, 0.954, and 0.982, respectively. To overcome the limitations of ROUGE in this field, we conducted a detailed evaluation of the proposed model architecture using keyword-based evaluation criteria for the generated repetition instructions. This evaluation criterion calculates various keyword-based metrics based on the segmented results of the repetition instruction text. In the keyword-based evaluation criteria, the constructed model achieved an overall accuracy of 98.8% on the area control dataset and 81.8% on the tower control dataset. In terms of generalization, RoBERTa-RL improved accuracy by 56% compared to the model before improvement and achieved a 47.5% improvement compared to various comparative models. These results indicate that employing reinforcement learning strategies to enhance deep learning algorithms can effectively mitigate the issue of poor generalization in text generation tasks, and this approach holds promise for future application in other related domains.",
"Sybrandt, Justin; Safro, Ilya",,"Safro, Ilya/0000-0001-6284-7408",CBAG: Conditional biomedical abstract generation,16,7,10.1371/journal.pone.0253905 ,Article ,2021.0,"Biomedical research papers often combine disjoint concepts in novel ways, such as when describing a newly discovered relationship between an understudied gene with an important disease. These concepts are often explicitly encoded as metadata keywords, such as the author-provided terms included with many documents in the MEDLINE database. While substantial recent work has addressed the problem of text generation in a more general context, applications, such as scientific writing assistants, or hypothesis generation systems, could benefit from the capacity to select the specific set of concepts that underpin a generated biomedical text. We propose a conditional language model following the transformer architecture. This model uses the encoder stack to encode concepts that a user wishes to discuss in the generated text. The decoder stack then follows the masked self-attention pattern to perform text generation, using both prior tokens as well as the encoded condition. We demonstrate that this approach provides significant control, while still producing reasonable biomedical text.",1932-6203,,,, ,  ,,"Title:CBAG: Conditional biomedical abstract generation

 Biomedical research papers often combine disjoint concepts in novel ways, such as when describing a newly discovered relationship between an understudied gene with an important disease. These concepts are often explicitly encoded as metadata keywords, such as the author-provided terms included with many documents in the MEDLINE database. While substantial recent work has addressed the problem of text generation in a more general context, applications, such as scientific writing assistants, or hypothesis generation systems, could benefit from the capacity to select the specific set of concepts that underpin a generated biomedical text. We propose a conditional language model following the transformer architecture. This model uses the encoder stack to encode concepts that a user wishes to discuss in the generated text. The decoder stack then follows the masked self-attention pattern to perform text generation, using both prior tokens as well as the encoded condition. We demonstrate that this approach provides significant control, while still producing reasonable biomedical text.",
"Yang, Boya; Peng, Wanli; Xue, Yiming; Zhong, Ping","Peng, Wanli/HOA-9045-2023","Peng, Wanli/0000-0001-9636-6928",A Generation-based Text Steganography by Maintaining Consistency of Probability Distribution,15,11,10.3837/tiis.2021.11.017 ,Article ,2021.0,"Text steganography combined with natural language generation has become increasingly popular. The existing methods usually embed secret information in the generated word by controlling the sampling in the process of text generation. A candidate pool will be constructed by greedy strategy, and only the words with high probability will be encoded, which damages the statistical law of the texts and seriously affects the security of steganography. In order to reduce the influence of the candidate pool on the statistical imperceptibility of steganography, we propose a steganography method based on a new sampling strategy. Instead of just consisting of words with high probability, we select words with relatively small difference from the actual sample of the language model to build a candidate pool, thus keeping consistency with the probability distribution of the language model. What's more, we encode the candidate words according to their probability similarity with the target word, which can further maintain the probability distribution. Experimental results show that the proposed method can outperform the state-of-the-art steganographic methods in terms of security performance.",1976-7277,,,4184-4202, ,  ,,"Title:A Generation-based Text Steganography by Maintaining Consistency of Probability Distribution

 Text steganography combined with natural language generation has become increasingly popular. The existing methods usually embed secret information in the generated word by controlling the sampling in the process of text generation. A candidate pool will be constructed by greedy strategy, and only the words with high probability will be encoded, which damages the statistical law of the texts and seriously affects the security of steganography. In order to reduce the influence of the candidate pool on the statistical imperceptibility of steganography, we propose a steganography method based on a new sampling strategy. Instead of just consisting of words with high probability, we select words with relatively small difference from the actual sample of the language model to build a candidate pool, thus keeping consistency with the probability distribution of the language model. What's more, we encode the candidate words according to their probability similarity with the target word, which can further maintain the probability distribution. Experimental results show that the proposed method can outperform the state-of-the-art steganographic methods in terms of security performance.",
"Xu, Chen; Zhao, Jianyu; Li, Rang; Hu, Changjian; Xiao, Chuangbai","cheng, cheng/JBR-8359-2023; Hu, Changjian/AAI-7633-2020","Zhao, Jianyu/0009-0006-1586-1024",Change or Not: A Simple Approach for Plug and Play Language Models on Sentiment Control,35,, ,Proceedings Paper ,2021.0,"Text generation with sentiment control is difficult without fine-tuning or modifying the model architecture. Plug and Play Language Model (PPLM) utilizes an external sentiment classifier to update the hidden states of GPT-2 at each time step. It does not change the parameters but achieves competitive performance. However, fluency is impaired due to the instability of the hidden states. Moreover, the classifier is not strong because of the way it is trained with partial texts, hence it is difficult to guide the generation in the process. To solve the above problems, in this paper, we first propose a fixed threshold method based on the Valence-Arousal-Dominance (VAD) lexicon to decide whether to change a word, which keeps the fluency of the original LM to the greatest extent. Furthermore, for the improvement of sentiment alignment, we propose a dynamic threshold method that utilizes VAD-based loss to make the threshold dynamic. Experiments demonstrate that our methods outperform the baseline with a great margin significantly both on fluency and sentiment accuracy.",2159-5399,2374-3468,978-1-57735-866-4,15935-15936, , 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence ,,"Title:Change or Not: A Simple Approach for Plug and Play Language Models on Sentiment Control

 Text generation with sentiment control is difficult without fine-tuning or modifying the model architecture. Plug and Play Language Model (PPLM) utilizes an external sentiment classifier to update the hidden states of GPT-2 at each time step. It does not change the parameters but achieves competitive performance. However, fluency is impaired due to the instability of the hidden states. Moreover, the classifier is not strong because of the way it is trained with partial texts, hence it is difficult to guide the generation in the process. To solve the above problems, in this paper, we first propose a fixed threshold method based on the Valence-Arousal-Dominance (VAD) lexicon to decide whether to change a word, which keeps the fluency of the original LM to the greatest extent. Furthermore, for the improvement of sentiment alignment, we propose a dynamic threshold method that utilizes VAD-based loss to make the threshold dynamic. Experiments demonstrate that our methods outperform the baseline with a great margin significantly both on fluency and sentiment accuracy.",
"Krishna, Kalpesh; Iyyer, Mohit",,,Generating Question-Answer Hierarchies,,, ,Proceedings Paper ,2019.0,"The process of knowledge acquisition can be viewed as a question-answer game between a student and a teacher in which the student typically starts by asking broad, open-ended questions before drilling down into specifics (Hintikka, 1981; Hakkarainen and Sintonen, 2002). This pedagogical perspective motivates a new way of representing documents. In this paper, we present SQUASH (Specificity-controlled Question-Answer Hierarchies), a novel and challenging text generation task that converts an input document into a hierarchy of question-answer pairs. Users can click on high-level questions (e.g., Why did Frodo leave the Fellowship?) to reveal related but more specific questions (e.g., Who did Frodo leave with?). Using a question taxonomy loosely based on Lehnert (1978), we classify questions in existing reading comprehension datasets as either GENERAL or SPECIFIC. We then use these labels as input to a pipelined system centered around a conditional neural language model. We extensively evaluate the quality of the generated QA hierarchies through crowdsourced experiments and report strong empirical results.",,,978-1-950737-48-2,2321-2334, , 57th Annual Meeting of the Association-for-Computational-Linguistics (ACL)57th Annual Meeting of the Association-for-Computational-Linguistics (ACL) ,,"Title:Generating Question-Answer Hierarchies

 The process of knowledge acquisition can be viewed as a question-answer game between a student and a teacher in which the student typically starts by asking broad, open-ended questions before drilling down into specifics (Hintikka, 1981; Hakkarainen and Sintonen, 2002). This pedagogical perspective motivates a new way of representing documents. In this paper, we present SQUASH (Specificity-controlled Question-Answer Hierarchies), a novel and challenging text generation task that converts an input document into a hierarchy of question-answer pairs. Users can click on high-level questions (e.g., Why did Frodo leave the Fellowship?) to reveal related but more specific questions (e.g., Who did Frodo leave with?). Using a question taxonomy loosely based on Lehnert (1978), we classify questions in existing reading comprehension datasets as either GENERAL or SPECIFIC. We then use these labels as input to a pipelined system centered around a conditional neural language model. We extensively evaluate the quality of the generated QA hierarchies through crowdsourced experiments and report strong empirical results.",
"Ragsdale, Jarrod; Boppana, Rajendra V.",,,On Designing Low-Risk Honeypots Using Generative Pre-Trained Transformer Models With Curated Inputs,11,,10.1109/ACCESS.2023.3326104 ,Article ,2023.0,"Honeypots are utilized as defensive tools within a monitored environment to engage attackers and gather artifacts for the development of indicators of compromise. However, once these honeypots are deployed, they are rarely updated, making them obsolete and easier to fingerprint as time passes. Furthermore, using fully functional computing and networking devices as honeypots presents the risk of an attacker breaking out from the controlled environment. Large-scale text-generating models, commonly referred to as Large Language Models (LLMs), have seen wide implementation using generative-pretrained transformer (GPT) models. These models have seen an explosion in popularity and have been tuned for various use cases. This paper investigates the use of these models to simulate honeypots that are adaptive to threat engagement without the risk of unintended breakouts. This investigation finds that the method these models use to generate output has limitations that can reveal the deception to a dedicated attacker in extended sessions. To overcome this challenge, this paper presents a method to manage the inputs and outputs to reduce non-deterministic output and token usage of a model generating text in a way that simulates a terminal. An example honeypot is evaluated against a traditional low-risk honeypot, Cowrie, where greater similarity to an actual machine for single commands is achieved. Furthermore, in several multi-step attack scenarios, the proposed architecture reduced the token usage by up to 77% when compared to a baseline scenario that did not manage the inputs to and outputs from an example model. A discussion on the utilization of LLMs for cyber deception, as well as the limitations hindering their broader adoption indicates that LLMs exhibit promise for cyber deception but necessitate further research before achieving widespread implementation.",2169-3536,,,117528-117545, ,  ,,"Title:On Designing Low-Risk Honeypots Using Generative Pre-Trained Transformer Models With Curated Inputs

 Honeypots are utilized as defensive tools within a monitored environment to engage attackers and gather artifacts for the development of indicators of compromise. However, once these honeypots are deployed, they are rarely updated, making them obsolete and easier to fingerprint as time passes. Furthermore, using fully functional computing and networking devices as honeypots presents the risk of an attacker breaking out from the controlled environment. Large-scale text-generating models, commonly referred to as Large Language Models (LLMs), have seen wide implementation using generative-pretrained transformer (GPT) models. These models have seen an explosion in popularity and have been tuned for various use cases. This paper investigates the use of these models to simulate honeypots that are adaptive to threat engagement without the risk of unintended breakouts. This investigation finds that the method these models use to generate output has limitations that can reveal the deception to a dedicated attacker in extended sessions. To overcome this challenge, this paper presents a method to manage the inputs and outputs to reduce non-deterministic output and token usage of a model generating text in a way that simulates a terminal. An example honeypot is evaluated against a traditional low-risk honeypot, Cowrie, where greater similarity to an actual machine for single commands is achieved. Furthermore, in several multi-step attack scenarios, the proposed architecture reduced the token usage by up to 77% when compared to a baseline scenario that did not manage the inputs to and outputs from an example model. A discussion on the utilization of LLMs for cyber deception, as well as the limitations hindering their broader adoption indicates that LLMs exhibit promise for cyber deception but necessitate further research before achieving widespread implementation.",
"Monnot, M; Orbelo, D; Riccardo, L; Sikka, S; Ross, E","Orbelo, Diana/K-9319-2019","Orbelo, Diana/0000-0003-3035-2077",Acoustic analyses support subjective judgments of vocal emotion,1000,,10.1196/annals.1280.027 ,Article; Proceedings Paper ,2003.0,"Subjective human judgments of emotion in speech have been considered to be less reliable than acoustic analyses in scientific studies, but acoustic analyses have had limited ability to detect subtle vocal nuances that give useful social information about human intent and meaning to discourse partners. Two post hoc analyses were undertaken to determine if results from acoustic analyses of vocalizations were related to subjective judgments of vocal affect (affective prosody). Acoustic analyses of fundamental frequency (F-0) and subjective judgments of emotional content of vocal productions from two studies underwent statistical analyses: Study 1-vocal repetition of sentences using 6 basic emotions in 24 detoxified alcoholics and 15 controls; study 2-quality/quantity of motherese speech directed to 52 infants in Cambridge, England. Ratings of emotion indicators for both studies were done by female researchers of different ages and cultural/language backgrounds. In both studies, acoustic analyses of F-0 elements in utterances accounted for approximately 50% of the effect when modeling subjective emotion accuracy and emotion intensity ratings, using linear regression analyses. Acoustic analyses of F0 are positively associated with subjective judgments of emotion indicators, and speakers who cannot vary F-0 are unable to convey emotion accurately to communication partners. Yet acoustic analyses are limited in comparison to the exquisite complexity of the human auditory and cognitive systems. Subjective judgments of emotional meaning in speech can be a reliable variable in scientific inquiry and can be used for more complex, subtle studies of speech communication and intentionality than acoustic analyses.",0077-8923,,1-57331-464-1,288-292, ," Conference on Emotions Inside Out, 130 Years after Darwins the Expression of the Emotions in Man and AnimalsConference on Emotions Inside Out, 130 Years after Darwins the Expression of the Emotions in Man and Animals ",,"Title:Acoustic analyses support subjective judgments of vocal emotion

 Subjective human judgments of emotion in speech have been considered to be less reliable than acoustic analyses in scientific studies, but acoustic analyses have had limited ability to detect subtle vocal nuances that give useful social information about human intent and meaning to discourse partners. Two post hoc analyses were undertaken to determine if results from acoustic analyses of vocalizations were related to subjective judgments of vocal affect (affective prosody). Acoustic analyses of fundamental frequency (F-0) and subjective judgments of emotional content of vocal productions from two studies underwent statistical analyses: Study 1-vocal repetition of sentences using 6 basic emotions in 24 detoxified alcoholics and 15 controls; study 2-quality/quantity of motherese speech directed to 52 infants in Cambridge, England. Ratings of emotion indicators for both studies were done by female researchers of different ages and cultural/language backgrounds. In both studies, acoustic analyses of F-0 elements in utterances accounted for approximately 50% of the effect when modeling subjective emotion accuracy and emotion intensity ratings, using linear regression analyses. Acoustic analyses of F0 are positively associated with subjective judgments of emotion indicators, and speakers who cannot vary F-0 are unable to convey emotion accurately to communication partners. Yet acoustic analyses are limited in comparison to the exquisite complexity of the human auditory and cognitive systems. Subjective judgments of emotional meaning in speech can be a reliable variable in scientific inquiry and can be used for more complex, subtle studies of speech communication and intentionality than acoustic analyses.",
"Seo, Hyein; Jung, Sangkeun; Jung, Jeesu; Hwang, Taewook; Namgoong, Hyuk; Roh, Yoon-Hyung","Seo, Hyein/IZQ-3448-2023; Hwang, Taewook/JDC-4547-2023","Seo, Hyein/0000-0003-3107-0880; Hwang, Taewook/0000-0003-2440-4707; Jung, Jeesu/0000-0003-1684-0517",Controllable Text Generation Using Semantic Control Grammar,11,,10.1109/ACCESS.2023.3252017 ,Article ,2023.0,"Controllable text generation is the primary technique for controlling specific attributes such as topic, keywords and obtaining augmented data. This work proposes a novel controllable text generation framework to improve the controllability of generation models. First, we introduce semantic control grammar, a controllable input format to generate sentences that satisfy the constraints. Second, we adopt a generation and rerank method to obtain semantically reranked controlled sentences. Extensive experiments and analyses are conducted on benchmark, natural language understanding, data-to-text generation, and text classification datasets. Through automatic evaluations, we show that our method leads to improvement over strong baselines. The results show that our model can control sentence and word attributes and semantically generate natural sentences. Furthermore, our techniques improve the generation quality of the model.",2169-3536,,,26329-26343, ,  ,,"Title:Controllable Text Generation Using Semantic Control Grammar

 Controllable text generation is the primary technique for controlling specific attributes such as topic, keywords and obtaining augmented data. This work proposes a novel controllable text generation framework to improve the controllability of generation models. First, we introduce semantic control grammar, a controllable input format to generate sentences that satisfy the constraints. Second, we adopt a generation and rerank method to obtain semantically reranked controlled sentences. Extensive experiments and analyses are conducted on benchmark, natural language understanding, data-to-text generation, and text classification datasets. Through automatic evaluations, we show that our method leads to improvement over strong baselines. The results show that our model can control sentence and word attributes and semantically generate natural sentences. Furthermore, our techniques improve the generation quality of the model.",
"Zheng, Yanan; Wang, Yan; Wen, Lijie; Wang, Jianmin","LI, Wenhui/JCD-9947-2023; feng, chen/JLM-8296-2023","Wang, Jianmin/0000-0001-6841-7943",How to Generate Reasonable Texts with Controlled Attributes,12113,,10.1007/978-3-030-59416-9_15 ,Proceedings Paper ,2020.0,"The controllable text generation (CTG) task is crucial for text-related applications, such as goal-oriented dialogue systems and text style-transfer applications, etc. However, existing CTG methods commonly ignore the co-occurrence dependencies between multiple controlled attributes, which are implicit in domain knowledge. As a result, rarely cooccurring controlled values are highly likely to be given by users, which finally leads to non-committal generated texts that are out of control. To address this problem, we initially propose the Dependency-aware Controllable Text Generation (DCTG) model that reduces trivial generations by automatically learning the co-occurrence dependencies and adjusting rarely co-occurring controlled values. Our DCTG highlights in (1) modeling the co-occurrence dependencies between controlled attributes with neural networks, (2) integrating dependency losses to guide each component of our model to collaboratively work for generating reasonable texts based on the learned dependencies, and (3) proposing a novel Reasonableness metric measuring to which degree generations comply with real co-occurrence dependencies. Experiments prove that DCTG outperforms state-of-the-art baselines on three datasets in multiple aspects.",0302-9743,1611-3349,978-3-030-59415-2; 978-3-030-59416-9,245-262, , 25th International Conference on Database Systems for Advanced Applications (DASFAA)25th International Conference on Database Systems for Advanced Applications (DASFAA) ,,"Title:How to Generate Reasonable Texts with Controlled Attributes

 The controllable text generation (CTG) task is crucial for text-related applications, such as goal-oriented dialogue systems and text style-transfer applications, etc. However, existing CTG methods commonly ignore the co-occurrence dependencies between multiple controlled attributes, which are implicit in domain knowledge. As a result, rarely cooccurring controlled values are highly likely to be given by users, which finally leads to non-committal generated texts that are out of control. To address this problem, we initially propose the Dependency-aware Controllable Text Generation (DCTG) model that reduces trivial generations by automatically learning the co-occurrence dependencies and adjusting rarely co-occurring controlled values. Our DCTG highlights in (1) modeling the co-occurrence dependencies between controlled attributes with neural networks, (2) integrating dependency losses to guide each component of our model to collaboratively work for generating reasonable texts based on the learned dependencies, and (3) proposing a novel Reasonableness metric measuring to which degree generations comply with real co-occurrence dependencies. Experiments prove that DCTG outperforms state-of-the-art baselines on three datasets in multiple aspects.",
"Tu, Haoqin; Yang, Zhongliang; Yang, Jinshuai; Zhang, Siyu; Huang, Yongfeng","yang, zhongliang/N-6016-2019; li, bai/JNE-1502-2023; xuan, li/JNI-7432-2023; Wang, Jiacheng/ABE-5948-2020","yang, zhongliang/0000-0002-8027-9560; Yang, Jinshuai/0000-0002-6293-1981",PCAE: A framework of plug-in conditional auto-encoder for controllable text generation,256,,10.1016/j.knosys.2022.109766 ,Article ,2022.0,"Controllable text generation has taken a gigantic step forward these days. Yet existing methods are either constrained in a one-off pattern or not efficient enough for receiving multiple conditions at every generation stage. We propose a model-agnostic framework Plug-in Conditional Auto-Encoder for Controllable Text Generation (PCAE) towards flexible and semi-supervised text generation. Our framework is plug-and-playwith partial parameters to be fine-tuned in the pre-trained model (less than a half). Crucial to the success of PCAE is the proposed broadcasting label fusion network for navigating the global latent code to a specified local and confined space. Visualization of the local latent prior well confirms the primary devotion in hidden space of the proposed model. Moreover, extensive experiments across five related generation tasks (from 2 conditions up to 10 conditions) on both RNN-based and pre-trained BART [26] based auto-encoders reveal the high capability of PCAE, which enables generation that is highly manipulable, syntactically diverse and time-saving with minimum labeled samples. We will release our code at https://github.com/ImKeTT/pcae.(c) 2022 Elsevier B.V. All rights reserved.",0950-7051,1872-7409,,, ,  ,,"Title:PCAE: A framework of plug-in conditional auto-encoder for controllable text generation

 Controllable text generation has taken a gigantic step forward these days. Yet existing methods are either constrained in a one-off pattern or not efficient enough for receiving multiple conditions at every generation stage. We propose a model-agnostic framework Plug-in Conditional Auto-Encoder for Controllable Text Generation (PCAE) towards flexible and semi-supervised text generation. Our framework is plug-and-playwith partial parameters to be fine-tuned in the pre-trained model (less than a half). Crucial to the success of PCAE is the proposed broadcasting label fusion network for navigating the global latent code to a specified local and confined space. Visualization of the local latent prior well confirms the primary devotion in hidden space of the proposed model. Moreover, extensive experiments across five related generation tasks (from 2 conditions up to 10 conditions) on both RNN-based and pre-trained BART [26] based auto-encoders reveal the high capability of PCAE, which enables generation that is highly manipulable, syntactically diverse and time-saving with minimum labeled samples. We will release our code at https://github.com/ImKeTT/pcae.(c) 2022 Elsevier B.V. All rights reserved.",
"Cho, JinUk; Jeong, MinSu; Bak, JinYeong; Cheong, Yun-Gyung",,,Genre-Controllable Story Generation via Supervised Contrastive Learning,,,10.1145/3485447.3512004 ,Proceedings Paper ,2022.0,"While controllable text generation has received attention due to the recent advances in large-scale pre-trained language models, there is a lack of research that focuses on story-specific controllability. To address this, we present Story Control via Supervised Contrastive learning model (SCSC), to create a story conditioned on genre. For this, we design a supervised contrastive objective combined with log-likelihood objective, to capture the intrinsic differences among the stories in different genres. The results of our automated evaluation and user study demonstrate that the proposed method is effective in genre-controlled story generation.",,,978-1-4503-9096-5,2839-2849, , 31st ACM Web Conference (WWW)31st ACM Web Conference (WWW) ,,"Title:Genre-Controllable Story Generation via Supervised Contrastive Learning

 While controllable text generation has received attention due to the recent advances in large-scale pre-trained language models, there is a lack of research that focuses on story-specific controllability. To address this, we present Story Control via Supervised Contrastive learning model (SCSC), to create a story conditioned on genre. For this, we design a supervised contrastive objective combined with log-likelihood objective, to capture the intrinsic differences among the stories in different genres. The results of our automated evaluation and user study demonstrate that the proposed method is effective in genre-controlled story generation.",
"Hu, Zhiting; Li, Li Erran",,,A Causal Lens for Controllable Text Generation,34,, ,Proceedings Paper ,2021.0,"Controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing text to possess desired attributes (i.e., text attribute transfer). Extensive prior work has largely studied the two problems separately, and developed different conditional models which, however, are prone to producing biased text (e.g., various gender stereotypes). This paper proposes to formulate controllable text generation from a principled causal perspective which models the two tasks with a unified framework. A direct advantage of the causal formulation is the use of rich causality tools to mitigate generation biases and improve control. We treat the two tasks as interventional and counterfactual causal inference based on a structural causal model, respectively. We then apply the framework to the challenging practical setting where confounding factors (that induce spurious correlations) are observable only on a small fraction of data. Experiments show significant superiority of the causal approach over previous conditional models for improved control accuracy and reduced bias.",1049-5258,,*****************,, , 35th Conference on Neural Information Processing Systems (NeurIPS)35th Conference on Neural Information Processing Systems (NeurIPS) ,,"Title:A Causal Lens for Controllable Text Generation

 Controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing text to possess desired attributes (i.e., text attribute transfer). Extensive prior work has largely studied the two problems separately, and developed different conditional models which, however, are prone to producing biased text (e.g., various gender stereotypes). This paper proposes to formulate controllable text generation from a principled causal perspective which models the two tasks with a unified framework. A direct advantage of the causal formulation is the use of rich causality tools to mitigate generation biases and improve control. We treat the two tasks as interventional and counterfactual causal inference based on a structural causal model, respectively. We then apply the framework to the challenging practical setting where confounding factors (that induce spurious correlations) are observable only on a small fraction of data. Experiments show significant superiority of the causal approach over previous conditional models for improved control accuracy and reduced bias.",
"Zhang, Hanqing; Song, Haolin; Li, Shaoyu; Zhou, Ming; Song, Dawei",,,A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models,56,3,10.1145/3617680 ,Article ,2024.0,"Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.",0360-0300,1557-7341,,, ,  ,,"Title:A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models

 Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.",
"Wang, Shuo; Zheng, Qiushuo; Su, Zherong; Na, Chongning; Qi, Guilin","Zhang, Han/JMR-0670-2023",,MEED: A Multimodal Event Extraction Dataset,1466,,10.1007/978-981-16-6471-7_23 ,Proceedings Paper ,2021.0,"Multimodal tasks are gradually attracting the attention of the research community, and the lack of multimodal event extraction datasets restricts the development of multimodal event extraction. We introduce the new Multimodal Event Extraction Dataset (MEED) to fill the gap, we define event types and argument roles that can be used on multimodal data, then use controllable text generation to generate the textual modality based on visual event extraction dataset. In this paper, we aim to make full use of multimodal resources in the event extraction task by constructing a large-scale and high-quality multimodal event extraction dataset and promote researches in the field of multimodal event extraction.",1865-0929,1865-0937,978-981-16-6471-7; 978-981-16-6470-0,288-294, , 6th China Conference on Knowledge Graph and Semantic Computing (CCKS)6th China Conference on Knowledge Graph and Semantic Computing (CCKS) ,,"Title:MEED: A Multimodal Event Extraction Dataset

 Multimodal tasks are gradually attracting the attention of the research community, and the lack of multimodal event extraction datasets restricts the development of multimodal event extraction. We introduce the new Multimodal Event Extraction Dataset (MEED) to fill the gap, we define event types and argument roles that can be used on multimodal data, then use controllable text generation to generate the textual modality based on visual event extraction dataset. In this paper, we aim to make full use of multimodal resources in the event extraction task by constructing a large-scale and high-quality multimodal event extraction dataset and promote researches in the field of multimodal event extraction.",
"Zhang, Siyu; Yang, Zhongliang; Yang, Jinshuai; Huang, Yongfeng","yang, zhongliang/N-6016-2019","yang, zhongliang/0000-0002-8027-9560; Yang, Jinshuai/0000-0002-6293-1981; Huang, Yongfeng/0000-0003-3825-2230",Linguistic Steganography: From Symbolic Space to Semantic Space,28,,10.1109/LSP.2020.3042413 ,Article ,2021.0,"Previous works about linguistic steganography such as synonym substitution and sampling-based methods usually manipulate observed symbols explicitly to conceal secret information, which may give rise to security risks. In this letter, in order to preclude straightforward operation on observed symbols, we explored generation-based linguistic steganography in latent space by means of encoding secret messages in the selection of implicit attributes (semanteme) of natural language. We proposed a novel framework of linguistic semantic steganography based on rejection sampling strategy. Concretely, we utilized controllable text generation model for embedding and semantic classifier for extraction. In experiments, a model based on CTRL and BERT is implemented for further quantitative assessment. Results reveal that our approach is able to achieve satisfactory efficiency as well as nearly perfect imperceptibility. Our code is available at https://github.com/YangzlTHU/Linguistic-Steganography-and-Steganalysis/tree/master/Steganography/Linguistic-Semantic-Steganography.",1070-9908,1558-2361,,11-15, ,  ,,"Title:Linguistic Steganography: From Symbolic Space to Semantic Space

 Previous works about linguistic steganography such as synonym substitution and sampling-based methods usually manipulate observed symbols explicitly to conceal secret information, which may give rise to security risks. In this letter, in order to preclude straightforward operation on observed symbols, we explored generation-based linguistic steganography in latent space by means of encoding secret messages in the selection of implicit attributes (semanteme) of natural language. We proposed a novel framework of linguistic semantic steganography based on rejection sampling strategy. Concretely, we utilized controllable text generation model for embedding and semantic classifier for extraction. In experiments, a model based on CTRL and BERT is implemented for further quantitative assessment. Results reveal that our approach is able to achieve satisfactory efficiency as well as nearly perfect imperceptibility. Our code is available at https://github.com/YangzlTHU/Linguistic-Steganography-and-Steganalysis/tree/master/Steganography/Linguistic-Semantic-Steganography.",
"Hao Fei; Li, Chenliang; Ji, Donghong; Li, Fei",,,Mutual Disentanglement Learning for Joint Fine-Grained Sentiment Classification and Controllable Text Generation,,,10.1145/3477495.3532029 ,Proceedings Paper ,2022.0,"Fine-grained sentiment classification (FGSC) task and fine-grained controllable text generation (FGSG) task are two representative applications of sentiment analysis, two of which together can actually form an inverse task prediction, i.e., the former aims to infer the fine-grained sentiment polarities given a text piece, while the latter generates text content that describes the input fine-grained opinions. Most of the existing work solves the FGSC and the FGSG tasks in isolation, while ignoring the complementary benefits in between. This paper combines FGSC and FGSG as a joint dual learning system, encouraging them to learn the advantages from each other. Based on the dual learning framework, we further propose decoupling the feature representations in two tasks into fine-grained aspect-oriented opinion variables and content variables respectively, by performing mutual disentanglement learning upon them. We also propose to transform the difficult data-to-text generation fashion widely used in FGSG into an easier text-to-text generation fashion by creating surrogate natural language text as the model inputs. Experimental results on 7 sentiment analysis benchmarks including both the document-level and sentence-level datasets show that our method significantly outperforms the current strong-performing baselines on both the FGSC and FGSG tasks. Automatic and human evaluations demonstrate that our FGSG model successfully generates fluent, diverse and rich content conditioned on fine-grained sentiments.",,,978-1-4503-8732-3,1555-1565, , 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) ,,"Title:Mutual Disentanglement Learning for Joint Fine-Grained Sentiment Classification and Controllable Text Generation

 Fine-grained sentiment classification (FGSC) task and fine-grained controllable text generation (FGSG) task are two representative applications of sentiment analysis, two of which together can actually form an inverse task prediction, i.e., the former aims to infer the fine-grained sentiment polarities given a text piece, while the latter generates text content that describes the input fine-grained opinions. Most of the existing work solves the FGSC and the FGSG tasks in isolation, while ignoring the complementary benefits in between. This paper combines FGSC and FGSG as a joint dual learning system, encouraging them to learn the advantages from each other. Based on the dual learning framework, we further propose decoupling the feature representations in two tasks into fine-grained aspect-oriented opinion variables and content variables respectively, by performing mutual disentanglement learning upon them. We also propose to transform the difficult data-to-text generation fashion widely used in FGSG into an easier text-to-text generation fashion by creating surrogate natural language text as the model inputs. Experimental results on 7 sentiment analysis benchmarks including both the document-level and sentence-level datasets show that our method significantly outperforms the current strong-performing baselines on both the FGSC and FGSG tasks. Automatic and human evaluations demonstrate that our FGSG model successfully generates fluent, diverse and rich content conditioned on fine-grained sentiments.",
"Knochelmann, Jonas P.; Cardona-Rivera, Rogelio E.",,,Bronco: A Universal Authoring Language for Controllable Text Generation,13762,,10.1007/978-3-031-22298-6_35 ,Proceedings Paper ,2022.0,"We present Bronco: an in-development authoring language for Turing-complete procedural text generation. Our language emerged from a close examination of existing tools. This analysis led to our desire of supporting users in specifying yielding grammars, a formalism we invented that is more expressive than what several popular and available solutions offer. With this formalism as our basis, we detail the qualities of Bronco that expose its power in author-focused ways.",0302-9743,1611-3349,978-3-031-22297-9; 978-3-031-22298-6,541-558, , 15th International Conference on Interactive Digital Storytelling (ICIDS)15th International Conference on Interactive Digital Storytelling (ICIDS) ,,"Title:Bronco: A Universal Authoring Language for Controllable Text Generation

 We present Bronco: an in-development authoring language for Turing-complete procedural text generation. Our language emerged from a close examination of existing tools. This analysis led to our desire of supporting users in specifying yielding grammars, a formalism we invented that is more expressive than what several popular and available solutions offer. With this formalism as our basis, we detail the qualities of Bronco that expose its power in author-focused ways.",
"Hu, Zhe; Cao, Zhiwei; Chan, Hou Pong; Liu, Jiachen; Xiao, Xinyan; Su, Jinsong; Wu, Hua",,"Cao, Zhiwei/0000-0001-5674-0444",Controllable Dialogue Generation With Disentangled Multi-Grained Style Specification and Attribute Consistency Reward,31,,10.1109/TASLP.2022.3221002 ,Article ,2023.0,"Controllable text generation is an appealing but challenging task, which allows users to specify particular attributes of the generated outputs. In this paper, we propose a controllable dialogue generation model to steer response generation under multi-attribute constraints. Specifically, we define and categorize the commonly-used control attributes into global and local ones, which possess different granularities of effects on response generation. Then, we significantly extend the conventional seq2seq framework by introducing a novel two-stage decoder, which first uses a multi-grained style specification layer to impose the stylistic constraints and determine word-level control states of responses based on the attributes, and then employs a response generation layer to generate final responses maintaining both semantic relevancy to the contexts and fidelity to the attributes. Furthermore, we train our model with an attribute consistency reward to promote response control with explicit supervision signals. Extensive experiments and in-depth analyses on two datasets indicate that our model can significantly outperform competitive baselines in terms of response quality, content diversity and controllability.",2329-9290,2329-9304,,188-199, ,  ,,"Title:Controllable Dialogue Generation With Disentangled Multi-Grained Style Specification and Attribute Consistency Reward

 Controllable text generation is an appealing but challenging task, which allows users to specify particular attributes of the generated outputs. In this paper, we propose a controllable dialogue generation model to steer response generation under multi-attribute constraints. Specifically, we define and categorize the commonly-used control attributes into global and local ones, which possess different granularities of effects on response generation. Then, we significantly extend the conventional seq2seq framework by introducing a novel two-stage decoder, which first uses a multi-grained style specification layer to impose the stylistic constraints and determine word-level control states of responses based on the attributes, and then employs a response generation layer to generate final responses maintaining both semantic relevancy to the contexts and fidelity to the attributes. Furthermore, we train our model with an attribute consistency reward to promote response control with explicit supervision signals. Extensive experiments and in-depth analyses on two datasets indicate that our model can significantly outperform competitive baselines in terms of response quality, content diversity and controllability.",
"Chen, Mingda; Tang, Qingming; Wiseman, Sam; Gimpel, Kevin",,"Chen, Mingda/0000-0002-1824-5263; Wiseman, Sam/0000-0003-0923-1086",Controllable Paraphrase Generation with a Syntactic Exemplar,,, ,Proceedings Paper ,2019.0,"Prior work on controllable text generation usually assumes that the controlled attribute can take on one of a small set of values known a priori. In this work, we propose a novel task, where the syntax of a generated sentence is controlled rather by a sentential exemplar. To evaluate quantitatively with standard metrics, we create a novel dataset with human annotations. We also develop a variational model with a neural module specifically designed for capturing syntactic knowledge and several multitask training objectives to promote disentangled representation learning. Empirically, the proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics.(1)",,,978-1-950737-48-2,5972-5984, , 57th Annual Meeting of the Association-for-Computational-Linguistics (ACL)57th Annual Meeting of the Association-for-Computational-Linguistics (ACL) ,,"Title:Controllable Paraphrase Generation with a Syntactic Exemplar

 Prior work on controllable text generation usually assumes that the controlled attribute can take on one of a small set of values known a priori. In this work, we propose a novel task, where the syntax of a generated sentence is controlled rather by a sentential exemplar. To evaluate quantitatively with standard metrics, we create a novel dataset with human annotations. We also develop a variational model with a neural module specifically designed for capturing syntactic knowledge and several multitask training objectives to promote disentangled representation learning. Empirically, the proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics.(1)",
"Zhai, You; Yang, Jian; Wang, Zixiang; He, Longtao; Yang, Liqun; Li, Zhoujun",,,CDGA: A GAN-based Controllable Domain Generation Algorithm,,,10.1109/TrustCom56396.2022.00056 ,Proceedings Paper ,2022.0,"Recently Command and Control (C&C) servers have attracted considerable attention in botnets and domain generation algorithms (DGAs) further enhance the stealth of C&C servers. However, Algorithmically Generated Domains (AGDs) generated by DGAs can be easily detected by previous DGA detection approaches. More specifically, the previous DGAs are hard to satisfy domain name rules, low repetition rate, and anti-detection in practical scenarios simultaneously. Designing an outstanding DGA has become a crucial issue from the botnet owner's perspective. To mitigate these problems, we propose CDGA, a Controllable DGA via Generative Adversarial Networks (GAN), which is a popular backbone model for text generation in the natural language processing (NLP) community.Controllable text generation approaches are adopted by CDGA to ensure no repetition in the generated domain names and compliance with the domain rules. In addition to cheating DGA detectors, GANs are exploited to equip CDGA with a powerful anti-detection ability. Furthermore, our proposed method uses the technique of NLP to force the AGDs to meet language rules, where the generated domain names are difficult for recognition by human. By utilizing the time-dependent seed, CDGA can dynamically generate domain names, ensuring that the malware can connect to the C&C server conditioned on a specific time stamp. Experimental results demonstrate that the domain names generated by our method are realistic enough to be resistant to the state-of-the-art DGA detectors.",2324-898X,,978-1-6654-9425-0,352-360, ," 21st IEEE International Conference on Trust, Security and Privacy in Computing and Communications (IEEE TrustCom)21st IEEE International Conference on Trust, Security and Privacy in Computing and Communications (IEEE TrustCom) ",,"Title:CDGA: A GAN-based Controllable Domain Generation Algorithm

 Recently Command and Control (C&C) servers have attracted considerable attention in botnets and domain generation algorithms (DGAs) further enhance the stealth of C&C servers. However, Algorithmically Generated Domains (AGDs) generated by DGAs can be easily detected by previous DGA detection approaches. More specifically, the previous DGAs are hard to satisfy domain name rules, low repetition rate, and anti-detection in practical scenarios simultaneously. Designing an outstanding DGA has become a crucial issue from the botnet owner's perspective. To mitigate these problems, we propose CDGA, a Controllable DGA via Generative Adversarial Networks (GAN), which is a popular backbone model for text generation in the natural language processing (NLP) community.Controllable text generation approaches are adopted by CDGA to ensure no repetition in the generated domain names and compliance with the domain rules. In addition to cheating DGA detectors, GANs are exploited to equip CDGA with a powerful anti-detection ability. Furthermore, our proposed method uses the technique of NLP to force the AGDs to meet language rules, where the generated domain names are difficult for recognition by human. By utilizing the time-dependent seed, CDGA can dynamically generate domain names, ensuring that the malware can connect to the C&C server conditioned on a specific time stamp. Experimental results demonstrate that the domain names generated by our method are realistic enough to be resistant to the state-of-the-art DGA detectors.",
"Chakrabarty, Tuhin; Hidey, Christopher; Muresan, Smaranda",,,ENTRUST: Argument Reframing with Language Models and Entailment,,, ,Proceedings Paper ,2021.0,"Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker (Emman, 1983). Differences in lexical framing, the focus of our work, can have large effects on peoples' opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for connotations to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a postdecoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear.",,,978-1-954085-46-6,4958-4971, , Conference of the North-American-Chapter of the Association-for-Computational-Linguistics - Human Language Technologies (NAACL-HLT)Conference of the North-American-Chapter of the Association-for-Computational-Linguistics - Human Language Technologies (NAACL-HLT) ,,"Title:ENTRUST: Argument Reframing with Language Models and Entailment

 Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker (Emman, 1983). Differences in lexical framing, the focus of our work, can have large effects on peoples' opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for connotations to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a postdecoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear.",
"Al-Khatib, Khalid; Hou, Yufang; Wachsmuth, Henning; Jochim, Charles; Bonin, Francesca; Stein, Benno","Al Khatib, Khalid/HKW-0253-2023; Wachsmuth, Henning/AAH-7299-2021","Al-Khatib, Khalid/0009-0006-7255-5349",End-to-End Argumentation Knowledge Graph Construction,34,, ,Proceedings Paper ,2020.0,"This paper studies the end-to-end construction of an argumentation knowledge graph that is intended to support argument synthesis, argumentative question answering, or fake news detection, among others. The study is motivated by the proven effectiveness of knowledge graphs for interpretable and controllable text generation and exploratory search. Original in our work is that we propose a model of the knowledge encapsulated in arguments. Based on this model, we build a new corpus that comprises about 16k manual annotations of 4740 claims with instances of the model's elements, and we develop an end-to-end framework that automatically identifies all modeled types of instances. The results of experiments show the potential of the framework for building a web-based argumentation graph that is of high quality and large scale.",2159-5399,2374-3468,978-1-57735-835-0,7367-7374, , 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence ,,"Title:End-to-End Argumentation Knowledge Graph Construction

 This paper studies the end-to-end construction of an argumentation knowledge graph that is intended to support argument synthesis, argumentative question answering, or fake news detection, among others. The study is motivated by the proven effectiveness of knowledge graphs for interpretable and controllable text generation and exploratory search. Original in our work is that we propose a model of the knowledge encapsulated in arguments. Based on this model, we build a new corpus that comprises about 16k manual annotations of 4740 claims with instances of the model's elements, and we develop an end-to-end framework that automatically identifies all modeled types of instances. The results of experiments show the potential of the framework for building a web-based argumentation graph that is of high quality and large scale.",
"Carlsson, Fredrik; Ohman, Joey; Liu, Fangyu; Verlinden, Severine; Nivre, Joakim; Sahlgren, Magnus","Liu, Fangyu/AHA-5291-2022","Liu, Fangyu/0000-0001-7038-3623",Fine-Grained Controllable Text Generation Using Non-Residual Prompting,,, ,Proceedings Paper ,2022.0,"The introduction of immensely large causal language models (CLMs) has rejuvenated the interest in open-ended text generation. However, controlling the generative process for these Transformer-based models is at large an unsolved problem. Earlier work has explored either plug-and-play decoding strategies or more powerful but blunt approaches such as prompting. There hence currently exists a trade-off between fine-grained control and the capability for more expressive high-level instructions. To alleviate this trade-off, we propose an encoder-decoder architecture that enables intermediate text prompts at arbitrary time steps. We propose a resource-efficient method for converting a pre-trained CLM into this architecture and demonstrate its potential in various experiments, including the novel task of contextualized word inclusion. Our method provides strong results in multiple experimental settings, proving itself to be both expressive and versatile.(1)",,,978-1-955917-21-6,6837-6857, , 60th Annual Meeting of the Association-for-Computational-Linguistics (ACL)60th Annual Meeting of the Association-for-Computational-Linguistics (ACL) ,,"Title:Fine-Grained Controllable Text Generation Using Non-Residual Prompting

 The introduction of immensely large causal language models (CLMs) has rejuvenated the interest in open-ended text generation. However, controlling the generative process for these Transformer-based models is at large an unsolved problem. Earlier work has explored either plug-and-play decoding strategies or more powerful but blunt approaches such as prompting. There hence currently exists a trade-off between fine-grained control and the capability for more expressive high-level instructions. To alleviate this trade-off, we propose an encoder-decoder architecture that enables intermediate text prompts at arbitrary time steps. We propose a resource-efficient method for converting a pre-trained CLM into this architecture and demonstrate its potential in various experiments, including the novel task of contextualized word inclusion. Our method provides strong results in multiple experimental settings, proving itself to be both expressive and versatile.(1)",
"Amplayo, Reinald Kim; Brazinskas, Arthur; Suhara, Yoshi; Wang, Xiaolan; Liu, Bing","王, 小兰/HOC-9816-2023",,Beyond Opinion Mining: Summarizing Opinions of Customer Reviews,,,10.1145/3477495.3532676 ,Proceedings Paper ,2022.0,"Customer reviews are vital for making purchasing decisions in the Information Age. Such reviews can be automatically summarized to provide the user with an overview of opinions. In this tutorial, we present various aspects of opinion summarization that are useful for researchers and practitioners. First, we will introduce the task and major challenges. Then, we will present existing opinion summarization solutions, both pre-neural and neural. We will discuss how summarizers can be trained in the unsupervised, fewshot, and supervised regimes. Each regime has roots in different machine learning methods, such as auto-encoding, controllable text generation, and variational inference. Finally, we will discuss resources and evaluation methods and conclude with the future directions. This three-hour tutorial will provide a comprehensive overview over major advances in opinion summarization. The listeners will be well-equipped with the knowledge that is both useful for research and practical applications.",,,978-1-4503-8732-3,3447-3450, , 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) ,,"Title:Beyond Opinion Mining: Summarizing Opinions of Customer Reviews

 Customer reviews are vital for making purchasing decisions in the Information Age. Such reviews can be automatically summarized to provide the user with an overview of opinions. In this tutorial, we present various aspects of opinion summarization that are useful for researchers and practitioners. First, we will introduce the task and major challenges. Then, we will present existing opinion summarization solutions, both pre-neural and neural. We will discuss how summarizers can be trained in the unsupervised, fewshot, and supervised regimes. Each regime has roots in different machine learning methods, such as auto-encoding, controllable text generation, and variational inference. Finally, we will discuss resources and evaluation methods and conclude with the future directions. This three-hour tutorial will provide a comprehensive overview over major advances in opinion summarization. The listeners will be well-equipped with the knowledge that is both useful for research and practical applications.",
"Mori, Yusuke; Yamane, Hiroaki; Shimizu, Ryohei; Harada, Tatsuya",,,Plug-and-Play Controller for Story Completion: A Pilot Study toward Emotion-aware Story Writing Assistance,,, ,Proceedings Paper ,2022.0,"Emotions are essential for storytelling and narrative generation, and as such, the relationship between stories and emotions has been extensively studied. The authors of this paper, including a professional novelist, have examined the use of natural language processing to address the problems of novelists from the perspective of practical creative writing. In particular, the story completion task, which requires understanding the existing unfinished context, was studied from the perspective of creative support for human writers, to generate appropriate content to complete the unfinished parts. It was found that unsupervised pre-trained large neural models of the sequence-to-sequence type are useful for this task. Furthermore, based on the plug-and-play module for controllable text generation using GPT-2, an additional module was implemented to consider emotions. Although this is a preliminary study, and the results leave room for improvement before incorporating the model into a practical system, this effort is an important step in complementing the emotional trajectory of the story.",,,978-1-955917-39-1,46-57, , 1st Workshop on Intelligent and Interactive Writing Assistants (In2Writing)1st Workshop on Intelligent and Interactive Writing Assistants (In2Writing) ,,"Title:Plug-and-Play Controller for Story Completion: A Pilot Study toward Emotion-aware Story Writing Assistance

 Emotions are essential for storytelling and narrative generation, and as such, the relationship between stories and emotions has been extensively studied. The authors of this paper, including a professional novelist, have examined the use of natural language processing to address the problems of novelists from the perspective of practical creative writing. In particular, the story completion task, which requires understanding the existing unfinished context, was studied from the perspective of creative support for human writers, to generate appropriate content to complete the unfinished parts. It was found that unsupervised pre-trained large neural models of the sequence-to-sequence type are useful for this task. Furthermore, based on the plug-and-play module for controllable text generation using GPT-2, an additional module was implemented to consider emotions. Although this is a preliminary study, and the results leave room for improvement before incorporating the model into a practical system, this effort is an important step in complementing the emotional trajectory of the story.",
"Utama, Prasetya Ajie; Bambrick, Joshua; Moosavi, Nafise Sadat; Gurevych, Iryna",,"Moosavi, Nafise Sadat/0000-0002-8332-307X",FALSESUM: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization,,, ,Proceedings Paper ,2022.0,"Neural abstractive summarization models are prone to generate summaries which are factually inconsistent with their source documents. Previous work has introduced the task of recognizing such factual inconsistency as a downstream application of natural language inference (NLI). However, state-of-the-art NLI models perform poorly in this context due to their inability to generalize to the target task. In this work, we show that NLI models can be effective for this task when the training data is augmented with high-quality task-oriented examples. We introduce FALSESUM, a data generation pipeline leveraging a controllable text generation model to perturb human-annotated summaries, introducing varying types of factual inconsistencies. Unlike previously introduced document-level NLI datasets, our generated dataset contains examples that are diverse and inconsistent yet plausible. We show that models trained on a Falsesum-augmented NLI dataset improve the state-of-the-art performance across four benchmarks for detecting factual inconsistency in summarization.(1)",,,978-1-955917-71-1,2763-2776, , Conference of the North-American-Chapter-of-the-Association-for-Computational-Linguistics (NAAACL) - Human Language TechnologiesConference of the North-American-Chapter-of-the-Association-for-Computational-Linguistics (NAAACL) - Human Language Technologies ,,"Title:FALSESUM: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization

 Neural abstractive summarization models are prone to generate summaries which are factually inconsistent with their source documents. Previous work has introduced the task of recognizing such factual inconsistency as a downstream application of natural language inference (NLI). However, state-of-the-art NLI models perform poorly in this context due to their inability to generalize to the target task. In this work, we show that NLI models can be effective for this task when the training data is augmented with high-quality task-oriented examples. We introduce FALSESUM, a data generation pipeline leveraging a controllable text generation model to perturb human-annotated summaries, introducing varying types of factual inconsistencies. Unlike previously introduced document-level NLI datasets, our generated dataset contains examples that are diverse and inconsistent yet plausible. We show that models trained on a Falsesum-augmented NLI dataset improve the state-of-the-art performance across four benchmarks for detecting factual inconsistency in summarization.(1)",
"Shi, Kaize; Peng, Xueping; Lu, Hao; Zhu, Yifan; Niu, Zhendong","Zhu, Yifan/AAF-3987-2020; zhu, yifan/JMR-2845-2023","Zhu, Yifan/0000-0002-7695-1633; lu, hao/0000-0002-8065-8499; Peng, Xueping/0000-0002-8901-1472; Shi, Kaize/0000-0003-3561-3627",Multiple Knowledge-Enhanced Meteorological Social Briefing Generation,,,10.1109/TCSS.2023.3298252 ,Article; Early Access ,,"Frequent meteorological disasters present new challenges for decision-making in disaster response. As a timely and effective source of intelligent information, social media plays a vital role in detecting and monitoring these situations. Meteorological social briefings summarize valuable information from numerous social media posts, providing essential decision-support services. This article proposes a multi-knowledge-enhanced summarization (MKES) model for automatically generating meteorological social briefing content from multiple Sina Weibo posts. The MKES model consists of a summary generation module and a knowledge enhancement module. The knowledge enhancement module guides and constrains the summary generation process using meteorological events and geographical location knowledge, resulting in summaries that focus on describing specific knowledge from the source text. The MKES model outperforms baseline models in content evaluation, as measured by ROUGE-1, ROUGE-2, and ROUGE-L scores, and in sentiment evaluation, as measured by F1 scores. Based on the MKES model, a framework for generating meteorological social briefings is developed, providing decision support services for the China Meteorological Administration (CMA).",2329-924X,,,, ,  ,,"Title:Multiple Knowledge-Enhanced Meteorological Social Briefing Generation

 Frequent meteorological disasters present new challenges for decision-making in disaster response. As a timely and effective source of intelligent information, social media plays a vital role in detecting and monitoring these situations. Meteorological social briefings summarize valuable information from numerous social media posts, providing essential decision-support services. This article proposes a multi-knowledge-enhanced summarization (MKES) model for automatically generating meteorological social briefing content from multiple Sina Weibo posts. The MKES model consists of a summary generation module and a knowledge enhancement module. The knowledge enhancement module guides and constrains the summary generation process using meteorological events and geographical location knowledge, resulting in summaries that focus on describing specific knowledge from the source text. The MKES model outperforms baseline models in content evaluation, as measured by ROUGE-1, ROUGE-2, and ROUGE-L scores, and in sentiment evaluation, as measured by F1 scores. Based on the MKES model, a framework for generating meteorological social briefings is developed, providing decision support services for the China Meteorological Administration (CMA).",
"Lyu, Yiwei; Liang, Paul Pu; Pham, Hai; Hovy, Eduard; Poczos, Barnabas; Salakhutdinov, Ruslan; Morency, Louis-Philippe","Morency, Louis-Philippe/B-2006-2008; Liang, Paul/AAL-4346-2020","Hovy, Eduard/0000-0002-3270-7903",STYLEPTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer,,, ,Proceedings Paper ,2021.0,"Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, STYLEPTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on STYLEPTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, STYLEPTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.",,,978-1-954085-46-6,2116-2138, , Conference of the North-American-Chapter of the Association-for-Computational-Linguistics - Human Language Technologies (NAACL-HLT)Conference of the North-American-Chapter of the Association-for-Computational-Linguistics - Human Language Technologies (NAACL-HLT) ,,"Title:STYLEPTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer

 Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, STYLEPTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on STYLEPTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, STYLEPTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.",
"Ahmad, Zishan; Mukuntha, N. S.; Ekbal, Asif; Bhattacharyya, Pushpak","Ekbal, Asif/JKI-7638-2023",,Tweet to News Conversion: An Investigation into Unsupervised Controllable Text Generation,,,10.1109/ijcnn48605.2020.9206620 ,Proceedings Paper ,2020.0,"Text generator systems have become extremely popular with the advent of recent deep learning models such as encoder-decoder. Controlling the information and style of the generated output without supervision is an important and challenging Natural Language Processing (NLP) task. In this paper we define the task of constructing a coherent paragraph from a set of disaster domain tweets, without any parallel data. We tackle the problem by building two systems in pipeline. The first system focuses on unsupervised style transfer to convert the individual tweets into news sentences. The second system stitches together the outputs from the first system to form a coherent news paragraph. We also propose a novel training mechanism, by splitting the sentences into propositions and training the second system to merge the sentences. We create a validation and test set consisting of tweet-sets and their equivalent news paragraphs to perform empirical evaluation. We also perform human evaluations on our model. In a completely unsupervised setting our model was able to achieve a BLEU score of 19.32, while successfully transferring styles and joining tweets to form a meaningful news paragraph.",2161-4393,,978-1-7281-6926-2,, , International Joint Conference on Neural Networks (IJCNN) held as part of the IEEE World Congress on Computational Intelligence (IEEE WCCI)International Joint Conference on Neural Networks (IJCNN) held as part of the IEEE World Congress on Computational Intelligence (IEEE WCCI) ,,"Title:Tweet to News Conversion: An Investigation into Unsupervised Controllable Text Generation

 Text generator systems have become extremely popular with the advent of recent deep learning models such as encoder-decoder. Controlling the information and style of the generated output without supervision is an important and challenging Natural Language Processing (NLP) task. In this paper we define the task of constructing a coherent paragraph from a set of disaster domain tweets, without any parallel data. We tackle the problem by building two systems in pipeline. The first system focuses on unsupervised style transfer to convert the individual tweets into news sentences. The second system stitches together the outputs from the first system to form a coherent news paragraph. We also propose a novel training mechanism, by splitting the sentences into propositions and training the second system to merge the sentences. We create a validation and test set consisting of tweet-sets and their equivalent news paragraphs to perform empirical evaluation. We also perform human evaluations on our model. In a completely unsupervised setting our model was able to achieve a BLEU score of 19.32, while successfully transferring styles and joining tweets to form a meaningful news paragraph.",
"Shen, Chenhui; Cheng, Liying; Zhou, Ran; Bing, Lidong; You, Yang; Si, Luo",,,MReD: A Meta-Review Dataset for Structure-Controllable Text Generation,,, ,Proceedings Paper ,2022.0,"When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited. A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences. A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with a deep understanding of the domain knowledge. Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc. We present experimental results on start-of-the-art summarization models, and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data. By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain.(1)",,,978-1-955917-25-4,2521-2535, , 60th Annual Meeting of the Association-for-Computational-Linguistics (ACL)60th Annual Meeting of the Association-for-Computational-Linguistics (ACL) ,,"Title:MReD: A Meta-Review Dataset for Structure-Controllable Text Generation

 When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited. A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences. A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with a deep understanding of the domain knowledge. Motivated by this vision, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated with one of the 9 carefully defined categories, including abstract, strength, decision, etc. We present experimental results on start-of-the-art summarization models, and propose methods for structure-controlled generation with both extractive and abstractive models using our annotated data. By exploring various settings and analyzing the model behavior with respect to the control signal, we demonstrate the challenges of our proposed task and the values of our dataset MReD. Meanwhile, MReD also allows us to have a better understanding of the meta-review domain.(1)",
"Zellers, Rowan; Holtzman, Ari; Rashkin, Hannah; Bisk, Yonatan; Farhadi, Ali; Roesner, Franziska; Choi, Yejin",,,Defending Against Neural Fake News,32,, ,Proceedings Paper ,2019.0,"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like 'Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias - and sampling strategies that alleviate its effects - both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",1049-5258,,*****************,, , 33rd Conference on Neural Information Processing Systems (NeurIPS)33rd Conference on Neural Information Processing Systems (NeurIPS) ,,"Title:Defending Against Neural Fake News

 Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like 'Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias - and sampling strategies that alleviate its effects - both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",
"Lin, Fuqiang; Ma, Xingkong; Chen, Yaofeng; Zhou, Jiajun; Liu, Bo",,,PC-SAN: Pretraining-Based Contextual Self-Attention Model for Topic Essay Generation,14,8,10.3837/tiis.2020.08.001 ,Article ,2020.0,"Automatic topic essay generation (TEG) is a controllable text generation task that aims to generate informative, diverse, and topic-consistent essays based on multiple topics. To make the generated essays of high quality, a reasonable method should consider both diversity and topic-consistency. Another essential issue is the intrinsic link of the topics, which contributes to making the essays closely surround the semantics of provided topics. However, it remains challenging for TEG to fill the semantic gap between source topic words and target output, and a more powerful model is needed to capture the semantics of given topics. To this end, we propose a pretraining-based contextual self-attention (PC-SAN) model that is built upon the seq2seq framework. For the encoder of our model, we employ a dynamic weight sum of layers from BERT to fully utilize the semantics of topics, which is of great help to fill the gap and improve the quality of the generated essays. In the decoding phase, we also transform the target-side contextual history information into the query layers to alleviate the lack of context in typical self-attention networks (SANs). Experimental results on large-scale paragraph-level Chinese corpora verify that our model is capable of generating diverse, topic-consistent text and essentially makes improvements as compare to strong baselines. Furthermore, extensive analysis validates the effectiveness of contextual embeddings from BERT and contextual history information in SANs.",1976-7277,,,3168-3186, ,  ,,"Title:PC-SAN: Pretraining-Based Contextual Self-Attention Model for Topic Essay Generation

 Automatic topic essay generation (TEG) is a controllable text generation task that aims to generate informative, diverse, and topic-consistent essays based on multiple topics. To make the generated essays of high quality, a reasonable method should consider both diversity and topic-consistency. Another essential issue is the intrinsic link of the topics, which contributes to making the essays closely surround the semantics of provided topics. However, it remains challenging for TEG to fill the semantic gap between source topic words and target output, and a more powerful model is needed to capture the semantics of given topics. To this end, we propose a pretraining-based contextual self-attention (PC-SAN) model that is built upon the seq2seq framework. For the encoder of our model, we employ a dynamic weight sum of layers from BERT to fully utilize the semantics of topics, which is of great help to fill the gap and improve the quality of the generated essays. In the decoding phase, we also transform the target-side contextual history information into the query layers to alleviate the lack of context in typical self-attention networks (SANs). Experimental results on large-scale paragraph-level Chinese corpora verify that our model is capable of generating diverse, topic-consistent text and essentially makes improvements as compare to strong baselines. Furthermore, extensive analysis validates the effectiveness of contextual embeddings from BERT and contextual history information in SANs.",
"Chevasson, Raphael; Laclau, Charlotte; Gravier, Christophe",,,Diverse Paraphrasing with Insertion Models for Few-Shot Intent Detection,13876,,10.1007/978-3-031-30047-9_6 ,Proceedings Paper ,2023.0,"In contrast to classic autoregressive generation, insertion-based models can predict in a order-free way multiple tokens at a time, which make their generation uniquely controllable: it can be constrained to strictly include an ordered list of tokens. We propose to exploit this feature in a new diverse paraphrasing framework: first, we extract important tokens or keywords in the source sentence; second, we augment them; third, we generate new samples around them by using insertion models. We show that the generated paraphrases are competitive with state of the art autoregressive paraphrasers, not only in diversity but also in quality. We further investigate their potential to create new pseudo-labelled samples for data augmentation, using a meta-learning classification framework, and find equally competitive result. In addition to proving non-autoregressive (NAR) viability for paraphrasing, we contribute our open-source framework as a starting point for further research into controllable NAR generation.",0302-9743,1611-3349,978-3-031-30046-2; 978-3-031-30047-9,65-76, , 21st International Symposium on Intelligent Data Analysis (IDA)21st International Symposium on Intelligent Data Analysis (IDA) ,,"Title:Diverse Paraphrasing with Insertion Models for Few-Shot Intent Detection

 In contrast to classic autoregressive generation, insertion-based models can predict in a order-free way multiple tokens at a time, which make their generation uniquely controllable: it can be constrained to strictly include an ordered list of tokens. We propose to exploit this feature in a new diverse paraphrasing framework: first, we extract important tokens or keywords in the source sentence; second, we augment them; third, we generate new samples around them by using insertion models. We show that the generated paraphrases are competitive with state of the art autoregressive paraphrasers, not only in diversity but also in quality. We further investigate their potential to create new pseudo-labelled samples for data augmentation, using a meta-learning classification framework, and find equally competitive result. In addition to proving non-autoregressive (NAR) viability for paraphrasing, we contribute our open-source framework as a starting point for further research into controllable NAR generation.",
"Lin, Haitao; Zhu, Junnan; Xiang, Lu; Zhai, Feifei; Zhou, Yu; Zhang, Jiajun; Zong, Chengqing",,"Zong, Chengqing/0000-0002-9864-3818",Topic-Oriented Dialogue Summarization,31,,10.1109/TASLP.2023.3271118 ,Article ,2023.0,"A multi-turn dialogue often contains multiple discussion topics. In several scenarios (e.g., customer service dispute, public opinion monitoring), people are only interested in the gist of a specific topic in the dialogue. Therefore, we propose a novel summarization task, i.e., Topic-Oriented Dialogue Summarization (TODS). Given a dialogue with a topic label, TODS aims to produce a summary covering the main content of the given topic in the dialogue. To model the relationship between dialogues and topics, three key abilities are needed for TODS: (1) Learning the semantic information of different topics. (2) Locating the topic-related content in the dialogue. (3) Distinguishing summaries for different topics in the same dialogue. Thus, we propose three topic-related auxiliary tasks to make the summarization model learn the three abilities above. First, the topic identification task aims at generating all the topics in the dialogue. Second, the topic attention restriction task tries to constrain the attention distribution on topic-related utterances. Third, the topic summary distinguishing task focuses on increasing the difference of summaries for different topics in the same dialogue. Experimental results on two public TODS datasets show that all auxiliary tasks are critical for TODS and help generate high-quality summaries. We also point out the expansions and challenges in TODS for future research.",2329-9290,2329-9304,,1797-1810, ,  ,,"Title:Topic-Oriented Dialogue Summarization

 A multi-turn dialogue often contains multiple discussion topics. In several scenarios (e.g., customer service dispute, public opinion monitoring), people are only interested in the gist of a specific topic in the dialogue. Therefore, we propose a novel summarization task, i.e., Topic-Oriented Dialogue Summarization (TODS). Given a dialogue with a topic label, TODS aims to produce a summary covering the main content of the given topic in the dialogue. To model the relationship between dialogues and topics, three key abilities are needed for TODS: (1) Learning the semantic information of different topics. (2) Locating the topic-related content in the dialogue. (3) Distinguishing summaries for different topics in the same dialogue. Thus, we propose three topic-related auxiliary tasks to make the summarization model learn the three abilities above. First, the topic identification task aims at generating all the topics in the dialogue. Second, the topic attention restriction task tries to constrain the attention distribution on topic-related utterances. Third, the topic summary distinguishing task focuses on increasing the difference of summaries for different topics in the same dialogue. Experimental results on two public TODS datasets show that all auxiliary tasks are critical for TODS and help generate high-quality summaries. We also point out the expansions and challenges in TODS for future research.",
"Chen, Qibin; Lin, Junyang; Zhang, Yichang; Yang, Hongxia; Zhou, Jingren; Tang, Jie","Chen, Qibin/AAT-3197-2020",,Towards Knowledge-Based Personalized Product Description Generation in E-commerce,,,10.1145/3292500.3330725 ,Proceedings Paper ,2019.0,"Quality product descriptions are critical for providing competitive customer experience in an e-commerce platform. An accurate and attractive description not only helps customers make an informed decision but also improves the likelihood of purchase. However, crafting a successful product description is tedious and highly time-consuming. Due to its importance, automating the product description generation has attracted considerable interest from both research and industrial communities. Existing methods mainly use templates or statistical methods, and their performance could be rather limited. In this paper, we explore a new way to generate personalized product descriptions by combining the power of neural networks and knowledge base. Specifically, we propose a KnOwledge Based pErsonalized (or KOBE) product description generation model in the context of e-commerce. In KOBE, we extend the encoder-decoder framework, the Transformer, to a sequence modeling formulation using self-attention. In order to make the description both informative and personalized, KOBE considers a variety of important factors during text generation, including product aspects, user categories, and knowledge base. Experiments on real-world datasets demonstrate that the proposed method outperforms the baseline on various metrics. 1 KOBE can achieve an improvement of 9.7% over state-of-the-arts in terms of BLEU. We also present several case studies as the anecdotal evidence to further prove the effectiveness of the proposed approach. The framework has been deployed in Taobao, 2 the largest online e-commerce platform in China.",,,978-1-4503-6201-6,3040-3050, , 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD)25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) ,,"Title:Towards Knowledge-Based Personalized Product Description Generation in E-commerce

 Quality product descriptions are critical for providing competitive customer experience in an e-commerce platform. An accurate and attractive description not only helps customers make an informed decision but also improves the likelihood of purchase. However, crafting a successful product description is tedious and highly time-consuming. Due to its importance, automating the product description generation has attracted considerable interest from both research and industrial communities. Existing methods mainly use templates or statistical methods, and their performance could be rather limited. In this paper, we explore a new way to generate personalized product descriptions by combining the power of neural networks and knowledge base. Specifically, we propose a KnOwledge Based pErsonalized (or KOBE) product description generation model in the context of e-commerce. In KOBE, we extend the encoder-decoder framework, the Transformer, to a sequence modeling formulation using self-attention. In order to make the description both informative and personalized, KOBE considers a variety of important factors during text generation, including product aspects, user categories, and knowledge base. Experiments on real-world datasets demonstrate that the proposed method outperforms the baseline on various metrics. 1 KOBE can achieve an improvement of 9.7% over state-of-the-arts in terms of BLEU. We also present several case studies as the anecdotal evidence to further prove the effectiveness of the proposed approach. The framework has been deployed in Taobao, 2 the largest online e-commerce platform in China.",
"Chen Z,Liu Z",,,Fixed global memory for controllable long text generation,53,11,10.1007/S10489-022-04197-6 , Journal Article,2023.0,"Generating coherent conversation is an impor-
tant and challenging long text generation task,
as it has various applications such as daily enter-
tainment, education, or building conversational
AI to facilitate human-computer interaction.
However, current generation models often fail
to effectively utilize rich linguistic and world
knowledge to generate conversations just like
humans. In this work, we introduce a novel con-
versation generation framework to effectively
incorporate human knowledge and conversa-
tion structures with both controllability and in-
terpretability for better conversation generation.
Specifically, we first generate the prototype con-
versations from short descriptions. We then
gradually and strategically incorporate differ-
ent levels of conversation structures including
the action triples, dialogue acts, and discourse
relations via diffusion models to directly edit
the prototype conversations. We demonstrate
the effectiveness of our framework through ex-
periments on two datasets by comparing our
method with the state-of-the-art baseline mod-
els1.",,,,, Appl. Intell.,  ,,"Title:Fixed global memory for controllable long text generation

 Generating coherent conversation is an impor-
tant and challenging long text generation task,
as it has various applications such as daily enter-
tainment, education, or building conversational
AI to facilitate human-computer interaction.
However, current generation models often fail
to effectively utilize rich linguistic and world
knowledge to generate conversations just like
humans. In this work, we introduce a novel con-
versation generation framework to effectively
incorporate human knowledge and conversa-
tion structures with both controllability and in-
terpretability for better conversation generation.
Specifically, we first generate the prototype con-
versations from short descriptions. We then
gradually and strategically incorporate differ-
ent levels of conversation structures including
the action triples, dialogue acts, and discourse
relations via diffusion models to directly edit
the prototype conversations. We demonstrate
the effectiveness of our framework through ex-
periments on two datasets by comparing our
method with the state-of-the-art baseline mod-
els1.",
"Feng Y,Yi X,Wang X,Lakshmanan LV,Xie X",,,DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation,,,10.18653/V1/2023.ACL-LONG.488 , Conference Paper,2023.0,"AbstractSelf-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented only by self-generated pseudo text, generation models over-exploit the previously learned text space and fail to explore a larger one, suffering from a restricted generalization boundary and limited controllability. In this work, we propose DuNST, a novel ST framework to tackle these problems. DuNST jointly models text generation and classification as a dual process and further perturbs and escapes from the collapsed space by adding two kinds of flexible noise. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We theoretically demonstrate that DuNST can be regarded as enhancing the exploration of the potentially larger real text space while maintaining exploitation, guaranteeing improved performance. Experiments on three controllable generation tasks show that DuNST significantly boosts control accuracy with comparable generation fluency and diversity against several strong baselines.",,,,,Association for Computational Linguistics ,"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023  ",,"Title:DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation

 AbstractSelf-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented only by self-generated pseudo text, generation models over-exploit the previously learned text space and fail to explore a larger one, suffering from a restricted generalization boundary and limited controllability. In this work, we propose DuNST, a novel ST framework to tackle these problems. DuNST jointly models text generation and classification as a dual process and further perturbs and escapes from the collapsed space by adding two kinds of flexible noise. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We theoretically demonstrate that DuNST can be regarded as enhancing the exploration of the potentially larger real text space while maintaining exploitation, guaranteeing improved performance. Experiments on three controllable generation tasks show that DuNST significantly boosts control accuracy with comparable generation fluency and diversity against several strong baselines.",
"Gu Y,Feng X,Ma S,Zhang L,Gong H,Zhong W,Qin B",,,Controllable Text Generation via Probability Density Estimation in the Latent Space,,,10.18653/V1/2023.ACL-LONG.704 , Conference Paper,2023.0,"Previous work on controllable text generation
has explored the idea of control from the latent
space, such as optimizing a representation with
attribute-specific classifiers or sampling one
from relevant discrete samples. However, they
cannot effectively model a complex space with
diverse attributes, high dimensionality, and
asymmetric structure, leaving subsequent con-
trols unsatisfying. In this work, we propose a
novel control framework using probability den-
sity estimation in the latent space. Our method
utilizes an invertible transformation function,
the Normalizing Flow, that maps the complex
distributions in the latent space to simple Gaus-
sian distributions in the prior space. Thus, we
can perform sophisticated and flexible controls
in the prior space and feed the control effects
back into the latent space owing to the bijection
property of invertible transformations. Exper-
iments on single-attribute and multi-attribute
control reveal that our method outperforms sev-
eral strong baselines on attribute relevance and
text quality, achieving a new SOTA. Further
analysis of control strength adjustment demon-
strates the flexibility of our control strategy1.",,,,,Association for Computational Linguistics ,"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023  ",,"Title:Controllable Text Generation via Probability Density Estimation in the Latent Space

 Previous work on controllable text generation
has explored the idea of control from the latent
space, such as optimizing a representation with
attribute-specific classifiers or sampling one
from relevant discrete samples. However, they
cannot effectively model a complex space with
diverse attributes, high dimensionality, and
asymmetric structure, leaving subsequent con-
trols unsatisfying. In this work, we propose a
novel control framework using probability den-
sity estimation in the latent space. Our method
utilizes an invertible transformation function,
the Normalizing Flow, that maps the complex
distributions in the latent space to simple Gaus-
sian distributions in the prior space. Thus, we
can perform sophisticated and flexible controls
in the prior space and feed the control effects
back into the latent space owing to the bijection
property of invertible transformations. Exper-
iments on single-attribute and multi-attribute
control reveal that our method outperforms sev-
eral strong baselines on attribute relevance and
text quality, achieving a new SOTA. Further
analysis of control strength adjustment demon-
strates the flexibility of our control strategy1.",
"Huang X,Liu Z,Li P,Li T,Sun M,Liu Y",,,An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text Generation,,,10.18653/V1/2023.ACL-LONG.849 , Conference Paper,2023.0,"Abstract:Recently, multi-aspect controllable text generation that controls the generated text in multiple aspects (e.g., sentiment, topic, and keywords) has attracted increasing attention. Although methods based on parameter efficient tuning like prefix-tuning could achieve multi-aspect controlling in a plug-and-play way, the mutual interference of multiple prefixes leads to significant degeneration of constraints and limits their extensibility to training-time unseen aspect combinations. In this work, we provide a theoretical lower bound for the interference and empirically found that the interference grows with the number of layers where prefixes are inserted. Based on these analyses, we propose using trainable gates to normalize the intervention of prefixes to restrain the growing interference. As a result, controlling training-time unseen combinations of aspects can be realized by simply concatenating corresponding plugins such that new constraints can be extended at a lower cost. In addition, we propose a unified way to process both categorical and free-form constraints. Experiments on text generation and machine translation demonstrate the superiority of our approach over baselines on constraint accuracy, text quality, and extensibility.",,,,,Association for Computational Linguistics ,"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023  ",,"Title:An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text Generation

 Abstract:Recently, multi-aspect controllable text generation that controls the generated text in multiple aspects (e.g., sentiment, topic, and keywords) has attracted increasing attention. Although methods based on parameter efficient tuning like prefix-tuning could achieve multi-aspect controlling in a plug-and-play way, the mutual interference of multiple prefixes leads to significant degeneration of constraints and limits their extensibility to training-time unseen aspect combinations. In this work, we provide a theoretical lower bound for the interference and empirically found that the interference grows with the number of layers where prefixes are inserted. Based on these analyses, we propose using trainable gates to normalize the intervention of prefixes to restrain the growing interference. As a result, controlling training-time unseen combinations of aspects can be realized by simply concatenating corresponding plugins such that new constraints can be extended at a lower cost. In addition, we propose a unified way to process both categorical and free-form constraints. Experiments on text generation and machine translation demonstrate the superiority of our approach over baselines on constraint accuracy, text quality, and extensibility.",
"Ma C,Zhao T,Shing M,Sawada K,Okumura M",,,Focused Prefix Tuning for Controllable Text Generation,,,10.18653/V1/2023.ACL-SHORT.96 , Conference Paper,2023.0,"AbstractIn a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning (FPT) to mitigate the problem and to enable the control to focus on the desired attribute. Experimental results show that FPT can achieve better control accuracy and text fluency than baseline models in single-attribute control tasks. In multi-attribute control tasks, FPT achieves comparable control accuracy with the state-of-the-art approach while keeping the flexibility to control new attributes without retraining existing models.",,,,,Association for Computational Linguistics ,"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023  ",,"Title:Focused Prefix Tuning for Controllable Text Generation

 AbstractIn a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning (FPT) to mitigate the problem and to enable the control to focus on the desired attribute. Experimental results show that FPT can achieve better control accuracy and text fluency than baseline models in single-attribute control tasks. In multi-attribute control tasks, FPT achieves comparable control accuracy with the state-of-the-art approach while keeping the flexibility to control new attributes without retraining existing models.",
"Wen Z,Tian Z,Huang Z,Yang Y,Jian Z,Wang C,Li D",,,GRACE: Gradient-guided Controllable Retrieval for Augmenting Attribute-based Text Generation,,,10.18653/V1/2023.FINDINGS-ACL.530 , Conference Paper,2023.0,"AbstractAttribute-based generation methods are of growing significance in controlling the generation of large pre-trained language models (PLMs). Existing studies control the generation by (1) finetuning the model with attributes or (2) guiding the inference processing toward control signals while freezing the PLM. However, finetuning approaches infuse domain bias into generation, making it hard to generate out-of-domain texts. Besides, many methods guide the inference in its word-by-word generation, pushing the word probability to the target attributes, resulting in less fluent sentences. We argue that distilling controlling information from natural texts can produce fluent sentences while maintaining high controllability. In this paper, we propose GRAdient-guided Controllable rEtrieval (GRACE), a retrieval-augmented generation framework to facilitate the generation of fluent sentences with high attribute relevance. GRACE memorizes the semantic and attribute information from unlabeled corpora and applies a controllable retrieval to obtain desired information. For the generation, we design techniques to eliminate the domain bias from the retrieval results and integrate it into the generation model. Additionally, we propose a gradient-guided generation scheme that iteratively steers generation toward higher attribute relevance. Experimental results and quantities of examples verify the effectiveness of our method.",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023  ",,"Title:GRACE: Gradient-guided Controllable Retrieval for Augmenting Attribute-based Text Generation

 AbstractAttribute-based generation methods are of growing significance in controlling the generation of large pre-trained language models (PLMs). Existing studies control the generation by (1) finetuning the model with attributes or (2) guiding the inference processing toward control signals while freezing the PLM. However, finetuning approaches infuse domain bias into generation, making it hard to generate out-of-domain texts. Besides, many methods guide the inference in its word-by-word generation, pushing the word probability to the target attributes, resulting in less fluent sentences. We argue that distilling controlling information from natural texts can produce fluent sentences while maintaining high controllability. In this paper, we propose GRAdient-guided Controllable rEtrieval (GRACE), a retrieval-augmented generation framework to facilitate the generation of fluent sentences with high attribute relevance. GRACE memorizes the semantic and attribute information from unlabeled corpora and applies a controllable retrieval to obtain desired information. For the generation, we design techniques to eliminate the domain bias from the retrieval results and integrate it into the generation model. Additionally, we propose a gradient-guided generation scheme that iteratively steers generation toward higher attribute relevance. Experimental results and quantities of examples verify the effectiveness of our method.",
"Zheng C,Ke P,Zhang Z,Huang M",,,Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning,,,10.18653/V1/2023.FINDINGS-ACL.65 , Conference Paper,2023.0,"(1) Generating Multiple Contiuations
Language
Model
Prompt
(2) Sample Construction
Likelihood
High
Low
Label
Function
Label
(3) Contrastive Learning
Language
Model
Language
Figure 1: Overview of CLICK. It contains three steps:
(1) Generating multiple continuations given a prompt,
which are labeled as positive / negative by a label func-
tion. (2) Constructing contrastive samples by pairing
each negative sample with the positive one whose
likelihood ranks highest but lower than the former
(§ 2.3). (3) Training the language model with the addi-
tional contrastive loss (§ 2.2).",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023  ",,"Title:Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning

 (1) Generating Multiple Contiuations
Language
Model
Prompt
(2) Sample Construction
Likelihood
High
Low
Label
Function
Label
(3) Contrastive Learning
Language
Model
Language
Figure 1: Overview of CLICK. It contains three steps:
(1) Generating multiple continuations given a prompt,
which are labeled as positive / negative by a label func-
tion. (2) Constructing contrastive samples by pairing
each negative sample with the positive one whose
likelihood ranks highest but lower than the former
(§ 2.3). (3) Training the language model with the addi-
tional contrastive loss (§ 2.2).",
"Forristal J,Mireshghallah F,Durrett G,Berg-Kirkpatrick T",,,A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation,,, , Conference Paper,2023.0,"AbstractRecent work has shown that energy-based language modeling is an effective framework for controllable text generation because it enables flexible integration of arbitrary discriminators. However, because energy-based LMs are globally normalized, approximate techniques like Metropolis-Hastings (MH) are required for inference. Past work has largely explored simple proposal distributions that modify a single token at a time, like in Gibbs sampling. In this paper, we develop a novel MH sampler that, in contrast, proposes re-writes of the entire sequence in each step via iterative prompting of a large language model. Our new sampler (a) allows for more efficient and accurate sampling from a target distribution and (b) allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required. We perform experiments on two controlled generation tasks, showing both downstream performance gains and more accurate target distribution sampling in comparison with single-token proposal techniques.",,,,,Association for Computational Linguistics ,"Proceedings of the 27th Conference on Computational Natural Language Learning, CoNLL 2023, Singapore, December 6-7, 2023  ",,"Title:A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation

 AbstractRecent work has shown that energy-based language modeling is an effective framework for controllable text generation because it enables flexible integration of arbitrary discriminators. However, because energy-based LMs are globally normalized, approximate techniques like Metropolis-Hastings (MH) are required for inference. Past work has largely explored simple proposal distributions that modify a single token at a time, like in Gibbs sampling. In this paper, we develop a novel MH sampler that, in contrast, proposes re-writes of the entire sequence in each step via iterative prompting of a large language model. Our new sampler (a) allows for more efficient and accurate sampling from a target distribution and (b) allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required. We perform experiments on two controlled generation tasks, showing both downstream performance gains and more accurate target distribution sampling in comparison with single-token proposal techniques.",
"Avrahami O,Hayes T,Gafni O,Gupta S,Taigman Y,Parikh D,Lischinski D,Fried O,Yin X",,,SpaText: Spatio-Textual Representation for Controllable Image Generation,,,10.1109/CVPR52729.2023.01762 , Conference Paper,2023.0,"Abstract:Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText - a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control.",,,,,IEEE ,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023  ",,"Title:SpaText: Spatio-Textual Representation for Controllable Image Generation

 Abstract:Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText - a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control.",
"Ding H,Pang L,Wei Z,Shen H,Cheng X,Chua TS",,,MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space,,, , Conference Paper,2023.0,"Multi-aspect controllable text generation aims
to generate fluent sentences that possess mul-
tiple desired attributes simultaneously. Tradi-
tional methods either require expensive itera-
tion / searching within the discrete text space
during the decoding stage, or train separate
controllers for each aspect, resulting in a degra-
dation of text quality due to the discrepancy
between different aspects. To address these
limitations, we introduce a novel approach for
Multi-aspect control, namely MacLaSa, that
estimates compact Latent space for multiple
aspects, and performs efficient Sampling with
a fast sampler. To eliminate the domain discrep-
ancies between different aspects, we first uti-
lize a variational autoencoder (VAE) network to
map text sequences from various data sources
into close latent representations. The estimated
latent space enables the formulation of joint
energy-based models and the plugging in of ar-
bitrary attribute discriminators to achieve multi-
aspect control. Afterwards, we draw latent sam-
ples with a fast sampler based on ordinary dif-
ferential equations and feed sampled examples
to the VAE decoder to produce target text se-
quences. Experimental results demonstrate that
MacLaSa outperforms strong baselines on both
attribute relevance and textual quality while
maintaining a high inference speed.",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023  ",,"Title:MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space

 Multi-aspect controllable text generation aims
to generate fluent sentences that possess mul-
tiple desired attributes simultaneously. Tradi-
tional methods either require expensive itera-
tion / searching within the discrete text space
during the decoding stage, or train separate
controllers for each aspect, resulting in a degra-
dation of text quality due to the discrepancy
between different aspects. To address these
limitations, we introduce a novel approach for
Multi-aspect control, namely MacLaSa, that
estimates compact Latent space for multiple
aspects, and performs efficient Sampling with
a fast sampler. To eliminate the domain discrep-
ancies between different aspects, we first uti-
lize a variational autoencoder (VAE) network to
map text sequences from various data sources
into close latent representations. The estimated
latent space enables the formulation of joint
energy-based models and the plugging in of ar-
bitrary attribute discriminators to achieve multi-
aspect control. Afterwards, we draw latent sam-
ples with a fast sampler based on ordinary dif-
ferential equations and feed sampled examples
to the VAE decoder to produce target text se-
quences. Experimental results demonstrate that
MacLaSa outperforms strong baselines on both
attribute relevance and textual quality while
maintaining a high inference speed.",
"Zhong T,Wang Q,Han J,Zhang Y,Mao Z",,,Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation,,, , Conference Paper,2023.0,"AbstractControllable text generation (CTG) aims to generate text with desired attributes, and decoding-time-based methods have shown promising performance on this task. However, in this paper, we identify the phenomenon of Attribute Collapse for the first time. It causes the fluency of generated text to rapidly decrease when the control strength exceeds a critical value, rendering the text completely unusable. This limitation hinders the effectiveness of decoding methods in achieving high levels of controllability. To address this problem, we propose a novel lightweight decoding framework named Air-Decoding. Its main idea is reconstructing the attribute distributions to balance the weights between attribute words and non-attribute words to generate more fluent text. Specifically, we train prefixes by prefix-tuning to obtain attribute distributions. Then we design a novel attribute distribution reconstruction method to balance the obtained distributions and use the reconstructed distributions to guide language models for generation, effectively avoiding the issue of Attribute Collapse. Experiments on multiple CTG tasks prove that our method achieves a new state-of-the-art control performance.",,,,,Association for Computational Linguistics ,"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023  ",,"Title:Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation

 AbstractControllable text generation (CTG) aims to generate text with desired attributes, and decoding-time-based methods have shown promising performance on this task. However, in this paper, we identify the phenomenon of Attribute Collapse for the first time. It causes the fluency of generated text to rapidly decrease when the control strength exceeds a critical value, rendering the text completely unusable. This limitation hinders the effectiveness of decoding methods in achieving high levels of controllability. To address this problem, we propose a novel lightweight decoding framework named Air-Decoding. Its main idea is reconstructing the attribute distributions to balance the weights between attribute words and non-attribute words to generate more fluent text. Specifically, we train prefixes by prefix-tuning to obtain attribute distributions. Then we design a novel attribute distribution reconstruction method to balance the obtained distributions and use the reconstructed distributions to guide language models for generation, effectively avoiding the issue of Attribute Collapse. Experiments on multiple CTG tasks prove that our method achieves a new state-of-the-art control performance.",
"Feng Y,Yi X,Lakshmanan LV,Xie X",,,KEST: Kernel Distance Based Efficient Self-Training for Improving Controllable Text Generation,,,10.24963/IJCAI.2023/561 , Conference Paper,2023.0,"Self-training (ST) has come to fruition in language understanding tasks by producing pseudo labels, which reduces the labeling bottleneck of language model fine-tuning. Nevertheless, in facilitating semi-supervised controllable language generation, ST faces two key challenges. First, augmented by self-generated pseudo text, generation models tend to over-exploit the previously learned text distribution, suffering from mode collapse and poor generation diversity. Second, generating pseudo text in each iteration is time-consuming, severely decelerating the training process. In this work, we propose KEST, a novel and efficient self-training framework to handle these problems. KEST utilizes a kernel-based loss, rather than standard cross entropy, to learn from the soft pseudo text produced by a shared non-autoregressive generator. We demonstrate both theoretically and empirically that KEST can benefit from more diverse pseudo text in an efficient manner, which allows not only refining and exploiting the previously fitted distribution but also enhanced exploration towards a larger potential text space, providing a guarantee of improved performance. Experiments on three controllable generation tasks demonstrate that KEST significantly improves control accuracy while maintaining comparable text fluency and generation diversity against several strong baselines.",,,,,ijcai.org ,"Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China  ",,"Title:KEST: Kernel Distance Based Efficient Self-Training for Improving Controllable Text Generation

 Self-training (ST) has come to fruition in language understanding tasks by producing pseudo labels, which reduces the labeling bottleneck of language model fine-tuning. Nevertheless, in facilitating semi-supervised controllable language generation, ST faces two key challenges. First, augmented by self-generated pseudo text, generation models tend to over-exploit the previously learned text distribution, suffering from mode collapse and poor generation diversity. Second, generating pseudo text in each iteration is time-consuming, severely decelerating the training process. In this work, we propose KEST, a novel and efficient self-training framework to handle these problems. KEST utilizes a kernel-based loss, rather than standard cross entropy, to learn from the soft pseudo text produced by a shared non-autoregressive generator. We demonstrate both theoretically and empirically that KEST can benefit from more diverse pseudo text in an efficient manner, which allows not only refining and exploiting the previously fitted distribution but also enhanced exploration towards a larger potential text space, providing a guarantee of improved performance. Experiments on three controllable generation tasks demonstrate that KEST significantly improves control accuracy while maintaining comparable text fluency and generation diversity against several strong baselines.",
"Kou Z,Pei S,Tian Y,Zhang X",,,Character As Pixels: A Controllable Prompt Adversarial Attacking Framework for Black-Box Text Guided Image Generation Models,,,10.24963/IJCAI.2023/109 , Conference Paper,2023.0,"In this paper, we study a controllable prompt adversarial attacking problem for text guided image generation (Text2Image) models in the black-box scenario, where the goal is to attack specific visual subjects (e.g., changing a brown dog to white) in a generated image by slightly, if not imperceptibly, perturbing the characters of the driven prompt (e.g., ``brown'' to ``br0wn''). Our study is motivated by the limitations of current Text2Image attacking approaches that still rely on manual trials to create adversarial prompts. To address such limitations, we develop CharGrad, a character-level gradient based attacking framework that replaces specific characters of a prompt with pixel-level similar ones by interactively learning the perturbation direction for the prompt and updating the attacking examiner for the generated image based on a novel proxy perturbation representation for characters.  We evaluate CharGrad using the texts from two public image captioning datasets. Results demonstrate that CharGrad outperforms existing text adversarial attacking approaches on attacking various subjects of generated images by black-box Text2Image models in a more effective and efficient way with less perturbation on the characters of the prompts.",,,,,ijcai.org ,"Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China  ",,"Title:Character As Pixels: A Controllable Prompt Adversarial Attacking Framework for Black-Box Text Guided Image Generation Models

 In this paper, we study a controllable prompt adversarial attacking problem for text guided image generation (Text2Image) models in the black-box scenario, where the goal is to attack specific visual subjects (e.g., changing a brown dog to white) in a generated image by slightly, if not imperceptibly, perturbing the characters of the driven prompt (e.g., ``brown'' to ``br0wn''). Our study is motivated by the limitations of current Text2Image attacking approaches that still rely on manual trials to create adversarial prompts. To address such limitations, we develop CharGrad, a character-level gradient based attacking framework that replaces specific characters of a prompt with pixel-level similar ones by interactively learning the perturbation direction for the prompt and updating the attacking examiner for the generated image based on a novel proxy perturbation representation for characters.  We evaluate CharGrad using the texts from two public image captioning datasets. Results demonstrate that CharGrad outperforms existing text adversarial attacking approaches on attacking various subjects of generated images by black-box Text2Image models in a more effective and efficient way with less perturbation on the characters of the prompts.",
"Chen Y,Pan Y,Li Y,Yao T,Mei T",,,Control3D: Towards Controllable Text-to-3D Generation,,,10.1145/3581783.3612489 , Conference Paper,2023.0,"Abstract:Recent remarkable advances in large-scale text-to-image diffusion models have inspired a significant breakthrough in text-to-3D generation, pursuing 3D content creation solely from a given text prompt. However, existing text-to-3D techniques lack a crucial ability in the creative process: interactively control and shape the synthetic 3D contents according to users' desired specifications (e.g., sketch). To alleviate this issue, we present the first attempt for text-to-3D generation conditioning on the additional hand-drawn sketch, namely Control3D, which enhances controllability for users. In particular, a 2D conditioned diffusion model (ControlNet) is remoulded to guide the learning of 3D scene parameterized as NeRF, encouraging each view of 3D scene aligned with the given text prompt and hand-drawn sketch. Moreover, we exploit a pre-trained differentiable photo-to-sketch model to directly estimate the sketch of the rendered image over synthetic 3D scene. Such estimated sketch along with each sampled view is further enforced to be geometrically consistent with the given sketch, pursuing better controllable text-to-3D generation. Through extensive experiments, we demonstrate that our proposal can generate accurate and faithful 3D scenes that align closely with the input text prompts and sketches.",,,,,ACM ,"Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 2023- 3 November 2023  ",,"Title:Control3D: Towards Controllable Text-to-3D Generation

 Abstract:Recent remarkable advances in large-scale text-to-image diffusion models have inspired a significant breakthrough in text-to-3D generation, pursuing 3D content creation solely from a given text prompt. However, existing text-to-3D techniques lack a crucial ability in the creative process: interactively control and shape the synthetic 3D contents according to users' desired specifications (e.g., sketch). To alleviate this issue, we present the first attempt for text-to-3D generation conditioning on the additional hand-drawn sketch, namely Control3D, which enhances controllability for users. In particular, a 2D conditioned diffusion model (ControlNet) is remoulded to guide the learning of 3D scene parameterized as NeRF, encouraging each view of 3D scene aligned with the given text prompt and hand-drawn sketch. Moreover, we exploit a pre-trained differentiable photo-to-sketch model to directly estimate the sketch of the rendered image over synthetic 3D scene. Such estimated sketch along with each sampled view is further enforced to be geometrically consistent with the given sketch, pursuing better controllable text-to-3D generation. Through extensive experiments, we demonstrate that our proposal can generate accurate and faithful 3D scenes that align closely with the input text prompts and sketches.",
"Du X,Peng J,Zhou Y,Zhang J,Chen S,Jiang G,Sun X,Ji R",,,PixelFace+: Towards Controllable Face Generation and Manipulation with Text Descriptions and Segmentation Masks,,,10.1145/3581783.3612067 , Conference Paper,2023.0,"ABSTRACT
Synthesizing vivid human portraits is a research hot spot in image generation with a wide scope of applications. In addition to fidelity, generation controllability is another key factor that has long plagued its development. To address this issue, existing solutions usually adopt either textual or visual conditions for the target face synthesis, e.g., descriptions or segmentation masks, which still cannot fully control the generation due to the intrinsic shortages of each condition. In this paper, we propose to make use of both types of prior information to facilitate controllable face generation. In particular, we hope to produce coarse-grained information about faces based on the segmentation masks, such as face shapes and poses, and the text description is used to render detailed face attributes, e.g., face color, makeup and gender. More importantly, we hope that the generation can be easily controlled via interactively editing both types of information, making face generation more applicable to real-world applications. To accomplish this target, we propose a novel face generation model termed PixelFace+. In PixelFace+, both the text and mask are encoded as pixel-wise priors, based on which the pixel synthesis process is conducted to produce the expected portraits. Meanwhile, the loss objectives are also carefully designed to make sure that the generated faces are semantically aligned with both text and mask inputs. To validate the proposed PixelFace+, we conducted a comprehensive set of experiments on the widely recognized benchmark called MMCelebA. We not only quantitatively compare PixelFace+ with a bunch of newly proposed Text-to-Face(T2F) generation methods, but also give plenty of qualitative analyses. The experimental results demonstrate that PixelFace+ not only outperforms existing generation methods in both image quality and conditional matching but also shows a much superior controllability of face generation. More importantly, PixelFace+ presents a convenient and interactive way of face generation and manipulation via editing the text and mask inputs. Our SOURCE CODE and DEMO are given in our supplementary materials.

                    References
                Mahmoud Afifi, Marcus A Brubaker, and Michael S Brown. 2021. Histogan: Controlling colors of gan-generated and real images via color histograms. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 7941--7950.Google ScholarCross RefIvan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, and Denis Korzhenkov. 2021. Image generators with conditionally-independent pixel synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14278--14287.Google ScholarCross RefMartin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein generative adversarial networks. In International conference on machine learning. PMLR, 214--223.Google ScholarAndrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large scale GAN training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096 (2018).Google ScholarAnpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su, and Jingyi Yu. 2022. Sofgan: A portrait image generator with dynamic styling. ACM Transactions on Graphics (TOG), Vol. 41, 1 (2022), 1--26.Google ScholarDigital LibraryQifeng Chen and Vladlen Koltun. 2017. Photographic image synthesis with cascaded refinement networks. In Proceedings of the IEEE international conference on computer vision. 1511--1520.Google ScholarCross RefShu-Yu Chen, Wanchao Su, Lin Gao, Shihong Xia, and Hongbo Fu. 2020. DeepFaceDrawing: Deep generation of face images from sketches. ACM Transactions on Graphics (TOG), Vol. 39, 4 (2020), 72--1.Google ScholarDigital LibraryXiang Chen, Lingbo Qing, Xiaohai He, Xiaodong Luo, and Yining Xu. 2019. FTGAN: A fully-trained generative adversarial networks for text to face generation. arXiv preprint arXiv:1904.05729 (2019).Google ScholarAntonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. 2018. Generative adversarial networks: An overview. IEEE signal processing magazine, Vol. 35, 1 (2018), 53--65.Google ScholarHelisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab, Gregory D Hager, Federico Tombari, and Christian Rupprecht. 2020. Semantic image manipulation using scene graphs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 5213--5222.Google ScholarCross RefPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, Vol. 34 (2021), 8780--8794.Google ScholarHao Dong, Simiao Yu, Chao Wu, and Yike Guo. 2017. Semantic image synthesis via adversarial learning. In Proceedings of the IEEE international conference on computer vision. 5706--5714.Google ScholarCross RefIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial networks. Commun. ACM, Vol. 63, 11 (2020), 139--144.Google ScholarDigital LibraryJing He, Yiyi Zhou, Qi Zhang, Jun Peng, Yunhang Shen, Xiaoshuai Sun, Chao Chen, and Rongrong Ji. 2022. PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. In Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XIV. Springer, 643--660.Google ScholarZhenliang He, Meina Kan, and Shiguang Shan. 2021. Eigengan: Layer-wise eigen-learning for gans. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 14408--14417.Google ScholarCross RefJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, Vol. 33 (2020), 6840--6851.Google ScholarXianxu Hou, Xiaokang Zhang, Yudong Li, and Linlin Shen. 2022. TextFace: Text-to-Style Mapping based Face Generation and Manipulation. IEEE Transactions on Multimedia (2022).Google ScholarJie Hu, Liujuan Cao, Yao Lu, ShengChuan Zhang, Yan Wang, Ke Li, Feiyue Huang, Ling Shao, and Rongrong Ji. 2021. Istr: End-to-end instance segmentation with transformers. arXiv preprint arXiv:2105.00637 (2021).Google ScholarJie Hu, Linyan Huang, Tianhe Ren, Shengchuan Zhang, Rongrong Ji, and Liujuan Cao. 2023. You Only Segment Once: Towards Real-Time Panoptic Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 17819--17829.Google ScholarCross RefZiqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. 2023. Collaborative Diffusion for Multi-Modal Face Generation and Editing. arXiv preprint arXiv:2304.10530 (2023).Google ScholarPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1125--1134.Google ScholarCross RefYoungjoo Jo and Jongyoul Park. 2019. Sc-fegan: Face editing generative adversarial network with user's sketch and color. In Proceedings of the IEEE/CVF international conference on computer vision. 1745--1753.Google ScholarCross RefJustin Johnson, Agrim Gupta, and Li Fei-Fei. 2018. Image generation from scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1219--1228.Google ScholarCross RefTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196 (2017).Google ScholarTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 4401--4410.Google ScholarCross RefTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8110--8119.Google ScholarCross RefAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2017. Imagenet classification with deep convolutional neural networks. Commun. ACM, Vol. 60, 6 (2017), 84--90.Google ScholarDigital LibraryCheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. 2020. Maskgan: Towards diverse and interactive facial image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5549--5558.Google ScholarCross RefBowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip HS Torr. 2020. Manigan: Text-guided image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7880--7889.Google ScholarCross RefChenxing Li, Yiping Duan, Qiyuan Du, Chengkang Pan, Guangyi Liu, and Xiaoming Tao. 2022. Image Generation from Scene Graph with Object Edges. In 2022 IEEE 96th Vehicular Technology Conference (VTC2022-Fall). IEEE, 1--7.Google ScholarCross RefYikang Li, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, and Xiaogang Wang. 2019. Pastegan: A semi-parametric method to generate image from scene graph. Advances in Neural Information Processing Systems, Vol. 32 (2019).Google ScholarJie Liang, Hui Zeng, and Lei Zhang. 2021. High-resolution photorealistic image translation in real-time: A laplacian pyramid translation network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9392--9400.Google ScholarCross RefWentong Liao, Kai Hu, Michael Ying Yang, and Bodo Rosenhahn. 2022. Text to image generation with semantic-spatial aware gan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18187--18196.Google ScholarCross RefChieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, and Hwann-Tzong Chen. 2019. Coco-gan: Generation by parts via conditional coordinating. In Proceedings of the IEEE/CVF international conference on computer vision. 4512--4521.Google ScholarCross RefJi Lin, Richard Zhang, Frieder Ganz, Song Han, and Jun-Yan Zhu. 2021. Anycost gans for interactive image synthesis and editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14986--14996.Google ScholarCross RefRui Liu, Yixiao Ge, Ching Lam Choi, Xiaogang Wang, and Hongsheng Li. 2021. Divco: Diverse conditional image synthesis via contrastive generative adversarial network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16377--16386.Google ScholarCross RefRosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski. 2018. An intriguing failing of convolutional neural networks and the coordconv solution. Advances in neural information processing systems, Vol. 31 (2018).Google ScholarXihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, et al. 2019. Learning to predict layout-to-image conditional convolutions for semantic image synthesis. Advances in Neural Information Processing Systems, Vol. 32 (2019).Google ScholarYahui Liu, Marco De Nadai, Deng Cai, Huayang Li, Xavier Alameda-Pineda, Nicu Sebe, and Bruno Lepri. 2020. Describe what to change: A text-guided unsupervised image-to-image translation approach. In Proceedings of the 28th ACM International Conference on Multimedia. 1357--1365.Google ScholarDigital LibraryGen Luo, Yiyi Zhou, Rongrong Ji, Xiaoshuai Sun, Jinsong Su, Chia-Wen Lin, and Qi Tian. 2020a. Cascade grouped attention network for referring expression segmentation. In Proceedings of the 28th ACM International Conference on Multimedia. 1274--1282.Google ScholarDigital LibraryGen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. 2020b. Multi-task collaborative network for joint referring expression comprehension and segmentation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. 10034--10043.Google ScholarCross RefGen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan Cao, Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2022. Towards lightweight transformer via group-wise transformation for vision-and-language tasks. IEEE Transactions on Image Processing, Vol. 31 (2022), 3386--3398.Google ScholarDigital LibraryMehdi Mirza and Simon Osindero. 2014. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784 (2014).Google ScholarGaurav Mittal, Shubham Agrawal, Anuva Agarwal, Sushant Mehta, and Tanya Marwah. 2019. Interactive image generation using scene graphs. arXiv preprint arXiv:1905.03743 (2019).Google ScholarYongqiang Mou, Lei Tan, Hui Yang, Jingying Chen, Leyuan Liu, Rui Yan, and Yaohong Huang. 2020. Plugnet: Degradation aware scene text recognition supervised by a pluggable super-resolution unit. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV 16. Springer, 158--174.Google ScholarSeonghyeon Nam, Yunji Kim, and Seon Joo Kim. 2018. Text-adaptive generative adversarial networks: manipulating images with natural language. Advances in neural information processing systems, Vol. 31 (2018).Google ScholarOsaid Rehman Nasir, Shailesh Kumar Jha, Manraj Singh Grover, Yi Yu, Ajit Kumar, and Rajiv Ratn Shah. 2019. Text2facegan: Face generation from fine grained textual descriptions. In 2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM). IEEE, 58--67.Google ScholarCross RefTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2337--2346.Google ScholarCross RefOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021. Styleclip: Text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2085--2094.Google ScholarCross RefJun Peng, Xiaoxiong Du, Yiyi Zhou, Jing He, Yunhang Shen, Xiaoshuai Sun, and Rongrong Ji. 2022a. Learning Dynamic Prior Knowledge for Text-to-Face Pixel Synthesis. In Proceedings of the 30th ACM International Conference on Multimedia. 5132--5141.Google ScholarDigital LibraryJun Peng, Han Pan, Yiyi Zhou, Jing He, Xiaoshuai Sun, Yan Wang, Yongjian Wu, and Rongrong Ji. 2022b. Towards Open-Ended Text-to-Face Generation, Combination and Manipulation. In Proceedings of the 30th ACM International Conference on Multimedia. 5045--5054.Google ScholarDigital LibraryJun Peng, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2021. Knowledge-driven generative adversarial network for text-to-image synthesis. IEEE Transactions on Multimedia, Vol. 24 (2021), 4356--4366.Google ScholarCross RefXiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun. 2018. Semi-parametric image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 8808--8816.Google ScholarCross RefAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748--8763.Google ScholarAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International Conference on Machine Learning. PMLR, 8821--8831.Google ScholarScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image synthesis. In International conference on machine learning. PMLR, 1060--1069.Google ScholarElad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. 2021. Encoding in style: a stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2287--2296.Google ScholarCross RefRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684--10695.Google ScholarCross RefTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. 2016. Improved techniques for training gans. Advances in neural information processing systems, Vol. 29 (2016).Google ScholarIvan Skorokhodov, Savva Ignatyev, and Mohamed Elhoseiny. 2021. Adversarial generation of continuous images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10753--10764.Google ScholarCross RefJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning. PMLR, 2256--2265.Google ScholarJiaming Song, Chenlin Meng, and Stefano Ermon. 2020a. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020).Google ScholarYang Song and Stefano Ermon. 2020. Improved techniques for training score-based generative models. Advances in neural information processing systems, Vol. 33 (2020), 12438--12448.Google ScholarYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020b. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 (2020).Google ScholarDavid Stap, Maurits Bleeker, Sarah Ibrahimi, and Maartje Ter Hoeve. 2020. Conditional image generation and manipulation for user-specified content. arXiv preprint arXiv:2005.04909 (2020).Google ScholarJianxin Sun, Qi Li, Weining Wang, Jian Zhao, and Zhenan Sun. 2021. Multi-caption text-to-face synthesis: Dataset and algorithm. In Proceedings of the 29th ACM International Conference on Multimedia. 2290--2298.Google ScholarDigital LibraryVadim Sushko, Edgar Schönfeld, Dan Zhang, Juergen Gall, Bernt Schiele, and Anna Khoreva. 2020. You only need adversarial supervision for semantic image synthesis. arXiv preprint arXiv:2012.04781 (2020).Google ScholarHao Tang, Song Bai, Li Zhang, Philip HS Torr, and Nicu Sebe. 2020. Xinggan for person image generation. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXV 16. Springer, 717--734.Google ScholarMing Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiaoyuan Jing, Fei Wu, and
 Bingkun Bao. 2020a. Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis. arXiv preprint arXiv:2008.05865 (2020).Google ScholarMing Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. 2020b. Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis. arXiv preprint arXiv:2008.05865 (2020).Google ScholarTianren Wang, Teng Zhang, and Brian Lovell. 2021b. Faces a la carte: Text-to-face generation via attribute disentanglement. In Proceedings of the IEEE/CVF winter conference on applications of computer vision. 3380--3388.Google ScholarCross RefTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. 2018. High-resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition. 8798--8807.Google ScholarCross RefYi Wang, Lu Qi, Ying-Cong Chen, Xiangyu Zhang, and Jiaya Jia. 2021a. Image synthesis via semantic composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 13749--13758.Google ScholarCross RefWeihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. 2021. Tedigan: Text-guided diverse face image generation and manipulation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2256--2265.Google ScholarCross RefTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1316--1324.Google ScholarCross RefHan Xue, Zhiwu Huang, Qianru Sun, Li Song, and Wenjun Zhang. 2023. Freestyle Layout-to-Image Synthesis. arXiv preprint arXiv:2303.14412 (2023).Google ScholarHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international conference on computer vision. 5907--5915.Google ScholarCross RefRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586--595.Google ScholarCross RefYiyi Zhou, Rongrong Ji, Gen Luo, Xiaoshuai Sun, Jinsong Su, Xinghao Ding, Chia-Wen Lin, and Qi Tian. 2021a. A real-time global inference network for one-stage referring expression comprehension. IEEE Transactions on Neural Networks and Learning Systems (2021).Google ScholarCross RefYiyi Zhou, Rongrong Ji, Xiaoshuai Sun, Jinsong Su, Deyu Meng, Yue Gao, and Chunhua Shen. 2019. Plenty is plague: Fine-grained learning for visual question answering. IEEE transactions on pattern analysis and machine intelligence, Vol. 44, 2 (2019), 697--709.Google ScholarYiyi Zhou, Tianhe Ren, Chaoyang Zhu, Xiaoshuai Sun, Jianzhuang Liu, Xinghao Ding, Mingliang Xu, and Rongrong Ji. 2021b. Trar: Routing the attention spans in transformer for visual question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2074--2084.Google ScholarCross RefMinfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. 2019. Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 5802--5810.Google ScholarCross RefPeihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. 2020a. Sean: Image synthesis with semantic region-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5104--5113.Google ScholarCross RefZhen Zhu, Zhiliang Xu, Ansheng You, and Xiang Bai. 2020b. Semantically multi-modal image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 5467--5476.Google ScholarCross Ref


Cited ByView all







Index Terms

PixelFace+: Towards Controllable Face Generation and Manipulation with Text Descriptions and Segmentation MasksComputing methodologiesArtificial intelligenceComputer visionComputer vision representationsImage representationsComputer graphicsImage manipulationImage processingInformation systemsInformation systems applicationsMultimedia information systemsMultimedia content creation

 Recommendations 
Facial Expression Recognition with the advent of face masksMUM '20: Proceedings of the 19th International Conference on Mobile and Ubiquitous Multimedia  
 With the worldwide spread of COVID-19, wearing face masks while interaction in public is becoming a common behavior to protect against infection. Thus, how to improve effectiveness of existing facial expression recognition (FER) technology on masked ...Read MoreTowards Age-Invariant Face RecognitionDespite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As ...Read MoreRange Face Segmentation: Face Detection and Segmentation for Authentication in Mobile Device Range ImagesMoMM '13: Proceedings of International Conference on Advances in Mobile Computing & Multimedia  
Face detection (finding faces of different perspectives in images) is an important task as prerequisite to face recognition. This is especially difficult in the mobile domain, as bad image quality and illumination conditions lead to overall reduced face ...Read More





 Comments 
Please enable JavaScript to view thecomments powered by Disqus.",,,,,ACM ,"Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 2023- 3 November 2023  ",,"Title:PixelFace+: Towards Controllable Face Generation and Manipulation with Text Descriptions and Segmentation Masks

 ABSTRACT
Synthesizing vivid human portraits is a research hot spot in image generation with a wide scope of applications. In addition to fidelity, generation controllability is another key factor that has long plagued its development. To address this issue, existing solutions usually adopt either textual or visual conditions for the target face synthesis, e.g., descriptions or segmentation masks, which still cannot fully control the generation due to the intrinsic shortages of each condition. In this paper, we propose to make use of both types of prior information to facilitate controllable face generation. In particular, we hope to produce coarse-grained information about faces based on the segmentation masks, such as face shapes and poses, and the text description is used to render detailed face attributes, e.g., face color, makeup and gender. More importantly, we hope that the generation can be easily controlled via interactively editing both types of information, making face generation more applicable to real-world applications. To accomplish this target, we propose a novel face generation model termed PixelFace+. In PixelFace+, both the text and mask are encoded as pixel-wise priors, based on which the pixel synthesis process is conducted to produce the expected portraits. Meanwhile, the loss objectives are also carefully designed to make sure that the generated faces are semantically aligned with both text and mask inputs. To validate the proposed PixelFace+, we conducted a comprehensive set of experiments on the widely recognized benchmark called MMCelebA. We not only quantitatively compare PixelFace+ with a bunch of newly proposed Text-to-Face(T2F) generation methods, but also give plenty of qualitative analyses. The experimental results demonstrate that PixelFace+ not only outperforms existing generation methods in both image quality and conditional matching but also shows a much superior controllability of face generation. More importantly, PixelFace+ presents a convenient and interactive way of face generation and manipulation via editing the text and mask inputs. Our SOURCE CODE and DEMO are given in our supplementary materials.

                    References
                Mahmoud Afifi, Marcus A Brubaker, and Michael S Brown. 2021. Histogan: Controlling colors of gan-generated and real images via color histograms. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 7941--7950.Google ScholarCross RefIvan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, and Denis Korzhenkov. 2021. Image generators with conditionally-independent pixel synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14278--14287.Google ScholarCross RefMartin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein generative adversarial networks. In International conference on machine learning. PMLR, 214--223.Google ScholarAndrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large scale GAN training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096 (2018).Google ScholarAnpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su, and Jingyi Yu. 2022. Sofgan: A portrait image generator with dynamic styling. ACM Transactions on Graphics (TOG), Vol. 41, 1 (2022), 1--26.Google ScholarDigital LibraryQifeng Chen and Vladlen Koltun. 2017. Photographic image synthesis with cascaded refinement networks. In Proceedings of the IEEE international conference on computer vision. 1511--1520.Google ScholarCross RefShu-Yu Chen, Wanchao Su, Lin Gao, Shihong Xia, and Hongbo Fu. 2020. DeepFaceDrawing: Deep generation of face images from sketches. ACM Transactions on Graphics (TOG), Vol. 39, 4 (2020), 72--1.Google ScholarDigital LibraryXiang Chen, Lingbo Qing, Xiaohai He, Xiaodong Luo, and Yining Xu. 2019. FTGAN: A fully-trained generative adversarial networks for text to face generation. arXiv preprint arXiv:1904.05729 (2019).Google ScholarAntonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. 2018. Generative adversarial networks: An overview. IEEE signal processing magazine, Vol. 35, 1 (2018), 53--65.Google ScholarHelisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab, Gregory D Hager, Federico Tombari, and Christian Rupprecht. 2020. Semantic image manipulation using scene graphs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 5213--5222.Google ScholarCross RefPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, Vol. 34 (2021), 8780--8794.Google ScholarHao Dong, Simiao Yu, Chao Wu, and Yike Guo. 2017. Semantic image synthesis via adversarial learning. In Proceedings of the IEEE international conference on computer vision. 5706--5714.Google ScholarCross RefIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial networks. Commun. ACM, Vol. 63, 11 (2020), 139--144.Google ScholarDigital LibraryJing He, Yiyi Zhou, Qi Zhang, Jun Peng, Yunhang Shen, Xiaoshuai Sun, Chao Chen, and Rongrong Ji. 2022. PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. In Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XIV. Springer, 643--660.Google ScholarZhenliang He, Meina Kan, and Shiguang Shan. 2021. Eigengan: Layer-wise eigen-learning for gans. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 14408--14417.Google ScholarCross RefJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, Vol. 33 (2020), 6840--6851.Google ScholarXianxu Hou, Xiaokang Zhang, Yudong Li, and Linlin Shen. 2022. TextFace: Text-to-Style Mapping based Face Generation and Manipulation. IEEE Transactions on Multimedia (2022).Google ScholarJie Hu, Liujuan Cao, Yao Lu, ShengChuan Zhang, Yan Wang, Ke Li, Feiyue Huang, Ling Shao, and Rongrong Ji. 2021. Istr: End-to-end instance segmentation with transformers. arXiv preprint arXiv:2105.00637 (2021).Google ScholarJie Hu, Linyan Huang, Tianhe Ren, Shengchuan Zhang, Rongrong Ji, and Liujuan Cao. 2023. You Only Segment Once: Towards Real-Time Panoptic Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 17819--17829.Google ScholarCross RefZiqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. 2023. Collaborative Diffusion for Multi-Modal Face Generation and Editing. arXiv preprint arXiv:2304.10530 (2023).Google ScholarPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1125--1134.Google ScholarCross RefYoungjoo Jo and Jongyoul Park. 2019. Sc-fegan: Face editing generative adversarial network with user's sketch and color. In Proceedings of the IEEE/CVF international conference on computer vision. 1745--1753.Google ScholarCross RefJustin Johnson, Agrim Gupta, and Li Fei-Fei. 2018. Image generation from scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1219--1228.Google ScholarCross RefTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196 (2017).Google ScholarTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 4401--4410.Google ScholarCross RefTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8110--8119.Google ScholarCross RefAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2017. Imagenet classification with deep convolutional neural networks. Commun. ACM, Vol. 60, 6 (2017), 84--90.Google ScholarDigital LibraryCheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. 2020. Maskgan: Towards diverse and interactive facial image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5549--5558.Google ScholarCross RefBowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip HS Torr. 2020. Manigan: Text-guided image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7880--7889.Google ScholarCross RefChenxing Li, Yiping Duan, Qiyuan Du, Chengkang Pan, Guangyi Liu, and Xiaoming Tao. 2022. Image Generation from Scene Graph with Object Edges. In 2022 IEEE 96th Vehicular Technology Conference (VTC2022-Fall). IEEE, 1--7.Google ScholarCross RefYikang Li, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, and Xiaogang Wang. 2019. Pastegan: A semi-parametric method to generate image from scene graph. Advances in Neural Information Processing Systems, Vol. 32 (2019).Google ScholarJie Liang, Hui Zeng, and Lei Zhang. 2021. High-resolution photorealistic image translation in real-time: A laplacian pyramid translation network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9392--9400.Google ScholarCross RefWentong Liao, Kai Hu, Michael Ying Yang, and Bodo Rosenhahn. 2022. Text to image generation with semantic-spatial aware gan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18187--18196.Google ScholarCross RefChieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, and Hwann-Tzong Chen. 2019. Coco-gan: Generation by parts via conditional coordinating. In Proceedings of the IEEE/CVF international conference on computer vision. 4512--4521.Google ScholarCross RefJi Lin, Richard Zhang, Frieder Ganz, Song Han, and Jun-Yan Zhu. 2021. Anycost gans for interactive image synthesis and editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14986--14996.Google ScholarCross RefRui Liu, Yixiao Ge, Ching Lam Choi, Xiaogang Wang, and Hongsheng Li. 2021. Divco: Diverse conditional image synthesis via contrastive generative adversarial network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16377--16386.Google ScholarCross RefRosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski. 2018. An intriguing failing of convolutional neural networks and the coordconv solution. Advances in neural information processing systems, Vol. 31 (2018).Google ScholarXihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, et al. 2019. Learning to predict layout-to-image conditional convolutions for semantic image synthesis. Advances in Neural Information Processing Systems, Vol. 32 (2019).Google ScholarYahui Liu, Marco De Nadai, Deng Cai, Huayang Li, Xavier Alameda-Pineda, Nicu Sebe, and Bruno Lepri. 2020. Describe what to change: A text-guided unsupervised image-to-image translation approach. In Proceedings of the 28th ACM International Conference on Multimedia. 1357--1365.Google ScholarDigital LibraryGen Luo, Yiyi Zhou, Rongrong Ji, Xiaoshuai Sun, Jinsong Su, Chia-Wen Lin, and Qi Tian. 2020a. Cascade grouped attention network for referring expression segmentation. In Proceedings of the 28th ACM International Conference on Multimedia. 1274--1282.Google ScholarDigital LibraryGen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. 2020b. Multi-task collaborative network for joint referring expression comprehension and segmentation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. 10034--10043.Google ScholarCross RefGen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan Cao, Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2022. Towards lightweight transformer via group-wise transformation for vision-and-language tasks. IEEE Transactions on Image Processing, Vol. 31 (2022), 3386--3398.Google ScholarDigital LibraryMehdi Mirza and Simon Osindero. 2014. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784 (2014).Google ScholarGaurav Mittal, Shubham Agrawal, Anuva Agarwal, Sushant Mehta, and Tanya Marwah. 2019. Interactive image generation using scene graphs. arXiv preprint arXiv:1905.03743 (2019).Google ScholarYongqiang Mou, Lei Tan, Hui Yang, Jingying Chen, Leyuan Liu, Rui Yan, and Yaohong Huang. 2020. Plugnet: Degradation aware scene text recognition supervised by a pluggable super-resolution unit. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV 16. Springer, 158--174.Google ScholarSeonghyeon Nam, Yunji Kim, and Seon Joo Kim. 2018. Text-adaptive generative adversarial networks: manipulating images with natural language. Advances in neural information processing systems, Vol. 31 (2018).Google ScholarOsaid Rehman Nasir, Shailesh Kumar Jha, Manraj Singh Grover, Yi Yu, Ajit Kumar, and Rajiv Ratn Shah. 2019. Text2facegan: Face generation from fine grained textual descriptions. In 2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM). IEEE, 58--67.Google ScholarCross RefTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2337--2346.Google ScholarCross RefOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021. Styleclip: Text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2085--2094.Google ScholarCross RefJun Peng, Xiaoxiong Du, Yiyi Zhou, Jing He, Yunhang Shen, Xiaoshuai Sun, and Rongrong Ji. 2022a. Learning Dynamic Prior Knowledge for Text-to-Face Pixel Synthesis. In Proceedings of the 30th ACM International Conference on Multimedia. 5132--5141.Google ScholarDigital LibraryJun Peng, Han Pan, Yiyi Zhou, Jing He, Xiaoshuai Sun, Yan Wang, Yongjian Wu, and Rongrong Ji. 2022b. Towards Open-Ended Text-to-Face Generation, Combination and Manipulation. In Proceedings of the 30th ACM International Conference on Multimedia. 5045--5054.Google ScholarDigital LibraryJun Peng, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2021. Knowledge-driven generative adversarial network for text-to-image synthesis. IEEE Transactions on Multimedia, Vol. 24 (2021), 4356--4366.Google ScholarCross RefXiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun. 2018. Semi-parametric image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 8808--8816.Google ScholarCross RefAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748--8763.Google ScholarAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International Conference on Machine Learning. PMLR, 8821--8831.Google ScholarScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image synthesis. In International conference on machine learning. PMLR, 1060--1069.Google ScholarElad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. 2021. Encoding in style: a stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2287--2296.Google ScholarCross RefRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684--10695.Google ScholarCross RefTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. 2016. Improved techniques for training gans. Advances in neural information processing systems, Vol. 29 (2016).Google ScholarIvan Skorokhodov, Savva Ignatyev, and Mohamed Elhoseiny. 2021. Adversarial generation of continuous images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10753--10764.Google ScholarCross RefJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning. PMLR, 2256--2265.Google ScholarJiaming Song, Chenlin Meng, and Stefano Ermon. 2020a. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020).Google ScholarYang Song and Stefano Ermon. 2020. Improved techniques for training score-based generative models. Advances in neural information processing systems, Vol. 33 (2020), 12438--12448.Google ScholarYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020b. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 (2020).Google ScholarDavid Stap, Maurits Bleeker, Sarah Ibrahimi, and Maartje Ter Hoeve. 2020. Conditional image generation and manipulation for user-specified content. arXiv preprint arXiv:2005.04909 (2020).Google ScholarJianxin Sun, Qi Li, Weining Wang, Jian Zhao, and Zhenan Sun. 2021. Multi-caption text-to-face synthesis: Dataset and algorithm. In Proceedings of the 29th ACM International Conference on Multimedia. 2290--2298.Google ScholarDigital LibraryVadim Sushko, Edgar Schönfeld, Dan Zhang, Juergen Gall, Bernt Schiele, and Anna Khoreva. 2020. You only need adversarial supervision for semantic image synthesis. arXiv preprint arXiv:2012.04781 (2020).Google ScholarHao Tang, Song Bai, Li Zhang, Philip HS Torr, and Nicu Sebe. 2020. Xinggan for person image generation. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXV 16. Springer, 717--734.Google ScholarMing Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiaoyuan Jing, Fei Wu, and
 Bingkun Bao. 2020a. Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis. arXiv preprint arXiv:2008.05865 (2020).Google ScholarMing Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. 2020b. Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis. arXiv preprint arXiv:2008.05865 (2020).Google ScholarTianren Wang, Teng Zhang, and Brian Lovell. 2021b. Faces a la carte: Text-to-face generation via attribute disentanglement. In Proceedings of the IEEE/CVF winter conference on applications of computer vision. 3380--3388.Google ScholarCross RefTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. 2018. High-resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition. 8798--8807.Google ScholarCross RefYi Wang, Lu Qi, Ying-Cong Chen, Xiangyu Zhang, and Jiaya Jia. 2021a. Image synthesis via semantic composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 13749--13758.Google ScholarCross RefWeihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. 2021. Tedigan: Text-guided diverse face image generation and manipulation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2256--2265.Google ScholarCross RefTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1316--1324.Google ScholarCross RefHan Xue, Zhiwu Huang, Qianru Sun, Li Song, and Wenjun Zhang. 2023. Freestyle Layout-to-Image Synthesis. arXiv preprint arXiv:2303.14412 (2023).Google ScholarHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international conference on computer vision. 5907--5915.Google ScholarCross RefRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586--595.Google ScholarCross RefYiyi Zhou, Rongrong Ji, Gen Luo, Xiaoshuai Sun, Jinsong Su, Xinghao Ding, Chia-Wen Lin, and Qi Tian. 2021a. A real-time global inference network for one-stage referring expression comprehension. IEEE Transactions on Neural Networks and Learning Systems (2021).Google ScholarCross RefYiyi Zhou, Rongrong Ji, Xiaoshuai Sun, Jinsong Su, Deyu Meng, Yue Gao, and Chunhua Shen. 2019. Plenty is plague: Fine-grained learning for visual question answering. IEEE transactions on pattern analysis and machine intelligence, Vol. 44, 2 (2019), 697--709.Google ScholarYiyi Zhou, Tianhe Ren, Chaoyang Zhu, Xiaoshuai Sun, Jianzhuang Liu, Xinghao Ding, Mingliang Xu, and Rongrong Ji. 2021b. Trar: Routing the attention spans in transformer for visual question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2074--2084.Google ScholarCross RefMinfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. 2019. Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 5802--5810.Google ScholarCross RefPeihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. 2020a. Sean: Image synthesis with semantic region-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5104--5113.Google ScholarCross RefZhen Zhu, Zhiliang Xu, Ansheng You, and Xiang Bai. 2020b. Semantically multi-modal image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 5467--5476.Google ScholarCross Ref


Cited ByView all







Index Terms

PixelFace+: Towards Controllable Face Generation and Manipulation with Text Descriptions and Segmentation MasksComputing methodologiesArtificial intelligenceComputer visionComputer vision representationsImage representationsComputer graphicsImage manipulationImage processingInformation systemsInformation systems applicationsMultimedia information systemsMultimedia content creation

 Recommendations 
Facial Expression Recognition with the advent of face masksMUM '20: Proceedings of the 19th International Conference on Mobile and Ubiquitous Multimedia  
 With the worldwide spread of COVID-19, wearing face masks while interaction in public is becoming a common behavior to protect against infection. Thus, how to improve effectiveness of existing facial expression recognition (FER) technology on masked ...Read MoreTowards Age-Invariant Face RecognitionDespite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As ...Read MoreRange Face Segmentation: Face Detection and Segmentation for Authentication in Mobile Device Range ImagesMoMM '13: Proceedings of International Conference on Advances in Mobile Computing & Multimedia  
Face detection (finding faces of different perspectives in images) is an important task as prerequisite to face recognition. This is especially difficult in the mobile domain, as bad image quality and illumination conditions lead to overall reduced face ...Read More





 Comments 
Please enable JavaScript to view thecomments powered by Disqus.",
"Yan H,Zhang H,Hou J,Fan J,Zhang Z",,,InspirNET: An Unsupervised Generative Adversarial Network with Controllable Fine-grained Texture Disentanglement for Fashion Generation,,,10.1145/3581783.3612130 , Conference Paper,2023.0,"ABSTRACT
Texture constitutes the color and fabric of fashion items. Its choice in fashion items can directly express the personality and emotional state of a wearer. Despite the rapid development of intelligence-driven fashion design, it remains challenging to achieve independent control over texture without affecting other attributes, due to the highly intertwined nature of texture space. To accomplish fine-grained texture disentanglement, we propose InspirNET, an unsupervised disentangled generative adversarial framework, that manipulates textures in a fine-grained latent space so as to produce new textures effectively, aiming to broaden the range of fashion options available to common users with distinct textures as well as boosting designers' potential for fashion innovation and inspiration. Specifically, we first introduce an auto-fashion attribute encoder to map the input fashion item into texture and structure spaces. To achieve unsupervised fine-grained texture disentanglement, our model proposes a K-textures disentanglement module that decomposes the texture space into several orthogonal vectors, each of which is empowered to control an independent texture element. In particular, by employing an orthogonal eigenvector to interpolate with another, a multitude of new textures can be generated easily. Qualitative and quantitative experiments demonstrate that our InspirNET can effectively utilize decomposed orthogonal vectors to generate a wide range of fashion items with diverse textures. Our model exhibits superior performance over state-of-the-art methods in terms of maintaining the authenticity of texture transfer.

                    References
                Rameen Abdal, Yipeng Qin, and Peter Wonka. 2019. Image2stylegan: How to embed images into the stylegan latent space?. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 4432--4441.Google ScholarCross RefWen-Huang Cheng, Sijie Song, Chieh-Yun Chen, Shintami Chusnul Hidayati, and Jiaying Liu. 2021. Fashion meets computer vision: A survey. ACM Computing Surveys (CSUR), Vol. 54, 4 (2021), 1--41.Google ScholarDigital LibraryYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. 2020. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8188--8197.Google ScholarCross RefYi Rui Cui, Qi Liu, Cheng Ying Gao, and Zhongbo Su. 2018. FashionGAN: display your fashion design using conditional generative adversarial nets. In Computer Graphics Forum, Vol. 37. Wiley Online Library, 109--119.Google ScholarYingying Deng, Fan Tang, Weiming Dong, Wen Sun, Feiyue Huang, and Changsheng Xu. 2020. Arbitrary style transfer via multi-adaptation network. In Proceedings of the 28th ACM international conference on multimedia. 2719--2727.Google ScholarDigital LibraryHaoye Dong, Xiaodan Liang, Yixuan Zhang, Xujie Zhang, Xiaohui Shen, Zhenyu Xie, Bowen Wu, and Jian Yin. 2020. Fashion editing with adversarial parsing learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8120--8128.Google ScholarCross RefLeon Gatys, Alexander S Ecker, and Matthias Bethge. 2015. Texture synthesis using convolutional neural networks. Advances in neural information processing systems, Vol. 28 (2015).Google ScholarLeon A Gatys, Alexander S Ecker, and Matthias Bethge. 2016. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2414--2423.Google ScholarCross RefLeon A Gatys, Alexander S Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. 2017. Controlling perceptual factors in neural style transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3985--3993.Google ScholarCross RefKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770--778.Google ScholarCross RefXun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. 2018. Multimodal unsupervised image-to-image translation. In Proceedings of the European conference on computer vision (ECCV). 172--189.Google ScholarDigital LibraryPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1125--1134.Google ScholarCross RefShuhui Jiang, Jun Li, and Yun Fu. 2021. Deep learning for fashion style generation. IEEE Transactions on Neural Networks and Learning Systems (2021).Google ScholarYongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, and Mingli Song. 2019. Neural style transfer: A review. IEEE transactions on visualization and computer graphics, Vol. 26, 11 (2019), 3365--3385.Google ScholarTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 4401--4410.Google ScholarCross RefTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8110--8119.Google ScholarCross RefHsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. 2018. Diverse image-to-image translation via disentangled representations. In Proceedings of the European conference on computer vision (ECCV). 35--51.Google ScholarDigital LibraryJieun Lee, Hyeonwoo Kim, Jonghwa Shim, and Eenjun Hwang. 2022. Cartoon-Flow: A Flow-Based Generative Adversarial Network for Arbitrary-Style Photo Cartoonization. In Proceedings of the 30th ACM International Conference on Multimedia. 1241--1251.Google ScholarDigital LibraryGuang-Hai Liu and Jing-Yu Yang. 2013. Content-based image retrieval using color difference histogram. Pattern recognition, Vol. 46, 1 (2013), 188--198.Google ScholarMing-Yu Liu, Thomas Breuel, and Jan Kautz. 2017. Unsupervised image-to-image translation networks. Advances in neural information processing systems, Vol. 30 (2017).Google ScholarYu Liu, Wei Chen, Li Liu, and Michael S Lew. 2019. Swapgan: A multistage generative approach for person-to-person fashion style transfer. IEEE Transactions on Multimedia, Vol. 21, 9 (2019), 2209--2222.Google ScholarCross RefYujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou. 2020. Interfacegan: Interpreting the disentangled face representation learned by gans. IEEE transactions on pattern analysis and machine intelligence (2020).Google ScholarHao Tang, Hong Liu, Dan Xu, Philip HS Torr, and Nicu Sebe. 2021. Attentiongan: Unpaired image-to-image translation using attention-guided generative adversarial networks. IEEE transactions on neural networks and learning systems (2021).Google ScholarOmer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. 2021. Designing an encoder for stylegan image manipulation. ACM Transactions on Graphics (TOG), Vol. 40, 4 (2021), 1--14.Google ScholarDigital LibraryZhizhong Wang, Zhanjie Zhang, Lei Zhao, Zhiwen Zuo, Ailin Li, Wei Xing, and Dongming Lu. 2022. AesUST: towards aesthetic-enhanced universal style transfer. In Proceedings of the 30th ACM International Conference on Multimedia. 1095--1106.Google ScholarDigital LibraryWeihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. 2022. Gan inversion: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).Google ScholarWenqi Xian, Patsorn Sangkloy, Varun Agrawal, Amit Raj, Jingwan Lu, Chen Fang, Fisher Yu, and James Hays. 2018. Texturegan: Controlling deep image synthesis with texture patches. In Proceedings of the IEEE conference on computer vision and pattern recognition. 8456--8465.Google ScholarCross RefHan Yan, Haijun Zhang, Linlin Liu, Dongliang Zhou, Xiaofei Xu, Zhao Zhang, and Shuicheng Yan. 2022. Toward intelligent design: An ai-based fashion designer using generative adversarial networks aided by sketch and rendering generators. IEEE Transactions on Multimedia (2022).Google ScholarHan Yan, Haijun Zhang, Jianyang Shi, Jianghong Ma, and Xiaofei Xu. 2023 a. Inspiration transfer for intelligent design: A generative adversarial network with fashion attributes disentanglement. IEEE Transactions on Consumer Electronics (2023).Google ScholarHan Yan, Haijun Zhang, Jianyang Shi, Jianghong Ma, and Xiaofei Xu. 2023 b. Toward intelligent fashion design: A texture and shape disentangled generative adversarial network. ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 19, 3 (2023), 1--23.Google ScholarDigital LibraryShuai Yang, Liming Jiang, Ziwei Liu, and Chen Change Loy. 2022. Pastiche master: exemplar-based high-resolution portrait style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7693--7702.Google ScholarCross RefJaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, and Jung-Woo Ha. 2019. Photorealistic style transfer via wavelet transforms. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 9036--9045.Google ScholarCross RefRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE CVPR. 586--595.Google ScholarYuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Tong-Yee Lee, and Changsheng Xu. 2022. Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning. In ACM SIGGRAPH.Google ScholarJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017a. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision. 2223--2232.Google ScholarCross RefShizhan Zhu, Raquel Urtasun, Sanja Fidler, Dahua Lin, and Chen Change Loy. 2017b. Be your own prada: Fashion synthesis with structural coherence. In Proceedings of the IEEE international conference on computer vision. 1680--1688.Google ScholarCross Ref


Cited ByView all







Index Terms

InspirNET: An Unsupervised Generative Adversarial Network with Controllable Fine-grained Texture Disentanglement for Fashion GenerationApplied computingArts and humanitiesComputing methodologiesArtificial intelligenceComputer visionComputer vision tasks

 Recommendations 
FCBoost-Net: A Generative Network for Synthesizing Multiple Collocated Outfits via Fashion Compatibility BoostingMM '23: Proceedings of the 31st ACM International Conference on Multimedia  
Outfit generation is a challenging task in the field of fashion technology, in which the aim is to create a collocated set of fashion items that complement a given set of items. Previous studies in this area have been limited to generating a unique set ...Read MoreFashionDiff: A Controllable Diffusion Model Using Pairwise Fashion Elements for Intelligent DesignMM '23: Proceedings of the 31st ACM International Conference on Multimedia  
The process of fashion design involves creative expression through various methods, including sketch drawing, brush painting, and choices of textures and colors, all of which are employed to characterize the originality and uniqueness of the designed ...Read MoreToward Intelligent Fashion Design: A Texture and Shape Disentangled Generative Adversarial NetworkTexture and shape in fashion, constituting essential elements of garments, characterize the body and surface of the fabric and outline the silhouette of clothing, respectively. The selection of texture and shape plays a critical role in the design process,...Read More





 Comments 
Please enable JavaScript to view thecomments powered by Disqus.",,,,,ACM ,"Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 2023- 3 November 2023  ",,"Title:InspirNET: An Unsupervised Generative Adversarial Network with Controllable Fine-grained Texture Disentanglement for Fashion Generation

 ABSTRACT
Texture constitutes the color and fabric of fashion items. Its choice in fashion items can directly express the personality and emotional state of a wearer. Despite the rapid development of intelligence-driven fashion design, it remains challenging to achieve independent control over texture without affecting other attributes, due to the highly intertwined nature of texture space. To accomplish fine-grained texture disentanglement, we propose InspirNET, an unsupervised disentangled generative adversarial framework, that manipulates textures in a fine-grained latent space so as to produce new textures effectively, aiming to broaden the range of fashion options available to common users with distinct textures as well as boosting designers' potential for fashion innovation and inspiration. Specifically, we first introduce an auto-fashion attribute encoder to map the input fashion item into texture and structure spaces. To achieve unsupervised fine-grained texture disentanglement, our model proposes a K-textures disentanglement module that decomposes the texture space into several orthogonal vectors, each of which is empowered to control an independent texture element. In particular, by employing an orthogonal eigenvector to interpolate with another, a multitude of new textures can be generated easily. Qualitative and quantitative experiments demonstrate that our InspirNET can effectively utilize decomposed orthogonal vectors to generate a wide range of fashion items with diverse textures. Our model exhibits superior performance over state-of-the-art methods in terms of maintaining the authenticity of texture transfer.

                    References
                Rameen Abdal, Yipeng Qin, and Peter Wonka. 2019. Image2stylegan: How to embed images into the stylegan latent space?. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 4432--4441.Google ScholarCross RefWen-Huang Cheng, Sijie Song, Chieh-Yun Chen, Shintami Chusnul Hidayati, and Jiaying Liu. 2021. Fashion meets computer vision: A survey. ACM Computing Surveys (CSUR), Vol. 54, 4 (2021), 1--41.Google ScholarDigital LibraryYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. 2020. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8188--8197.Google ScholarCross RefYi Rui Cui, Qi Liu, Cheng Ying Gao, and Zhongbo Su. 2018. FashionGAN: display your fashion design using conditional generative adversarial nets. In Computer Graphics Forum, Vol. 37. Wiley Online Library, 109--119.Google ScholarYingying Deng, Fan Tang, Weiming Dong, Wen Sun, Feiyue Huang, and Changsheng Xu. 2020. Arbitrary style transfer via multi-adaptation network. In Proceedings of the 28th ACM international conference on multimedia. 2719--2727.Google ScholarDigital LibraryHaoye Dong, Xiaodan Liang, Yixuan Zhang, Xujie Zhang, Xiaohui Shen, Zhenyu Xie, Bowen Wu, and Jian Yin. 2020. Fashion editing with adversarial parsing learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8120--8128.Google ScholarCross RefLeon Gatys, Alexander S Ecker, and Matthias Bethge. 2015. Texture synthesis using convolutional neural networks. Advances in neural information processing systems, Vol. 28 (2015).Google ScholarLeon A Gatys, Alexander S Ecker, and Matthias Bethge. 2016. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2414--2423.Google ScholarCross RefLeon A Gatys, Alexander S Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. 2017. Controlling perceptual factors in neural style transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3985--3993.Google ScholarCross RefKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770--778.Google ScholarCross RefXun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. 2018. Multimodal unsupervised image-to-image translation. In Proceedings of the European conference on computer vision (ECCV). 172--189.Google ScholarDigital LibraryPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1125--1134.Google ScholarCross RefShuhui Jiang, Jun Li, and Yun Fu. 2021. Deep learning for fashion style generation. IEEE Transactions on Neural Networks and Learning Systems (2021).Google ScholarYongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, and Mingli Song. 2019. Neural style transfer: A review. IEEE transactions on visualization and computer graphics, Vol. 26, 11 (2019), 3365--3385.Google ScholarTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 4401--4410.Google ScholarCross RefTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8110--8119.Google ScholarCross RefHsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. 2018. Diverse image-to-image translation via disentangled representations. In Proceedings of the European conference on computer vision (ECCV). 35--51.Google ScholarDigital LibraryJieun Lee, Hyeonwoo Kim, Jonghwa Shim, and Eenjun Hwang. 2022. Cartoon-Flow: A Flow-Based Generative Adversarial Network for Arbitrary-Style Photo Cartoonization. In Proceedings of the 30th ACM International Conference on Multimedia. 1241--1251.Google ScholarDigital LibraryGuang-Hai Liu and Jing-Yu Yang. 2013. Content-based image retrieval using color difference histogram. Pattern recognition, Vol. 46, 1 (2013), 188--198.Google ScholarMing-Yu Liu, Thomas Breuel, and Jan Kautz. 2017. Unsupervised image-to-image translation networks. Advances in neural information processing systems, Vol. 30 (2017).Google ScholarYu Liu, Wei Chen, Li Liu, and Michael S Lew. 2019. Swapgan: A multistage generative approach for person-to-person fashion style transfer. IEEE Transactions on Multimedia, Vol. 21, 9 (2019), 2209--2222.Google ScholarCross RefYujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou. 2020. Interfacegan: Interpreting the disentangled face representation learned by gans. IEEE transactions on pattern analysis and machine intelligence (2020).Google ScholarHao Tang, Hong Liu, Dan Xu, Philip HS Torr, and Nicu Sebe. 2021. Attentiongan: Unpaired image-to-image translation using attention-guided generative adversarial networks. IEEE transactions on neural networks and learning systems (2021).Google ScholarOmer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. 2021. Designing an encoder for stylegan image manipulation. ACM Transactions on Graphics (TOG), Vol. 40, 4 (2021), 1--14.Google ScholarDigital LibraryZhizhong Wang, Zhanjie Zhang, Lei Zhao, Zhiwen Zuo, Ailin Li, Wei Xing, and Dongming Lu. 2022. AesUST: towards aesthetic-enhanced universal style transfer. In Proceedings of the 30th ACM International Conference on Multimedia. 1095--1106.Google ScholarDigital LibraryWeihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. 2022. Gan inversion: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).Google ScholarWenqi Xian, Patsorn Sangkloy, Varun Agrawal, Amit Raj, Jingwan Lu, Chen Fang, Fisher Yu, and James Hays. 2018. Texturegan: Controlling deep image synthesis with texture patches. In Proceedings of the IEEE conference on computer vision and pattern recognition. 8456--8465.Google ScholarCross RefHan Yan, Haijun Zhang, Linlin Liu, Dongliang Zhou, Xiaofei Xu, Zhao Zhang, and Shuicheng Yan. 2022. Toward intelligent design: An ai-based fashion designer using generative adversarial networks aided by sketch and rendering generators. IEEE Transactions on Multimedia (2022).Google ScholarHan Yan, Haijun Zhang, Jianyang Shi, Jianghong Ma, and Xiaofei Xu. 2023 a. Inspiration transfer for intelligent design: A generative adversarial network with fashion attributes disentanglement. IEEE Transactions on Consumer Electronics (2023).Google ScholarHan Yan, Haijun Zhang, Jianyang Shi, Jianghong Ma, and Xiaofei Xu. 2023 b. Toward intelligent fashion design: A texture and shape disentangled generative adversarial network. ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 19, 3 (2023), 1--23.Google ScholarDigital LibraryShuai Yang, Liming Jiang, Ziwei Liu, and Chen Change Loy. 2022. Pastiche master: exemplar-based high-resolution portrait style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7693--7702.Google ScholarCross RefJaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, and Jung-Woo Ha. 2019. Photorealistic style transfer via wavelet transforms. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 9036--9045.Google ScholarCross RefRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE CVPR. 586--595.Google ScholarYuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Tong-Yee Lee, and Changsheng Xu. 2022. Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning. In ACM SIGGRAPH.Google ScholarJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017a. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision. 2223--2232.Google ScholarCross RefShizhan Zhu, Raquel Urtasun, Sanja Fidler, Dahua Lin, and Chen Change Loy. 2017b. Be your own prada: Fashion synthesis with structural coherence. In Proceedings of the IEEE international conference on computer vision. 1680--1688.Google ScholarCross Ref


Cited ByView all







Index Terms

InspirNET: An Unsupervised Generative Adversarial Network with Controllable Fine-grained Texture Disentanglement for Fashion GenerationApplied computingArts and humanitiesComputing methodologiesArtificial intelligenceComputer visionComputer vision tasks

 Recommendations 
FCBoost-Net: A Generative Network for Synthesizing Multiple Collocated Outfits via Fashion Compatibility BoostingMM '23: Proceedings of the 31st ACM International Conference on Multimedia  
Outfit generation is a challenging task in the field of fashion technology, in which the aim is to create a collocated set of fashion items that complement a given set of items. Previous studies in this area have been limited to generating a unique set ...Read MoreFashionDiff: A Controllable Diffusion Model Using Pairwise Fashion Elements for Intelligent DesignMM '23: Proceedings of the 31st ACM International Conference on Multimedia  
The process of fashion design involves creative expression through various methods, including sketch drawing, brush painting, and choices of textures and colors, all of which are employed to characterize the originality and uniqueness of the designed ...Read MoreToward Intelligent Fashion Design: A Texture and Shape Disentangled Generative Adversarial NetworkTexture and shape in fashion, constituting essential elements of garments, characterize the body and surface of the fabric and outline the silhouette of clothing, respectively. The selection of texture and shape plays a critical role in the design process,...Read More





 Comments 
Please enable JavaScript to view thecomments powered by Disqus.",
"Yu C,Zhou Q,Li J,Zhang Z,Wang Z,Wang F",,,Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation,,,10.1145/3581783.3612232 , Conference Paper,2023.0,"Abstract:Text-to-3D generation has recently garnered significant attention, fueled by 2D diffusion models trained on billions of image-text pairs. Existing methods primarily rely on score distillation to leverage the 2D diffusion priors to supervise the generation of 3D models, e.g., NeRF. However, score distillation is prone to suffer the view inconsistency problem, and implicit NeRF modeling can also lead to an arbitrary shape, thus leading to less realistic and uncontrollable 3D generation. In this work, we propose a flexible framework of Points-to-3D to bridge the gap between sparse yet freely available 3D points and realistic shape-controllable 3D generation by distilling the knowledge from both 2D and 3D diffusion models. The core idea of Points-to-3D is to introduce controllable sparse 3D points to guide the text-to-3D generation. Specifically, we use the sparse point cloud generated from the 3D diffusion model, Point-E, as the geometric prior, conditioned on a single reference image. To better utilize the sparse 3D points, we propose an efficient point cloud guidance loss to adaptively drive the NeRF's geometry to align with the shape of the sparse 3D points. In addition to controlling the geometry, we propose to optimize the NeRF for a more view-consistent appearance. To be specific, we perform score distillation to the publicly available 2D image diffusion model ControlNet, conditioned on text as well as depth map of the learned compact geometry. Qualitative and quantitative comparisons demonstrate that Points-to-3D improves view consistency and achieves good shape controllability for text-to-3D generation. Points-to-3D provides users with a new way to improve and control text-to-3D generation.",,,,,ACM ,"Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 2023- 3 November 2023  ",,"Title:Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation

 Abstract:Text-to-3D generation has recently garnered significant attention, fueled by 2D diffusion models trained on billions of image-text pairs. Existing methods primarily rely on score distillation to leverage the 2D diffusion priors to supervise the generation of 3D models, e.g., NeRF. However, score distillation is prone to suffer the view inconsistency problem, and implicit NeRF modeling can also lead to an arbitrary shape, thus leading to less realistic and uncontrollable 3D generation. In this work, we propose a flexible framework of Points-to-3D to bridge the gap between sparse yet freely available 3D points and realistic shape-controllable 3D generation by distilling the knowledge from both 2D and 3D diffusion models. The core idea of Points-to-3D is to introduce controllable sparse 3D points to guide the text-to-3D generation. Specifically, we use the sparse point cloud generated from the 3D diffusion model, Point-E, as the geometric prior, conditioned on a single reference image. To better utilize the sparse 3D points, we propose an efficient point cloud guidance loss to adaptively drive the NeRF's geometry to align with the shape of the sparse 3D points. In addition to controlling the geometry, we propose to optimize the NeRF for a more view-consistent appearance. To be specific, we perform score distillation to the publicly available 2D image diffusion model ControlNet, conditioned on text as well as depth map of the learned compact geometry. Qualitative and quantitative comparisons demonstrate that Points-to-3D improves view consistency and achieves good shape controllability for text-to-3D generation. Points-to-3D provides users with a new way to improve and control text-to-3D generation.",
"Tu H,Yang B,Zhao X",,,ZeroGen: Zero-Shot Multimodal Controllable Text Generation with Multiple Oracles,14303,,10.1007/978-3-031-44696-2_39 , Conference Paper,2023.0,"Abstract:Automatically generating textual content with desired attributes is an ambitious task that people have pursued long. Existing works have made a series of progress in incorporating unimodal controls into language models (LMs), whereas how to generate controllable sentences with multimodal signals and high efficiency remains an open question. To tackle the puzzle, we propose a new paradigm of zero-shot controllable text generation with multimodal signals (\textsc{ZeroGen}). Specifically, \textsc{ZeroGen} leverages controls of text and image successively from token-level to sentence-level and maps them into a unified probability space at decoding, which customizes the LM outputs by weighted addition without extra training. To achieve better inter-modal trade-offs, we further introduce an effective dynamic weighting mechanism to regulate all control weights. Moreover, we conduct substantial experiments to probe the relationship of being in-depth or in-width between signals from distinct modalities. Encouraging empirical results on three downstream tasks show that \textsc{ZeroGen} not only outperforms its counterparts on captioning tasks by a large margin but also shows great potential in multimodal news generation with a higher degree of control. Our code will be released at this https URL.",,,,,Springer ,"Natural Language Processing and Chinese Computing - 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12-15, 2023, Proceedings, Part II  ",,"Title:ZeroGen: Zero-Shot Multimodal Controllable Text Generation with Multiple Oracles

 Abstract:Automatically generating textual content with desired attributes is an ambitious task that people have pursued long. Existing works have made a series of progress in incorporating unimodal controls into language models (LMs), whereas how to generate controllable sentences with multimodal signals and high efficiency remains an open question. To tackle the puzzle, we propose a new paradigm of zero-shot controllable text generation with multimodal signals (\textsc{ZeroGen}). Specifically, \textsc{ZeroGen} leverages controls of text and image successively from token-level to sentence-level and maps them into a unified probability space at decoding, which customizes the LM outputs by weighted addition without extra training. To achieve better inter-modal trade-offs, we further introduce an effective dynamic weighting mechanism to regulate all control weights. Moreover, we conduct substantial experiments to probe the relationship of being in-depth or in-width between signals from distinct modalities. Encouraging empirical results on three downstream tasks show that \textsc{ZeroGen} not only outperforms its counterparts on captioning tasks by a large margin but also shows great potential in multimodal news generation with a higher degree of control. Our code will be released at this https URL.",
"Nishikino K,Kobayashi K",,,Adversarial Imitation Learning with Controllable Rewards for Text Generation,14169,,10.1007/978-3-031-43412-9_8 , Conference Paper,2023.0,"Abstract:Generative Adversarial Networks (GANs) for text generation have recently received many criticisms, as they perform worse than their MLE counterparts. We suspect previous text GANs' inferior performance is due to the lack of a reliable guiding signal in their discriminators. To address this problem, we propose a generative adversarial imitation learning framework for text generation that uses large pre-trained language models to provide more reliable reward guidance. Our approach uses contrastive discriminator, and proximal policy optimization (PPO) to stabilize and improve text generation performance. For evaluation, we conduct experiments on a diverse set of unconditional and conditional text generation tasks. Experimental results show that TextGAIL achieves better performance in terms of both quality and diversity than the MLE baseline. We also validate our intuition that TextGAIL's discriminator demonstrates the capability of providing reasonable rewards with an additional task.",,,,,Springer ,"Machine Learning and Knowledge Discovery in Databases: Research Track - European Conference, ECML PKDD 2023, Turin, Italy, September 18-22, 2023, Proceedings, Part I  ",,"Title:Adversarial Imitation Learning with Controllable Rewards for Text Generation

 Abstract:Generative Adversarial Networks (GANs) for text generation have recently received many criticisms, as they perform worse than their MLE counterparts. We suspect previous text GANs' inferior performance is due to the lack of a reliable guiding signal in their discriminators. To address this problem, we propose a generative adversarial imitation learning framework for text generation that uses large pre-trained language models to provide more reliable reward guidance. Our approach uses contrastive discriminator, and proximal policy optimization (PPO) to stabilize and improve text generation performance. For evaluation, we conduct experiments on a diverse set of unconditional and conditional text generation tasks. Experimental results show that TextGAIL achieves better performance in terms of both quality and diversity than the MLE baseline. We also validate our intuition that TextGAIL's discriminator demonstrates the capability of providing reasonable rewards with an additional task.",
"Yang J,Wang H,Xiao R,Wu S,Chen G,Zhao J",,,Controllable Textual Inversion for Personalized Text-to-Image Generation,abs/2304.05265,,10.48550/ARXIV.2304.05265 , Journal Article,2023.0,"Abstract:Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new ""words"" in the embedding space of a frozen text-to-image model. These ""words"" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.
Our code, data and new words will be available at: this https URL",,,,, CoRR,  ,,"Title:Controllable Textual Inversion for Personalized Text-to-Image Generation

 Abstract:Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new ""words"" in the embedding space of a frozen text-to-image model. These ""words"" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.
Our code, data and new words will be available at: this https URL",
"Kalpakchi D,Boye J",,,SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish,abs/2304.13994,,10.48550/ARXIV.2304.13994 , Journal Article,2023.0,"Abstract:We present SweCTRL-Mini, a large Swedish language model that can be used for inference and fine-tuning on a single consumer-grade GPU. The model is based on the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019), which means that users of the SweCTRL-Mini model can control the genre of the generated text by inserting special tokens in the generation prompts. SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a set of Swedish novels. In this article, we provide (1) a detailed account of the utilized training data and text pre-processing steps, to the extent that it is possible to check whether a specific phrase/source was a part of the training data, and (2) an evaluation of the model on both discriminative tasks, using automatic evaluation methods, and generative tasks, using human referees. We also compare the generative capabilities of the model with those of GPT-3. SweCTRL-Mini is fully open and available for download.",,,,, CoRR,  ,,"Title:SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish

 Abstract:We present SweCTRL-Mini, a large Swedish language model that can be used for inference and fine-tuning on a single consumer-grade GPU. The model is based on the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019), which means that users of the SweCTRL-Mini model can control the genre of the generated text by inserting special tokens in the generation prompts. SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a set of Swedish novels. In this article, we provide (1) a detailed account of the utilized training data and text pre-processing steps, to the extent that it is possible to check whether a specific phrase/source was a part of the training data, and (2) an evaluation of the model on both discriminative tasks, using automatic evaluation methods, and generative tasks, using human referees. We also compare the generative capabilities of the model with those of GPT-3. SweCTRL-Mini is fully open and available for download.",
"Zhang Y,Wei Y,Jiang D,Zhang X,Zuo W,Tian Q",,,ControlVideo: Training-free Controllable Text-to-Video Generation,abs/2305.13077,,10.48550/ARXIV.2305.13077 , Journal Article,2023.0,"Abstract:Text-driven diffusion models have unlocked unprecedented abilities in image generation, whereas their video counterpart still lags behind due to the excessive training cost of temporal modeling. Besides the training burden, the generated videos also suffer from appearance inconsistency and structural flickers, especially in long video synthesis. To address these challenges, we design a \emph{training-free} framework called \textbf{ControlVideo} to enable natural and efficient text-to-video generation. ControlVideo, adapted from ControlNet, leverages coarsely structural consistency from input motion sequences, and introduces three modules to improve video generation. Firstly, to ensure appearance coherence between frames, ControlVideo adds fully cross-frame interaction in self-attention modules. Secondly, to mitigate the flicker effect, it introduces an interleaved-frame smoother that employs frame interpolation on alternated frames. Finally, to produce long videos efficiently, it utilizes a hierarchical sampler that separately synthesizes each short clip with holistic coherency. Empowered with these modules, ControlVideo outperforms the state-of-the-arts on extensive motion-prompt pairs quantitatively and qualitatively. Notably, thanks to the efficient designs, it generates both short and long videos within several minutes using one NVIDIA 2080Ti. Code is available at this https URL.",,,,, CoRR,  ,,"Title:ControlVideo: Training-free Controllable Text-to-Video Generation

 Abstract:Text-driven diffusion models have unlocked unprecedented abilities in image generation, whereas their video counterpart still lags behind due to the excessive training cost of temporal modeling. Besides the training burden, the generated videos also suffer from appearance inconsistency and structural flickers, especially in long video synthesis. To address these challenges, we design a \emph{training-free} framework called \textbf{ControlVideo} to enable natural and efficient text-to-video generation. ControlVideo, adapted from ControlNet, leverages coarsely structural consistency from input motion sequences, and introduces three modules to improve video generation. Firstly, to ensure appearance coherence between frames, ControlVideo adds fully cross-frame interaction in self-attention modules. Secondly, to mitigate the flicker effect, it introduces an interleaved-frame smoother that employs frame interpolation on alternated frames. Finally, to produce long videos efficiently, it utilizes a hierarchical sampler that separately synthesizes each short clip with holistic coherency. Empowered with these modules, ControlVideo outperforms the state-of-the-arts on extensive motion-prompt pairs quantitatively and qualitatively. Notably, thanks to the efficient designs, it generates both short and long videos within several minutes using one NVIDIA 2080Ti. Code is available at this https URL.",
"Chen W,Wu J,Xie P,Wu H,Li J,Xia X,Xiao X,Lin L",,,Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models,abs/2305.13840,,10.48550/ARXIV.2305.13840 , Journal Article,2023.0,"Abstract:Recent advancements in diffusion models have unlocked unprecedented abilities in visual creation. However, current text-to-video generation models struggle with the trade-off among movement range, action coherence and object consistency. To mitigate this issue, we present a controllable text-to-video (T2V) diffusion model, called Control-A-Video, capable of maintaining consistency while customizable video synthesis. Based on a pre-trained conditional text-to-image (T2I) diffusion model, our model aims to generate videos conditioned on a sequence of control signals, such as edge or depth maps. For the purpose of improving object consistency, Control-A-Video integrates motion priors and content priors into video generation. We propose two motion-adaptive noise initialization strategies, which are based on pixel residual and optical flow, to introduce motion priors from input videos, producing more coherent videos. Moreover, a first-frame conditioned controller is proposed to generate videos from content priors of the first frame, which facilitates the semantic alignment with text and allows longer video generation in an auto-regressive manner. With the proposed architecture and strategies, our model achieves resource-efficient convergence and generate consistent and coherent videos with fine-grained control. Extensive experiments demonstrate its success in various video generative tasks such as video editing and video style transfer, outperforming previous methods in terms of consistency and quality.",,,,, CoRR,  ,,"Title:Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models

 Abstract:Recent advancements in diffusion models have unlocked unprecedented abilities in visual creation. However, current text-to-video generation models struggle with the trade-off among movement range, action coherence and object consistency. To mitigate this issue, we present a controllable text-to-video (T2V) diffusion model, called Control-A-Video, capable of maintaining consistency while customizable video synthesis. Based on a pre-trained conditional text-to-image (T2I) diffusion model, our model aims to generate videos conditioned on a sequence of control signals, such as edge or depth maps. For the purpose of improving object consistency, Control-A-Video integrates motion priors and content priors into video generation. We propose two motion-adaptive noise initialization strategies, which are based on pixel residual and optical flow, to introduce motion priors from input videos, producing more coherent videos. Moreover, a first-frame conditioned controller is proposed to generate videos from content priors of the first frame, which facilitates the semantic alignment with text and allows longer video generation in an auto-regressive manner. With the proposed architecture and strategies, our model achieves resource-efficient convergence and generate consistent and coherent videos with fine-grained control. Extensive experiments demonstrate its success in various video generative tasks such as video editing and video style transfer, outperforming previous methods in terms of consistency and quality.",
"Li D,Li J,Hoi SC",,,BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing,abs/2305.14720,,10.48550/ARXIV.2305.14720 , Journal Article,2023.0,"Abstract:Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Code and models will be released at this https URL. Project page at this https URL.",,,,, CoRR,  ,,"Title:BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing

 Abstract:Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Code and models will be released at this https URL. Project page at this https URL.",
"Zhang T,Zhang Y,Vineet V,Joshi N,Wang X",,,Controllable Text-to-Image Generation with GPT-4,abs/2305.18583,,10.48550/ARXIV.2305.18583 , Journal Article,2023.0,"Abstract:Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning. On the other hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable precision in generating code snippets for sketching out text inputs graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide the diffusion-based text-to-image pipelines with programmatic sketches generated by GPT-4, enhancing their abilities for instruction following. Control-GPT works by querying GPT-4 to write TikZ code, and the generated sketches are used as references alongside the text instructions for diffusion models (e.g., ControlNet) to generate photo-realistic images. One major challenge to training our pipeline is the lack of a dataset containing aligned text, images, and sketches. We address the issue by converting instance masks in existing datasets into polygons to mimic the sketches used at test time. As a result, Control-GPT greatly boosts the controllability of image generation. It establishes a new state-of-art on the spatial arrangement and object positioning generation and enhances users' control of object positions, sizes, etc., nearly doubling the accuracy of prior models. Our work, as a first attempt, shows the potential for employing LLMs to enhance the performance in computer vision tasks.",,,,, CoRR,  ,,"Title:Controllable Text-to-Image Generation with GPT-4

 Abstract:Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning. On the other hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable precision in generating code snippets for sketching out text inputs graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide the diffusion-based text-to-image pipelines with programmatic sketches generated by GPT-4, enhancing their abilities for instruction following. Control-GPT works by querying GPT-4 to write TikZ code, and the generated sketches are used as references alongside the text instructions for diffusion models (e.g., ControlNet) to generate photo-realistic images. One major challenge to training our pipeline is the lack of a dataset containing aligned text, images, and sketches. We address the issue by converting instance masks in existing datasets into polygons to mimic the sketches used at test time. As a result, Control-GPT greatly boosts the controllability of image generation. It establishes a new state-of-art on the spatial arrangement and object positioning generation and enhances users' control of object positions, sizes, etc., nearly doubling the accuracy of prior models. Our work, as a first attempt, shows the potential for employing LLMs to enhance the performance in computer vision tasks.",
"Goel A,Hira M,Anand A,Bangar S,Shah RR",,,Advancements in Scientific Controllable Text Generation Methods,abs/2307.05538,,10.48550/ARXIV.2307.05538 , Journal Article,2023.0,"Neural controllable text generation is an important area gaining attention due to its plethora of
applications. Although there is a large body of prior work in controllable text generation, there
is no unifying theme. In this work, we provide a new schema of the pipeline of the generation
process by classifying it into ﬁve modules. The control of attributes in the generation process
requires modiﬁcation of these modules. We present an overview of different techniques used to
perform the modulation of these modules. We also provide an analysis on the advantages and
disadvantages of these techniques. We further pave ways to develop new architectures based on
the combination of the modules described in this paper.
Introduction
Controllable text generation is the task of generating natural sentences whose attributes can be controlled.
The attributes to control can range from being stylistic such politeness, sentiment, formality, etc.; demo-
graphic attributes of the person writing the text such as gender, age, etc.; content such as information,
keywords, entities, etc.; ordering of information, events, like plot summaries etc. Controlling various
attributes of text generation has manifold applications. For instance in dialogue response generation task,
work has been done in controlling persona (Zhang et al., 2018; Li et al., 2016b), controlling various
aspects of the response such as politeness (Niu and Bansal, 2018), formality, authority etc, grounding the
responses in external source of information (Zhou et al., 2018; Dinan et al., 2018; Ghazvininejad et al.,
2018), and controlling topic sequence (Tang et al., 2019; Prabhumoye et al., 2020). Another application is
story generation where you can control the ending (Peng et al., 2018), the persona (Chandu et al., 2019),
the plot (Yao et al., 2019), and the topic sequence (Huang et al., 2019). Controllable text generation is
also used to modulate the formality and politeness of emails (Madaan et al., 2020). Report generation
can be controlled by pulling disparate source documents into a coherent uniﬁed whole, which can use a
shared set of sources such as Wikipedia article generation (Liu et al., 2018; Prabhumoye et al., 2019).
Although there is a large body of prior work in controllable text generation, there is no unifying theme.
Each work addresses a speciﬁc task in a speciﬁc context. In this paper we outline a new schema which
connects prior work and provides an insight into various aspects of controllable text generation. The
schema contains ﬁve modules that cover the overall generation pipeline and provide an understanding of
the effect of each component on the generation process. Prior work has focused on speciﬁc parts of the
schema that we outline here and we provide insights into their similarities. We provide an overview of
these modules and also present an exploration of the various techniques used to control and update each
of these modules.
Most of the controllable text generation tasks can be framed as conditional language generation tasks.
They have an input or a source sequence U and an output or a target sequence Y to be generated. In
this case, we model the probability of the target sequence conditioned on the source sequence given by
P(Y|U) = �T
t=1 P(yt|U, y<t). The generation of the target tokens of the sequence Y unfolds as a time
series where each token yt is generated at a time step t. At a given time step t, a generative model takes
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/.",,,,, CoRR,  ,,"Title:Advancements in Scientific Controllable Text Generation Methods

 Neural controllable text generation is an important area gaining attention due to its plethora of
applications. Although there is a large body of prior work in controllable text generation, there
is no unifying theme. In this work, we provide a new schema of the pipeline of the generation
process by classifying it into ﬁve modules. The control of attributes in the generation process
requires modiﬁcation of these modules. We present an overview of different techniques used to
perform the modulation of these modules. We also provide an analysis on the advantages and
disadvantages of these techniques. We further pave ways to develop new architectures based on
the combination of the modules described in this paper.
Introduction
Controllable text generation is the task of generating natural sentences whose attributes can be controlled.
The attributes to control can range from being stylistic such politeness, sentiment, formality, etc.; demo-
graphic attributes of the person writing the text such as gender, age, etc.; content such as information,
keywords, entities, etc.; ordering of information, events, like plot summaries etc. Controlling various
attributes of text generation has manifold applications. For instance in dialogue response generation task,
work has been done in controlling persona (Zhang et al., 2018; Li et al., 2016b), controlling various
aspects of the response such as politeness (Niu and Bansal, 2018), formality, authority etc, grounding the
responses in external source of information (Zhou et al., 2018; Dinan et al., 2018; Ghazvininejad et al.,
2018), and controlling topic sequence (Tang et al., 2019; Prabhumoye et al., 2020). Another application is
story generation where you can control the ending (Peng et al., 2018), the persona (Chandu et al., 2019),
the plot (Yao et al., 2019), and the topic sequence (Huang et al., 2019). Controllable text generation is
also used to modulate the formality and politeness of emails (Madaan et al., 2020). Report generation
can be controlled by pulling disparate source documents into a coherent uniﬁed whole, which can use a
shared set of sources such as Wikipedia article generation (Liu et al., 2018; Prabhumoye et al., 2019).
Although there is a large body of prior work in controllable text generation, there is no unifying theme.
Each work addresses a speciﬁc task in a speciﬁc context. In this paper we outline a new schema which
connects prior work and provides an insight into various aspects of controllable text generation. The
schema contains ﬁve modules that cover the overall generation pipeline and provide an understanding of
the effect of each component on the generation process. Prior work has focused on speciﬁc parts of the
schema that we outline here and we provide insights into their similarities. We provide an overview of
these modules and also present an exploration of the various techniques used to control and update each
of these modules.
Most of the controllable text generation tasks can be framed as conditional language generation tasks.
They have an input or a source sequence U and an output or a target sequence Y to be generated. In
this case, we model the probability of the target sequence conditioned on the source sequence given by
P(Y|U) = �T
t=1 P(yt|U, y<t). The generation of the target tokens of the sequence Y unfolds as a time
series where each token yt is generated at a time step t. At a given time step t, a generative model takes
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/.",
"Hwang S,Hyung J,Choo J",,,Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model,abs/2309.03550,,10.48550/ARXIV.2309.03550 , Journal Article,2023.0,"Abstract:Recent advances in diffusion models such as ControlNet have enabled geometrically controllable, high-fidelity text-to-image generation. However, none of them addresses the question of adding such controllability to text-to-3D generation. In response, we propose Text2Control3D, a controllable text-to-3D avatar generation method whose facial expression is controllable given a monocular video casually captured with hand-held camera. Our main strategy is to construct the 3D avatar in Neural Radiance Fields (NeRF) optimized with a set of controlled viewpoint-aware images that we generate from ControlNet, whose condition input is the depth map extracted from the input video. When generating the viewpoint-aware images, we utilize cross-reference attention to inject well-controlled, referential facial expression and appearance via cross attention. We also conduct low-pass filtering of Gaussian latent of the diffusion model in order to ameliorate the viewpoint-agnostic texture problem we observed from our empirical analysis, where the viewpoint-aware images contain identical textures on identical pixel positions that are incomprehensible in 3D. Finally, to train NeRF with the images that are viewpoint-aware yet are not strictly consistent in geometry, our approach considers per-image geometric variation as a view of deformation from a shared 3D canonical space. Consequently, we construct the 3D avatar in a canonical space of deformable NeRF by learning a set of per-image deformation via deformation field table. We demonstrate the empirical results and discuss the effectiveness of our method.",,,,, CoRR,  ,,"Title:Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model

 Abstract:Recent advances in diffusion models such as ControlNet have enabled geometrically controllable, high-fidelity text-to-image generation. However, none of them addresses the question of adding such controllability to text-to-3D generation. In response, we propose Text2Control3D, a controllable text-to-3D avatar generation method whose facial expression is controllable given a monocular video casually captured with hand-held camera. Our main strategy is to construct the 3D avatar in Neural Radiance Fields (NeRF) optimized with a set of controlled viewpoint-aware images that we generate from ControlNet, whose condition input is the depth map extracted from the input video. When generating the viewpoint-aware images, we utilize cross-reference attention to inject well-controlled, referential facial expression and appearance via cross attention. We also conduct low-pass filtering of Gaussian latent of the diffusion model in order to ameliorate the viewpoint-agnostic texture problem we observed from our empirical analysis, where the viewpoint-aware images contain identical textures on identical pixel positions that are incomprehensible in 3D. Finally, to train NeRF with the images that are viewpoint-aware yet are not strictly consistent in geometry, our approach considers per-image geometric variation as a view of deformation from a shared 3D canonical space. Consequently, we construct the 3D avatar in a canonical space of deformable NeRF by learning a set of per-image deformation via deformation field table. We demonstrate the empirical results and discuss the effectiveness of our method.",
"Zheng X,Lin H,Han X,Sun L",,,Toward Unified Controllable Text Generation via Regular Expression Instruction,abs/2309.10447,,10.48550/ARXIV.2309.10447 , Journal Article,2023.0,"‡University of Cambridge
fl339@cam.ac.uk
The introduction of immensely large causal
language models (CLMs) has rejuvenated the
interest in open-ended text generation. How-
ever, controlling the generative process for
these Transformer-based models is at large
an unsolved problem.
Earlier work has ex-
plored either plug-and-play decoding strate-
gies or more powerful but blunt approaches
such as prompting. There hence currently ex-
ists a trade-off between fine-grained control
and the capability for more expressive high-
level instructions. To alleviate this trade-off,
we propose an encoder-decoder architecture
that enables intermediate text prompts at ar-
bitrary time steps.
We propose a resource-
efficient method for converting a pre-trained
CLM into this architecture and demonstrate its
potential in various experiments, including the
novel task of contextualized word inclusion.
Our method provides strong results in multi-
ple experimental settings, proving itself to be
both expressive and versatile.1",,,,, CoRR,  ,,"Title:Toward Unified Controllable Text Generation via Regular Expression Instruction

 ‡University of Cambridge
fl339@cam.ac.uk
The introduction of immensely large causal
language models (CLMs) has rejuvenated the
interest in open-ended text generation. How-
ever, controlling the generative process for
these Transformer-based models is at large
an unsolved problem.
Earlier work has ex-
plored either plug-and-play decoding strate-
gies or more powerful but blunt approaches
such as prompting. There hence currently ex-
ists a trade-off between fine-grained control
and the capability for more expressive high-
level instructions. To alleviate this trade-off,
we propose an encoder-decoder architecture
that enables intermediate text prompts at ar-
bitrary time steps.
We propose a resource-
efficient method for converting a pre-trained
CLM into this architecture and demonstrate its
potential in various experiments, including the
novel task of contextualized word inclusion.
Our method provides strong results in multi-
ple experimental settings, proving itself to be
both expressive and versatile.1",
"Zhang H,Si S,Wu H,Song D",,,Controllable Text Generation with Residual Memory Transformer,abs/2309.16231,,10.48550/ARXIV.2309.16231 , Journal Article,2023.0,"Neural controllable text generation is an important area gaining attention due to its plethora of
applications. Although there is a large body of prior work in controllable text generation, there
is no unifying theme. In this work, we provide a new schema of the pipeline of the generation
process by classifying it into ﬁve modules. The control of attributes in the generation process
requires modiﬁcation of these modules. We present an overview of different techniques used to
perform the modulation of these modules. We also provide an analysis on the advantages and
disadvantages of these techniques. We further pave ways to develop new architectures based on
the combination of the modules described in this paper.
Introduction
Controllable text generation is the task of generating natural sentences whose attributes can be controlled.
The attributes to control can range from being stylistic such politeness, sentiment, formality, etc.; demo-
graphic attributes of the person writing the text such as gender, age, etc.; content such as information,
keywords, entities, etc.; ordering of information, events, like plot summaries etc. Controlling various
attributes of text generation has manifold applications. For instance in dialogue response generation task,
work has been done in controlling persona (Zhang et al., 2018; Li et al., 2016b), controlling various
aspects of the response such as politeness (Niu and Bansal, 2018), formality, authority etc, grounding the
responses in external source of information (Zhou et al., 2018; Dinan et al., 2018; Ghazvininejad et al.,
2018), and controlling topic sequence (Tang et al., 2019; Prabhumoye et al., 2020). Another application is
story generation where you can control the ending (Peng et al., 2018), the persona (Chandu et al., 2019),
the plot (Yao et al., 2019), and the topic sequence (Huang et al., 2019). Controllable text generation is
also used to modulate the formality and politeness of emails (Madaan et al., 2020). Report generation
can be controlled by pulling disparate source documents into a coherent uniﬁed whole, which can use a
shared set of sources such as Wikipedia article generation (Liu et al., 2018; Prabhumoye et al., 2019).
Although there is a large body of prior work in controllable text generation, there is no unifying theme.
Each work addresses a speciﬁc task in a speciﬁc context. In this paper we outline a new schema which
connects prior work and provides an insight into various aspects of controllable text generation. The
schema contains ﬁve modules that cover the overall generation pipeline and provide an understanding of
the effect of each component on the generation process. Prior work has focused on speciﬁc parts of the
schema that we outline here and we provide insights into their similarities. We provide an overview of
these modules and also present an exploration of the various techniques used to control and update each
of these modules.
Most of the controllable text generation tasks can be framed as conditional language generation tasks.
They have an input or a source sequence U and an output or a target sequence Y to be generated. In
this case, we model the probability of the target sequence conditioned on the source sequence given by
P(Y|U) = �T
t=1 P(yt|U, y<t). The generation of the target tokens of the sequence Y unfolds as a time
series where each token yt is generated at a time step t. At a given time step t, a generative model takes
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/.",,,,, CoRR,  ,,"Title:Controllable Text Generation with Residual Memory Transformer

 Neural controllable text generation is an important area gaining attention due to its plethora of
applications. Although there is a large body of prior work in controllable text generation, there
is no unifying theme. In this work, we provide a new schema of the pipeline of the generation
process by classifying it into ﬁve modules. The control of attributes in the generation process
requires modiﬁcation of these modules. We present an overview of different techniques used to
perform the modulation of these modules. We also provide an analysis on the advantages and
disadvantages of these techniques. We further pave ways to develop new architectures based on
the combination of the modules described in this paper.
Introduction
Controllable text generation is the task of generating natural sentences whose attributes can be controlled.
The attributes to control can range from being stylistic such politeness, sentiment, formality, etc.; demo-
graphic attributes of the person writing the text such as gender, age, etc.; content such as information,
keywords, entities, etc.; ordering of information, events, like plot summaries etc. Controlling various
attributes of text generation has manifold applications. For instance in dialogue response generation task,
work has been done in controlling persona (Zhang et al., 2018; Li et al., 2016b), controlling various
aspects of the response such as politeness (Niu and Bansal, 2018), formality, authority etc, grounding the
responses in external source of information (Zhou et al., 2018; Dinan et al., 2018; Ghazvininejad et al.,
2018), and controlling topic sequence (Tang et al., 2019; Prabhumoye et al., 2020). Another application is
story generation where you can control the ending (Peng et al., 2018), the persona (Chandu et al., 2019),
the plot (Yao et al., 2019), and the topic sequence (Huang et al., 2019). Controllable text generation is
also used to modulate the formality and politeness of emails (Madaan et al., 2020). Report generation
can be controlled by pulling disparate source documents into a coherent uniﬁed whole, which can use a
shared set of sources such as Wikipedia article generation (Liu et al., 2018; Prabhumoye et al., 2019).
Although there is a large body of prior work in controllable text generation, there is no unifying theme.
Each work addresses a speciﬁc task in a speciﬁc context. In this paper we outline a new schema which
connects prior work and provides an insight into various aspects of controllable text generation. The
schema contains ﬁve modules that cover the overall generation pipeline and provide an understanding of
the effect of each component on the generation process. Prior work has focused on speciﬁc parts of the
schema that we outline here and we provide insights into their similarities. We provide an overview of
these modules and also present an exploration of the various techniques used to control and update each
of these modules.
Most of the controllable text generation tasks can be framed as conditional language generation tasks.
They have an input or a source sequence U and an output or a target sequence Y to be generated. In
this case, we model the probability of the target sequence conditioned on the source sequence given by
P(Y|U) = �T
t=1 P(yt|U, y<t). The generation of the target tokens of the sequence Y unfolds as a time
series where each token yt is generated at a time step t. At a given time step t, a generative model takes
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/.",
"Fang C,Hu X,Luo K,Tan P",,,Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints,abs/2310.03602,,10.48550/ARXIV.2310.03602 , Journal Article,2023.0,"Abstract


            Text-driven 3D indoor scene generation could be useful for gaming, film industry, and AR/VR applications. 
            However, existing methods cannot faithfully capture the room layout, nor do they allow flexible editing of individual objects in the room.
          

            To address these problems, we present Ctrl-Room, which is able to generate convincing 3D rooms with designer-style layouts and high-fidelity textures from just a text prompt. 
            Moreover, Ctrl-Room enables versatile interactive editing operations such as resizing or moving individual furniture items. 
            Our key insight is to separate the modeling of layouts and appearance. Our proposed method consists of two stages, a `Layout Generation Stage' and an `Appearance Generation Stage'.  
            The `Layout Generation Stage' trains a text-conditional diffusion model to learn the layout distribution with our holistic scene code parameterization. 
            Next, the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a vivid panoramic image of the room guided by the 3D scene layout and text prompt.
          

            In this way, we achieve a high-quality 3D room with convincing layouts and lively textures. 
            Benefiting from the scene code parameterization, we can easily edit the generated room model through our mask-guided editing module, 
            without expensive editing-specific training. 
            Extensive experiments on the Structured3D dataset demonstrate that our method outperforms existing methods in producing more reasonable, 
            view-consistent, and editable 3D rooms from natural language prompts.",,,,, CoRR,  ,,"Title:Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints

 Abstract


            Text-driven 3D indoor scene generation could be useful for gaming, film industry, and AR/VR applications. 
            However, existing methods cannot faithfully capture the room layout, nor do they allow flexible editing of individual objects in the room.
          

            To address these problems, we present Ctrl-Room, which is able to generate convincing 3D rooms with designer-style layouts and high-fidelity textures from just a text prompt. 
            Moreover, Ctrl-Room enables versatile interactive editing operations such as resizing or moving individual furniture items. 
            Our key insight is to separate the modeling of layouts and appearance. Our proposed method consists of two stages, a `Layout Generation Stage' and an `Appearance Generation Stage'.  
            The `Layout Generation Stage' trains a text-conditional diffusion model to learn the layout distribution with our holistic scene code parameterization. 
            Next, the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a vivid panoramic image of the room guided by the 3D scene layout and text prompt.
          

            In this way, we achieve a high-quality 3D room with convincing layouts and lively textures. 
            Benefiting from the scene code parameterization, we can easily edit the generated room model through our mask-guided editing module, 
            without expensive editing-specific training. 
            Extensive experiments on the Structured3D dataset demonstrate that our method outperforms existing methods in producing more reasonable, 
            view-consistent, and editable 3D rooms from natural language prompts.",
"Li Z,Chen Y,Zhao L,Liu P",,,MVControl: Adding Conditional Control to Multi-view Diffusion for Controllable Text-to-3D Generation,abs/2311.14494,,10.48550/ARXIV.2311.14494 , Journal Article,2023.0,"Abstract:We introduce MVControl, a novel neural network architecture that enhances existing pre-trained multi-view 2D diffusion models by incorporating additional input conditions, e.g. edge maps. Our approach enables the generation of controllable multi-view images and view-consistent 3D content. To achieve controllable multi-view image generation, we leverage MVDream as our base model, and train a new neural network module as additional plugin for end-to-end task-specific condition learning. To precisely control the shapes and views of generated images, we innovatively propose a new conditioning mechanism that predicts an embedding encapsulating the input spatial and view conditions, which is then injected to the network globally. Once MVControl is trained, score-distillation (SDS) loss based optimization can be performed to generate 3D content, in which process we propose to use a hybrid diffusion prior. The hybrid prior relies on a pre-trained Stable-Diffusion network and our trained MVControl for additional guidance. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 3D content. Code available at this https URL.",,,,, CoRR,  ,,"Title:MVControl: Adding Conditional Control to Multi-view Diffusion for Controllable Text-to-3D Generation

 Abstract:We introduce MVControl, a novel neural network architecture that enhances existing pre-trained multi-view 2D diffusion models by incorporating additional input conditions, e.g. edge maps. Our approach enables the generation of controllable multi-view images and view-consistent 3D content. To achieve controllable multi-view image generation, we leverage MVDream as our base model, and train a new neural network module as additional plugin for end-to-end task-specific condition learning. To precisely control the shapes and views of generated images, we innovatively propose a new conditioning mechanism that predicts an embedding encapsulating the input spatial and view conditions, which is then injected to the network globally. Once MVControl is trained, score-distillation (SDS) loss based optimization can be performed to generate 3D content, in which process we propose to use a hybrid diffusion prior. The hybrid prior relies on a pre-trained Stable-Diffusion network and our trained MVControl for additional guidance. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 3D content. Code available at this https URL.",
"Jiang Y,Yang S,Qiu H,Wu W,Loy CC,Liu Z",,,Text2Human: text-driven controllable human image generation,41,4,10.1145/3528223.3530104 , Journal Article,2022.0,"Generating high-quality and diverse human images is an important yet challenging task in vision and graphics. However, existing generative models often fall short under the high diversity of clothing shapes and textures. Furthermore, the generation process is even desired to be intuitively controllable for layman users. In this work, we present a text-driven controllable framework, Text2Human, for a high-quality and diverse human generation. We synthesize full-body human images starting from a given human pose with two dedicated steps. 1) With some texts describing the shapes of clothes, the given human pose is first translated to a human parsing map. 2) The final human image is then generated by providing the system with more attributes about the textures of clothes. Specifically, to model the diversity of clothing textures, we build a hierarchical texture-aware codebook that stores multi-scale neural representations for each type of texture. The codebook at the coarse level includes the structural representations of textures, while the codebook at the fine level focuses on the details of textures. To make use of the learned hierarchical codebook to synthesize desired images, a diffusion-based transformer sampler with mixture of experts is firstly employed to sample indices from the coarsest level of the codebook, which then is used to predict the indices of the codebook at finer levels. The predicted indices at different levels are translated to human images by the decoder learned accompanied with hierarchical codebooks. The use of mixture-of-experts allows for the generated image conditioned on the fine-grained text input. The prediction for finer level indices refines the quality of clothing textures. Extensive quantitative and qualitative evaluations demonstrate that our proposed Text2Human framework can generate more diverse and realistic human images compared to state-of-the-art methods. Our project page is https://yumingj.github.io/projects/Text2Human.html. Code and pretrained models are available at https://github.com/yumingj/Text2Human.",,,,, ACM Trans. Graph.,  ,,"Title:Text2Human: text-driven controllable human image generation

 Generating high-quality and diverse human images is an important yet challenging task in vision and graphics. However, existing generative models often fall short under the high diversity of clothing shapes and textures. Furthermore, the generation process is even desired to be intuitively controllable for layman users. In this work, we present a text-driven controllable framework, Text2Human, for a high-quality and diverse human generation. We synthesize full-body human images starting from a given human pose with two dedicated steps. 1) With some texts describing the shapes of clothes, the given human pose is first translated to a human parsing map. 2) The final human image is then generated by providing the system with more attributes about the textures of clothes. Specifically, to model the diversity of clothing textures, we build a hierarchical texture-aware codebook that stores multi-scale neural representations for each type of texture. The codebook at the coarse level includes the structural representations of textures, while the codebook at the fine level focuses on the details of textures. To make use of the learned hierarchical codebook to synthesize desired images, a diffusion-based transformer sampler with mixture of experts is firstly employed to sample indices from the coarsest level of the codebook, which then is used to predict the indices of the codebook at finer levels. The predicted indices at different levels are translated to human images by the decoder learned accompanied with hierarchical codebooks. The use of mixture-of-experts allows for the generated image conditioned on the fine-grained text input. The prediction for finer level indices refines the quality of clothing textures. Extensive quantitative and qualitative evaluations demonstrate that our proposed Text2Human framework can generate more diverse and realistic human images compared to state-of-the-art methods. Our project page is https://yumingj.github.io/projects/Text2Human.html. Code and pretrained models are available at https://github.com/yumingj/Text2Human.",
"Mireshghallah F,Goyal K,Berg-Kirkpatrick T",,,Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models,,,10.18653/V1/2022.ACL-LONG.31 , Conference Paper,2022.0,"AbstractRecent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.",,,,,Association for Computational Linguistics ,"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022  ",,"Title:Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models

 AbstractRecent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.",
"Hu Y,Luo C,Chen Z",,,Make It Move: Controllable Image-to-Video Generation with Text Descriptions,,,10.1109/CVPR52688.2022.01768 , Conference Paper,2022.0,"Abstract:Generating controllable videos conforming to user intentions is an appealing yet challenging topic in computer vision. To enable maneuverable control in line with user intentions, a novel video generation task, named Text-Image-to-Video generation (TI2V), is proposed. With both controllable appearance and motion, TI2V aims at generating videos from a static image and a text description. The key challenges of TI2V task lie both in aligning appearance and motion from different modalities, and in handling uncertainty in text descriptions. To address these challenges, we propose a Motion Anchor-based video GEnerator (MAGE) with an innovative motion anchor (MA) structure to store appearance-motion aligned representation. To model the uncertainty and increase the diversity, it further allows the injection of explicit condition and implicit randomness. Through three-dimensional axial transformers, MA is interacted with given image to generate next frames recursively with satisfying controllability and diversity. Accompanying the new task, we build two new video-text paired datasets based on MNIST and CATER for evaluation. Experiments conducted on these datasets verify the effectiveness of MAGE and show appealing potentials of TI2V task. Source code for model and datasets will be available soon.",,,,,IEEE ,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022  ",,"Title:Make It Move: Controllable Image-to-Video Generation with Text Descriptions

 Abstract:Generating controllable videos conforming to user intentions is an appealing yet challenging topic in computer vision. To enable maneuverable control in line with user intentions, a novel video generation task, named Text-Image-to-Video generation (TI2V), is proposed. With both controllable appearance and motion, TI2V aims at generating videos from a static image and a text description. The key challenges of TI2V task lie both in aligning appearance and motion from different modalities, and in handling uncertainty in text descriptions. To address these challenges, we propose a Motion Anchor-based video GEnerator (MAGE) with an innovative motion anchor (MA) structure to store appearance-motion aligned representation. To model the uncertainty and increase the diversity, it further allows the injection of explicit condition and implicit randomness. Through three-dimensional axial transformers, MA is interacted with given image to generate next frames recursively with satisfying controllability and diversity. Accompanying the new task, we build two new video-text paired datasets based on MNIST and CATER for evaluation. Experiments conducted on these datasets verify the effectiveness of MAGE and show appealing potentials of TI2V task. Source code for model and datasets will be available soon.",
"Gu Y,Feng X,Ma S,Zhang L,Gong H,Qin B",,,A Distributional Lens for Multi-Aspect Controllable Text Generation,,,10.18653/V1/2022.EMNLP-MAIN.67 , Conference Paper,2022.0,"Multi-aspect controllable text generation is a
more challenging and practical task than single-
aspect control. Existing methods achieve com-
plex multi-aspect control by fusing multiple
controllers learned from single-aspect, but suf-
fer from attribute degeneration caused by the
mutual interference of these controllers. To
address this, we provide observations on at-
tribute fusion from a distributional perspective
and propose to directly search for the intersec-
tion areas of multiple attribute distributions as
their combination for generation. Our method
first estimates the attribute space with an au-
toencoder structure. Afterward, we iteratively
approach the intersections by jointly minimiz-
ing distances to points representing different
attributes. Finally, we map them to attribute-
relevant sentences with a prefix-tuning-based
decoder. Experiments on the three-aspect con-
trol task, including sentiment, topic, and detox-
ification aspects, reveal that our method outper-
forms several strong baselines on attribute rele-
vance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory
support for the effectiveness of our approach1.",,,,,Association for Computational Linguistics ,"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022  ",,"Title:A Distributional Lens for Multi-Aspect Controllable Text Generation

 Multi-aspect controllable text generation is a
more challenging and practical task than single-
aspect control. Existing methods achieve com-
plex multi-aspect control by fusing multiple
controllers learned from single-aspect, but suf-
fer from attribute degeneration caused by the
mutual interference of these controllers. To
address this, we provide observations on at-
tribute fusion from a distributional perspective
and propose to directly search for the intersec-
tion areas of multiple attribute distributions as
their combination for generation. Our method
first estimates the attribute space with an au-
toencoder structure. Afterward, we iteratively
approach the intersections by jointly minimiz-
ing distances to points representing different
attributes. Finally, we map them to attribute-
relevant sentences with a prefix-tuning-based
decoder. Experiments on the three-aspect con-
trol task, including sentiment, topic, and detox-
ification aspects, reveal that our method outper-
forms several strong baselines on attribute rele-
vance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory
support for the effectiveness of our approach1.",
"Zhang H,Song D",,,DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation,,,10.18653/V1/2022.EMNLP-MAIN.223 , Conference Paper,2022.0,"Abstract:Prompt learning with immensely large Casual Language Models (CLMs) has been shown promising for attribute-controllable text generation (CTG). However, vanilla prompt tuning tends to imitate training corpus characteristics beyond the control attributes, resulting in a poor generalization ability. Moreover, it is less able to capture the relationship between different attributes, further limiting the control performance. In this paper, we propose a new CTG approach, namely DisCup, which incorporates the attribute knowledge of discriminator to optimize the control-prompts, steering a frozen CLM to produce attribute-specific texts. Specifically, the frozen CLM model, capable of producing multitudinous texts, is first used to generate the next-token candidates based on the context, so as to ensure the diversity of tokens to be predicted. Then, we leverage an attribute-discriminator to select desired/undesired tokens from those candidates, providing the inter-attribute knowledge. Finally, we bridge the above two traits by an unlikelihood objective for prompt-tuning. Extensive experimental results show that DisCup can achieve a new state-of-the-art control performance while maintaining an efficient and high-quality text generation, only relying on around 10 virtual tokens.",,,,,Association for Computational Linguistics ,"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022  ",,"Title:DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation

 Abstract:Prompt learning with immensely large Casual Language Models (CLMs) has been shown promising for attribute-controllable text generation (CTG). However, vanilla prompt tuning tends to imitate training corpus characteristics beyond the control attributes, resulting in a poor generalization ability. Moreover, it is less able to capture the relationship between different attributes, further limiting the control performance. In this paper, we propose a new CTG approach, namely DisCup, which incorporates the attribute knowledge of discriminator to optimize the control-prompts, steering a frozen CLM to produce attribute-specific texts. Specifically, the frozen CLM model, capable of producing multitudinous texts, is first used to generate the next-token candidates based on the context, so as to ensure the diversity of tokens to be predicted. Then, we leverage an attribute-discriminator to select desired/undesired tokens from those candidates, providing the inter-attribute knowledge. Finally, we bridge the above two traits by an unlikelihood objective for prompt-tuning. Extensive experimental results show that DisCup can achieve a new state-of-the-art control performance while maintaining an efficient and high-quality text generation, only relying on around 10 virtual tokens.",
Peng Nviolet,,,Controllable Text Generation for Open-Domain Creativity and Fairness,,,10.24963/IJCAI.2022/818 , Conference Paper,2022.0,"Recent advances in large pre-trained language models have demonstrated strong results in generating natural languages and significantly improved performances for many natural language generation (NLG) applications such as machine translation and text summarization. However, when the generation tasks are more open-ended and the content is under-specified, existing techniques struggle to generate long-term coherent and creative content. Moreover, the models exhibit and even amplify social biases that are learned from the training corpora. This happens because the generation models are trained to capture the surface patterns (i.e. sequences of words), instead of capturing underlying semantics and discourse structures, as well as background knowledge including social norms. In this paper, I introduce our recent works on controllable text generation to enhance the creativity and fairness of language generation models. We explore hierarchical generation and constrained decoding, with applications to creative language generation including story, poetry, and figurative languages, and bias mitigation for generation models.",,,,,ijcai.org ,"Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022  ",,"Title:Controllable Text Generation for Open-Domain Creativity and Fairness

 Recent advances in large pre-trained language models have demonstrated strong results in generating natural languages and significantly improved performances for many natural language generation (NLG) applications such as machine translation and text summarization. However, when the generation tasks are more open-ended and the content is under-specified, existing techniques struggle to generate long-term coherent and creative content. Moreover, the models exhibit and even amplify social biases that are learned from the training corpora. This happens because the generation models are trained to capture the surface patterns (i.e. sequences of words), instead of capturing underlying semantics and discourse structures, as well as background knowledge including social norms. In this paper, I introduce our recent works on controllable text generation to enhance the creativity and fairness of language generation models. We explore hierarchical generation and constrained decoding, with applications to creative language generation including story, poetry, and figurative languages, and bias mitigation for generation models.",
"Landsman D,Chen JZ,Zaidi H",,,BeamR: Beam Reweighing with Attribute Discriminators for Controllable Text Generation,,, , Conference Paper,2022.0,"AbstractRecent advances in natural language processing have led to the availability of large pre-trained language models (LMs), with rich generative capabilities. Although these models are able to produce fluent and coherent text, it remains a challenge to control various attributes of the generation, including sentiment, formality, topic and many others. We propose a Beam Reweighing (BeamR) method, building on top of standard beam search, in order to control different attributes. BeamR combines any generative LM with any attribute discriminator, offering full flexibility of generation style and attribute, while the beam search backbone maintains fluency across different domains. Notably, BeamR allows practitioners to leverage pre-trained models without the need to train generative LMs together with discriminators. We evaluate BeamR in two diverse tasks: sentiment steering, and machine translation formality. Our results show that BeamR performs on par with or better than existing state-of-the-art approaches (including fine-tuned methods), and highlight the flexiblity of BeamR in both causal and seq2seq language modeling tasks.",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, Online only, November 20-23, 2022  ",,"Title:BeamR: Beam Reweighing with Attribute Discriminators for Controllable Text Generation

 AbstractRecent advances in natural language processing have led to the availability of large pre-trained language models (LMs), with rich generative capabilities. Although these models are able to produce fluent and coherent text, it remains a challenge to control various attributes of the generation, including sentiment, formality, topic and many others. We propose a Beam Reweighing (BeamR) method, building on top of standard beam search, in order to control different attributes. BeamR combines any generative LM with any attribute discriminator, offering full flexibility of generation style and attribute, while the beam search backbone maintains fluency across different domains. Notably, BeamR allows practitioners to leverage pre-trained models without the need to train generative LMs together with discriminators. We evaluate BeamR in two diverse tasks: sentiment steering, and machine translation formality. Our results show that BeamR performs on par with or better than existing state-of-the-art approaches (including fine-tuned methods), and highlight the flexiblity of BeamR in both causal and seq2seq language modeling tasks.",
"Li X,Thickstun J,Gulrajani I,Liang P,Hashimoto TB",,,Diffusion-LM Improves Controllable Text Generation,,, , Conference Paper,2022.0,"Abstract:Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.",,,,, ,NeurIPS  ,,"Title:Diffusion-LM Improves Controllable Text Generation

 Abstract:Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.",
"Lu X,Welleck S,Hessel J,Jiang L,Qin L,West P,Ammanabrolu P,Choi Y",,,QUARK: Controllable Text Generation with Reinforced Unlearning,,, , Conference Paper,2022.0,"Abstract:Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives.",,,,, ,NeurIPS  ,,"Title:QUARK: Controllable Text Generation with Reinforced Unlearning

 Abstract:Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives.",
"Meng T,Lu S,Peng N,Chang KW",,,Controllable Text Generation with Neurally-Decomposed Oracle,,, , Conference Paper,2022.0,"Abstract:We propose a general and efficient framework to control auto-regressive generation models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained base language model and a sequence-level boolean oracle function, we propose to decompose the oracle function into token-level guidance to steer the base model in text generation. Specifically, the token-level guidance is approximated by a neural model trained with examples sampled from the base model, demanding no additional auxiliary labeled data. Based on posterior regularization, we present the closed-form optimal solution to incorporate the token-level guidance into the base model for controllable generation. We further provide a theoretical analysis of how the approximation quality of NADO affects the controllable generation results. Experiments conducted on two applications: (1) text generation with lexical constraints and (2) machine translation with formality control demonstrate that our framework efficiently guides the base model towards the given oracle while maintaining high generation quality.",,,,, ,NeurIPS  ,,"Title:Controllable Text Generation with Neurally-Decomposed Oracle

 Abstract:We propose a general and efficient framework to control auto-regressive generation models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained base language model and a sequence-level boolean oracle function, we propose to decompose the oracle function into token-level guidance to steer the base model in text generation. Specifically, the token-level guidance is approximated by a neural model trained with examples sampled from the base model, demanding no additional auxiliary labeled data. Based on posterior regularization, we present the closed-form optimal solution to incorporate the token-level guidance into the base model for controllable generation. We further provide a theoretical analysis of how the approximation quality of NADO affects the controllable generation results. Experiments conducted on two applications: (1) text generation with lexical constraints and (2) machine translation with formality control demonstrate that our framework efficiently guides the base model towards the given oracle while maintaining high generation quality.",
"Peng X,Sollami M",,,XFBoost: Improving Text Generation with Controllable Decoders,abs/2202.08124,, , Journal Article,2022.0,"Weighted decoding methods composed of the
pretrained language model (LM) and the con-
troller have achieved promising results for con-
trollable text generation. However, these mod-
els often suffer from a control strength/fluency
trade-off problem as higher control strength is
more likely to generate incoherent and repeti-
tive text. In this paper, we illustrate this trade-
off is arisen by the controller imposing the tar-
get attribute on the LM at improper positions.
And we propose a novel framework based on
existing weighted decoding methods called
CAT-PAW1, which introduces a lightweight reg-
ulator to adjust bias signals from the controller
at different decoding positions. Experiments on
positive sentiment control, topic control, and
language detoxification show the effectiveness
of our CAT-PAW upon 4 SOTA models2.",,,,, CoRR,  ,,"Title:XFBoost: Improving Text Generation with Controllable Decoders

 Weighted decoding methods composed of the
pretrained language model (LM) and the con-
troller have achieved promising results for con-
trollable text generation. However, these mod-
els often suffer from a control strength/fluency
trade-off problem as higher control strength is
more likely to generate incoherent and repeti-
tive text. In this paper, we illustrate this trade-
off is arisen by the controller imposing the tar-
get attribute on the LM at improper positions.
And we propose a novel framework based on
existing weighted decoding methods called
CAT-PAW1, which introduces a lightweight reg-
ulator to adjust bias signals from the controller
at different decoding positions. Experiments on
positive sentiment control, topic control, and
language detoxification show the effectiveness
of our CAT-PAW upon 4 SOTA models2.",
"Sitdikov A,Balagansky N,Gavrilov D,Markov A",,,Classifiers are Better Experts for Controllable Text Generation,abs/2205.07276,,10.48550/ARXIV.2205.07276 , Journal Article,2022.0,"Recent work on controlled text generation has
either required attribute-based fine-tuning of the
base language model (LM), or has restricted the
parameterization of the attribute discriminator
to be compatible with the base autoregressive
LM. In this work, we propose Mix and Match
LM, a global score-based alternative for con-
trollable text generation that combines arbitrary
pre-trained black-box models for achieving the
desired attributes in the generated text without
involving any fine-tuning or structural assump-
tions about the black-box models. We interpret
the task of controllable generation as drawing
samples from an energy-based model whose
energy values are a linear combination of scores
from black-box models that are separately
responsible for fluency, the control attribute,
and faithfulness to any conditioning context.
We use a Metropolis-Hastings sampling scheme
to sample from this energy-based model using
bidirectional context and global attribute
features. We validate the effectiveness of our
approach on various controlled generation and
style-based text revision tasks by outperforming
recently proposed methods that involve extra
training, fine-tuning, or restrictive assumptions
over the form of models.",,,,, CoRR,  ,,"Title:Classifiers are Better Experts for Controllable Text Generation

 Recent work on controlled text generation has
either required attribute-based fine-tuning of the
base language model (LM), or has restricted the
parameterization of the attribute discriminator
to be compatible with the base autoregressive
LM. In this work, we propose Mix and Match
LM, a global score-based alternative for con-
trollable text generation that combines arbitrary
pre-trained black-box models for achieving the
desired attributes in the generated text without
involving any fine-tuning or structural assump-
tions about the black-box models. We interpret
the task of controllable generation as drawing
samples from an energy-based model whose
energy values are a linear combination of scores
from black-box models that are separately
responsible for fluency, the control attribute,
and faithfulness to any conditioning context.
We use a Metropolis-Hastings sampling scheme
to sample from this energy-based model using
bidirectional context and global attribute
features. We validate the effectiveness of our
approach on various controlled generation and
style-based text revision tasks by outperforming
recently proposed methods that involve extra
training, fine-tuning, or restrictive assumptions
over the form of models.",
"Vychegzhanin SV,Kotelnikov EV",,,Collocation2Text: Controllable Text Generation from Guide Phrases in Russian,abs/2206.09248,,10.48550/ARXIV.2206.09248 , Journal Article,2022.0,"Abstract:Large pre-trained language models are capable of generating varied and fluent texts. Starting from the prompt, these models generate a narrative that can develop unpredictably. The existing methods of controllable text generation, which guide the narrative in the text in the user-specified direction, require creating a training corpus and an additional time-consuming training procedure. The paper proposes and investigates Collocation2Text, a plug-and-play method for automatic controllable text generation in Russian, which does not require fine-tuning. The method is based on two interacting models: the autoregressive language ruGPT-3 model and the autoencoding language ruRoBERTa model. The idea of the method is to shift the output distribution of the autoregressive model according to the output distribution of the autoencoding model in order to ensure a coherent transition of the narrative in the text towards the guide phrase, which can contain single words or collocations. The autoencoding model, which is able to take into account the left and right contexts of the token, ""tells"" the autoregressive model which tokens are the most and least logical at the current generation step, increasing or decreasing the probabilities of the corresponding tokens. The experiments on generating news articles using the proposed method showed its effectiveness for automatically generated fluent texts which contain coherent transitions between user-specified phrases.",,,,, CoRR,  ,,"Title:Collocation2Text: Controllable Text Generation from Guide Phrases in Russian

 Abstract:Large pre-trained language models are capable of generating varied and fluent texts. Starting from the prompt, these models generate a narrative that can develop unpredictably. The existing methods of controllable text generation, which guide the narrative in the text in the user-specified direction, require creating a training corpus and an additional time-consuming training procedure. The paper proposes and investigates Collocation2Text, a plug-and-play method for automatic controllable text generation in Russian, which does not require fine-tuning. The method is based on two interacting models: the autoregressive language ruGPT-3 model and the autoencoding language ruRoBERTa model. The idea of the method is to shift the output distribution of the autoregressive model according to the output distribution of the autoencoding model in order to ensure a coherent transition of the narrative in the text towards the guide phrase, which can contain single words or collocations. The autoencoding model, which is able to take into account the left and right contexts of the token, ""tells"" the autoregressive model which tokens are the most and least logical at the current generation step, increasing or decreasing the probabilities of the corresponding tokens. The experiments on generating news articles using the proposed method showed its effectiveness for automatically generated fluent texts which contain coherent transitions between user-specified phrases.",
"Huang S,Ma S,Li Y,Li Y,Lin S,Zheng HT,Shen Y",,,Towards Attribute-Entangled Controllable Text Generation: A Pilot Study of Blessing Generation,abs/2210.16557,,10.48550/ARXIV.2210.16557 , Journal Article,2022.0,"AbstractControllable Text Generation (CTG) has obtained great success due to its fine-grained generation ability obtained by focusing on multiple attributes. However, most existing CTG researches overlook how to utilize the attribute entanglement to enhance the diversity of the controlled generated texts. Facing this dilemma, we focus on a novel CTG scenario, i.e., blessing generation which is challenging because high-quality blessing texts require CTG models to comprehensively consider the entanglement between multiple attributes (e.g., objects and occasions). To promote the research on blessing generation, we present EBleT, a large-scale Entangled Blessing Text dataset containing 293K English sentences annotated with multiple attributes. Furthermore, we propose novel evaluation metrics to measure the quality of the blessing texts generated by the baseline models we designed. Our study opens a new research direction for controllable text generation and enables the development of attribute-entangled CTG models.",,,,, CoRR,  ,,"Title:Towards Attribute-Entangled Controllable Text Generation: A Pilot Study of Blessing Generation

 AbstractControllable Text Generation (CTG) has obtained great success due to its fine-grained generation ability obtained by focusing on multiple attributes. However, most existing CTG researches overlook how to utilize the attribute entanglement to enhance the diversity of the controlled generated texts. Facing this dilemma, we focus on a novel CTG scenario, i.e., blessing generation which is challenging because high-quality blessing texts require CTG models to comprehensively consider the entanglement between multiple attributes (e.g., objects and occasions). To promote the research on blessing generation, we present EBleT, a large-scale Entangled Blessing Text dataset containing 293K English sentences annotated with multiple attributes. Furthermore, we propose novel evaluation metrics to measure the quality of the blessing texts generated by the baseline models we designed. Our study opens a new research direction for controllable text generation and enables the development of attribute-entangled CTG models.",
"Zhang K,Sun M,Sun J,Zhao B,Zhang K,Sun Z,Tan T",,,HumanDiffusion: a Coarse-to-Fine Alignment Diffusion Framework for Controllable Text-Driven Person Image Generation,abs/2211.06235,,10.48550/ARXIV.2211.06235 , Journal Article,2022.0,"Abstract:Text-driven person image generation is an emerging and challenging task in cross-modality image generation. Controllable person image generation promotes a wide range of applications such as digital human interaction and virtual try-on. However, previous methods mostly employ single-modality information as the prior condition (e.g. pose-guided person image generation), or utilize the preset words for text-driven human synthesis. Introducing a sentence composed of free words with an editable semantic pose map to describe person appearance is a more user-friendly way. In this paper, we propose HumanDiffusion, a coarse-to-fine alignment diffusion framework, for text-driven person image generation. Specifically, two collaborative modules are proposed, the Stylized Memory Retrieval (SMR) module for fine-grained feature distillation in data processing and the Multi-scale Cross-modality Alignment (MCA) module for coarse-to-fine feature alignment in diffusion. These two modules guarantee the alignment quality of the text and image, from image-level to feature-level, from low-resolution to high-resolution. As a result, HumanDiffusion realizes open-vocabulary person image generation with desired semantic poses. Extensive experiments conducted on DeepFashion demonstrate the superiority of our method compared with previous approaches. Moreover, better results could be obtained for complicated person images with various details and uncommon poses.",,,,, CoRR,  ,,"Title:HumanDiffusion: a Coarse-to-Fine Alignment Diffusion Framework for Controllable Text-Driven Person Image Generation

 Abstract:Text-driven person image generation is an emerging and challenging task in cross-modality image generation. Controllable person image generation promotes a wide range of applications such as digital human interaction and virtual try-on. However, previous methods mostly employ single-modality information as the prior condition (e.g. pose-guided person image generation), or utilize the preset words for text-driven human synthesis. Introducing a sentence composed of free words with an editable semantic pose map to describe person appearance is a more user-friendly way. In this paper, we propose HumanDiffusion, a coarse-to-fine alignment diffusion framework, for text-driven person image generation. Specifically, two collaborative modules are proposed, the Stylized Memory Retrieval (SMR) module for fine-grained feature distillation in data processing and the Multi-scale Cross-modality Alignment (MCA) module for coarse-to-fine feature alignment in diffusion. These two modules guarantee the alignment quality of the text and image, from image-level to feature-level, from low-resolution to high-resolution. As a result, HumanDiffusion realizes open-vocabulary person image generation with desired semantic poses. Extensive experiments conducted on DeepFashion demonstrate the superiority of our method compared with previous approaches. Moreover, better results could be obtained for complicated person images with various details and uncommon poses.",
"Gu N,Hahnloser RH",,,Controllable Citation Text Generation,abs/2211.07066,,10.48550/ARXIV.2211.07066 , Journal Article,2022.0,"AbstractThis work introduces Focused-Variation Network (FVN), a novel model to control language generation. The main problems in previous controlled language generation models range from the difficulty of generating text according to the given attributes, to the lack of diversity of the generated texts. FVN addresses these issues by learning disjoint discrete latent spaces for each attribute inside codebooks, which allows for both controllability and diversity, while at the same time generating fluent text. We evaluate FVN on two text generation datasets with annotated content and style, and show state-of-the-art performance as assessed by automatic and human evaluations.",,,,, CoRR,  ,,"Title:Controllable Citation Text Generation

 AbstractThis work introduces Focused-Variation Network (FVN), a novel model to control language generation. The main problems in previous controlled language generation models range from the difficulty of generating text according to the given attributes, to the lack of diversity of the generated texts. FVN addresses these issues by learning disjoint discrete latent spaces for each attribute inside codebooks, which allows for both controllability and diversity, while at the same time generating fluent text. We evaluate FVN on two text generation datasets with annotated content and style, and show state-of-the-art performance as assessed by automatic and human evaluations.",
"Tu H,Li Y",,,An Overview on Controllable Text Generation via Variational Auto-Encoders,abs/2211.07954,,10.48550/ARXIV.2211.07954 , Journal Article,2022.0,"While variational autoencoders (VAEs) have
been widely applied in text generation tasks,
they are troubled by two challenges: insufﬁ-
cient representation capacity and poor control-
lability. The former results from the posterior
collapse and restrictive assumption, which im-
pede better representation learning. The lat-
ter arises as continuous latent variables in tra-
ditional formulations hinder VAEs from inter-
pretability and controllability.
In this paper,
we propose Dictionary Prior (DPrior), a new
data-driven prior that enjoys the merits of ex-
pressivity and controllability. To facilitate con-
trolled text generation with DPrior, we pro-
pose to employ contrastive learning to separate
the latent space into several parts. Extensive
experiments on both language modeling and
controlled text generation demonstrate the ef-
fectiveness of the proposed approach.",,,,, CoRR,  ,,"Title:An Overview on Controllable Text Generation via Variational Auto-Encoders

 While variational autoencoders (VAEs) have
been widely applied in text generation tasks,
they are troubled by two challenges: insufﬁ-
cient representation capacity and poor control-
lability. The former results from the posterior
collapse and restrictive assumption, which im-
pede better representation learning. The lat-
ter arises as continuous latent variables in tra-
ditional formulations hinder VAEs from inter-
pretability and controllability.
In this paper,
we propose Dictionary Prior (DPrior), a new
data-driven prior that enjoys the merits of ex-
pressivity and controllability. To facilitate con-
trolled text generation with DPrior, we pro-
pose to employ contrastive learning to separate
the latent space into several parts. Extensive
experiments on both language modeling and
controlled text generation demonstrate the ef-
fectiveness of the proposed approach.",
"Dong Z,Wei P,Lin L",,,DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning,abs/2211.11337,,10.48550/ARXIV.2211.11337 , Journal Article,2022.0,"Abstract:Large-scale text-to-image generation models have achieved remarkable progress in synthesizing high-quality, feature-rich images with high resolution guided by texts. However, these models often struggle with novel concepts, eg, new styles, object entities, etc. Although recent attempts have employed fine-tuning or prompt-tuning strategies to teach the pre-trained diffusion model novel concepts from a reference image set,they have the drawback of overfitting to the given reference images, particularly in one-shot applications, which is harmful to generate diverse and high-quality images while maintaining generation controllability.
To tackle this challenge, we present a simple yet effective method called DreamArtist, which employs a positive-negative prompt-tuning learning strategy. Specifically, DreamArtist incorporates both positive and negative embeddings and jointly trains them. The positive embedding aggressively captures the salient characteristics of the reference image to drive diversified generation and the negative embedding rectifies inadequacies from the positive embedding. It learns not only what is correct, but also what can be avoided or improved. We have conducted extensive experiments and evaluated the proposed method from image similarity and diversity, generation controllability, and style cloning. And our DreamArtist has achieved a superior generation performance over existing methods. Besides, our additional evaluation on extended tasks, including concept compositions and prompt-guided image editing, demonstrates its effectiveness for more applications.",,,,, CoRR,  ,,"Title:DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning

 Abstract:Large-scale text-to-image generation models have achieved remarkable progress in synthesizing high-quality, feature-rich images with high resolution guided by texts. However, these models often struggle with novel concepts, eg, new styles, object entities, etc. Although recent attempts have employed fine-tuning or prompt-tuning strategies to teach the pre-trained diffusion model novel concepts from a reference image set,they have the drawback of overfitting to the given reference images, particularly in one-shot applications, which is harmful to generate diverse and high-quality images while maintaining generation controllability.
To tackle this challenge, we present a simple yet effective method called DreamArtist, which employs a positive-negative prompt-tuning learning strategy. Specifically, DreamArtist incorporates both positive and negative embeddings and jointly trains them. The positive embedding aggressively captures the salient characteristics of the reference image to drive diversified generation and the negative embedding rectifies inadequacies from the positive embedding. It learns not only what is correct, but also what can be avoided or improved. We have conducted extensive experiments and evaluated the proposed method from image similarity and diversity, generation controllability, and style cloning. And our DreamArtist has achieved a superior generation performance over existing methods. Besides, our additional evaluation on extended tasks, including concept compositions and prompt-guided image editing, demonstrates its effectiveness for more applications.",
"Chen H,Li H,Chen D,Narasimhan K",,,Controllable Text Generation with Language Constraints,abs/2212.10466,,10.48550/ARXIV.2212.10466 , Journal Article,2022.0,"Abstract:We consider the task of text generation in language models with constraints specified in natural language. To this end, we first create a challenging benchmark Cognac that provides as input to the model a topic with example text, along with a constraint on text to be avoided. Unlike prior work, our benchmark contains knowledge-intensive constraints sourced from databases like Wordnet and Wikidata, which allows for straightforward evaluation while striking a balance between broad attribute-level and narrow lexical-level controls. We find that even state-of-the-art language models like GPT-3 fail often on this task, and propose a solution to leverage a language model's own internal knowledge to guide generation. Our method, called CognacGen, first queries the language model to generate guidance terms for a specified topic or constraint, and uses the guidance to modify the model's token generation probabilities. We propose three forms of guidance (binary verifier, top-k tokens, textual example), and employ prefix-tuning approaches to distill the guidance to tackle diverse natural language constraints. Through extensive empirical evaluations, we demonstrate that CognacGen can successfully generalize to unseen instructions and outperform competitive baselines in generating constraint conforming text.",,,,, CoRR,  ,,"Title:Controllable Text Generation with Language Constraints

 Abstract:We consider the task of text generation in language models with constraints specified in natural language. To this end, we first create a challenging benchmark Cognac that provides as input to the model a topic with example text, along with a constraint on text to be avoided. Unlike prior work, our benchmark contains knowledge-intensive constraints sourced from databases like Wordnet and Wikidata, which allows for straightforward evaluation while striking a balance between broad attribute-level and narrow lexical-level controls. We find that even state-of-the-art language models like GPT-3 fail often on this task, and propose a solution to leverage a language model's own internal knowledge to guide generation. Our method, called CognacGen, first queries the language model to generate guidance terms for a specified topic or constraint, and uses the guidance to modify the model's token generation probabilities. We propose three forms of guidance (binary verifier, top-k tokens, textual example), and employ prefix-tuning approaches to distill the guidance to tackle diverse natural language constraints. Through extensive empirical evaluations, we demonstrate that CognacGen can successfully generalize to unseen instructions and outperform competitive baselines in generating constraint conforming text.",
"Lu Y,Lin H,Xu J,Han X,Tang J,Li A,Sun L,Liao M,Chen S",,,Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction,,,10.18653/V1/2021.ACL-LONG.217 , Conference Paper,2021.0,"AbstractEvent extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",,,,,Association for Computational Linguistics ,"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021  ",,"Title:Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction

 AbstractEvent extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
"Shao H,Wang J,Lin H,Zhang X,Zhang A,Ji H,Abdelzaher TF",,,Controllable and Diverse Text Generation in E-commerce,,,10.1145/3442381.3449838 , Conference Paper,2021.0,"One of the challenges in text generation is to
control text generation as intended by the user.
Previous studies proposed specifying the key-
words that should be included in the generated
text. However, this approach is insufﬁcient to
generate text that reﬂect the user’s intent. For
example, placing an important keyword at the
beginning of the text would help attract the
reader’s attention; however, existing methods
do not enable such ﬂexible control. In this pa-
per, we tackle a novel task of controlling not
only keywords but also the position of each
keyword in the text generation. To this end, we
propose a task-independent method that uses
special tokens to control the relative position
of keywords. Experimental results on summa-
rization and story generation tasks show that
the proposed method can control keywords and
their positions. The experimental results also
demonstrate that controlling the keyword posi-
tions can generate summary texts that are closer
to the user’s intent than baseline.",,,,,ACM / IW3C2 ,"WWW '21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021  ",,"Title:Controllable and Diverse Text Generation in E-commerce

 One of the challenges in text generation is to
control text generation as intended by the user.
Previous studies proposed specifying the key-
words that should be included in the generated
text. However, this approach is insufﬁcient to
generate text that reﬂect the user’s intent. For
example, placing an important keyword at the
beginning of the text would help attract the
reader’s attention; however, existing methods
do not enable such ﬂexible control. In this pa-
per, we tackle a novel task of controlling not
only keywords but also the position of each
keyword in the text generation. To this end, we
propose a task-independent method that uses
special tokens to control the relative position
of keywords. Experimental results on summa-
rization and story generation tasks show that
the proposed method can control keywords and
their positions. The experimental results also
demonstrate that controlling the keyword posi-
tions can generate summary texts that are closer
to the user’s intent than baseline.",
"Shen C,Cheng L,Zhou R,Bing L,You Y,Si L",,,MReD: A Meta-Review Dataset for Controllable Text Generation,abs/2110.07474,, , Journal Article,2021.0,"[Over-
all, the reviewers found that the paper presents interesting ob-
servations and promising experimental results.]←STRENGTH
[However, they also raised concerns in their initial reviews,
regarding the clarity of the paper, its theoretical foundations
and its positioning (notably regarding the bias/variance tradeoff
of uncorrected n-step returns) and parts of the experimental
results. ]←WEAKNESS [In the absence of rebuttal or revised
manuscript from the authors, not much discussion was trig-
gered.]←REBUTTAL PROCESS [Based on the initial reviews,
the AC cannot recommend accepting this paper, but the au-
thors are encouraged to pursue this interesting research direc-
tion.]←DECISION
Table 1: An example of annotated meta-review. CATE-
GORY indicates the category of each sentence.
into three clusters: more-to-less, less-to-more, and
neck-to-neck. The more-to-less text generation
tasks output a concise piece of text from some
more abundant input, such as text summarization
(Tan et al., 2017; Kry´sci´nski et al., 2018). The less-
to-more generation tasks generate a more abundant
output from some obviously simpler input, such as
prompt-based story generation (Fan et al., 2018b).
The neck-to-neck generation aims at generating
an output text which conveys the same quantity
of knowledge as the input but in natural language,
such as typical RDF triples to text tasks (Gardent
et al., 2017).
To some extent, the existing task settings are not
so adequate because they do not have a deep un-
derstanding of the domains they are working on,
i.e., domain knowledge. Taking text summariza-
tion as an example, the most well-experimented
dataset CNN/Daily Mail (Nallapati et al., 2016)
is composed of the training pairs of news content
and human-written summary bullets. However, it
does not tell why a particular piece of news con-
tent should have that corresponding summary, for
example for the same earnings report, why one",,,,, CoRR,  ,,"Title:MReD: A Meta-Review Dataset for Controllable Text Generation

 [Over-
all, the reviewers found that the paper presents interesting ob-
servations and promising experimental results.]←STRENGTH
[However, they also raised concerns in their initial reviews,
regarding the clarity of the paper, its theoretical foundations
and its positioning (notably regarding the bias/variance tradeoff
of uncorrected n-step returns) and parts of the experimental
results. ]←WEAKNESS [In the absence of rebuttal or revised
manuscript from the authors, not much discussion was trig-
gered.]←REBUTTAL PROCESS [Based on the initial reviews,
the AC cannot recommend accepting this paper, but the au-
thors are encouraged to pursue this interesting research direc-
tion.]←DECISION
Table 1: An example of annotated meta-review. CATE-
GORY indicates the category of each sentence.
into three clusters: more-to-less, less-to-more, and
neck-to-neck. The more-to-less text generation
tasks output a concise piece of text from some
more abundant input, such as text summarization
(Tan et al., 2017; Kry´sci´nski et al., 2018). The less-
to-more generation tasks generate a more abundant
output from some obviously simpler input, such as
prompt-based story generation (Fan et al., 2018b).
The neck-to-neck generation aims at generating
an output text which conveys the same quantity
of knowledge as the input but in natural language,
such as typical RDF triples to text tasks (Gardent
et al., 2017).
To some extent, the existing task settings are not
so adequate because they do not have a deep un-
derstanding of the domains they are working on,
i.e., domain knowledge. Taking text summariza-
tion as an example, the most well-experimented
dataset CNN/Daily Mail (Nallapati et al., 2016)
is composed of the training pairs of news content
and human-written summary bullets. However, it
does not tell why a particular piece of news con-
tent should have that corresponding summary, for
example for the same earnings report, why one",
"Casas N,Fonollosa JA,Costa-jussà MR",,,Syntax-driven Iterative Expansion Language Models for Controllable Text Generation,,,10.18653/V1/2020.SPNLP-1.1 , Conference Paper,2020.0,"The dominant language modeling paradigm
handles text as a sequence of discrete to-
kens. While that approach can capture the la-
tent structure of the text, it is inherently con-
strained to sequential dynamics for text gener-
ation. We propose a new paradigm for intro-
ducing a syntactic inductive bias into neural
text generation, where the dependency parse
tree is used to drive the Transformer model to
generate sentences iteratively.
Our experiments show that this paradigm is
effective at text generation, with quality be-
tween LSTMs and Transformers, and compa-
rable diversity, requiring less than half their de-
coding steps, and its generation process allows
direct control over the syntactic constructions
of the generated text, enabling the induction of
stylistic variations.",,,,,Association for Computational Linguistics ,"Proceedings of the Fourth Workshop on Structured Prediction for NLP@EMNLP 2020, Online, November 20, 2020  ",,"Title:Syntax-driven Iterative Expansion Language Models for Controllable Text Generation

 The dominant language modeling paradigm
handles text as a sequence of discrete to-
kens. While that approach can capture the la-
tent structure of the text, it is inherently con-
strained to sequential dynamics for text gener-
ation. We propose a new paradigm for intro-
ducing a syntactic inductive bias into neural
text generation, where the dependency parse
tree is used to drive the Transformer model to
generate sentences iteratively.
Our experiments show that this paradigm is
effective at text generation, with quality be-
tween LSTMs and Transformers, and compa-
rable diversity, requiring less than half their de-
coding steps, and its generation process allows
direct control over the syntactic constructions
of the generated text, enabling the induction of
stylistic variations.",
"Prabhumoye S,Black AW,Salakhutdinov R",,,Exploring Controllable Text Generation Techniques,,,10.18653/V1/2020.COLING-MAIN.1 , Conference Paper,2020.0,"AbstractNeural controllable text generation is an important area gaining attention due to its plethora of applications. Although there is a large body of prior work in controllable text generation, there is no unifying theme. In this work, we provide a new schema of the pipeline of the generation process by classifying it into five modules. The control of attributes in the generation process requires modification of these modules. We present an overview of different techniques used to perform the modulation of these modules. We also provide an analysis on the advantages and disadvantages of these techniques. We further pave ways to develop new architectures based on the combination of the modules described in this paper.",,,,,International Committee on Computational Linguistics ,"Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020  ",,"Title:Exploring Controllable Text Generation Techniques

 AbstractNeural controllable text generation is an important area gaining attention due to its plethora of applications. Although there is a large body of prior work in controllable text generation, there is no unifying theme. In this work, we provide a new schema of the pipeline of the generation process by classifying it into five modules. The control of attributes in the generation process requires modification of these modules. We present an overview of different techniques used to perform the modulation of these modules. We also provide an analysis on the advantages and disadvantages of these techniques. We further pave ways to develop new architectures based on the combination of the modules described in this paper.",
"Shu L,Papangelis A,Wang YC,Tür G,Xu H,Feizollahi Z,Liu B,Molino P",,,Controllable Text Generation with Focused Variation,EMNLP 2020,,10.18653/V1/2020.FINDINGS-EMNLP.339 , Conference Paper,2020.0,"This work introduces Focused-Variation Net-
work (FVN), a novel model to control lan-
guage generation. The main problems in pre-
vious controlled language generation models
range from the difﬁculty of generating text ac-
cording to the given attributes, to the lack of di-
versity of the generated texts. FVN addresses
these issues by learning disjoint discrete la-
tent spaces for each attribute inside codebooks,
which allows for both controllability and diver-
sity, while at the same time generating ﬂuent
text. We evaluate FVN on two text generation
datasets with annotated content and style, and
show state-of-the-art performance as assessed
by automatic and human evaluations.",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020  ",,"Title:Controllable Text Generation with Focused Variation

 This work introduces Focused-Variation Net-
work (FVN), a novel model to control lan-
guage generation. The main problems in pre-
vious controlled language generation models
range from the difﬁculty of generating text ac-
cording to the given attributes, to the lack of di-
versity of the generated texts. FVN addresses
these issues by learning disjoint discrete la-
tent spaces for each attribute inside codebooks,
which allows for both controllability and diver-
sity, while at the same time generating ﬂuent
text. We evaluate FVN on two text generation
datasets with annotated content and style, and
show state-of-the-art performance as assessed
by automatic and human evaluations.",
"Kedzie C,McKeown KR",,,Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies,,,10.18653/V1/2020.EMNLP-MAIN.419 , Conference Paper,2020.0,"AbstractWe study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation. Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness. Additionally, we evaluate how a phrase-based data augmentation method can improve performance. We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model. Data augmentation further improves control on difficult, randomly generated utterance plans.",,,,,Association for Computational Linguistics ,"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020  ",,"Title:Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies

 AbstractWe study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation. Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness. Additionally, we evaluate how a phrase-based data augmentation method can improve performance. We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model. Data augmentation further improves control on difficult, randomly generated utterance plans.",
"Yang Z,Gong B,Li Y,Yang J,Hu Z,Huang Y",,,Graph-Stega: Semantic Controllable Steganographic Text Generation Guided by Knowledge Graph,abs/2006.08339,, , Journal Article,2020.0,"Abstract:Most of the existing text generative steganographic methods are based on coding the conditional probability distribution of each word during the generation process, and then selecting specific words according to the secret information, so as to achieve information hiding. Such methods have their limitations which may bring potential security risks. Firstly, with the increase of embedding rate, these models will choose words with lower conditional probability, which will reduce the quality of the generated steganographic texts; secondly, they can not control the semantic expression of the final generated steganographic text. This paper proposes a new text generative steganography method which is quietly different from the existing models. We use a Knowledge Graph (KG) to guide the generation of steganographic sentences. On the one hand, we hide the secret information by coding the path in the knowledge graph, but not the conditional probability of each generated word; on the other hand, we can control the semantic expression of the generated steganographic text to a certain extent. The experimental results show that the proposed model can guarantee both the quality of the generated text and its semantic expression, which is a supplement and improvement to the current text generation steganography.",,,,, CoRR,  ,,"Title:Graph-Stega: Semantic Controllable Steganographic Text Generation Guided by Knowledge Graph

 Abstract:Most of the existing text generative steganographic methods are based on coding the conditional probability distribution of each word during the generation process, and then selecting specific words according to the secret information, so as to achieve information hiding. Such methods have their limitations which may bring potential security risks. Firstly, with the increase of embedding rate, these models will choose words with lower conditional probability, which will reduce the quality of the generated steganographic texts; secondly, they can not control the semantic expression of the final generated steganographic text. This paper proposes a new text generative steganography method which is quietly different from the existing models. We use a Knowledge Graph (KG) to guide the generation of steganographic sentences. On the one hand, we hide the secret information by coding the path in the knowledge graph, but not the conditional probability of each generated word; on the other hand, we can control the semantic expression of the generated steganographic text to a certain extent. The experimental results show that the proposed model can guarantee both the quality of the generated text and its semantic expression, which is a supplement and improvement to the current text generation steganography.",
"Mishra A,Chowdhury MF,Manohar S,Gutfreund D,Sankaranarayanan K",,,Template Controllable keywords-to-text Generation,abs/2011.03722,, , Journal Article,2020.0,"Neural controllable text generation is an important area gaining attention due to its plethora of
applications. Although there is a large body of prior work in controllable text generation, there
is no unifying theme. In this work, we provide a new schema of the pipeline of the generation
process by classifying it into ﬁve modules. The control of attributes in the generation process
requires modiﬁcation of these modules. We present an overview of different techniques used to
perform the modulation of these modules. We also provide an analysis on the advantages and
disadvantages of these techniques. We further pave ways to develop new architectures based on
the combination of the modules described in this paper.
Introduction
Controllable text generation is the task of generating natural sentences whose attributes can be controlled.
The attributes to control can range from being stylistic such politeness, sentiment, formality, etc.; demo-
graphic attributes of the person writing the text such as gender, age, etc.; content such as information,
keywords, entities, etc.; ordering of information, events, like plot summaries etc. Controlling various
attributes of text generation has manifold applications. For instance in dialogue response generation task,
work has been done in controlling persona (Zhang et al., 2018; Li et al., 2016b), controlling various
aspects of the response such as politeness (Niu and Bansal, 2018), formality, authority etc, grounding the
responses in external source of information (Zhou et al., 2018; Dinan et al., 2018; Ghazvininejad et al.,
2018), and controlling topic sequence (Tang et al., 2019; Prabhumoye et al., 2020). Another application is
story generation where you can control the ending (Peng et al., 2018), the persona (Chandu et al., 2019),
the plot (Yao et al., 2019), and the topic sequence (Huang et al., 2019). Controllable text generation is
also used to modulate the formality and politeness of emails (Madaan et al., 2020). Report generation
can be controlled by pulling disparate source documents into a coherent uniﬁed whole, which can use a
shared set of sources such as Wikipedia article generation (Liu et al., 2018; Prabhumoye et al., 2019).
Although there is a large body of prior work in controllable text generation, there is no unifying theme.
Each work addresses a speciﬁc task in a speciﬁc context. In this paper we outline a new schema which
connects prior work and provides an insight into various aspects of controllable text generation. The
schema contains ﬁve modules that cover the overall generation pipeline and provide an understanding of
the effect of each component on the generation process. Prior work has focused on speciﬁc parts of the
schema that we outline here and we provide insights into their similarities. We provide an overview of
these modules and also present an exploration of the various techniques used to control and update each
of these modules.
Most of the controllable text generation tasks can be framed as conditional language generation tasks.
They have an input or a source sequence U and an output or a target sequence Y to be generated. In
this case, we model the probability of the target sequence conditioned on the source sequence given by
P(Y|U) = �T
t=1 P(yt|U, y<t). The generation of the target tokens of the sequence Y unfolds as a time
series where each token yt is generated at a time step t. At a given time step t, a generative model takes
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/.",,,,, CoRR,  ,,"Title:Template Controllable keywords-to-text Generation

 Neural controllable text generation is an important area gaining attention due to its plethora of
applications. Although there is a large body of prior work in controllable text generation, there
is no unifying theme. In this work, we provide a new schema of the pipeline of the generation
process by classifying it into ﬁve modules. The control of attributes in the generation process
requires modiﬁcation of these modules. We present an overview of different techniques used to
perform the modulation of these modules. We also provide an analysis on the advantages and
disadvantages of these techniques. We further pave ways to develop new architectures based on
the combination of the modules described in this paper.
Introduction
Controllable text generation is the task of generating natural sentences whose attributes can be controlled.
The attributes to control can range from being stylistic such politeness, sentiment, formality, etc.; demo-
graphic attributes of the person writing the text such as gender, age, etc.; content such as information,
keywords, entities, etc.; ordering of information, events, like plot summaries etc. Controlling various
attributes of text generation has manifold applications. For instance in dialogue response generation task,
work has been done in controlling persona (Zhang et al., 2018; Li et al., 2016b), controlling various
aspects of the response such as politeness (Niu and Bansal, 2018), formality, authority etc, grounding the
responses in external source of information (Zhou et al., 2018; Dinan et al., 2018; Ghazvininejad et al.,
2018), and controlling topic sequence (Tang et al., 2019; Prabhumoye et al., 2020). Another application is
story generation where you can control the ending (Peng et al., 2018), the persona (Chandu et al., 2019),
the plot (Yao et al., 2019), and the topic sequence (Huang et al., 2019). Controllable text generation is
also used to modulate the formality and politeness of emails (Madaan et al., 2020). Report generation
can be controlled by pulling disparate source documents into a coherent uniﬁed whole, which can use a
shared set of sources such as Wikipedia article generation (Liu et al., 2018; Prabhumoye et al., 2019).
Although there is a large body of prior work in controllable text generation, there is no unifying theme.
Each work addresses a speciﬁc task in a speciﬁc context. In this paper we outline a new schema which
connects prior work and provides an insight into various aspects of controllable text generation. The
schema contains ﬁve modules that cover the overall generation pipeline and provide an understanding of
the effect of each component on the generation process. Prior work has focused on speciﬁc parts of the
schema that we outline here and we provide insights into their similarities. We provide an overview of
these modules and also present an exploration of the various techniques used to control and update each
of these modules.
Most of the controllable text generation tasks can be framed as conditional language generation tasks.
They have an input or a source sequence U and an output or a target sequence Y to be generated. In
this case, we model the probability of the target sequence conditioned on the source sequence given by
P(Y|U) = �T
t=1 P(yt|U, y<t). The generation of the target tokens of the sequence Y unfolds as a time
series where each token yt is generated at a time step t. At a given time step t, a generative model takes
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/.",
"Shen X,Suzuki J,Inui K,Su H,Klakow D,Sekine S",,,Select and Attend: Towards Controllable Content Selection in Text Generation,,,10.18653/V1/D19-1054 , Conference Paper,2019.0,"AbstractIn this work we study user controlled table-to-text generation where users explore the content in a table by selecting cells and reading a natural language description thereof automatically produce by a natural language generator. Such generation models usually learn from carefully selected cell combinations (clean cell selections); however, in practice users may select unexpected, redundant, or incoherent cell combinations (noisy cell selections). In experiments, we find that models perform well on test sets coming from the same distribution as the train data but their performance drops when evaluated on realistic noisy user inputs. We propose a fine-tuning regime with additional user-simulated noisy cell selections. Models fine-tuned with the proposed regime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test cases; and achieve comparable state-of-the-art performance on the ToTTo dataset.",,,,,Association for Computational Linguistics ,"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019  ",,"Title:Select and Attend: Towards Controllable Content Selection in Text Generation

 AbstractIn this work we study user controlled table-to-text generation where users explore the content in a table by selecting cells and reading a natural language description thereof automatically produce by a natural language generator. Such generation models usually learn from carefully selected cell combinations (clean cell selections); however, in practice users may select unexpected, redundant, or incoherent cell combinations (noisy cell selections). In experiments, we find that models perform well on test sets coming from the same distribution as the train data but their performance drops when evaluated on realistic noisy user inputs. We propose a fine-tuning regime with additional user-simulated noisy cell selections. Models fine-tuned with the proposed regime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test cases; and achieve comparable state-of-the-art performance on the ToTTo dataset.",
"Li B,Qi X,Lukasiewicz T,Torr PH",,,Controllable Text-to-Image Generation,,, , Conference Paper,2019.0,"Abstract:In this paper, we propose a novel controllable text-to-image generative adversarial network (ControlGAN), which can effectively synthesise high-quality images and also control parts of the image generation according to natural language descriptions. To achieve this, we introduce a word-level spatial and channel-wise attention-driven generator that can disentangle different visual attributes, and allow the model to focus on generating and manipulating subregions corresponding to the most relevant words. Also, a word-level discriminator is proposed to provide fine-grained supervisory feedback by correlating words with image regions, facilitating training an effective generator which is able to manipulate specific visual attributes without affecting the generation of other content. Furthermore, perceptual loss is adopted to reduce the randomness involved in the image generation, and to encourage the generator to manipulate specific attributes required in the modified text. Extensive experiments on benchmark datasets demonstrate that our method outperforms existing state of the art, and is able to effectively manipulate synthetic images using natural language descriptions. Code is available at this https URL.",,,,, ,"Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada  ",,"Title:Controllable Text-to-Image Generation

 Abstract:In this paper, we propose a novel controllable text-to-image generative adversarial network (ControlGAN), which can effectively synthesise high-quality images and also control parts of the image generation according to natural language descriptions. To achieve this, we introduce a word-level spatial and channel-wise attention-driven generator that can disentangle different visual attributes, and allow the model to focus on generating and manipulating subregions corresponding to the most relevant words. Also, a word-level discriminator is proposed to provide fine-grained supervisory feedback by correlating words with image regions, facilitating training an effective generator which is able to manipulate specific visual attributes without affecting the generation of other content. Furthermore, perceptual loss is adopted to reduce the randomness involved in the image generation, and to encourage the generator to manipulate specific attributes required in the modified text. Extensive experiments on benchmark datasets demonstrate that our method outperforms existing state of the art, and is able to effectively manipulate synthetic images using natural language descriptions. Code is available at this https URL.",
"Xu P,Cao Y,Cheung JC",,,Unsupervised Controllable Text Generation with Global Variation Discovery and Disentanglement,abs/1905.11975,, , Journal Article,2019.0,"Neural controllable text generation is an important area gaining attention due to its plethora of
applications. Although there is a large body of prior work in controllable text generation, there
is no unifying theme. In this work, we provide a new schema of the pipeline of the generation
process by classifying it into ﬁve modules. The control of attributes in the generation process
requires modiﬁcation of these modules. We present an overview of different techniques used to
perform the modulation of these modules. We also provide an analysis on the advantages and
disadvantages of these techniques. We further pave ways to develop new architectures based on
the combination of the modules described in this paper.
Introduction
Controllable text generation is the task of generating natural sentences whose attributes can be controlled.
The attributes to control can range from being stylistic such politeness, sentiment, formality, etc.; demo-
graphic attributes of the person writing the text such as gender, age, etc.; content such as information,
keywords, entities, etc.; ordering of information, events, like plot summaries etc. Controlling various
attributes of text generation has manifold applications. For instance in dialogue response generation task,
work has been done in controlling persona (Zhang et al., 2018; Li et al., 2016b), controlling various
aspects of the response such as politeness (Niu and Bansal, 2018), formality, authority etc, grounding the
responses in external source of information (Zhou et al., 2018; Dinan et al., 2018; Ghazvininejad et al.,
2018), and controlling topic sequence (Tang et al., 2019; Prabhumoye et al., 2020). Another application is
story generation where you can control the ending (Peng et al., 2018), the persona (Chandu et al., 2019),
the plot (Yao et al., 2019), and the topic sequence (Huang et al., 2019). Controllable text generation is
also used to modulate the formality and politeness of emails (Madaan et al., 2020). Report generation
can be controlled by pulling disparate source documents into a coherent uniﬁed whole, which can use a
shared set of sources such as Wikipedia article generation (Liu et al., 2018; Prabhumoye et al., 2019).
Although there is a large body of prior work in controllable text generation, there is no unifying theme.
Each work addresses a speciﬁc task in a speciﬁc context. In this paper we outline a new schema which
connects prior work and provides an insight into various aspects of controllable text generation. The
schema contains ﬁve modules that cover the overall generation pipeline and provide an understanding of
the effect of each component on the generation process. Prior work has focused on speciﬁc parts of the
schema that we outline here and we provide insights into their similarities. We provide an overview of
these modules and also present an exploration of the various techniques used to control and update each
of these modules.
Most of the controllable text generation tasks can be framed as conditional language generation tasks.
They have an input or a source sequence U and an output or a target sequence Y to be generated. In
this case, we model the probability of the target sequence conditioned on the source sequence given by
P(Y|U) = �T
t=1 P(yt|U, y<t). The generation of the target tokens of the sequence Y unfolds as a time
series where each token yt is generated at a time step t. At a given time step t, a generative model takes
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/.",,,,, CoRR,  ,,"Title:Unsupervised Controllable Text Generation with Global Variation Discovery and Disentanglement

 Neural controllable text generation is an important area gaining attention due to its plethora of
applications. Although there is a large body of prior work in controllable text generation, there
is no unifying theme. In this work, we provide a new schema of the pipeline of the generation
process by classifying it into ﬁve modules. The control of attributes in the generation process
requires modiﬁcation of these modules. We present an overview of different techniques used to
perform the modulation of these modules. We also provide an analysis on the advantages and
disadvantages of these techniques. We further pave ways to develop new architectures based on
the combination of the modules described in this paper.
Introduction
Controllable text generation is the task of generating natural sentences whose attributes can be controlled.
The attributes to control can range from being stylistic such politeness, sentiment, formality, etc.; demo-
graphic attributes of the person writing the text such as gender, age, etc.; content such as information,
keywords, entities, etc.; ordering of information, events, like plot summaries etc. Controlling various
attributes of text generation has manifold applications. For instance in dialogue response generation task,
work has been done in controlling persona (Zhang et al., 2018; Li et al., 2016b), controlling various
aspects of the response such as politeness (Niu and Bansal, 2018), formality, authority etc, grounding the
responses in external source of information (Zhou et al., 2018; Dinan et al., 2018; Ghazvininejad et al.,
2018), and controlling topic sequence (Tang et al., 2019; Prabhumoye et al., 2020). Another application is
story generation where you can control the ending (Peng et al., 2018), the persona (Chandu et al., 2019),
the plot (Yao et al., 2019), and the topic sequence (Huang et al., 2019). Controllable text generation is
also used to modulate the formality and politeness of emails (Madaan et al., 2020). Report generation
can be controlled by pulling disparate source documents into a coherent uniﬁed whole, which can use a
shared set of sources such as Wikipedia article generation (Liu et al., 2018; Prabhumoye et al., 2019).
Although there is a large body of prior work in controllable text generation, there is no unifying theme.
Each work addresses a speciﬁc task in a speciﬁc context. In this paper we outline a new schema which
connects prior work and provides an insight into various aspects of controllable text generation. The
schema contains ﬁve modules that cover the overall generation pipeline and provide an understanding of
the effect of each component on the generation process. Prior work has focused on speciﬁc parts of the
schema that we outline here and we provide insights into their similarities. We provide an overview of
these modules and also present an exploration of the various techniques used to control and update each
of these modules.
Most of the controllable text generation tasks can be framed as conditional language generation tasks.
They have an input or a source sequence U and an output or a target sequence Y to be generated. In
this case, we model the probability of the target sequence conditioned on the source sequence given by
P(Y|U) = �T
t=1 P(yt|U, y<t). The generation of the target tokens of the sequence Y unfolds as a time
series where each token yt is generated at a time step t. At a given time step t, a generative model takes
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/.",
"Hu Z,Yang Z,Liang X,Salakhutdinov R,Xing EP",,,Controllable Text Generation,abs/1703.00955,, , Journal Article,2017.0,"Neural controllable text generation is an important area gaining attention due to its plethora of
applications. Although there is a large body of prior work in controllable text generation, there
is no unifying theme. In this work, we provide a new schema of the pipeline of the generation
process by classifying it into ﬁve modules. The control of attributes in the generation process
requires modiﬁcation of these modules. We present an overview of different techniques used to
perform the modulation of these modules. We also provide an analysis on the advantages and
disadvantages of these techniques. We further pave ways to develop new architectures based on
the combination of the modules described in this paper.
Introduction
Controllable text generation is the task of generating natural sentences whose attributes can be controlled.
The attributes to control can range from being stylistic such politeness, sentiment, formality, etc.; demo-
graphic attributes of the person writing the text such as gender, age, etc.; content such as information,
keywords, entities, etc.; ordering of information, events, like plot summaries etc. Controlling various
attributes of text generation has manifold applications. For instance in dialogue response generation task,
work has been done in controlling persona (Zhang et al., 2018; Li et al., 2016b), controlling various
aspects of the response such as politeness (Niu and Bansal, 2018), formality, authority etc, grounding the
responses in external source of information (Zhou et al., 2018; Dinan et al., 2018; Ghazvininejad et al.,
2018), and controlling topic sequence (Tang et al., 2019; Prabhumoye et al., 2020). Another application is
story generation where you can control the ending (Peng et al., 2018), the persona (Chandu et al., 2019),
the plot (Yao et al., 2019), and the topic sequence (Huang et al., 2019). Controllable text generation is
also used to modulate the formality and politeness of emails (Madaan et al., 2020). Report generation
can be controlled by pulling disparate source documents into a coherent uniﬁed whole, which can use a
shared set of sources such as Wikipedia article generation (Liu et al., 2018; Prabhumoye et al., 2019).
Although there is a large body of prior work in controllable text generation, there is no unifying theme.
Each work addresses a speciﬁc task in a speciﬁc context. In this paper we outline a new schema which
connects prior work and provides an insight into various aspects of controllable text generation. The
schema contains ﬁve modules that cover the overall generation pipeline and provide an understanding of
the effect of each component on the generation process. Prior work has focused on speciﬁc parts of the
schema that we outline here and we provide insights into their similarities. We provide an overview of
these modules and also present an exploration of the various techniques used to control and update each
of these modules.
Most of the controllable text generation tasks can be framed as conditional language generation tasks.
They have an input or a source sequence U and an output or a target sequence Y to be generated. In
this case, we model the probability of the target sequence conditioned on the source sequence given by
P(Y|U) = �T
t=1 P(yt|U, y<t). The generation of the target tokens of the sequence Y unfolds as a time
series where each token yt is generated at a time step t. At a given time step t, a generative model takes
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/.",,,,, CoRR,  ,,"Title:Controllable Text Generation

 Neural controllable text generation is an important area gaining attention due to its plethora of
applications. Although there is a large body of prior work in controllable text generation, there
is no unifying theme. In this work, we provide a new schema of the pipeline of the generation
process by classifying it into ﬁve modules. The control of attributes in the generation process
requires modiﬁcation of these modules. We present an overview of different techniques used to
perform the modulation of these modules. We also provide an analysis on the advantages and
disadvantages of these techniques. We further pave ways to develop new architectures based on
the combination of the modules described in this paper.
Introduction
Controllable text generation is the task of generating natural sentences whose attributes can be controlled.
The attributes to control can range from being stylistic such politeness, sentiment, formality, etc.; demo-
graphic attributes of the person writing the text such as gender, age, etc.; content such as information,
keywords, entities, etc.; ordering of information, events, like plot summaries etc. Controlling various
attributes of text generation has manifold applications. For instance in dialogue response generation task,
work has been done in controlling persona (Zhang et al., 2018; Li et al., 2016b), controlling various
aspects of the response such as politeness (Niu and Bansal, 2018), formality, authority etc, grounding the
responses in external source of information (Zhou et al., 2018; Dinan et al., 2018; Ghazvininejad et al.,
2018), and controlling topic sequence (Tang et al., 2019; Prabhumoye et al., 2020). Another application is
story generation where you can control the ending (Peng et al., 2018), the persona (Chandu et al., 2019),
the plot (Yao et al., 2019), and the topic sequence (Huang et al., 2019). Controllable text generation is
also used to modulate the formality and politeness of emails (Madaan et al., 2020). Report generation
can be controlled by pulling disparate source documents into a coherent uniﬁed whole, which can use a
shared set of sources such as Wikipedia article generation (Liu et al., 2018; Prabhumoye et al., 2019).
Although there is a large body of prior work in controllable text generation, there is no unifying theme.
Each work addresses a speciﬁc task in a speciﬁc context. In this paper we outline a new schema which
connects prior work and provides an insight into various aspects of controllable text generation. The
schema contains ﬁve modules that cover the overall generation pipeline and provide an understanding of
the effect of each component on the generation process. Prior work has focused on speciﬁc parts of the
schema that we outline here and we provide insights into their similarities. We provide an overview of
these modules and also present an exploration of the various techniques used to control and update each
of these modules.
Most of the controllable text generation tasks can be framed as conditional language generation tasks.
They have an input or a source sequence U and an output or a target sequence Y to be generated. In
this case, we model the probability of the target sequence conditioned on the source sequence given by
P(Y|U) = �T
t=1 P(yt|U, y<t). The generation of the target tokens of the sequence Y unfolds as a time
series where each token yt is generated at a time step t. At a given time step t, a generative model takes
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/.",
"Yu S,Lee C,Lee H,Yoon S",,,Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor,abs/2311.07430,,10.48550/ARXIV.2311.07430 , Journal Article,2023.0,"Abstract:Despite recent progress in language models, generating constrained text for specific domains remains a challenge, particularly when utilizing black-box models that lack domain-specific knowledge. In this paper, we introduce ScoPE (Score-based Progressive Editor) generation, a novel approach for controlled text generation for black-box language models. We employ ScoPE to facilitate text generation in the target domain by integrating it with language models through a cascading approach. Trained to enhance the target domain score of the edited text, ScoPE progressively edits intermediate output discrete tokens to align with the target attributes throughout the auto-regressive generation process of the language model. This iterative process guides subsequent steps to produce desired output texts for the target domain. Our experimental results on diverse controlled generations demonstrate that ScoPE effectively facilitates controlled text generation for black-box language models in both in-domain and out-of-domain conditions, which is challenging for existing methods.",,,,, CoRR,  ,,"Title:Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor

 Abstract:Despite recent progress in language models, generating constrained text for specific domains remains a challenge, particularly when utilizing black-box models that lack domain-specific knowledge. In this paper, we introduce ScoPE (Score-based Progressive Editor) generation, a novel approach for controlled text generation for black-box language models. We employ ScoPE to facilitate text generation in the target domain by integrating it with language models through a cascading approach. Trained to enhance the target domain score of the edited text, ScoPE progressively edits intermediate output discrete tokens to align with the target attributes throughout the auto-regressive generation process of the language model. This iterative process guides subsequent steps to produce desired output texts for the target domain. Our experimental results on diverse controlled generations demonstrate that ScoPE effectively facilitates controlled text generation for black-box language models in both in-domain and out-of-domain conditions, which is challenging for existing methods.",
"Dathathri S,Madotto A,Lan J,Hung J,Frank E,Molino P,Yosinski J,Liu R",,,Plug and Play Language Models: A Simple Approach to Controlled Text Generation,,, , Conference Paper,2020.0,"AbstractLarge pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.",,,,,OpenReview.net ,"8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020  ",,"Title:Plug and Play Language Models: A Simple Approach to Controlled Text Generation

 AbstractLarge pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.",
"Zhang X,Wan X",,,MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning,,,10.18653/V1/2023.ACL-LONG.11 , Conference Paper,2023.0,"Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MIL-Decoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning (MIL) network.MIL model is trained on a corpus with a toxicity label for each text to predict the overall toxicity and the toxicity of each token in its context. Intuitively, the MIL network computes a toxicity distribution over next tokens according to the generated context which supplements the original language model to avoid toxicity. We evaluate MIL-Decoding with automatic metrics and human evaluation, where MIL-Decoding outperforms other baselines in detoxification while it only hurts generation fluency a little bit.",,,,,Association for Computational Linguistics ,"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023  ",,"Title:MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning

 Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MIL-Decoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning (MIL) network.MIL model is trained on a corpus with a toxicity label for each text to predict the overall toxicity and the toxicity of each token in its context. Intuitively, the MIL network computes a toxicity distribution over next tokens according to the generated context which supplements the original language model to avoid toxicity. We evaluate MIL-Decoding with automatic metrics and human evaluation, where MIL-Decoding outperforms other baselines in detoxification while it only hurts generation fluency a little bit.",
"Leong CT,Cheng Y,Wang J,Wang J,Li W",,,Self-Detoxifying Language Models via Toxification Reversal,,, , Conference Paper,2023.0,"Language model detoxification aims to mini-
mize the risk of generating offensive or harmful
content in pretrained language models (PLMs)
for safer deployment. Existing methods can
be roughly categorized as finetuning-based and
decoding-based. However, the former is of-
ten resource-intensive, while the latter relies
on additional components and potentially com-
promises the generation fluency. In this pa-
per, we propose a more lightweight approach
that enables the PLM itself to achieve “self-
detoxification”. Our method is built upon the
observation that prepending a negative steer-
ing prompt can effectively induce PLMs to
generate toxic content. At the same time, we
are inspired by the recent research in the inter-
pretability field, which formulates the evolving
contextualized representations within the PLM
as an information stream facilitated by the at-
tention layers. Drawing on this idea, we devise
a method to identify the toxification direction
from the normal generation process to the one
prompted with the negative prefix, and then
steer the generation to the reversed direction by
manipulating the information movement within
the attention layers. Experimental results show
that our approach, without any fine-tuning or
extra components, can achieve comparable per-
formance with state-of-the-art methods.1",,,,,Association for Computational Linguistics ,"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023  ",,"Title:Self-Detoxifying Language Models via Toxification Reversal

 Language model detoxification aims to mini-
mize the risk of generating offensive or harmful
content in pretrained language models (PLMs)
for safer deployment. Existing methods can
be roughly categorized as finetuning-based and
decoding-based. However, the former is of-
ten resource-intensive, while the latter relies
on additional components and potentially com-
promises the generation fluency. In this pa-
per, we propose a more lightweight approach
that enables the PLM itself to achieve “self-
detoxification”. Our method is built upon the
observation that prepending a negative steer-
ing prompt can effectively induce PLMs to
generate toxic content. At the same time, we
are inspired by the recent research in the inter-
pretability field, which formulates the evolving
contextualized representations within the PLM
as an information stream facilitated by the at-
tention layers. Drawing on this idea, we devise
a method to identify the toxification direction
from the normal generation process to the one
prompted with the negative prefix, and then
steer the generation to the reversed direction by
manipulating the information movement within
the attention layers. Experimental results show
that our approach, without any fine-tuning or
extra components, can achieve comparable per-
formance with state-of-the-art methods.1",
"Tang Z,Zhou K,Wang P,Ding Y,Li J,Zhang M",,,Detoxify Language Model Step-by-Step,abs/2308.08295,,10.48550/ARXIV.2308.08295 , Journal Article,2023.0,"Despite advances in large pre-trained neural
language models, they are prone to gener-
ating toxic language, which brings security
risks to their applications. We introduce MIL-
Decoding, which detoxifies language models
at token-level by interpolating it with a trained
multiple instance learning (MIL) network. MIL
model is trained on a corpus with a toxicity la-
bel for each text to predict the overall toxicity
and the toxicity of each token in its context.
Intuitively, the MIL network computes a toxi-
city distribution over next tokens according to
the generated context which supplements the
original language model to avoid toxicity. We
evaluate MIL-Decoding with automatic metrics
and human evaluation, where MIL-Decoding
outperforms other baselines in detoxification
while it only hurts generation fluency a little
bit.",,,,, CoRR,  ,,"Title:Detoxify Language Model Step-by-Step

 Despite advances in large pre-trained neural
language models, they are prone to gener-
ating toxic language, which brings security
risks to their applications. We introduce MIL-
Decoding, which detoxifies language models
at token-level by interpolating it with a trained
multiple instance learning (MIL) network. MIL
model is trained on a corpus with a toxicity la-
bel for each text to predict the overall toxicity
and the toxicity of each token in its context.
Intuitively, the MIL network computes a toxi-
city distribution over next tokens according to
the generated context which supplements the
original language model to avoid toxicity. We
evaluate MIL-Decoding with automatic metrics
and human evaluation, where MIL-Decoding
outperforms other baselines in detoxification
while it only hurts generation fluency a little
bit.",
"Wang B,Ping W,Xiao C,Xu P,Patwary M,Shoeybi M,Li B,Anandkumar A,Catanzaro B",,,Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,,, , Conference Paper,2022.0,"Abstract:Pre-trained language models (LMs) are shown to easily generate toxic language. In this work, we systematically explore domain-adaptive training to reduce the toxicity of language models. We conduct this study on three dimensions: training corpus, model size, and parameter efficiency. For the training corpus, we propose to leverage the generative power of LMs and generate nontoxic datasets for domain-adaptive training, which mitigates the exposure bias and is shown to be more data-efficient than using a curated pre-training corpus. We demonstrate that the self-generation method consistently outperforms the existing baselines across various model sizes on both automatic and human evaluations, even when it uses a 1/3 smaller training corpus. We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never been studied before. We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify. We also explore parameter-efficient training methods for detoxification. We demonstrate that adding and training adapter-only layers in LMs not only saves a lot of parameters but also achieves a better trade-off between toxicity and perplexity than whole model adaptation for the large-scale models.",,,,, ,NeurIPS  ,,"Title:Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models

 Abstract:Pre-trained language models (LMs) are shown to easily generate toxic language. In this work, we systematically explore domain-adaptive training to reduce the toxicity of language models. We conduct this study on three dimensions: training corpus, model size, and parameter efficiency. For the training corpus, we propose to leverage the generative power of LMs and generate nontoxic datasets for domain-adaptive training, which mitigates the exposure bias and is shown to be more data-efficient than using a curated pre-training corpus. We demonstrate that the self-generation method consistently outperforms the existing baselines across various model sizes on both automatic and human evaluations, even when it uses a 1/3 smaller training corpus. We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never been studied before. We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify. We also explore parameter-efficient training methods for detoxification. We demonstrate that adding and training adapter-only layers in LMs not only saves a lot of parameters but also achieves a better trade-off between toxicity and perplexity than whole model adaptation for the large-scale models.",
"Welbl J,Glaese A,Uesato J,Dathathri S,Mellor J,Hendricks LA,Anderson K,Kohli P,Coppin B,Huang PS",,,Challenges in Detoxifying Language Models,,,10.18653/V1/2021.FINDINGS-EMNLP.210 , Conference Paper,2021.0,"AbstractLarge language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions—highlighting further the nuances involved in careful evaluation of LM toxicity.",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021  ",,"Title:Challenges in Detoxifying Language Models

 AbstractLarge language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions—highlighting further the nuances involved in careful evaluation of LM toxicity.",
"Ganguli D,Askell A,Schiefer N,Liao TI,Lukosiute K,Chen A,Goldie A,Mirhoseini A,Olsson C,Hernandez D,Drain D,Li D,Tran-Johnson E,Perez E,Kernion J,Kerr J,Mueller J,Landau J,Ndousse K,Nguyen K,Lovitt L,Sellitto M,Elhage N,Mercado N,DasSarma N,Rausch O,Lasenby R,Larson R,Ringer S,Kundu S,Kadavath S,Johnston S,Kravec S,Showk SE,Lanham T,Telleen-Lawton T,Henighan T,Hume T,Bai Y,Hatfield-Dodds Z,Mann B,Amodei D,Joseph N,McCandlish S,Brown T,Olah C,Clark J,Bowman SR,Kaplan J",,,The Capacity for Moral Self-Correction in Large Language Models,abs/2302.07459,,10.48550/ARXIV.2302.07459 , Journal Article,2023.0,"Abstract:We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to ""morally self-correct"" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",,,,, CoRR,  ,,"Title:The Capacity for Moral Self-Correction in Large Language Models

 Abstract:We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to ""morally self-correct"" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",
"Gou Z,Shao Z,Gong Y,Shen Y,Yang Y,Duan N,Chen W",,,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,abs/2305.11738,,10.48550/ARXIV.2305.11738 , Journal Article,2023.0,"Abstract:Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially ""black boxes"" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.",,,,, CoRR,  ,,"Title:CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing

 Abstract:Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially ""black boxes"" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.",
"Zhao T,Wei M,Preston JS,Poon H",,,Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision,abs/2306.16564,,10.48550/ARXIV.2306.16564 , Journal Article,2023.0,"Abstract:Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area. Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification. An important source of calibration stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align with LLM output as well as other weak supervision sources. The model assigns higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction and classification tasks in biomedical and general domains demonstrate that the proposed risk score is highly correlated with the actual LLM error rate. By using a dynamic prompting strategy based on the risk score, we observed significant accuracy improvement for off-the-shelf LLMs, boosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",,,,, CoRR,  ,,"Title:Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision

 Abstract:Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area. Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification. An important source of calibration stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align with LLM output as well as other weak supervision sources. The model assigns higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction and classification tasks in biomedical and general domains demonstrate that the proposed risk score is highly correlated with the actual LLM error rate. By using a dynamic prompting strategy based on the risk score, we observed significant accuracy improvement for off-the-shelf LLMs, boosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
"Pan L,Saxon M,Xu W,Nathani D,Wang X,Wang WY",,,Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies,abs/2308.03188,,10.48550/ARXIV.2308.03188 , Journal Article,2023.0,"Abstract:Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.",,,,, CoRR,  ,,"Title:Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies

 Abstract:Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.",
"Poncelet J,Van Hamme H",,,Unsupervised Accent Adaptation Through Masked Language Model Correction Of Discrete Self-Supervised Speech Units,abs/2309.13994,,10.48550/ARXIV.2309.13994 , Journal Article,2023.0,"Abstract
We propose a neural language modeling system based on low-rank adaptation
(LoRA) for speech recognition output rescoring. Although pretrained language
models (LMs) like BERT have shown superior performance in second-pass
rescoring, the high computational cost of scaling up the pretraining stage and
adapting the pretrained models to specific domains limit their practical use in
rescoring. Here we present a method based on low-rank decomposition to train a
rescoring BERT model and adapt it to new domains using only a fraction (0.08%)
of the pretrained parameters. These inserted matrices are optimized through a
discriminative training objective along with a correlation-based regularization
loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is
evaluated on LibriSpeech and internal datasets with decreased training times by
factors between 5.4 and 3.6.",,,,, CoRR,  ,,"Title:Unsupervised Accent Adaptation Through Masked Language Model Correction Of Discrete Self-Supervised Speech Units

 Abstract
We propose a neural language modeling system based on low-rank adaptation
(LoRA) for speech recognition output rescoring. Although pretrained language
models (LMs) like BERT have shown superior performance in second-pass
rescoring, the high computational cost of scaling up the pretraining stage and
adapting the pretrained models to specific domains limit their practical use in
rescoring. Here we present a method based on low-rank decomposition to train a
rescoring BERT model and adapt it to new domains using only a fraction (0.08%)
of the pretrained parameters. These inserted matrices are optimized through a
discriminative training objective along with a correlation-based regularization
loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is
evaluated on LibriSpeech and internal datasets with decreased training times by
factors between 5.4 and 3.6.",
"Huang J,Chen X,Mishra S,Zheng HS,Yu AW,Song X,Zhou D",,,Large Language Models Cannot Self-Correct Reasoning Yet,abs/2310.01798,,10.48550/ARXIV.2310.01798 , Journal Article,2023.0,"Abstract:Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.",,,,, CoRR,  ,,"Title:Large Language Models Cannot Self-Correct Reasoning Yet

 Abstract:Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.",
Krishna S,,,On the Intersection of Self-Correction and Trust in Language Models,abs/2311.02801,,10.48550/ARXIV.2311.02801 , Journal Article,2023.0,"Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex cognitive tasks. However, their complexity and lack of transparency have raised several trustworthiness concerns, including the propagation of misinformation and toxicity. Recent research has explored the self-correction capabilities of LLMs to enhance their performance. In this work, we investigate whether these self-correction capabilities can be harnessed to improve the trustworthiness of LLMs. We conduct experiments focusing on two key aspects of trustworthiness: truthfulness and toxicity. Our findings reveal that self-correction can lead to improvements in toxicity and truthfulness, but the extent of these improvements varies depending on the specific aspect of trustworthiness and the nature of the task. Interestingly, our study also uncovers instances of ""self-doubt"" in LLMs during the self-correction process, introducing a new set of challenges that need to be addressed.
        


PDF



Abstract",,,,, CoRR,  ,,"Title:On the Intersection of Self-Correction and Trust in Language Models

 Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex cognitive tasks. However, their complexity and lack of transparency have raised several trustworthiness concerns, including the propagation of misinformation and toxicity. Recent research has explored the self-correction capabilities of LLMs to enhance their performance. In this work, we investigate whether these self-correction capabilities can be harnessed to improve the trustworthiness of LLMs. We conduct experiments focusing on two key aspects of trustworthiness: truthfulness and toxicity. Our findings reveal that self-correction can lead to improvements in toxicity and truthfulness, but the extent of these improvements varies depending on the specific aspect of trustworthiness and the nature of the task. Interestingly, our study also uncovers instances of ""self-doubt"" in LLMs during the self-correction process, introducing a new set of challenges that need to be addressed.
        


PDF



Abstract",
"Ke L,Li X,Bisk Y,Holtzman A,Gan Z,Liu J,Gao J,Choi Y,Srinivasa SS",,,Tactical Rewind: Self-Correction via Backtracking in Vision-And-Language Navigation,,,10.1109/CVPR.2019.00690 , Conference Paper,2019.0,"Abstract:We present the Frontier Aware Search with backTracking (FAST) Navigator, a general framework for action decoding, that achieves state-of-the-art results on the Room-to-Room (R2R) Vision-and-Language navigation challenge of Anderson et. al. (2018). Given a natural language instruction and photo-realistic image views of a previously unseen environment, the agent was tasked with navigating from source to target location as quickly as possible. While all current approaches make local action decisions or score entire trajectories using beam search, ours balances local and global signals when exploring an unobserved environment. Importantly, this lets us act greedily but use global signals to backtrack when necessary. Applying FAST framework to existing state-of-the-art models achieved a 17% relative gain, an absolute 6% gain on Success rate weighted by Path Length (SPL).",,,,,Computer Vision Foundation / IEEE ,"IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019  ",,"Title:Tactical Rewind: Self-Correction via Backtracking in Vision-And-Language Navigation

 Abstract:We present the Frontier Aware Search with backTracking (FAST) Navigator, a general framework for action decoding, that achieves state-of-the-art results on the Room-to-Room (R2R) Vision-and-Language navigation challenge of Anderson et. al. (2018). Given a natural language instruction and photo-realistic image views of a previously unseen environment, the agent was tasked with navigating from source to target location as quickly as possible. While all current approaches make local action decisions or score entire trajectories using beam search, ours balances local and global signals when exploring an unobserved environment. Importantly, this lets us act greedily but use global signals to backtrack when necessary. Applying FAST framework to existing state-of-the-art models achieved a 17% relative gain, an absolute 6% gain on Success rate weighted by Path Length (SPL).",
"Bonfante G,Godfroy H,Marion JY",,,A construction of a self-modifiying language with a formal correction proof,,,10.1109/MALWARE.2017.8323962 , Conference Paper,2017.0,Verified Abstract Interpretation Techniques for Disassembling Low-level Self-modifying Code,,,,,IEEE Computer Society ,"12th International Conference on Malicious and Unwanted Software, MALWARE 2017, Fajardo, PR, USA, October 11-14, 2017  ",,"Title:A construction of a self-modifiying language with a formal correction proof

 Verified Abstract Interpretation Techniques for Disassembling Low-level Self-modifying Code",
"Ramscar M,Yarlett D",,,Linguistic Self-Correction in the Absence of Feedback: A New Approach to the Logical Problem of Language Acquisition,31,6,10.1080/03640210701703576 , Journal Article,2007.0,"In a series of studies children show increasing mastery of irregular plural forms (such as <jats:italic>mice</jats:italic>) simply by producing erroneous over‐regularized versions of them (such as <jats:italic>mouses</jats:italic>). We explain this phenomenon in terms of successive approximation in imitation: Children over‐regularize early in acquisition because the representations of frequent, regular plural forms develop more quickly, such that at the earliest stages of production they interfere with children's attempts to imitatively reproduce irregular forms they have heard in the input. As the strength of the representations that determine children's productions settle asymptotically, the early advantage for the frequent regular forms is negated, and children's attempts to imitate the irregular forms they have observed become more likely to succeed (a process that produces the classic U‐shape in children's acquisition of plural inflection). These data show that children can acquire correct linguistic behavior without feedback in a situation where, as a result of philosophical and linguistic analyses, it has often been argued that it is logically impossible for them to do so.",,,,, Cogn. Sci.,  ,,"Title:Linguistic Self-Correction in the Absence of Feedback: A New Approach to the Logical Problem of Language Acquisition

 In a series of studies children show increasing mastery of irregular plural forms (such as <jats:italic>mice</jats:italic>) simply by producing erroneous over‐regularized versions of them (such as <jats:italic>mouses</jats:italic>). We explain this phenomenon in terms of successive approximation in imitation: Children over‐regularize early in acquisition because the representations of frequent, regular plural forms develop more quickly, such that at the earliest stages of production they interfere with children's attempts to imitatively reproduce irregular forms they have heard in the input. As the strength of the representations that determine children's productions settle asymptotically, the early advantage for the frequent regular forms is negated, and children's attempts to imitate the irregular forms they have observed become more likely to succeed (a process that produces the classic U‐shape in children's acquisition of plural inflection). These data show that children can acquire correct linguistic behavior without feedback in a situation where, as a result of philosophical and linguistic analyses, it has often been argued that it is logically impossible for them to do so.",
Moon N,,,The self-correction method for programming language tutoring system,1,4,10.5555/543121.543127 , Journal Article,2000.0,"One  view  of  self-correction  is  that  it  makes  a  virtue  out  of  necessity:  independent learners, having chosen to forego opportunities for assess-ing progress and correcting linguistic faults1 through classroom interac-tion  (Chaudron,  1983;  White,  1994,  1997),  simply  have  to  accept  their  misfortune, monitor their own progress without grumbling and quickly develop the ability to self-correct. However, as Hurd (2001) observes, the ‘teaching voice’ is present in the learning materials, which constitute the link  between  teacher  and  learner.  In  independent  contexts,  learners  are  accustomed to engaging with the materials without constant mediation by a tutor. Indeed, Little (1995) argues that even when there appears to be no social interaction, such as when a learner uses a textbook, the psycho-logical  process  involved  nevertheless  includes  a  covert,  internalised  version of it.Accordingly, an understanding of interaction and its associated cognitive processes can reveal much about second language acquisition (SLA) in these contexts. For instance, although classroom-based, the work of Aljaafreh and Lantolf (1994), who were among the fi  rst to investigate the fault correction/learning interface from within a theoretical, rather than a phenomenological stance,   is   highly   relevant.   Exploring   the   implications   for   learners   of   Vygotsky’s  ‘zone  of  proximal  development’  (ZPD)  (the  distance  between  learners’  actual  and  potential  development  levels),  a  concept  widely  dis-cussed  in  research  into  teacher-learner  interaction,  this  study  investigated  the  effect  of  negative  feedback  given  to  students  on  their  linguistic  faults.  Learning was found to improve when tutor-learner dialogue was increased, especially when intervention was graduated and contingent, and the help provided was ‘designed to discover the novice’s ZPD in order to offer the appropriate level of assistance and to encourage the learner to function at his or her potential level of ability’ (Aljaafreh & Lantolf, 1994: 468).",,,,, ACIS Int. J. Comput. Inf. Sci.,  ,,"Title:The self-correction method for programming language tutoring system

 One  view  of  self-correction  is  that  it  makes  a  virtue  out  of  necessity:  independent learners, having chosen to forego opportunities for assess-ing progress and correcting linguistic faults1 through classroom interac-tion  (Chaudron,  1983;  White,  1994,  1997),  simply  have  to  accept  their  misfortune, monitor their own progress without grumbling and quickly develop the ability to self-correct. However, as Hurd (2001) observes, the ‘teaching voice’ is present in the learning materials, which constitute the link  between  teacher  and  learner.  In  independent  contexts,  learners  are  accustomed to engaging with the materials without constant mediation by a tutor. Indeed, Little (1995) argues that even when there appears to be no social interaction, such as when a learner uses a textbook, the psycho-logical  process  involved  nevertheless  includes  a  covert,  internalised  version of it.Accordingly, an understanding of interaction and its associated cognitive processes can reveal much about second language acquisition (SLA) in these contexts. For instance, although classroom-based, the work of Aljaafreh and Lantolf (1994), who were among the fi  rst to investigate the fault correction/learning interface from within a theoretical, rather than a phenomenological stance,   is   highly   relevant.   Exploring   the   implications   for   learners   of   Vygotsky’s  ‘zone  of  proximal  development’  (ZPD)  (the  distance  between  learners’  actual  and  potential  development  levels),  a  concept  widely  dis-cussed  in  research  into  teacher-learner  interaction,  this  study  investigated  the  effect  of  negative  feedback  given  to  students  on  their  linguistic  faults.  Learning was found to improve when tutor-learner dialogue was increased, especially when intervention was graduated and contingent, and the help provided was ‘designed to discover the novice’s ZPD in order to offer the appropriate level of assistance and to encourage the learner to function at his or her potential level of ability’ (Aljaafreh & Lantolf, 1994: 468).",
"Liang, Xuyuan, Tian, Lihua, Li, Chen, Mandi, Zhang",,,CTG:A Controllable Text Generation Method based on the Joint Work of Language Model and Text Classifier,,,10.1109/CISCE52179.2021.9445873 , ,2021.0,"Text generation is an important research field in natural language processing. Today pre-trained models can generate text with high readability and fluency. However, the generated text content is usually not restricted, and the generated content cannot be controlled. Therefore, there is an urgent need for a controllable text generation method. Traditional controllable text generation methods usually have the disadvantages of high training cost and difficulty in adjustment. We propose a lightweight and easy-to-adjust method to update the language model through the discrimination results of the text classification model on the language model, thereby completing controllable text generation. And we have solved the problem of imbalance and quality degradation in text generation. Experiments show that the quality of the text generated by this method can also reach a higher level with strong subjectivity, can achieves the purpose of controlling text generation. And this method is easy to adjust, suitable for various scenarios.",,,,, ,"  2021 International Conference on Communications, Information System and Computer Engineering (CISCE)",,"Title:CTG:A Controllable Text Generation Method based on the Joint Work of Language Model and Text Classifier

 Text generation is an important research field in natural language processing. Today pre-trained models can generate text with high readability and fluency. However, the generated text content is usually not restricted, and the generated content cannot be controlled. Therefore, there is an urgent need for a controllable text generation method. Traditional controllable text generation methods usually have the disadvantages of high training cost and difficulty in adjustment. We propose a lightweight and easy-to-adjust method to update the language model through the discrimination results of the text classification model on the language model, thereby completing controllable text generation. And we have solved the problem of imbalance and quality degradation in text generation. Experiments show that the quality of the text generated by this method can also reach a higher level with strong subjectivity, can achieves the purpose of controlling text generation. And this method is easy to adjust, suitable for various scenarios.",
"Çağlayan, Cansen, Karakaya, Murat",,,Topic-Controlled Text Generation,,,10.1109/UBMK52708.2021.9558910 , ,2021.0,"Today, the text generation subject in the field of Natural Language Processing (NLP) has gained a lot of importance. In particular, the quality of the text generated with the emergence of new transformer-based models has reached high levels. In this way, controllable text generation has become an important research area. There are various methods applied for controllable text generation, but since these methods are mostly applied on Recurrent Neural Network (RNN) based encoder decoder models, which were used frequently, studies using transformer-based models are few. Transformer-based models are very successful in long sequences thanks to their parallel working ability. This study aimed to generate Turkish reviews on the desired topics by using a transformer-based language model. We used the method of adding the topic information to the sequential input. We concatenated input token embedding and topic embedding (control) at each time step during the training. As a result, we were able to create Turkish reviews on the specified topics.",,,,, ,  2021 6th International Conference on Computer Science and Engineering (UBMK),,"Title:Topic-Controlled Text Generation

 Today, the text generation subject in the field of Natural Language Processing (NLP) has gained a lot of importance. In particular, the quality of the text generated with the emergence of new transformer-based models has reached high levels. In this way, controllable text generation has become an important research area. There are various methods applied for controllable text generation, but since these methods are mostly applied on Recurrent Neural Network (RNN) based encoder decoder models, which were used frequently, studies using transformer-based models are few. Transformer-based models are very successful in long sequences thanks to their parallel working ability. This study aimed to generate Turkish reviews on the desired topics by using a transformer-based language model. We used the method of adding the topic information to the sequential input. We concatenated input token embedding and topic embedding (control) at each time step during the training. As a result, we were able to create Turkish reviews on the specified topics.",
"Xuyuan, Liang, Lihua, Tian, Chen, Li",,,TCTG:A Controllable Text Generation Method Using Text to Control Text Generation,,,10.1109/ICSIP52628.2021.9688767 , ,2021.0,"Text generation is an important research field in natural language processing, and current pre-trained models can generate text with very high readability and fluency. However, the content of the generated text cannot be controlled, and it is easy to generate text with weak relevance. Therefore, a controllable text generation method is urgently needed. Traditional controllable text generation methods often have the disadvantages of high training costs and difficult adjustments. This paper presents a method to control text generation by text. By extracting keywords from the data set built by yourself, the sequence of keywords is organized into text information that is controlled to be generated. Insert a module into the language model, use this module to make the initial text interact with the control text, and then perform subsequent generation. And two different training tasks are designed so that the model learns how to control text generation by controlling text information while maintaining a high degree of fluency and diversity of text generation. Experiments have shown that this method can better complete the task of controlling text generation through text, without greatly reducing the fluency and diversity of the generated text, and has a better control effect.",,,,, ,  2021 IEEE 6th International Conference on Signal and Image Processing (ICSIP),,"Title:TCTG:A Controllable Text Generation Method Using Text to Control Text Generation

 Text generation is an important research field in natural language processing, and current pre-trained models can generate text with very high readability and fluency. However, the content of the generated text cannot be controlled, and it is easy to generate text with weak relevance. Therefore, a controllable text generation method is urgently needed. Traditional controllable text generation methods often have the disadvantages of high training costs and difficult adjustments. This paper presents a method to control text generation by text. By extracting keywords from the data set built by yourself, the sequence of keywords is organized into text information that is controlled to be generated. Insert a module into the language model, use this module to make the initial text interact with the control text, and then perform subsequent generation. And two different training tasks are designed so that the model learns how to control text generation by controlling text information while maintaining a high degree of fluency and diversity of text generation. Experiments have shown that this method can better complete the task of controlling text generation through text, without greatly reducing the fluency and diversity of the generated text, and has a better control effect.",
"Imasato, Naomi, Miyazawa, Kazuki, Duncan, Caitlin, Nagai, Takayuki",,,Using a Language Model to Generate Music in Its Symbolic Domain While Controlling Its Perceived Emotion,,,10.1109/ACCESS.2023.3280603 , ,2023.0,"This work proposes a transformer-based model capable of generating music in its symbolic domain, in a controllable fashion. The ultimate goal of this is to build a system with which people can compose music collaboratively with a computer. Using an NLP model as a base (GPT-2), we take advantage of the similarities across symbolic music representation and written language to build a model capable of conditionally predicting musical sequences. Controllability is achieved without explicit programming for it, and does not require extensive retraining of the model. A study with 939 participants was performed to evaluate this controllability. The results of this suggest the proposed method is indeed effective and can be used to control the generation of music in its symbolic domain. The method itself is flexible to any desired “control”, but this work focuses specifically on the emotion conveyed when one listens to a piece of music.",,,,, ,  ,,"Title:Using a Language Model to Generate Music in Its Symbolic Domain While Controlling Its Perceived Emotion

 This work proposes a transformer-based model capable of generating music in its symbolic domain, in a controllable fashion. The ultimate goal of this is to build a system with which people can compose music collaboratively with a computer. Using an NLP model as a base (GPT-2), we take advantage of the similarities across symbolic music representation and written language to build a model capable of conditionally predicting musical sequences. Controllability is achieved without explicit programming for it, and does not require extensive retraining of the model. A study with 939 participants was performed to evaluate this controllability. The results of this suggest the proposed method is indeed effective and can be used to control the generation of music in its symbolic domain. The method itself is flexible to any desired “control”, but this work focuses specifically on the emotion conveyed when one listens to a piece of music.",
"Dugar, Abhinav, Singh, Gaurav, B, Navyasree, M, Anand Kumar",,,Unsupervised Abstractive Text Summarization with Length Controlled Autoencoder,,,10.1109/INDICON56171.2022.10040048 , ,2022.0,"This work deals with taking an unsupervised approach to abstractive text summarization where a large set of sentences is converted into a concise summary highlighting the essential details. This is achieved with the use of an adversarial autoencoder model. The model encodes the input to a smaller latent vector and the decoder decodes this latent code to generate the higher dimensional output with some loss. Unlike variational autoencoders, AAE’s use discriminators to learn using adversarial loss. K-Means clustering and language models are used to get the final summary. This model has been tested with different datasets like the Amazon, Rotten Tomatoes and Yelp reviews dataset to essentially do an opinion summarization task and this is finally evaluated using ROGUE-1, ROGUE-2,ROGUE-L and BLEU scores. The same task is also conducted on a dataset in Hindi. We obtain a ROGUE-1 score of around 24% for Amazon, Yelp and CNN/Daily Mail dataset and a score of 12% for Rotten Tomatoes while the score obtained for the Hindi news articles dataset is only 8%.",,,,, ,  2022 IEEE 19th India Council International Conference (INDICON),,"Title:Unsupervised Abstractive Text Summarization with Length Controlled Autoencoder

 This work deals with taking an unsupervised approach to abstractive text summarization where a large set of sentences is converted into a concise summary highlighting the essential details. This is achieved with the use of an adversarial autoencoder model. The model encodes the input to a smaller latent vector and the decoder decodes this latent code to generate the higher dimensional output with some loss. Unlike variational autoencoders, AAE’s use discriminators to learn using adversarial loss. K-Means clustering and language models are used to get the final summary. This model has been tested with different datasets like the Amazon, Rotten Tomatoes and Yelp reviews dataset to essentially do an opinion summarization task and this is finally evaluated using ROGUE-1, ROGUE-2,ROGUE-L and BLEU scores. The same task is also conducted on a dataset in Hindi. We obtain a ROGUE-1 score of around 24% for Amazon, Yelp and CNN/Daily Mail dataset and a score of 12% for Rotten Tomatoes while the score obtained for the Hindi news articles dataset is only 8%.",
"Mosallanezhad, Ahmadreza, Shu, Kai, Liu, Huan",,,Generating Topic-Preserving Synthetic News,,,10.1109/BigData52589.2021.9671623 , ,2021.0,"The text generation methods have witnessed great success in text summarization, machine translation, and synthetic news generation. However, these techniques may be abused to generate disinformation and fake news. To better understand the potential threats of synthetic news, we develop a novel generation method RLTG to generate topic-preserving news content. The majority of existing text generation methods are either controlled by specific attributes or lack topic consistency between the input claims and output news, making synthetic news less coherent and realistic. In this paper, we study the problem of topic-preserving synthetic news generation by proposing a novel deep reinforcement learning-based method to control the output of large pre-trained language models. Experiment results on real-world datasets demonstrate that the news contents generated by RLTG are topic-consistent and realistic.",,,,, ,  2021 IEEE International Conference on Big Data (Big Data),,"Title:Generating Topic-Preserving Synthetic News

 The text generation methods have witnessed great success in text summarization, machine translation, and synthetic news generation. However, these techniques may be abused to generate disinformation and fake news. To better understand the potential threats of synthetic news, we develop a novel generation method RLTG to generate topic-preserving news content. The majority of existing text generation methods are either controlled by specific attributes or lack topic consistency between the input claims and output news, making synthetic news less coherent and realistic. In this paper, we study the problem of topic-preserving synthetic news generation by proposing a novel deep reinforcement learning-based method to control the output of large pre-trained language models. Experiment results on real-world datasets demonstrate that the news contents generated by RLTG are topic-consistent and realistic.",
"Chang, Rong-Guey, Siao, Cheng-Yan, Lee, Chia-Ying",,,Postprocessing System for Word-Filling and Quantifiers Based on Chinese Story Generation,,,10.1109/ECICE55674.2022.10042910 , ,2022.0,"In recent years, many fields have been related to introducing artificial intelligence to natural language generation. Although these natural language models have excellent results and generate smooth sentences, they are still not effective learning features such as character relationships, especially in the Chinese language. When a sentence is generated, it is necessary to pay attention to the following words to correctly predict and generate, such as quantifiers, which causes the generated words to be inappropriate and then affects the generation of subsequent words. Therefore, we developed a set of attention mechanism enhancement models, aiming at the generative language model that controls the ordering of generated speech parts, revising the different mechanisms of character fighting relationship and quantifier design, and adopting the traditional classification model one-against-all support vector machine training. The results show that the generated sentences are arranged in the same order as the original ones so the generation can be reliably controlled.",,,,, ,"  2022 IEEE 4th Eurasia Conference on IOT, Communication and Engineering (ECICE)",,"Title:Postprocessing System for Word-Filling and Quantifiers Based on Chinese Story Generation

 In recent years, many fields have been related to introducing artificial intelligence to natural language generation. Although these natural language models have excellent results and generate smooth sentences, they are still not effective learning features such as character relationships, especially in the Chinese language. When a sentence is generated, it is necessary to pay attention to the following words to correctly predict and generate, such as quantifiers, which causes the generated words to be inappropriate and then affects the generation of subsequent words. Therefore, we developed a set of attention mechanism enhancement models, aiming at the generative language model that controls the ordering of generated speech parts, revising the different mechanisms of character fighting relationship and quantifier design, and adopting the traditional classification model one-against-all support vector machine training. The results show that the generated sentences are arranged in the same order as the original ones so the generation can be reliably controlled.",
"Saha, Tulika, Ananiadou, Sophia",,,Emotion-aware and Intent-controlled Empathetic Response Generation using Hierarchical Transformer Network,,,10.1109/IJCNN55064.2022.9892592 , ,2022.0,"Enriching any dialogue systems to exhibit empathy is fundamental for delivering human-like conversations. Empathetic interactions in the form of empathetic dialogue generation has been studied widely in recent times. Existing models either incorporate emotion as a feature at the encoding side or as a latent variable explicitly at the decoder to condition their response on. While understanding speaker emotion is integral to expressing empathy, another aspect of being empathetic necessitates responding with an appropriate emotion (also known as emotional regulating intents) to speaker's mental state. To integrate these multiple aspects, in this paper, we propose a Hierarchical Transformer Network (HTN), an amalgamation of the recently introduced Transformer model and Hierarchical Encoder Decoder (HRED) architecture to capture the speaker emotion and dialogic context. For generating intent controlled empathetic responses, we draw insights from Reinforcement Learning (RL) to optimize rewards implicitly. The proposed approach is demonstrated on two benchmark open-domain empathetic datasets. The empirical evaluation (both automated and manual) demonstrates the system capability by way of outperforming several baselines and the state of the art models.",,,,, ,  2022 International Joint Conference on Neural Networks (IJCNN),,"Title:Emotion-aware and Intent-controlled Empathetic Response Generation using Hierarchical Transformer Network

 Enriching any dialogue systems to exhibit empathy is fundamental for delivering human-like conversations. Empathetic interactions in the form of empathetic dialogue generation has been studied widely in recent times. Existing models either incorporate emotion as a feature at the encoding side or as a latent variable explicitly at the decoder to condition their response on. While understanding speaker emotion is integral to expressing empathy, another aspect of being empathetic necessitates responding with an appropriate emotion (also known as emotional regulating intents) to speaker's mental state. To integrate these multiple aspects, in this paper, we propose a Hierarchical Transformer Network (HTN), an amalgamation of the recently introduced Transformer model and Hierarchical Encoder Decoder (HRED) architecture to capture the speaker emotion and dialogic context. For generating intent controlled empathetic responses, we draw insights from Reinforcement Learning (RL) to optimize rewards implicitly. The proposed approach is demonstrated on two benchmark open-domain empathetic datasets. The empirical evaluation (both automated and manual) demonstrates the system capability by way of outperforming several baselines and the state of the art models.",
"Peng, Guan-Fu, Yang, Yi-Shian, Tsai, Chen-Yu, Soo, Von-Wun",,,Generate Modern Chinese Poems from News Based on Text Style Transfer Using GAN,,,10.1109/TAAI48200.2019.8959907 , ,2019.0,"In this paper, we investigate techniques that can transfer a news story into a poem. We train cycle-GAN that can conduct text style transfer from news style to poem style even lack of parallel corpus. We compare teacher forcing and free-running modes of training as well as different attention mechanisms in the GAN and cycle-GAN architectures. We found that there is a trade-off between degree of style transfer and content preserving that can be controlled by the ratio of reconstruction and transfer using different training modes of the discriminator and the generator. We show that both GAN and cycle-GAN can be trained to convert news into poems to some extent using non-parallel corpus.",,,,, ,  2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI),,"Title:Generate Modern Chinese Poems from News Based on Text Style Transfer Using GAN

 In this paper, we investigate techniques that can transfer a news story into a poem. We train cycle-GAN that can conduct text style transfer from news style to poem style even lack of parallel corpus. We compare teacher forcing and free-running modes of training as well as different attention mechanisms in the GAN and cycle-GAN architectures. We found that there is a trade-off between degree of style transfer and content preserving that can be controlled by the ratio of reconstruction and transfer using different training modes of the discriminator and the generator. We show that both GAN and cycle-GAN can be trained to convert news into poems to some extent using non-parallel corpus.",
"Safovich, Yuri, Azaria, Amos",,,Fiction Sentence Expansion and Enhancement via Focused Objective and Novelty Curve Sampling,,,10.1109/ICTAI50040.2020.00132 , ,2020.0,"We describe the task of sentence expansion and enhancement, in which a sentence provided by a human is expanded in some creative way. The expansion should be understandable, believably grammatical, and highly related to the original sentence. Sentence expansion and enhancement may serve as an authoring tool, or integrate in dynamic media, conversational agents, and advertising. We implement a neural sentence expander, which is trained on sentence compressions generated from a corpus of modern fiction. We modify the objective loss function to support the task by focusing on new words, and decode at test time with controlled curve-like novelty sampling. We run the sentence expander on sentences provided by human subjects and have humans evaluate these expansions. The generation methods are shown to be comparable to, and as well liked as, subjects' original input sentences, and preferred over baselines.",,,,, ,  2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),,"Title:Fiction Sentence Expansion and Enhancement via Focused Objective and Novelty Curve Sampling

 We describe the task of sentence expansion and enhancement, in which a sentence provided by a human is expanded in some creative way. The expansion should be understandable, believably grammatical, and highly related to the original sentence. Sentence expansion and enhancement may serve as an authoring tool, or integrate in dynamic media, conversational agents, and advertising. We implement a neural sentence expander, which is trained on sentence compressions generated from a corpus of modern fiction. We modify the objective loss function to support the task by focusing on new words, and decode at test time with controlled curve-like novelty sampling. We run the sentence expander on sentences provided by human subjects and have humans evaluate these expansions. The generation methods are shown to be comparable to, and as well liked as, subjects' original input sentences, and preferred over baselines.",
"Yang, Qichuan, Zhang, Liuxin, Zhang, Yang, Gao, Jinghua, Wang, Siyun",,,A Novel Distilled Generative Essay Polish System via Hierarchical Pre-Training,,,10.1109/IJCNN55064.2022.9892086 , ,2022.0,"In language processing tasks, the most important process in automated text polishment always consists of text correction and text supplementation. Finding that text polishment is a necessary step in the field of English essay reviewing, we are motivated to be the first of building an end-to-end automated English essay polish system, to support writing instruction. There were independent methods for text correction tasks and text supplementation tasks, but when combining them for essay polishment tasks, conflicts arise from their interplay. In this paper, we propose a polish system that elegantly performs text correction and text supplementation at the same time, achieving an improved revision quality. Furthermore, we design a closed-loop essay polishing process, made up of a Rewriting Model and a Scoring Model, which refers to modified GPT2 and ensembled Bert respectively. The rewriting process targets the deficiencies of the essays by a threshold controlled mechanism. Lastly, the performance of our proposed system is further enhanced by an optimization method. Intensive experiments on both real data and simulated data have shown score improvements on full essays by Scoring Model, as well as higher text correction accuracy and longer text supplementation length.",,,,, ,  2022 International Joint Conference on Neural Networks (IJCNN),,"Title:A Novel Distilled Generative Essay Polish System via Hierarchical Pre-Training

 In language processing tasks, the most important process in automated text polishment always consists of text correction and text supplementation. Finding that text polishment is a necessary step in the field of English essay reviewing, we are motivated to be the first of building an end-to-end automated English essay polish system, to support writing instruction. There were independent methods for text correction tasks and text supplementation tasks, but when combining them for essay polishment tasks, conflicts arise from their interplay. In this paper, we propose a polish system that elegantly performs text correction and text supplementation at the same time, achieving an improved revision quality. Furthermore, we design a closed-loop essay polishing process, made up of a Rewriting Model and a Scoring Model, which refers to modified GPT2 and ensembled Bert respectively. The rewriting process targets the deficiencies of the essays by a threshold controlled mechanism. Lastly, the performance of our proposed system is further enhanced by an optimization method. Intensive experiments on both real data and simulated data have shown score improvements on full essays by Scoring Model, as well as higher text correction accuracy and longer text supplementation length.",
"Majumder, Navonil, Ghosal, Deepanway, Hazarika, Devamanyu, Gelbukh, Alexander, Mihalcea, Rada, Poria, Soujanya",,,Exemplars-Guided Empathetic Response Generation Controlled by the Elements of Human Communication,,,10.1109/ACCESS.2022.3193159 , ,2022.0,"Empathy is fundamental to humans among other animals. It is key to strengthening social cohesion, the cornerstone of health and success of societies. Thus, empathy could be an important component of effective human-computer interactions through conversations. This has motivated a whole sub-field of research focused on empathetic response generation. The majority of existing methods for empathetic response generation rely on the emotion of the context to generate empathetic responses. However, empathy is much more than generating responses with an appropriate emotion. It also often entails subtle expressions of understanding and personal resonance with the situation of the other interlocutor. Unfortunately, such qualities are difficult to quantify, and the datasets lack relevant annotations. To address this issue, in this paper we propose an approach that relies on exemplars to cue the generative model on fine stylistic properties that signal empathy to the interlocutor. To this end, we employ dense passage retrieval to extract relevant exemplary responses from the training set. Three elements of human communication—emotional presence, interpretation, and exploration—and sentiment are additionally introduced using synthetic labels to guide the generation towards empathy. The human evaluation is also extended by these elements of human communication. We empirically show that these approaches yield significant improvements in empathetic response quality in terms of both automated and human-evaluated metrics. The implementation is available at https://github.com/declare-lab/exemplary-empathy.",,,,, ,  ,,"Title:Exemplars-Guided Empathetic Response Generation Controlled by the Elements of Human Communication

 Empathy is fundamental to humans among other animals. It is key to strengthening social cohesion, the cornerstone of health and success of societies. Thus, empathy could be an important component of effective human-computer interactions through conversations. This has motivated a whole sub-field of research focused on empathetic response generation. The majority of existing methods for empathetic response generation rely on the emotion of the context to generate empathetic responses. However, empathy is much more than generating responses with an appropriate emotion. It also often entails subtle expressions of understanding and personal resonance with the situation of the other interlocutor. Unfortunately, such qualities are difficult to quantify, and the datasets lack relevant annotations. To address this issue, in this paper we propose an approach that relies on exemplars to cue the generative model on fine stylistic properties that signal empathy to the interlocutor. To this end, we employ dense passage retrieval to extract relevant exemplary responses from the training set. Three elements of human communication—emotional presence, interpretation, and exploration—and sentiment are additionally introduced using synthetic labels to guide the generation towards empathy. The human evaluation is also extended by these elements of human communication. We empirically show that these approaches yield significant improvements in empathetic response quality in terms of both automated and human-evaluated metrics. The implementation is available at https://github.com/declare-lab/exemplary-empathy.",
"Chatzimina, Maria, Papadaki, Helen, Pontikoglou, Charalampos, Koumakis, Lefteris, Marias, Kostas, Tsiknakis, Manolis",,,Designing a conversational agent for patients with hematologic malignancies: Usability and Usefulness Study,,,10.1109/BHI50953.2021.9508587 , ,2021.0,"Empowering patients to record health-related information can provide further insights on treatment response, disease evolution, disease burden, Quality of Life (QoL) and enables direct measurement of the experiences of patients with chronic conditions, including hematologic malignancies. Evidence suggests that using electronic tools for the collection of patient health outcomes improves symptom control and enhances patient satisfaction. In parallel, recent advancement in machine learning and speech recognition have led to conversational agents, software systems mimicking written or spoken human speech, being increasingly adopted in the health care domain. In the present manuscript we present (i) a methodology for the implementation of a conversational agent able to collect family history and symptom-related information of patients with hematologic malignancies and (ii) its initial evaluation results from a relevant feasibility study. Our approach uses deep learning algorithms trained in conversations focused on hematologic malignancies. The evaluation of the model from the user experience perspective provides promising results regarding the acceptability and comprehensibility of the system.",,,,, ,  2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI),,"Title:Designing a conversational agent for patients with hematologic malignancies: Usability and Usefulness Study

 Empowering patients to record health-related information can provide further insights on treatment response, disease evolution, disease burden, Quality of Life (QoL) and enables direct measurement of the experiences of patients with chronic conditions, including hematologic malignancies. Evidence suggests that using electronic tools for the collection of patient health outcomes improves symptom control and enhances patient satisfaction. In parallel, recent advancement in machine learning and speech recognition have led to conversational agents, software systems mimicking written or spoken human speech, being increasingly adopted in the health care domain. In the present manuscript we present (i) a methodology for the implementation of a conversational agent able to collect family history and symptom-related information of patients with hematologic malignancies and (ii) its initial evaluation results from a relevant feasibility study. Our approach uses deep learning algorithms trained in conversations focused on hematologic malignancies. The evaluation of the model from the user experience perspective provides promising results regarding the acceptability and comprehensibility of the system.",
"Chen, Jin, Xiao, Guangyi, Han, Xu, Chen, Hao",,,Controllable and Editable Neural Story Plot Generation via Control-and-Edit Transformer,,,10.1109/ACCESS.2021.3094263 , ,2021.0,"Language-modeling-based methods for story plot generation aim to generate a plot with a language model (LM). LM methods have limitations of user-assist plot generation of goal control, refinement for editing, causing the generated plots not clear sense for specific goal, lack coherence, and edit flexible. We present a control-and-edit transformer technique which uses controlled imitation learning of editing distance from dynamic programming to support deleting policy, inserting policy, a weighting-reward with prepossess of corpus statistic, and measures continues reward for the controlled goal. Automated evaluation and Haman judgement show our method is promising in comparison with the baselines.",,,,, ,  ,,"Title:Controllable and Editable Neural Story Plot Generation via Control-and-Edit Transformer

 Language-modeling-based methods for story plot generation aim to generate a plot with a language model (LM). LM methods have limitations of user-assist plot generation of goal control, refinement for editing, causing the generated plots not clear sense for specific goal, lack coherence, and edit flexible. We present a control-and-edit transformer technique which uses controlled imitation learning of editing distance from dynamic programming to support deleting policy, inserting policy, a weighting-reward with prepossess of corpus statistic, and measures continues reward for the controlled goal. Automated evaluation and Haman judgement show our method is promising in comparison with the baselines.",
"Guan, Jiaqi, Li, Runzhe, Yu, Sheng, Zhang, Xuegong",,,A Method for Generating Synthetic Electronic Medical Record Text,,,10.1109/TCBB.2019.2948985 , ,2021.0,"Machine learning (ML) and Natural Language Processing (NLP) have achieved remarkable success in many fields and have brought new opportunities and high expectation in the analyses of medical data, of which the most common type is the massive free-text electronic medical records (EMR). However, the free EMR texts are lacking consistent standards, rich of private information, and limited in availability. Also, it is often hard to have a balanced number of samples for the types of diseases under study. These problems hinder the development of ML and NLP methods for EMR data analysis. To tackle these problems, we developed a model called Medical Text Generative Adversarial Network or mtGAN, to generate synthetic EMR text. It is based on the GAN framework and is trained by the REINFORCE algorithm. It takes disease tags as inputs and generates synthetic texts as EMRs for the corresponding diseases. We evaluate the model from micro-level, macro-level and application-level on a Chinese EMR text dataset. The results show that the method has a good capacity to fit real data and can generate realistic and diverse EMR samples. This provides a novel way to avoid potential leakage of patient privacy while still supply sufficient well-controlled cohort data for developing downstream ML and NLP methods.",,,,, ,  ,,"Title:A Method for Generating Synthetic Electronic Medical Record Text

 Machine learning (ML) and Natural Language Processing (NLP) have achieved remarkable success in many fields and have brought new opportunities and high expectation in the analyses of medical data, of which the most common type is the massive free-text electronic medical records (EMR). However, the free EMR texts are lacking consistent standards, rich of private information, and limited in availability. Also, it is often hard to have a balanced number of samples for the types of diseases under study. These problems hinder the development of ML and NLP methods for EMR data analysis. To tackle these problems, we developed a model called Medical Text Generative Adversarial Network or mtGAN, to generate synthetic EMR text. It is based on the GAN framework and is trained by the REINFORCE algorithm. It takes disease tags as inputs and generates synthetic texts as EMRs for the corresponding diseases. We evaluate the model from micro-level, macro-level and application-level on a Chinese EMR text dataset. The results show that the method has a good capacity to fit real data and can generate realistic and diverse EMR samples. This provides a novel way to avoid potential leakage of patient privacy while still supply sufficient well-controlled cohort data for developing downstream ML and NLP methods.",
"Lu, Tianhe, Liu, Gongshen, Zhang, Ru, Li, Peixuan, Ju, Tianjie",,,Robust Secret Data Hiding for Transformer-based Neural Machine Translation,,,10.1109/IJCNN54540.2023.10191984 , ,2023.0,"Hiding secret information in text is a research area of significant importance and a great challenge. In recent years, there have been huge developments and exciting advances in generation-based text information hiding techniques. Current generative text information hiding methods mainly establish correspondence between token and secret bits based on probability distributions given by language models. However, the semantic control of such methods is weak, and their robustness is not discussed. In this paper, we investigate an end-to-end generation-based text information hiding scheme. The proposed method uses a sequence-to-sequence model with adversarial training as a machine translation model. It converts the secret information into an embedding vector to be added to each position of the hidden state representation of the source language text, which in turn allows the model to automatically learn to produce translation results with the embedded secret information without using fixed rules. The semantics of the text with embedded secret messages obtained by translation can be controlled by the meaning of the source language text. Our experiments show that the proposed method can embed the secret message into the translation results with little loss of the translation quality and is robust to active attacks such as word deletion or synonym substitution.",,,,, ,  2023 International Joint Conference on Neural Networks (IJCNN),,"Title:Robust Secret Data Hiding for Transformer-based Neural Machine Translation

 Hiding secret information in text is a research area of significant importance and a great challenge. In recent years, there have been huge developments and exciting advances in generation-based text information hiding techniques. Current generative text information hiding methods mainly establish correspondence between token and secret bits based on probability distributions given by language models. However, the semantic control of such methods is weak, and their robustness is not discussed. In this paper, we investigate an end-to-end generation-based text information hiding scheme. The proposed method uses a sequence-to-sequence model with adversarial training as a machine translation model. It converts the secret information into an embedding vector to be added to each position of the hidden state representation of the source language text, which in turn allows the model to automatically learn to produce translation results with the embedded secret information without using fixed rules. The semantics of the text with embedded secret messages obtained by translation can be controlled by the meaning of the source language text. Our experiments show that the proposed method can embed the secret message into the translation results with little loss of the translation quality and is robust to active attacks such as word deletion or synonym substitution.",
"Madaan, Nishtha, Saha, Diptikalyan, Bedathur, Srikanta",,,Counterfactual Sentence Generation with Plug-and-Play Perturbation,,,10.1109/SaTML54575.2023.00028 , ,2023.0,"Generating counterfactual test-cases is an important backbone for testing NLP models and making them as robust and reliable as traditional software. In generating the test-cases, a desired property is the ability to control the test-case generation in a flexible manner to test for a large variety of failure cases and to explain and repair them in a targeted manner. In this direction, significant progress has been made in the prior works by manually writing rules for generating controlled counterfactuals. However, this approach requires heavy manual supervision and lacks the flexibility to easily introduce new controls. Motivated by the impressive flexibility of the plug-and-play approach of PPLM, we propose bringing the framework of plug-and-play to counterfactual test case generation task. We introduce CASPer, a plug-and-play counterfactual generation framework to generate test cases that satisfy goal attributes on demand. Our plug-and-play model can steer the test case generation process given any attribute model without requiring attribute-specific training of the model. In experiments, we show that CASPer effectively generates counterfactual text that follow the steering provided by an attribute model while also being fluent, diverse and preserving the original content. We also show that the generated counterfactuals from CASPer can be used for augmenting the training data and thereby fixing and making the test model more robust.",,,,, ,  2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML),,"Title:Counterfactual Sentence Generation with Plug-and-Play Perturbation

 Generating counterfactual test-cases is an important backbone for testing NLP models and making them as robust and reliable as traditional software. In generating the test-cases, a desired property is the ability to control the test-case generation in a flexible manner to test for a large variety of failure cases and to explain and repair them in a targeted manner. In this direction, significant progress has been made in the prior works by manually writing rules for generating controlled counterfactuals. However, this approach requires heavy manual supervision and lacks the flexibility to easily introduce new controls. Motivated by the impressive flexibility of the plug-and-play approach of PPLM, we propose bringing the framework of plug-and-play to counterfactual test case generation task. We introduce CASPer, a plug-and-play counterfactual generation framework to generate test cases that satisfy goal attributes on demand. Our plug-and-play model can steer the test case generation process given any attribute model without requiring attribute-specific training of the model. In experiments, we show that CASPer effectively generates counterfactual text that follow the steering provided by an attribute model while also being fluent, diverse and preserving the original content. We also show that the generated counterfactuals from CASPer can be used for augmenting the training data and thereby fixing and making the test model more robust.",
"Ding, Changhao, Fu, Zhangjie, Yang, Zhongliang, Yu, Qi, Li, Daqiu, Huang, Yongfeng",,,Context-Aware Linguistic Steganography Model Based on Neural Machine Translation,,,10.1109/TASLP.2023.3340601 , ,2024.0,"Linguistic steganography based on text generation is a hot topic in the field of text information hiding. Previous studies have managed to improve the syntactic quality of steganography texts using natural language processing techniques based on deep learning, but their steganography models still lack the ability to control the semantic and contextual characteristics in texts, which is caused by the shortage of relevant information they can obtain. This results in a great decline in the imperceptibility of steganographic texts. To address the problem, we propose a context-aware linguistic steganography method based on neural machine translation called NMT-Stega. The model generates translation containing secret messages based on the neural machine translation model with semantic fusion and language model reference units. In this way, the semantics and contexts of translation are controlled by the additional semantic and contextual features acquired from the text to be translated. Also, a new encoding that combines arithmetic coding with a waiting mechanism is proposed in our model. This method solves the low embedding capacity problem of waiting mechanism while ensuring the semantic and contextual characteristics of steganographic text are less modified. Experimental results show that our model outperforms the previous models and encoding methods in semantic correlation, embedding capacity and imperceptibility.",,,,, ,  ,,"Title:Context-Aware Linguistic Steganography Model Based on Neural Machine Translation

 Linguistic steganography based on text generation is a hot topic in the field of text information hiding. Previous studies have managed to improve the syntactic quality of steganography texts using natural language processing techniques based on deep learning, but their steganography models still lack the ability to control the semantic and contextual characteristics in texts, which is caused by the shortage of relevant information they can obtain. This results in a great decline in the imperceptibility of steganographic texts. To address the problem, we propose a context-aware linguistic steganography method based on neural machine translation called NMT-Stega. The model generates translation containing secret messages based on the neural machine translation model with semantic fusion and language model reference units. In this way, the semantics and contexts of translation are controlled by the additional semantic and contextual features acquired from the text to be translated. Also, a new encoding that combines arithmetic coding with a waiting mechanism is proposed in our model. This method solves the low embedding capacity problem of waiting mechanism while ensuring the semantic and contextual characteristics of steganographic text are less modified. Experimental results show that our model outperforms the previous models and encoding methods in semantic correlation, embedding capacity and imperceptibility.",
"Zou, Hanyi, Xu, Nan, Kong, Qingchao, Mao, Wenji",,,Controllable News Comment Generation based on Attribute Level Contrastive Learning,,,10.1109/ISI58743.2023.10297146 , ,2023.0,"News comments provide a convenient way for people to express opinions and exchange ideas. Positive comments en-courage a harmonious discussion atmosphere within news media communities. In contrast, offensive or insulting comments may result in cyberbullying and personal psychological trauma, which have particular practical impacts in security-related domains. The automatic generation of news comments with controllable attributes (e.g. sentiment) to assist users and news platform administrators is greatly needed. However, existing research for news comment generation has not addressed the controllable issue yet. On the other hand, existing methods for controllable text generation focus on token-level constraints, which are not applicable to controlling the sentence level attributes for news comment generation. To address this challenging issue, in this paper, we propose an attribute-level contrastive learning method for controllable news comment generation. To apply attribute level constraints on the generated text, our method considers the attributes of the generated comments and the pre-defined attributes as different views of the same attribute, and maximizes their similarity during the training process. We conduct experiments on two publicly available news comment datasets, and the experimental results show that our model achieves competitive performance in news comment generation and attribute controllability.",,,,, ,  2023 IEEE International Conference on Intelligence and Security Informatics (ISI),,"Title:Controllable News Comment Generation based on Attribute Level Contrastive Learning

 News comments provide a convenient way for people to express opinions and exchange ideas. Positive comments en-courage a harmonious discussion atmosphere within news media communities. In contrast, offensive or insulting comments may result in cyberbullying and personal psychological trauma, which have particular practical impacts in security-related domains. The automatic generation of news comments with controllable attributes (e.g. sentiment) to assist users and news platform administrators is greatly needed. However, existing research for news comment generation has not addressed the controllable issue yet. On the other hand, existing methods for controllable text generation focus on token-level constraints, which are not applicable to controlling the sentence level attributes for news comment generation. To address this challenging issue, in this paper, we propose an attribute-level contrastive learning method for controllable news comment generation. To apply attribute level constraints on the generated text, our method considers the attributes of the generated comments and the pre-defined attributes as different views of the same attribute, and maximizes their similarity during the training process. We conduct experiments on two publicly available news comment datasets, and the experimental results show that our model achieves competitive performance in news comment generation and attribute controllability.",
"Toyama, Keisuke, Sudoh, Katsuhito, Nakamura, Satoshi",,,Content Order-Controllable MR-to-Text,,,10.1109/ACCESS.2023.3334139 , ,2023.0,"Content order is critical in natural language generation (NLG) for emphasizing the focus of a generated text passage. In this paper, we propose a novel MR (meaning representation)-to-text method that controls the order of the MR values in a generated text passage based on the given order constraints. We use an MR-text dataset with additional value order annotations to train our order-controllable MR-to-text model. We also use it to train a text-to-MR model to check whether the generated text passage correctly reflects the original MR. Furthermore, we augment the dataset with synthetic MR-text pairs to mitigate the discrepancy in the number of non-empty attributes between the training and test conditions and use it to train another order-controllable MR-to-text model. Our proposed methods demonstrate better NLG performance than the baseline methods without order constraints in automatic and subjective evaluations. In particular, the augmented dataset effectively reduces the number of deletion, insertion, and substitution errors in the generated text passages.",,,,, ,  ,,"Title:Content Order-Controllable MR-to-Text

 Content order is critical in natural language generation (NLG) for emphasizing the focus of a generated text passage. In this paper, we propose a novel MR (meaning representation)-to-text method that controls the order of the MR values in a generated text passage based on the given order constraints. We use an MR-text dataset with additional value order annotations to train our order-controllable MR-to-text model. We also use it to train a text-to-MR model to check whether the generated text passage correctly reflects the original MR. Furthermore, we augment the dataset with synthetic MR-text pairs to mitigate the discrepancy in the number of non-empty attributes between the training and test conditions and use it to train another order-controllable MR-to-text model. Our proposed methods demonstrate better NLG performance than the baseline methods without order constraints in automatic and subjective evaluations. In particular, the augmented dataset effectively reduces the number of deletion, insertion, and substitution errors in the generated text passages.",
"Wong, Aslan B., Wu, Kaishun",,,Pronunciation Training through Sensing of Tongue and Lip Motion via Smartphone,,,10.1109/PerComWorkshops51409.2021.9430957 , ,2021.0,"Learning a foreign language pronunciation is the most challenging task for non-native speakers. Compared to scoring feedback on pronunciation assessment, informative feedback for a self-correct pronunciation training system is still left behind. To bring this to an end, we propose a pronunciation training system that can give articulation feedback to a user. Our approach focuses on correcting the user's articulation. We employ simultaneously two ranges of acoustic signals, both speech and ultrasonic signal, to recognize lip shape and tongue position. The proposed technique is also implemented into an off-the-shelf smartphone to be more accessible. Our system is currently working on 30 vowels in four languages: French, Japanese, Korean, and Mandarin Chinese.",,,,, ,  2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),,"Title:Pronunciation Training through Sensing of Tongue and Lip Motion via Smartphone

 Learning a foreign language pronunciation is the most challenging task for non-native speakers. Compared to scoring feedback on pronunciation assessment, informative feedback for a self-correct pronunciation training system is still left behind. To bring this to an end, we propose a pronunciation training system that can give articulation feedback to a user. Our approach focuses on correcting the user's articulation. We employ simultaneously two ranges of acoustic signals, both speech and ultrasonic signal, to recognize lip shape and tongue position. The proposed technique is also implemented into an off-the-shelf smartphone to be more accessible. Our system is currently working on 30 vowels in four languages: French, Japanese, Korean, and Mandarin Chinese.",
"Zhu, Zongxiao, Wu, Xianli, Liu, Sai, Tian, Wei, Chen, Li",,,The Research of Printed Yi Character Recognition,,,10.1109/CSIE.2009.563 , ,2009.0,"This paper describes the detailed process required to accomplish the printed Yi optical character recognition, including pretreatment, feature extraction and pattern classification. The arithmetic of Yi multifont’s feature extraction, the arithmetic of recognition required for dictionary building and the arithmetic of multilevel Yi character matching were analyzed with particular emphases. An OCR software based on Windows with self-correcting ability was developed. Its one-time accurate identification of samples was above 99.5%. This result proved that the arithmetic used is feasible and efficient.",,,,, ,  2009 WRI World Congress on Computer Science and Information Engineering,,"Title:The Research of Printed Yi Character Recognition

 This paper describes the detailed process required to accomplish the printed Yi optical character recognition, including pretreatment, feature extraction and pattern classification. The arithmetic of Yi multifont’s feature extraction, the arithmetic of recognition required for dictionary building and the arithmetic of multilevel Yi character matching were analyzed with particular emphases. An OCR software based on Windows with self-correcting ability was developed. Its one-time accurate identification of samples was above 99.5%. This result proved that the arithmetic used is feasible and efficient.",
"Tam, Yik-Cheung, Xu, Jiacheng, Zou, Jiakai, Wang, Zecheng, Liao, Tinglong, Yuan, Shuhan",,,Robust Unstructured Knowledge Access in Conversational Dialogue with ASR Errors,,,10.1109/ICASSP43922.2022.9746741 , ,2022.0,"Performance of spoken language understanding (SLU) can be degraded with automatic speech recognition (ASR) errors. We propose a novel approach to improve SLU robustness by randomly corrupting clean training text with an ASR error simulator, followed by self-correcting the errors and minimizing the target classification loss in a joint manner. In the proposed error simulator, we leverage confusion networks generated from an ASR decoder without human transcriptions to generate variety of error patterns for model training. We evaluate our approach on DSTC10 challenge targeted for knowledge-grounded task-oriented conversational dialogues with ASR errors. Experimental results show effectiveness of our proposed approach, boosting the knowledge-seeking turn detection (KTD) F1 significantly from 0.9433 to 0.9904. Knowledge cluster classification is boosted from 0.7924 to 0.9333 in Recall@1. After knowledge document re-ranking, our approach shows significant improvement in all knowledge selection metrics, from 0.7358 to 0.7806 in Recall@1, from 0.8301 to 0.9333 in Recall@5, and from 0.7798 to 0.8460 in MRR@5 (Mean Reciprocal Rank) on the test set. On the recent DSTC10 evaluation, our approach demonstrates significant improvement in knowledge selection, boosting Recall@1 from 0.495 to 0.7105 compared to the official baseline. Our source code is released in GitHub1.",,,,, ,"  ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,"Title:Robust Unstructured Knowledge Access in Conversational Dialogue with ASR Errors

 Performance of spoken language understanding (SLU) can be degraded with automatic speech recognition (ASR) errors. We propose a novel approach to improve SLU robustness by randomly corrupting clean training text with an ASR error simulator, followed by self-correcting the errors and minimizing the target classification loss in a joint manner. In the proposed error simulator, we leverage confusion networks generated from an ASR decoder without human transcriptions to generate variety of error patterns for model training. We evaluate our approach on DSTC10 challenge targeted for knowledge-grounded task-oriented conversational dialogues with ASR errors. Experimental results show effectiveness of our proposed approach, boosting the knowledge-seeking turn detection (KTD) F1 significantly from 0.9433 to 0.9904. Knowledge cluster classification is boosted from 0.7924 to 0.9333 in Recall@1. After knowledge document re-ranking, our approach shows significant improvement in all knowledge selection metrics, from 0.7358 to 0.7806 in Recall@1, from 0.8301 to 0.9333 in Recall@5, and from 0.7798 to 0.8460 in MRR@5 (Mean Reciprocal Rank) on the test set. On the recent DSTC10 evaluation, our approach demonstrates significant improvement in knowledge selection, boosting Recall@1 from 0.495 to 0.7105 compared to the official baseline. Our source code is released in GitHub1.",
"Spinellis, Diomidis, Avgeriou, Paris",,,Evolution of the Unix System Architecture: An Exploratory Case Study,,,10.1109/TSE.2019.2892149 , ,2021.0,"Unix has evolved for almost five decades, shaping modern operating systems, key software technologies, and development practices. Studying the evolution of this remarkable system from an architectural perspective can provide insights on how to manage the growth of large, complex, and long-lived software systems. Along main Unix releases leading to the FreeBSD lineage we examine core architectural design decisions, the number of features, and code complexity, based on the analysis of source code, reference documentation, and related publications. We report that the growth in size has been uniform, with some notable outliers, while cyclomatic complexity has been religiously safeguarded. A large number of Unix-defining design decisions were implemented right from the very early beginning, with most of them still playing a major role. Unix continues to evolve from an architectural perspective, but the rate of architectural innovation has slowed down over the system's lifetime. Architectural technical debt has accrued in the forms of functionality duplication and unused facilities, but in terms of cyclomatic complexity it is systematically being paid back through what appears to be a self-correcting process. Some unsung architectural forces that shaped Unix are the emphasis on conventions over rigid enforcement, the drive for portability, a sophisticated ecosystem of other operating systems and development organizations, and the emergence of a federated architecture, often through the adoption of third-party subsystems. These findings have led us to form an initial theory on the architecture evolution of large, complex operating system software.",,,,, ,  ,,"Title:Evolution of the Unix System Architecture: An Exploratory Case Study

 Unix has evolved for almost five decades, shaping modern operating systems, key software technologies, and development practices. Studying the evolution of this remarkable system from an architectural perspective can provide insights on how to manage the growth of large, complex, and long-lived software systems. Along main Unix releases leading to the FreeBSD lineage we examine core architectural design decisions, the number of features, and code complexity, based on the analysis of source code, reference documentation, and related publications. We report that the growth in size has been uniform, with some notable outliers, while cyclomatic complexity has been religiously safeguarded. A large number of Unix-defining design decisions were implemented right from the very early beginning, with most of them still playing a major role. Unix continues to evolve from an architectural perspective, but the rate of architectural innovation has slowed down over the system's lifetime. Architectural technical debt has accrued in the forms of functionality duplication and unused facilities, but in terms of cyclomatic complexity it is systematically being paid back through what appears to be a self-correcting process. Some unsung architectural forces that shaped Unix are the emphasis on conventions over rigid enforcement, the drive for portability, a sophisticated ecosystem of other operating systems and development organizations, and the emergence of a federated architecture, often through the adoption of third-party subsystems. These findings have led us to form an initial theory on the architecture evolution of large, complex operating system software.",
"Eddon, G., Reiss, S.",,,Myrrh: A Transaction-Based Model for Autonomic Recovery,,,10.1109/ICAC.2005.43 , ,2005.0,"As software comes under increasing scrutiny for its lack of safety and reliability, numerous static and partially dynamic tools (including model checking) have been proposed for verifying large and complex systems of interacting components. However, because these tools have been largely unsuccessful, it is essential to develop dynamic mechanisms able to enforce runtime safety properties. We aim to address this goal through a runtime system, called Myrrh, that can provide broad safety and reliability guarantees. Myrrh uses transactions, an abstraction commonly associated with database systems, in order to create autonomic capabilities that support automated recovery within the context of a general purpose programming language. The resulting code is self-correcting; exceptions cause faulty transactions to rollback and thus return the system to its previous state",,,,, ,  Second International Conference on Autonomic Computing (ICAC'05),,"Title:Myrrh: A Transaction-Based Model for Autonomic Recovery

 As software comes under increasing scrutiny for its lack of safety and reliability, numerous static and partially dynamic tools (including model checking) have been proposed for verifying large and complex systems of interacting components. However, because these tools have been largely unsuccessful, it is essential to develop dynamic mechanisms able to enforce runtime safety properties. We aim to address this goal through a runtime system, called Myrrh, that can provide broad safety and reliability guarantees. Myrrh uses transactions, an abstraction commonly associated with database systems, in order to create autonomic capabilities that support automated recovery within the context of a general purpose programming language. The resulting code is self-correcting; exceptions cause faulty transactions to rollback and thus return the system to its previous state",
"Liu, Ming, Zhang, Jingxu, Nyagoga, Lucy Michael, Liu, Li",,,Student-AI Question Co-Creation for Enhancing Reading Comprehension,,,10.1109/TLT.2023.3333439 , ,2023.0,"Student question generation (SQG) is an effective strategy for improving reading comprehension. It helps students improve their understanding of reading materials, metacognitively monitor their comprehension, and self-correct comprehension gaps. Internet technologies have been used to facilitate student question generation process through intensive peer support. However, the availability, level of task commitment, and capabilities of student peers have emerged as significant concerns, particularly in light of the global pandemic and the subsequent post-pandemic era. Thus, this article presents a student-Artificial Intelligence (AI) co-creation tool called CoAsker for supporting question generation. Following recent Human Computer Interaction (HCI) research in human-AI collaborative writing, CoAsker first allows students to provide question clues and answers and then uses a state-of-the-art pre-trained language model, T5-PEGASUS, to generate questions. Finally, the student can use this AI question directly or perform reflection by comparing his or her questions with the AI question. An empirical study was conducted to examine the quality of AI questions and the effect of this tool on student engagement and reading comprehension. The results of the study shows that students using this tool (treatment) were more engaged in generating low-level cognitive questions and performed better in acquiring knowledge than those using a traditional online question generation tool (control). These results indicate that student-AI question co-creation is beneficial to SQG training and educational assessment for reading comprehension, such as repeated practices.",,,,, ,  ,,"Title:Student-AI Question Co-Creation for Enhancing Reading Comprehension

 Student question generation (SQG) is an effective strategy for improving reading comprehension. It helps students improve their understanding of reading materials, metacognitively monitor their comprehension, and self-correct comprehension gaps. Internet technologies have been used to facilitate student question generation process through intensive peer support. However, the availability, level of task commitment, and capabilities of student peers have emerged as significant concerns, particularly in light of the global pandemic and the subsequent post-pandemic era. Thus, this article presents a student-Artificial Intelligence (AI) co-creation tool called CoAsker for supporting question generation. Following recent Human Computer Interaction (HCI) research in human-AI collaborative writing, CoAsker first allows students to provide question clues and answers and then uses a state-of-the-art pre-trained language model, T5-PEGASUS, to generate questions. Finally, the student can use this AI question directly or perform reflection by comparing his or her questions with the AI question. An empirical study was conducted to examine the quality of AI questions and the effect of this tool on student engagement and reading comprehension. The results of the study shows that students using this tool (treatment) were more engaged in generating low-level cognitive questions and performed better in acquiring knowledge than those using a traditional online question generation tool (control). These results indicate that student-AI question co-creation is beneficial to SQG training and educational assessment for reading comprehension, such as repeated practices.",
