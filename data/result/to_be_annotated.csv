id,text,Authors,Researcher Ids,ORCIDs,title,Volume,Issue,DOI,Document Type,Publication Date,Abstract,ISSN,eISSN,ISBN,Pages,Publisher,Proceedings title,Keywords,label,Comments
2459,"**Title**Sensitivity, Performance, Robustness: Deconstructing the Effect of Sociodemographic Prompting

**Abstract**Annotators' sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as toxic language detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique - it remains unclear for which tasks and scenarios it can help, and the role of the individual factors in sociodemographic prompting is still unexplored. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. We analyze its influence on model sensitivity, performance and robustness across seven datasets and six instruction-tuned model families. We show that sociodemographic information affects model predictions and can be beneficial for improving zero-shot learning in subjective NLP tasks. However, its outcomes largely vary for different model types, sizes, and datasets, and are subject to large variance with regards to prompt formulations. Most importantly, our results show that sociodemographic prompting should be used with care for sensitive applications, such as toxicity annotation or when studying LLM alignment.","Beck, Tilman; Schuff, Hendrik; Lauscher, Anne; Gurevych, Iryna",,,"Sensitivity, Performance, Robustness: Deconstructing the Effect of Sociodemographic Prompting",,, ,Proceedings Paper ,,"Annotators' sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as toxic language detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique - it remains unclear for which tasks and scenarios it can help, and the role of the individual factors in sociodemographic prompting is still unexplored. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. We analyze its influence on model sensitivity, performance and robustness across seven datasets and six instruction-tuned model families. We show that sociodemographic information affects model predictions and can be beneficial for improving zero-shot learning in subjective NLP tasks. However, its outcomes largely vary for different model types, sizes, and datasets, and are subject to large variance with regards to prompt formulations. Most importantly, our results show that sociodemographic prompting should be used with care for sensitive applications, such as toxicity annotation or when studying LLM alignment.",,,979-8-89176-088-2,2589-2615, , 18th Conference of the European-Chapter of the Association-for-Computational-Linguistics (EACL)18th Conference of the European-Chapter of the Association-for-Computational-Linguistics (EACL) ,,detection#detox#methodology,
2460,"**Title**DQAC: Detoxifying Query Auto-completion with Adapters

**Abstract**Recent Query Auto-completion (QAC) systems leverage natural language generation or pre-trained language models (PLMs) to demonstrate remarkable performance. However, these systems also suffer from biased and toxic completions. Efforts have been made to address language detoxification within PLMs using controllable text generation (CTG) techniques, involving training with nontoxic data and employing decoding time approaches. As the completions for QAC systems are usually short, these existing CTG methods based on decoding and training are not directly transferable. Towards these concerns, we propose the first public QAC detoxification model, Detoxifying Query Auto-Completion (or DQAC), which utilizes adapters in a CTG framework. DQAC operates on latent representations with no additional overhead. It leverages two adapters for toxic and non-toxic cases. During inference, we fuse these representations in a controlled manner that guides the generation of query completions towards nontoxicity. We evaluate toxicity levels in the generated completions across two realworld datasets using two classifiers: a publicly available (Detoxify) and a search query-specific classifier which we develop (QDETOXIFY). DQAC consistently outperforms all existing baselines and emerges as a state-of-the-art model providing high quality and low toxicity. We make the code publicly available1.(1 https://shorturl.at/zJ024)","Maheswaran, Aishwarya; Maurya, Kaushal Kumar; Gupta, Manish; Desarkar, Maunendra Sankar","Desarkar, Maunendra/Y-8696-2019",,DQAC: Detoxifying Query Auto-completion with Adapters,14650,,10.1007/978-981-97-2266-2_9 ,Proceedings Paper ,,"Recent Query Auto-completion (QAC) systems leverage natural language generation or pre-trained language models (PLMs) to demonstrate remarkable performance. However, these systems also suffer from biased and toxic completions. Efforts have been made to address language detoxification within PLMs using controllable text generation (CTG) techniques, involving training with nontoxic data and employing decoding time approaches. As the completions for QAC systems are usually short, these existing CTG methods based on decoding and training are not directly transferable. Towards these concerns, we propose the first public QAC detoxification model, Detoxifying Query Auto-Completion (or DQAC), which utilizes adapters in a CTG framework. DQAC operates on latent representations with no additional overhead. It leverages two adapters for toxic and non-toxic cases. During inference, we fuse these representations in a controlled manner that guides the generation of query completions towards nontoxicity. We evaluate toxicity levels in the generated completions across two realworld datasets using two classifiers: a publicly available (Detoxify) and a search query-specific classifier which we develop (QDETOXIFY). DQAC consistently outperforms all existing baselines and emerges as a state-of-the-art model providing high quality and low toxicity. We make the code publicly available1.(1 https://shorturl.at/zJ024)",2945-9133,1611-3349,978-981-97-2265-5; 978-981-97-2266-2,108-120, , 28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD) ,,detox,
2461,"**Title**Language Detoxification with Attribute-Discriminative Latent Space

**Abstract**Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to detoxify toxic texts using additional LMs or perturbations. However, previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application. To address such limitations, we propose an effective yet efficient method for language detoxification using an attribute-discriminative latent space. Specifically, we project the latent space of an original Transformer LM onto a discriminative latent space that well-separates texts by their attributes using a projection block and an attribute discriminator. This allows the LM to control the text generation to be nontoxic with minimal memory and computation overhead. We validate our model, Attribute-Discriminative Language Model (ADLM) on detoxified language and dialogue generation tasks, on which our method significantly outperforms baselines both in performance and efficiency.","Kwak, Jin Myung; Kim, Minseon; Hwang, Sung Ju","Hwang, Sung/A-8817-2018",,Language Detoxification with Attribute-Discriminative Latent Space,,, ,Proceedings Paper ,,"Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to detoxify toxic texts using additional LMs or perturbations. However, previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application. To address such limitations, we propose an effective yet efficient method for language detoxification using an attribute-discriminative latent space. Specifically, we project the latent space of an original Transformer LM onto a discriminative latent space that well-separates texts by their attributes using a projection block and an attribute discriminator. This allows the LM to control the text generation to be nontoxic with minimal memory and computation overhead. We validate our model, Attribute-Discriminative Language Model (ADLM) on detoxified language and dialogue generation tasks, on which our method significantly outperforms baselines both in performance and efficiency.",,,978-1-959429-72-2,10149-10171, , 61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL)61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL) ,,detox,
2462,"**Title**DAC: Quantized Optimal Transport Reward-based Reinforcement Learning Approach to Detoxify Query Auto-Completion

**Abstract**Modern Query Auto-Completion (QAC) systems utilize natural language generation (NLG) using large language models (LLM) to achieve remarkable performance. However, these systems are prone to generating biased and toxic completions due to inherent learning biases. Existing detoxification approaches exhibit two key limitations: (1) They primarily focus on mitigating toxicity for grammatically well-formed long sentences but struggle to adapt to the QAC task, where queries are short and structurally different (include spelling errors, do not follow grammatical rules and have relatively flexible word order). (2) These approaches often view detoxification through a binary lens where all text labeled as toxic is undesirable, and non-toxic is considered desirable. To address these limitations, we propose DAC, an intuitive and efficient reinforcement learning-based model to detoxify QAC. With DAC, we introduce an additional perspective of considering the third query class of addressable toxicity. These queries can encompass implicit toxicity, subjective toxicity, or non-toxic queries containing toxic words. We incorporate this three-class query behavior perspective into the proposed model through quantized optimal transport to learn distinctions and generate truly non-toxic completions. We evaluate toxicity levels in the generated completions by DAC across two real-world QAC datasets (Bing and AOL) using two classifiers: a publicly available generic classifier (Detoxify) and a search queryspecific classifier, which we develop (TClassify). We find that DAC consistently outperforms all existing baselines on the Bing dataset and achieves competitive performance on the AOL dataset for query detoxification. We make the code publicly available1.","Maheswaran, Aishwarya; Maurya, Kaushal Kumar; Gupta, Manish; Desarkar, Maunendra Sankar","Desarkar, Maunendra/Y-8696-2019",,DAC: Quantized Optimal Transport Reward-based Reinforcement Learning Approach to Detoxify Query Auto-Completion,,,10.1145/3626772.3657779 ,Proceedings Paper ,,"Modern Query Auto-Completion (QAC) systems utilize natural language generation (NLG) using large language models (LLM) to achieve remarkable performance. However, these systems are prone to generating biased and toxic completions due to inherent learning biases. Existing detoxification approaches exhibit two key limitations: (1) They primarily focus on mitigating toxicity for grammatically well-formed long sentences but struggle to adapt to the QAC task, where queries are short and structurally different (include spelling errors, do not follow grammatical rules and have relatively flexible word order). (2) These approaches often view detoxification through a binary lens where all text labeled as toxic is undesirable, and non-toxic is considered desirable. To address these limitations, we propose DAC, an intuitive and efficient reinforcement learning-based model to detoxify QAC. With DAC, we introduce an additional perspective of considering the third query class of addressable toxicity. These queries can encompass implicit toxicity, subjective toxicity, or non-toxic queries containing toxic words. We incorporate this three-class query behavior perspective into the proposed model through quantized optimal transport to learn distinctions and generate truly non-toxic completions. We evaluate toxicity levels in the generated completions by DAC across two real-world QAC datasets (Bing and AOL) using two classifiers: a publicly available generic classifier (Detoxify) and a search queryspecific classifier, which we develop (TClassify). We find that DAC consistently outperforms all existing baselines on the Bing dataset and achieves competitive performance on the AOL dataset for query detoxification. We make the code publicly available1.",,,979-8-4007-0431-4,608-618, , 47th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)47th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) ,,detox,
2464,"**Title**SAFECONV: Explaining and Correcting Conversational Unsafe Behavior

**Abstract**One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SAFECONV for the research of conversational safety: (1) Besides the utterance-level safety labels, SAFECONV also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SAFECONV provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. By virtue of the comprehensive annotation of SAFECONV, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at https://github.com/mianzhang/SafeConv.Warning: This paper contains cases that may be offensive or upsetting.","Zhang, Mian; Jin, Lifeng; Song, Linfeng; Mi, Haitao; Chen, Wenliang; Yu, Dong",,,SAFECONV: Explaining and Correcting Conversational Unsafe Behavior,,, ,Proceedings Paper ,,"One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SAFECONV for the research of conversational safety: (1) Besides the utterance-level safety labels, SAFECONV also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SAFECONV provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. By virtue of the comprehensive annotation of SAFECONV, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at https://github.com/mianzhang/SafeConv.Warning: This paper contains cases that may be offensive or upsetting.",,,978-1-959429-72-2,22-35, , 61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL)61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL) ,,Gen_dataset,
2465,"**Title**Detoxifying Text with MARCO: Controllable Revision with Experts and Anti-Experts

**Abstract**Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MARCO, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MARCO uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MARCO's rewrites are preferred 2.1x more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.","Hallinan, Skyler; Liu, Alisa; Choi, Yejin; Sap, Maarten",,,Detoxifying Text with MARCO: Controllable Revision with Experts and Anti-Experts,,, ,Proceedings Paper ,,"Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MARCO, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MARCO uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MARCO's rewrites are preferred 2.1x more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.",,,978-1-959429-71-5,228-242, , 61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL)61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL) ,,detox,
2466,"**Title**Understanding the Unintended Effects of Human-Machine Moderation in Addressing Harassment within Online Communities

**Abstract**We set out to explore the unintended effects of human-machine moderation in mitigating harassment within online communities. We examine communities that use a block-list type bot to prevent harassment from the source of harassment. Drawing from social categorization and selective exposure theories, we theorize that employing a machine alongside humans for community moderation will create unintended adverse effects. Specifically, within the moderated focal community, we hypothesize an emboldening effect characterized by an increase in harassment among community members directed at their outgroup members. Additionally, we expect a disengaging effect, that is, a downward trend in the focal community's membership. Finally, in neighboring communities that share the same topic of discussion, we expect a spillover effect, that is, an increase in harassment. Employing Detoxify, a Bidirectional Encoder Representations from Transformers (BERT)-based model, we evaluate harassment scores in the focal community by analyzing 4 million Reddit comments across various communities. These scores serve as inputs for Bayesian Structural Time Series analysis, revealing evidence for both disengaging and spillover effects. For the emboldening effect, we use community-specific keywords in a predefined computer-assisted document classification approach, Keyword Assisted Topic Model (keyATM), to identify the target of harassment. We use mean comparison and regression discontinuity to assess the change in the level of harassment targeting outgroup members before and after the human-machine moderation implementation.","Nguyen, An; Rai, Arun; Maruping, Likoebe",,"Nguyen, An/0009-0008-4292-5964; Rai, Arun/0000-0002-3655-7543; Maruping, Likoebe/0000-0001-5105-6635",Understanding the Unintended Effects of Human-Machine Moderation in Addressing Harassment within Online Communities,41,2.0,10.1080/07421222.2024.2340831 ,Article ,,"We set out to explore the unintended effects of human-machine moderation in mitigating harassment within online communities. We examine communities that use a block-list type bot to prevent harassment from the source of harassment. Drawing from social categorization and selective exposure theories, we theorize that employing a machine alongside humans for community moderation will create unintended adverse effects. Specifically, within the moderated focal community, we hypothesize an emboldening effect characterized by an increase in harassment among community members directed at their outgroup members. Additionally, we expect a disengaging effect, that is, a downward trend in the focal community's membership. Finally, in neighboring communities that share the same topic of discussion, we expect a spillover effect, that is, an increase in harassment. Employing Detoxify, a Bidirectional Encoder Representations from Transformers (BERT)-based model, we evaluate harassment scores in the focal community by analyzing 4 million Reddit comments across various communities. These scores serve as inputs for Bayesian Structural Time Series analysis, revealing evidence for both disengaging and spillover effects. For the emboldening effect, we use community-specific keywords in a predefined computer-assisted document classification approach, Keyword Assisted Topic Model (keyATM), to identify the target of harassment. We use mean comparison and regression discontinuity to assess the change in the level of harassment targeting outgroup members before and after the human-machine moderation implementation.",0742-1222,1557-928X,,341-366, ,  ,,detection,
2467,"**Title**GoldenWind at SemEval-2021 Task 5: Orthrus - An Ensemble Approach to Identify Toxicity

**Abstract**Many new developments to detect and mitigate toxicity are currently being evaluated. We are particularly interested in the correlation between toxicity and the emotions expressed in online posts. While toxicity may be disguised by amending the wording of posts, emotions will not. Therefore, we describe here an ensemble method to identify toxicity and classify the emotions expressed on a corpus of annotated posts published by Task 5 of SemEval 2021-our analysis shows that the majority of such posts express anger, sadness and fear. Our method to identify toxicity combines a lexicon-based approach, which on its own achieves an F1 score of 61.07%, with a supervised learning approach, which on its own achieves an F1 score of 60%. When both methods are combined,","Palomino, Marco; Grad, Dawid; Bedwell, James",,,GoldenWind at SemEval-2021 Task 5: Orthrus - An Ensemble Approach to Identify Toxicity,,, ,Proceedings Paper ,,"Many new developments to detect and mitigate toxicity are currently being evaluated. We are particularly interested in the correlation between toxicity and the emotions expressed in online posts. While toxicity may be disguised by amending the wording of posts, emotions will not. Therefore, we describe here an ensemble method to identify toxicity and classify the emotions expressed on a corpus of annotated posts published by Task 5 of SemEval 2021-our analysis shows that the majority of such posts express anger, sadness and fear. Our method to identify toxicity combines a lexicon-based approach, which on its own achieves an F1 score of 61.07%, with a supervised learning approach, which on its own achieves an F1 score of 60%. When both methods are combined,",,,978-1-954085-70-1,860-864, , 15th International Workshops on Semantic Evaluation (SemEval)15th International Workshops on Semantic Evaluation (SemEval) ,,detection#methodology,
2468,"**Title**Fine-grained detoxification framework via instance-level prefixes for large language models

**Abstract**Large Language Models (LLMs) have demonstrated remarkable performance across various natural language processing (NLP) tasks. However, their practical usability is often compromised by a propensity to generate toxic content, such as insults, threats, and profanity, particularly in response to adversarial prompts. Several fine-tuning and decoding approaches have been employed to address this challenge to mitigate toxicity. Nevertheless, these methods typically necessitate additional resources, such as high-quality training data or auxiliary models, thereby incurring extra costs. In this paper, we propose a novel detoxification framework, Fine-Grained Detoxification via Instance-Level Prefixes (FGDILP), which effectively mitigates the generation of toxic text without incurring additional training costs. Specifically, FGDILP leverages contextualized representations in attention space by contrasting a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This methodology facilitates the construction of fine-grained subtoxicity vectors, which are subsequently fused to adjust the original generation pathway when responding to raw prompts. Our results demonstrate that FGDILP enables controlled text generation concerning detoxification at both the utterance and context levels. While our method slightly impacts generation fluency and diversity, it significantly outperforms prompt-based baselines regarding detoxification effectiveness. Our code is available at https://github.com/xinykou/FGDILP.","Yi, Xin; Wang, Linlin; Wang, Xiaoling; He, Liang","He, Liang/CAF-0477-2022",,Fine-grained detoxification framework via instance-level prefixes for large language models,611,,10.1016/j.neucom.2024.128684 ,Article ,,"Large Language Models (LLMs) have demonstrated remarkable performance across various natural language processing (NLP) tasks. However, their practical usability is often compromised by a propensity to generate toxic content, such as insults, threats, and profanity, particularly in response to adversarial prompts. Several fine-tuning and decoding approaches have been employed to address this challenge to mitigate toxicity. Nevertheless, these methods typically necessitate additional resources, such as high-quality training data or auxiliary models, thereby incurring extra costs. In this paper, we propose a novel detoxification framework, Fine-Grained Detoxification via Instance-Level Prefixes (FGDILP), which effectively mitigates the generation of toxic text without incurring additional training costs. Specifically, FGDILP leverages contextualized representations in attention space by contrasting a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This methodology facilitates the construction of fine-grained subtoxicity vectors, which are subsequently fused to adjust the original generation pathway when responding to raw prompts. Our results demonstrate that FGDILP enables controlled text generation concerning detoxification at both the utterance and context levels. While our method slightly impacts generation fluency and diversity, it significantly outperforms prompt-based baselines regarding detoxification effectiveness. Our code is available at https://github.com/xinykou/FGDILP.",0925-2312,1872-8286,,, ,  ,,detox,
2469,"**Title**WLV-RIT at SemEval-2021 Task 5: A Neural Transformer Framework for Detecting Toxic Spans

**Abstract**In recent years, the widespread use of social media has led to an increase in the generation of toxic and offensive content on online platforms. In response, social media platforms have worked on developing automatic detection methods and employing human moderators to cope with this deluge of offensive content. While various state-of-the-art statistical models have been applied to detect toxic posts, there are only a few studies that focus on detecting the words or expressions that make a post offensive. This motivates the organization of the SemEval-2021 Task 5: Toxic Spans Detection competition, which has provided participants with a dataset containing toxic spans annotation in English posts. In this paper, we present the WLV-RIT entry for the SemEval2021 Task 5. Our best performing neural transformer model achieves an 0.68 F1-Score. Furthermore, we develop an open-source framework for multilingual detection of offensive spans, i.e., MUDES, based on neural transformers that detect toxic spans in texts.","Ranasinghe, Tharindu; Sarkar, Diptanu; Zampieri, Marcos; Ororbia, Alexander","Ranasinghe, Tharindu/AAL-5855-2021",,WLV-RIT at SemEval-2021 Task 5: A Neural Transformer Framework for Detecting Toxic Spans,,, ,Proceedings Paper ,,"In recent years, the widespread use of social media has led to an increase in the generation of toxic and offensive content on online platforms. In response, social media platforms have worked on developing automatic detection methods and employing human moderators to cope with this deluge of offensive content. While various state-of-the-art statistical models have been applied to detect toxic posts, there are only a few studies that focus on detecting the words or expressions that make a post offensive. This motivates the organization of the SemEval-2021 Task 5: Toxic Spans Detection competition, which has provided participants with a dataset containing toxic spans annotation in English posts. In this paper, we present the WLV-RIT entry for the SemEval2021 Task 5. Our best performing neural transformer model achieves an 0.68 F1-Score. Furthermore, we develop an open-source framework for multilingual detection of offensive spans, i.e., MUDES, based on neural transformers that detect toxic spans in texts.",,,978-1-954085-70-1,833-840, , 15th International Workshops on Semantic Evaluation (SemEval)15th International Workshops on Semantic Evaluation (SemEval) ,,detection,
2474,"**Title**Analyzing And Editing Inner Mechanisms of Backdoored Language Models

**Abstract**Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets.Trigger warning: Offensive language.","Lamparth, Max; Reuel, Anka",,"Reuel, Ann-Katrin/0000-0002-7913-9296",Analyzing And Editing Inner Mechanisms of Backdoored Language Models,,,10.1145/3630106.3659042 ,Proceedings Paper ,,"Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets.Trigger warning: Offensive language.",,,979-8-4007-0450-5,2362-2373, ," 6th ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)6th ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT) ",,methodology,
2476,"**Title**Toxicity on Social Media During the 2022 Mpox Public Health Emergency: Quantitative Study of Topical and Network Dynamics

**Abstract**Background: Toxicity on social media, encompassing behaviors such as harassment, bullying, hate speech, and the dissemination of misinformation, has become a pressing social concern in the digital age. Its prevalence intensifies during periods of social crises and unrest, eroding a sense of safety and community. Such toxic environments can adversely impact the mental well-being of those exposed and further deepen societal divisions and polarization. The 2022 mpox outbreak, initially called monkeypox but later renamed to reduce stigma and address societal concerns, provides a relevant context for this issue. Objective: In this study, we conducted a comprehensive analysis of the toxic online discourse surrounding the 2022 mpox outbreak. We aimed to dissect its origins, characterize its nature and content, trace its dissemination patterns, and assess its broader societal implications, with the goal of providing insights that can inform strategies to mitigate such toxicity in future crises. Methods: We collected >1.6 million unique tweets and analyzed them with 5 dimensions: context, extent, content, speaker, and intent. Using topic modeling based on bidirectional encoder representations from transformers and social network community clustering, we delineated the toxic dynamics on Twitter. Results: By categorizing topics, we identified 5 high-level categories in the toxic online discourse on Twitter, including disease (20,281/43,521, 46.6%), health policy and health care (8400/43,521, 19.3%), homophobia (10,402/43,521, 23.9%), politics (2611/43,521, 6%), and racism (1784/43,521, 4.1%). Across these categories, users displayed negativity or controversial views on the mpox outbreak, highlighting the escalating political tensions and the weaponization of stigma during this infodemic. Through the toxicity diffusion networks of mentions (17,437 vertices with 3628 clusters), retweets (59,749 vertices with 3015 clusters), and the top users with the highest in-degree centrality, we found that retweets of toxic content were widespread, while influential users rarely engaged with or countered this toxicity through retweets. Conclusions: Our study introduces a comprehensive workflow that combines topical and network analyses to decode emerging social issues during crises. By tracking topical dynamics, we can track the changing popularity of toxic content on the internet, providing a better understanding of societal challenges. Network dynamics highlight key social media influencers and their intentions, suggesting that engaging with these central figures in toxic discourse can improve crisis communication and guide policy making.","Fan, Lizhou; Li, Lingyao; Hemphill, Libby","Hemphill, Libby/AAH-7062-2019",,Toxicity on Social Media During the 2022 Mpox Public Health Emergency: Quantitative Study of Topical and Network Dynamics,26,,10.2196/52997 ,Article ,,"Background: Toxicity on social media, encompassing behaviors such as harassment, bullying, hate speech, and the dissemination of misinformation, has become a pressing social concern in the digital age. Its prevalence intensifies during periods of social crises and unrest, eroding a sense of safety and community. Such toxic environments can adversely impact the mental well-being of those exposed and further deepen societal divisions and polarization. The 2022 mpox outbreak, initially called monkeypox but later renamed to reduce stigma and address societal concerns, provides a relevant context for this issue. Objective: In this study, we conducted a comprehensive analysis of the toxic online discourse surrounding the 2022 mpox outbreak. We aimed to dissect its origins, characterize its nature and content, trace its dissemination patterns, and assess its broader societal implications, with the goal of providing insights that can inform strategies to mitigate such toxicity in future crises. Methods: We collected >1.6 million unique tweets and analyzed them with 5 dimensions: context, extent, content, speaker, and intent. Using topic modeling based on bidirectional encoder representations from transformers and social network community clustering, we delineated the toxic dynamics on Twitter. Results: By categorizing topics, we identified 5 high-level categories in the toxic online discourse on Twitter, including disease (20,281/43,521, 46.6%), health policy and health care (8400/43,521, 19.3%), homophobia (10,402/43,521, 23.9%), politics (2611/43,521, 6%), and racism (1784/43,521, 4.1%). Across these categories, users displayed negativity or controversial views on the mpox outbreak, highlighting the escalating political tensions and the weaponization of stigma during this infodemic. Through the toxicity diffusion networks of mentions (17,437 vertices with 3628 clusters), retweets (59,749 vertices with 3015 clusters), and the top users with the highest in-degree centrality, we found that retweets of toxic content were widespread, while influential users rarely engaged with or countered this toxicity through retweets. Conclusions: Our study introduces a comprehensive workflow that combines topical and network analyses to decode emerging social issues during crises. By tracking topical dynamics, we can track the changing popularity of toxic content on the internet, providing a better understanding of societal challenges. Network dynamics highlight key social media influencers and their intentions, suggesting that engaging with these central figures in toxic discourse can improve crisis communication and guide policy making.",1438-8871,,,, ,  ,,detection,
2477,"**Title**Development of an Automated Moderator for Deliberative Events

**Abstract**Online communication platforms have revolutionized interpersonal interactions by transcending geographical barriers. While facilitating connectivity, these platforms have introduced challenges such as overcoming linguistic differences and preventing spam and offensive content diffusion. This is particularly pertinent in the context of deliberative events, where online platforms could be used to extend the inclusion of citizens in democratic decision-making. In traditional deliberative events, human moderators and translators were used to facilitate conversation; however, the need for these figures imposed a limit on both the number of deliberative events that could be organized and the number of participants. In response, this paper proposes an automated moderator for deliberative events. The moderator is developed in Python for the online communication platform Discord and can be used, thanks to the integrated AI (Artificial Intelligence) tools, to automatically manage conversation agendas, prevent spam and inappropriate language, analyze the sentiment of the conversation, and translate messages into multiple languages. In particular, three classifiers, based on a pre-trained BERT (Bidirection Encoder Representations from Transformers), were fine-tuned for spam detection, toxic comments classification, and sentiment analysis. These allow the moderator to automatically detect and remove spam and offensive messages in different languages, send warnings to users, alert administrators, and, after repeated warnings, impose bans. Additionally, a built-in translator, based on Meta's No Language Left Behind NLLB model, translates messages into five languages (Italian, English, French, German, and Polish). The developed bot was tested in a simulated deliberative event on a Discord server, demonstrating its ability to manage conversations and prevent linguistic abuse.","Bonechi, Simone","Bonechi, Simone/AIC-5356-2022","BONECHI, SIMONE/0000-0002-5540-3742",Development of an Automated Moderator for Deliberative Events,13,3.0,10.3390/electronics13030544 ,Article ,,"Online communication platforms have revolutionized interpersonal interactions by transcending geographical barriers. While facilitating connectivity, these platforms have introduced challenges such as overcoming linguistic differences and preventing spam and offensive content diffusion. This is particularly pertinent in the context of deliberative events, where online platforms could be used to extend the inclusion of citizens in democratic decision-making. In traditional deliberative events, human moderators and translators were used to facilitate conversation; however, the need for these figures imposed a limit on both the number of deliberative events that could be organized and the number of participants. In response, this paper proposes an automated moderator for deliberative events. The moderator is developed in Python for the online communication platform Discord and can be used, thanks to the integrated AI (Artificial Intelligence) tools, to automatically manage conversation agendas, prevent spam and inappropriate language, analyze the sentiment of the conversation, and translate messages into multiple languages. In particular, three classifiers, based on a pre-trained BERT (Bidirection Encoder Representations from Transformers), were fine-tuned for spam detection, toxic comments classification, and sentiment analysis. These allow the moderator to automatically detect and remove spam and offensive messages in different languages, send warnings to users, alert administrators, and, after repeated warnings, impose bans. Additionally, a built-in translator, based on Meta's No Language Left Behind NLLB model, translates messages into five languages (Italian, English, French, German, and Polish). The developed bot was tested in a simulated deliberative event on a Discord server, demonstrating its ability to manage conversations and prevent linguistic abuse.",,2079-9292,,, ,  ,,detection,
2478,"**Title**A New Generation of Perspective API: Efficient Multilingual Character-level Transformers

**Abstract**On the world wide web, toxic content detectors are a crucial line of defense against potentially hateful and offensive messages. As such, building highly effective classifiers that enable a safer internet is an important research area. Moreover, the web is a highly multilingual, cross-cultural community that develops its own lingo over time. As such, it is crucial to develop models that are effective across a diverse range of languages, usages, and styles. In this paper, we present the fundamentals behind the next version of the Perspective API from Google Jigsaw. At the heart of the approach is a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks. We demonstrate that by forgoing static vocabularies, we gain flexibility across a variety of settings. We additionally outline the techniques employed to make such a byte-level model efficient and feasible for productionization. Through extensive experiments on multilingual toxic comment classification benchmarks derived from real API traffic and evaluation on an array of code-switching, covert toxicity, emoji-based hate, human-readable obfuscation, distribution shift, and bias evaluation settings, we show that our proposed approach outperforms strong baselines. Finally, we present our findings from deploying this system in production.","Lees, Alyssa; Tran, Vinh Q.; Tay, Yi; Sorensen, Jeffrey; Gupta, Jai; Metzler, Donald; Vasserman, Lucy",,,A New Generation of Perspective API: Efficient Multilingual Character-level Transformers,,,10.1145/3534678.3539147 ,Proceedings Paper ,,"On the world wide web, toxic content detectors are a crucial line of defense against potentially hateful and offensive messages. As such, building highly effective classifiers that enable a safer internet is an important research area. Moreover, the web is a highly multilingual, cross-cultural community that develops its own lingo over time. As such, it is crucial to develop models that are effective across a diverse range of languages, usages, and styles. In this paper, we present the fundamentals behind the next version of the Perspective API from Google Jigsaw. At the heart of the approach is a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks. We demonstrate that by forgoing static vocabularies, we gain flexibility across a variety of settings. We additionally outline the techniques employed to make such a byte-level model efficient and feasible for productionization. Through extensive experiments on multilingual toxic comment classification benchmarks derived from real API traffic and evaluation on an array of code-switching, covert toxicity, emoji-based hate, human-readable obfuscation, distribution shift, and bias evaluation settings, we show that our proposed approach outperforms strong baselines. Finally, we present our findings from deploying this system in production.",,,978-1-4503-9385-0,3197-3207, , 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KKD)28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KKD) ,,detection,
2480,"**Title**RETRACTED: An Automated Toxicity Classification on Social Media Using LSTM and Word Embedding (Retracted Article)

**Abstract**The automated identification of toxicity in texts is a crucial area in text analysis since the social media world is replete with unfiltered content that ranges from mildly abusive to downright hateful. Researchers have found an unintended bias and unfairness caused by training datasets, which caused an inaccurate classification of toxic words in context. In this paper, several approaches for locating toxicity in texts are assessed and presented aiming to enhance the overall quality of text classification. General unsupervised methods were used depending on the state-of-art models and external embeddings to improve the accuracy while relieving bias and enhancing F1-score. Suggested approaches used a combination of long short-term memory (LSTM) deep learning model with Glove word embeddings and LSTM with word embeddings generated by the Bidirectional Encoder Representations from Transformers (BERT), respectively. These models were trained and tested on large secondary qualitative data containing a large number of comments classified as toxic or not. Results found that acceptable accuracy of 94% and an F1-score of 0.89 were achieved using LSTM with BERT word embeddings in the binary classification of comments (toxic and nontoxic). A combination of LSTM and BERT performed better than both LSTM unaccompanied and LSTM with Glove word embedding. This paper tries to solve the problem of classifying comments with high accuracy by pertaining models with larger corpora of text (high-quality word embedding) rather than the training data solely.","Alsharef, Ahmad; Aggarwal, Karan; Sonia; Koundal, Deepika; Alyami, Hashem; Ameyed, Darine","Koundal, Deepika/I-9927-2019; Aggarwal, Karan/AAR-5617-2020",", Sonia/0000-0002-0779-8065; Alsharef, Ahmad/0000-0002-9113-7859",RETRACTED: An Automated Toxicity Classification on Social Media Using LSTM and Word Embedding (Retracted Article),2022,,10.1155/2022/8467349 ,Article; Retracted Publication ,,"The automated identification of toxicity in texts is a crucial area in text analysis since the social media world is replete with unfiltered content that ranges from mildly abusive to downright hateful. Researchers have found an unintended bias and unfairness caused by training datasets, which caused an inaccurate classification of toxic words in context. In this paper, several approaches for locating toxicity in texts are assessed and presented aiming to enhance the overall quality of text classification. General unsupervised methods were used depending on the state-of-art models and external embeddings to improve the accuracy while relieving bias and enhancing F1-score. Suggested approaches used a combination of long short-term memory (LSTM) deep learning model with Glove word embeddings and LSTM with word embeddings generated by the Bidirectional Encoder Representations from Transformers (BERT), respectively. These models were trained and tested on large secondary qualitative data containing a large number of comments classified as toxic or not. Results found that acceptable accuracy of 94% and an F1-score of 0.89 were achieved using LSTM with BERT word embeddings in the binary classification of comments (toxic and nontoxic). A combination of LSTM and BERT performed better than both LSTM unaccompanied and LSTM with Glove word embedding. This paper tries to solve the problem of classifying comments with high accuracy by pertaining models with larger corpora of text (high-quality word embedding) rather than the training data solely.",1687-5265,1687-5273,,, ,  ,,detection,
2482,"**Title**QCon at SemEval-2023 Task 10: Data Augmentation and Model Ensembling for Detection of Online Sexism

**Abstract**The web contains an abundance of user-generated content. While this content is useful for many applications, it poses many challenges due to the presence of offensive, biased, and overall toxic language. In this work, we present a system that identifies and classifies sexist content at different levels of granularity. Using transformer-based models, we explore the value of data augmentation, use of ensemble methods, and leverage in-context learning using foundation models to tackle the task. We evaluate the different components of our system both quantitatively and qualitatively. Our best systems achieve an F-1 score of 0.84 for the binary classification task - aiming to identify whether a given content is sexist or not and 0.64 and 0.47 for the two multi-class tasks that aim to identify the coarse and fine-grained types of sexism present in the given content respectively.","Feely, Weston; Gupta, Prabhakar; Mohanty, Manas; Chon, Timothy; Kundu, Tuhin; Singh, Vijit; Atluri, Sandeep; Roosta, Tanya; Ghaderi, Viviane; Schulam, Peter; Elfardy, Heba",,,QCon at SemEval-2023 Task 10: Data Augmentation and Model Ensembling for Detection of Online Sexism,,, ,Proceedings Paper ,,"The web contains an abundance of user-generated content. While this content is useful for many applications, it poses many challenges due to the presence of offensive, biased, and overall toxic language. In this work, we present a system that identifies and classifies sexist content at different levels of granularity. Using transformer-based models, we explore the value of data augmentation, use of ensemble methods, and leverage in-context learning using foundation models to tackle the task. We evaluate the different components of our system both quantitatively and qualitatively. Our best systems achieve an F-1 score of 0.84 for the binary classification task - aiming to identify whether a given content is sexist or not and 0.64 and 0.47 for the two multi-class tasks that aim to identify the coarse and fine-grained types of sexism present in the given content respectively.",,,978-1-959429-99-9,1260-1270, , 17th International Workshop on Semantic Evaluation (SemEval)17th International Workshop on Semantic Evaluation (SemEval) ,,detection,
2486,"**Title**DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models

**Abstract**Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance - where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially because GPT-4 follows (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/.","Wang, Boxin; Chen, Weixin; Pei, Hengzhi; Xie, Chulin; Kang, Mintong; Zhang, Chenhui; Xu, Chejian; Xiong, Zidi; Dutta, Ritik; Schaeffer, Rylan; Truong, Sang T.; Arora, Simran; Mazeika, Mantas; Hendrycks, Dan; Lin, Zinan; Cheng, Yu; Koyejo, Sanmi; Song, Dawn; Li, Bo","Xiong, Zidi/KBB-8747-2024; , /GZM-3682-2022","Arora, Simran/0009-0002-7044-4689; Truong, Sang/0000-0001-8069-9410",DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models,,, ,Proceedings Paper ,,"Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance - where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially because GPT-4 follows (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/.",1049-5258,,*****************,, , 37th Conference on Neural Information Processing Systems (NeurIPS)37th Conference on Neural Information Processing Systems (NeurIPS) ,,Gen_dataset#evaluation,
2491,"**Title**Exploring ChatGPT for Toxicity Detection in GitHub

**Abstract**Fostering a collaborative and inclusive environment is crucial for the sustained progress of open source development. However, the prevalence of negative discourse, often manifested as toxic comments, poses significant challenges to developer well-being and productivity. To identify such negativity in project communications, especially within large projects, automated toxicity detection models are necessary. To train these models effectively, we need large software engineering-specific toxicity datasets. However, such datasets are limited in availability and often exhibit imbalance (e.g., only 6 in 1000 GitHub issues are toxic) [1], posing challenges for training effective toxicity detection models. To address this problem, we explore a zero-shot LLM (ChatGPT) that is pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting toxicity in software-related text. Our preliminary evaluation indicates that ChatGPT shows promise in detecting toxicity in GitHub, and warrants further investigation. We experimented with various prompts, including those designed for justifying model outputs, thereby enhancing model interpretability and paving the way for potential integration of ChatGPT-enabled toxicity detection into developer communication channels.","Mishra, Shyamal; Chatterjee, Preetha","Chatterjee, Preetha/AAS-2995-2021","Chatterjee, Preetha/0000-0003-3057-7807",Exploring ChatGPT for Toxicity Detection in GitHub,,,10.1145/3639476.3639777 ,Proceedings Paper ,,"Fostering a collaborative and inclusive environment is crucial for the sustained progress of open source development. However, the prevalence of negative discourse, often manifested as toxic comments, poses significant challenges to developer well-being and productivity. To identify such negativity in project communications, especially within large projects, automated toxicity detection models are necessary. To train these models effectively, we need large software engineering-specific toxicity datasets. However, such datasets are limited in availability and often exhibit imbalance (e.g., only 6 in 1000 GitHub issues are toxic) [1], posing challenges for training effective toxicity detection models. To address this problem, we explore a zero-shot LLM (ChatGPT) that is pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting toxicity in software-related text. Our preliminary evaluation indicates that ChatGPT shows promise in detecting toxicity in GitHub, and warrants further investigation. We experimented with various prompts, including those designed for justifying model outputs, thereby enhancing model interpretability and paving the way for potential integration of ChatGPT-enabled toxicity detection into developer communication channels.",,,979-8-4007-0500-7,6-10, , IEEE/ACM 46th International Conference on Software Engineering - New Ideas and Emerging Results (ICSE-NIER)IEEE/ACM 46th International Conference on Software Engineering - New Ideas and Emerging Results (ICSE-NIER) ,,detection,
2492,"**Title**Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks

**Abstract**Many text classification tasks are inherently ambiguous, which results in automatic systems having a high risk of making mistakes, in spite of using advanced machine learning models. For example, toxicity detection in user-generated content is a subjective task, and notions of toxicity can be annotated according to a variety of definitions that can be in conflict with one another. Instead of relying solely on automatic solutions, moderation of the most difficult and ambiguous cases can be delegated to human workers. Potential mistakes in automated classification can be identified by using uncertainty estimation (UE) techniques. Although UE is a rapidly growing field within natural language processing, we find that state-of-the-art UE methods estimate only epistemic uncertainty and show poor performance, or under-perform trivial methods for ambiguous tasks such as toxicity detection. We argue that in order to create robust uncertainty estimation methods for ambiguous tasks it is necessary to account also for aleatoric uncertainty. In this paper, we propose a new uncertainty estimation method that combines epistemic and aleatoric UE methods. We show that by using our hybrid method, we can outperform state-of-the-art UE methods for toxicity detection and other ambiguous text classification tasks(1).","Vazhentsev, Artem; Kuzmin, Gleb; Tsvigun, Akim; Panchenko, Alexander; Panov, Maxim; Burtsev, Mikhail; Shelmanov, Artem","Panchenko, Alexander/AAQ-7808-2021; Shelmanov, Artem/AAE-2008-2019; Tsvigun, Akim/HTT-2881-2023; Burtsev, Mikhail/G-6293-2010; Kuzmin, Gleb/KUC-6084-2024",,Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks,,, ,Proceedings Paper ,,"Many text classification tasks are inherently ambiguous, which results in automatic systems having a high risk of making mistakes, in spite of using advanced machine learning models. For example, toxicity detection in user-generated content is a subjective task, and notions of toxicity can be annotated according to a variety of definitions that can be in conflict with one another. Instead of relying solely on automatic solutions, moderation of the most difficult and ambiguous cases can be delegated to human workers. Potential mistakes in automated classification can be identified by using uncertainty estimation (UE) techniques. Although UE is a rapidly growing field within natural language processing, we find that state-of-the-art UE methods estimate only epistemic uncertainty and show poor performance, or under-perform trivial methods for ambiguous tasks such as toxicity detection. We argue that in order to create robust uncertainty estimation methods for ambiguous tasks it is necessary to account also for aleatoric uncertainty. In this paper, we propose a new uncertainty estimation method that combines epistemic and aleatoric UE methods. We show that by using our hybrid method, we can outperform state-of-the-art UE methods for toxicity detection and other ambiguous text classification tasks(1).",,,978-1-959429-72-2,11659-11681, , 61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL)61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL) ,,detection,
2493,"**Title**Exploring Moral Principles Exhibited in OSS: A Case Study on GitHub Heated Issues

**Abstract**To foster collaboration and inclusivity in Open Source Software (OSS) projects, it is crucial to understand and detect patterns of toxic language that may drive contributors away, especially those from underrepresented communities. Although machine learning-based toxicity detection tools trained on domain-specific data have shown promise, their design lacks an understanding of the unique nature and triggers of toxicity in OSS discussions, highlighting the need for further investigation. In this study, we employ Moral Foundations Theory to examine the relationship between moral principles and toxicity in OSS. Specifically, we analyze toxic communications in GitHub issue threads to identify and understand five types of moral principles exhibited in text, and explore their potential association with toxic behavior. Our preliminary findings suggest a possible link between moral principles and toxic comments in OSS communications, with each moral principle associated with at least one type of toxicity. The potential of MFT in toxicity detection warrants further investigation.","Ehsani, Ramtin; Rezapour, Rezvaneh; Chatterjee, Preetha","Chatterjee, Preetha/AAS-2995-2021","Ehsani, Ramtin/0000-0003-1517-7135; Chatterjee, Preetha/0000-0003-3057-7807",Exploring Moral Principles Exhibited in OSS: A Case Study on GitHub Heated Issues,,,10.1145/3611643.3613077 ,Proceedings Paper ,,"To foster collaboration and inclusivity in Open Source Software (OSS) projects, it is crucial to understand and detect patterns of toxic language that may drive contributors away, especially those from underrepresented communities. Although machine learning-based toxicity detection tools trained on domain-specific data have shown promise, their design lacks an understanding of the unique nature and triggers of toxicity in OSS discussions, highlighting the need for further investigation. In this study, we employ Moral Foundations Theory to examine the relationship between moral principles and toxicity in OSS. Specifically, we analyze toxic communications in GitHub issue threads to identify and understand five types of moral principles exhibited in text, and explore their potential association with toxic behavior. Our preliminary findings suggest a possible link between moral principles and toxic comments in OSS communications, with each moral principle associated with at least one type of toxicity. The potential of MFT in toxicity detection warrants further investigation.",,,979-8-4007-0327-0,2092-2096, , 31st ACM Joint Meeting of the European Software Engineering Conference / Symposium on the Foundations-of-Software-Engineering (ESEC/FSE)31st ACM Joint Meeting of the European Software Engineering Conference / Symposium on the Foundations-of-Software-Engineering (ESEC/FSE) ,,detection,
2494,"**Title**Performance and Risk Trade-offs for Multi-word Text Prediction at Scale Warning: The paper contains examples which the reader might find offensive.

**Abstract**Large Language Models such as GPT-3 are well-suited for text prediction tasks, which can help and delight users during text composition. LLMs are known to generate ethically inappropriate predictions even for seemingly innocuous contexts. Toxicity detection followed by filtering is a common strategy for mitigating the harm from such predictions. However, as we shall argue in this paper, in the context of text prediction, it is not sufficient to detect and filter toxic content. One also needs to ensure factual correctness and group-level fairness of the predictions; failing to do so can make the system ineffective and nonsensical at best, and unfair and detrimental to the users at worst. We discuss the gaps and challenges of toxicity detection approaches - from blocklist-based approaches to sophisticated state-of-the-art neural classifiers - by evaluating them on the text prediction task for English against a manually crafted CheckList of harms targeted at different groups and different levels of severity.","Vashishtha, Aniket; Prasad, S. Sai Krishna; Bajaj, Payal; Chaudhary, Vishrav; Cook, Kate; Dandapat, Sandipan; Sitaram, Sunayana; Choudhury, Monojit",,,Performance and Risk Trade-offs for Multi-word Text Prediction at Scale Warning: The paper contains examples which the reader might find offensive.,,, ,Proceedings Paper ,,"Large Language Models such as GPT-3 are well-suited for text prediction tasks, which can help and delight users during text composition. LLMs are known to generate ethically inappropriate predictions even for seemingly innocuous contexts. Toxicity detection followed by filtering is a common strategy for mitigating the harm from such predictions. However, as we shall argue in this paper, in the context of text prediction, it is not sufficient to detect and filter toxic content. One also needs to ensure factual correctness and group-level fairness of the predictions; failing to do so can make the system ineffective and nonsensical at best, and unfair and detrimental to the users at worst. We discuss the gaps and challenges of toxicity detection approaches - from blocklist-based approaches to sophisticated state-of-the-art neural classifiers - by evaluating them on the text prediction task for English against a manually crafted CheckList of harms targeted at different groups and different levels of severity.",,,978-1-959429-47-0,2226-2242, , 17th Conference of the European-Chapter of the Association-for-Computational-Linguistics (EACL)17th Conference of the European-Chapter of the Association-for-Computational-Linguistics (EACL) ,,detection,
2496,"**Title**Promoting Positive Discourse: Advancing AI-Powered Content Moderation with Explainability and User Rephrasing

**Abstract**Nothing is good or bad only thinking makes it so - is a well-known phrase we might have heard while growing up. Social media and discussion platforms provide unsupervised and an unbound stage to share and discuss ideas, opinions and thoughts. Apart from these they knowingly or unknowingly pave way to a space that generates and harbors toxicity. The contents of these are sometimes targeted on a certain group of people based on gender, caste, religion and countries not keeping in mind the civic sense. This paper introduces an innovative AI-powered system for content moderation, designed to combat online toxicity. The system utilizes a rule-table-based filter for efficient initial screening, blocking content with clear violations. For nuanced toxicity detection, we employ an ensemble of RoBERTa and BiLSTM models. Promoting collaboration, we integrate GPT-3.5 to generate rephrasing suggestions, empow- ering users to modify their language constructively. The system incorporates explainable AI to clarify model decisions, enhancing transparency and understanding. Finally, a user feedback loop ensures the system's ongoing evolution, maintaining its relevance to community standards.","Ananthajothi, K.; Meenakshi, R.; Monica, S.","K, Ananthajothi/AGE-3388-2022",,Promoting Positive Discourse: Advancing AI-Powered Content Moderation with Explainability and User Rephrasing,,,10.1109/ACCAI61061.2024.10601796 ,Proceedings Paper ,,"Nothing is good or bad only thinking makes it so - is a well-known phrase we might have heard while growing up. Social media and discussion platforms provide unsupervised and an unbound stage to share and discuss ideas, opinions and thoughts. Apart from these they knowingly or unknowingly pave way to a space that generates and harbors toxicity. The contents of these are sometimes targeted on a certain group of people based on gender, caste, religion and countries not keeping in mind the civic sense. This paper introduces an innovative AI-powered system for content moderation, designed to combat online toxicity. The system utilizes a rule-table-based filter for efficient initial screening, blocking content with clear violations. For nuanced toxicity detection, we employ an ensemble of RoBERTa and BiLSTM models. Promoting collaboration, we integrate GPT-3.5 to generate rephrasing suggestions, empow- ering users to modify their language constructively. The system incorporates explainable AI to clarify model decisions, enhancing transparency and understanding. Finally, a user feedback loop ensures the system's ongoing evolution, maintaining its relevance to community standards.",,,979-8-3503-8943-2; 979-8-3503-8944-9; 979-8-3503-8945-6,, ," International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI) ",,detection#methodology,
2497,"**Title**A Wide Evaluation of ChatGPT on Affective Computing Tasks

**Abstract**With the rise of foundation models, a new artificial intelligence paradigm has emerged, by simply using general purpose foundation models with prompting to solve problems instead of training a separate machine learning model for each problem. Such models have been shown to have emergent properties of solving problems that they were not initially trained on. The studies for the effectiveness of such models are still quite limited. In this work, we widely study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13 affective computing problems, namely aspect extraction, aspect polarity classification, opinion extraction, sentiment analysis, sentiment intensity ranking, emotions intensity ranking, suicide tendency detection, toxicity detection, well-being assessment, engagement measurement, personality assessment, sarcasm detection, and subjectivity detection. We introduce a framework to evaluate the ChatGPT models on regression-based problems, such as intensity ranking problems, by modelling them as pairwise ranking classification. We compare ChatGPT against more traditional NLP methods, such as end-to-end recurrent neural networks and transformers. The results demonstrate the emergent abilities of the ChatGPT models on a wide range of affective computing problems, where GPT-3.5 and especially GPT-4 have shown strong performance on many problems, particularly the ones related to sentiment, emotions, or toxicity. The ChatGPT models fell short for problems with implicit signals, such as engagement measurement and subjectivity detection.","Amin, Mostafa M.; Mao, Rui; Cambria, Erik; Schuller, Bjoern W.","Mao, Rui/ABM-7006-2022",,A Wide Evaluation of ChatGPT on Affective Computing Tasks,15,4.0,10.1109/TAFFC.2024.3419593 ,Article ,,"With the rise of foundation models, a new artificial intelligence paradigm has emerged, by simply using general purpose foundation models with prompting to solve problems instead of training a separate machine learning model for each problem. Such models have been shown to have emergent properties of solving problems that they were not initially trained on. The studies for the effectiveness of such models are still quite limited. In this work, we widely study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13 affective computing problems, namely aspect extraction, aspect polarity classification, opinion extraction, sentiment analysis, sentiment intensity ranking, emotions intensity ranking, suicide tendency detection, toxicity detection, well-being assessment, engagement measurement, personality assessment, sarcasm detection, and subjectivity detection. We introduce a framework to evaluate the ChatGPT models on regression-based problems, such as intensity ranking problems, by modelling them as pairwise ranking classification. We compare ChatGPT against more traditional NLP methods, such as end-to-end recurrent neural networks and transformers. The results demonstrate the emergent abilities of the ChatGPT models on a wide range of affective computing problems, where GPT-3.5 and especially GPT-4 have shown strong performance on many problems, particularly the ones related to sentiment, emotions, or toxicity. The ChatGPT models fell short for problems with implicit signals, such as engagement measurement and subjectivity detection.",1949-3045,,,2204-2212, ,  ,,detection,
2498,"**Title**Chain of Explanation: New Prompting Method to Generate Quality Natural Language Explanation for Implicit Hate Speech

**Abstract**Recent studies have exploited advanced generative language models to generate Natural Language Explanations (NLE) for why a certain text could be hateful. We propose the Chain of Explanation (CoE) Prompting method, using the heuristic words and target group, to generate high-quality NLE for implicit hate speech. We improved the BLUE score from 44.0 to 62.3 for NLE generation by providing accurate target information. We then evaluate the quality of generated NLE using various automatic metrics and human annotations of informativeness and clarity scores.","Huang, Fan; Kwak, Haewoon; An, Jisun",,"Huang, Fan/0000-0002-3097-0484; Kwak, Haewoon/0000-0003-1418-0834",Chain of Explanation: New Prompting Method to Generate Quality Natural Language Explanation for Implicit Hate Speech,,,10.1145/3543873.3587320 ,Proceedings Paper ,,"Recent studies have exploited advanced generative language models to generate Natural Language Explanations (NLE) for why a certain text could be hateful. We propose the Chain of Explanation (CoE) Prompting method, using the heuristic words and target group, to generate high-quality NLE for implicit hate speech. We improved the BLUE score from 44.0 to 62.3 for NLE generation by providing accurate target information. We then evaluate the quality of generated NLE using various automatic metrics and human annotations of informativeness and clarity scores.",,,978-1-4503-9416-1,90-93, , 32nd World Wide Web Conference (WWW)32nd World Wide Web Conference (WWW) ,,detection,
2500,"**Title**TextCheater: A Query-Efficient Textual Adversarial Attack in the Hard-Label Setting

**Abstract**Designing a query-efficient attack strategy to generate high-quality adversarial examples under the hard-label black-box setting is a fundamental yet challenging problem, especially in natural language processing (NLP). The process of searching for adversarial examples has many uncertainties (e.g., an unknown impact on the target model's prediction of the added perturbation) when confidence scores cannot be accessed, which must be compensated for with a large number of queries. To address this issue, we propose TextCheater, a decision-based metaheuristic search method that performs a query-efficient textual adversarial attack task by prohibiting invalid searches. The strategies of multiple initialization points and Tabu search are also introduced to keep the search process from falling into a local optimum. We apply our approach to three state-of-the-art language models (i.e., BERT, wordLSTM, and wordCNN) across six benchmark datasets and eight real-world commercial sentiment analysis platforms/models. Furthermore, we evaluate the Robustly optimized BERT pretraining Approach (RoBERTa) and models that enhance their robustness by adversarial training on toxicity detection and text classification tasks. The results demonstrate that our method minimizes the number of queries required for crafting plausible adversarial text while outperforming existing attack methods in the attack success rate, fluency of output sentences, and similarity between the original text and its adversary.","Peng, Hao; Guo, Shixin; Zhao, Dandan; Zhang, Xuhong; Han, Jianmin; Ji, Shouling; Yang, Xing; Zhong, Ming","Zhong, Ming/AHC-9485-2022; peng, hao/C-2042-2016; Yang, Xing/HHS-6130-2022","peng, hao/0000-0003-0586-7132; Zhong, Ming/0000-0002-9132-3782; zhao, dandan/0000-0001-9375-2997; Guo, Shixin/0009-0003-1763-5221; Yang, Xing/0000-0002-8824-1356",TextCheater: A Query-Efficient Textual Adversarial Attack in the Hard-Label Setting,21,4.0,10.1109/TDSC.2023.3339802 ,Article ,,"Designing a query-efficient attack strategy to generate high-quality adversarial examples under the hard-label black-box setting is a fundamental yet challenging problem, especially in natural language processing (NLP). The process of searching for adversarial examples has many uncertainties (e.g., an unknown impact on the target model's prediction of the added perturbation) when confidence scores cannot be accessed, which must be compensated for with a large number of queries. To address this issue, we propose TextCheater, a decision-based metaheuristic search method that performs a query-efficient textual adversarial attack task by prohibiting invalid searches. The strategies of multiple initialization points and Tabu search are also introduced to keep the search process from falling into a local optimum. We apply our approach to three state-of-the-art language models (i.e., BERT, wordLSTM, and wordCNN) across six benchmark datasets and eight real-world commercial sentiment analysis platforms/models. Furthermore, we evaluate the Robustly optimized BERT pretraining Approach (RoBERTa) and models that enhance their robustness by adversarial training on toxicity detection and text classification tasks. The results demonstrate that our method minimizes the number of queries required for crafting plausible adversarial text while outperforming existing attack methods in the attack success rate, fluency of output sentences, and similarity between the original text and its adversary.",1545-5971,1941-0018,,3901-3916, ,  ,,detection#methodology,
2501,"**Title**Identifying Spurious Correlations for Robust Text Classification

**Abstract**The predictions of text classifiers are often driven by spurious correlations - e.g., the term Spielberg correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we propose a method to distinguish spurious and genuine correlations in text classification. We treat this as a supervised classification problem, using features derived from treatment effect estimators to distinguish spurious correlations from genuine ones. Due to the generic nature of these features and their small dimensionality, we find that the approach works well even with limited training examples, and that it is possible to transport the word classifier to new domains. Experiments on four datasets (sentiment classification and toxicity detection) suggest that using this approach to inform feature selection also leads to more robust classification, as measured by improved worst-case accuracy on the samples affected by spurious correlations.","Wang, Zhao; Culotta, Aron","C, Aron/JFS-4299-2023",,Identifying Spurious Correlations for Robust Text Classification,,, ,Proceedings Paper ,,"The predictions of text classifiers are often driven by spurious correlations - e.g., the term Spielberg correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we propose a method to distinguish spurious and genuine correlations in text classification. We treat this as a supervised classification problem, using features derived from treatment effect estimators to distinguish spurious correlations from genuine ones. Due to the generic nature of these features and their small dimensionality, we find that the approach works well even with limited training examples, and that it is possible to transport the word classifier to new domains. Experiments on four datasets (sentiment classification and toxicity detection) suggest that using this approach to inform feature selection also leads to more robust classification, as measured by improved worst-case accuracy on the samples affected by spurious correlations.",,,978-1-952148-90-3,, , Meeting of the Association-for-Computational-Linguistics (ACL-EMNLP)Meeting of the Association-for-Computational-Linguistics (ACL-EMNLP) ,,detection#methodology,
2502,"**Title**Designing Closed-Loop Models for Task Allocation

**Abstract**Automatically assigning tasks to people is challenging because human performance can vary across tasks for many reasons. This challenge is further compounded in real-life settings in which no oracle exists to assess the quality of human decisions and task assignments made. Instead, we find ourselves in a closed decision-making loop in which the same fallible human decisions we rely on in practice must also be used to guide task allocation. How can imperfect and potentially biased human decisions train an accurate allocation model? Our key insight is to exploit weak prior information on human-task similarity to bootstrap model training. We show that the use of such a weak prior can improve task allocation accuracy, even when human decision-makers are fallible and biased. We present both theoretical analysis and empirical evaluation over synthetic data and a social media toxicity detection task. Results demonstrate the efficacy of our approach.","Keswani, Vijay; Celis, Elisa; Kenthapadi, Krishnaram; Lease, Matthew",,,Designing Closed-Loop Models for Task Allocation,368,,10.3233/FAIA230072 ,Proceedings Paper ,,"Automatically assigning tasks to people is challenging because human performance can vary across tasks for many reasons. This challenge is further compounded in real-life settings in which no oracle exists to assess the quality of human decisions and task assignments made. Instead, we find ourselves in a closed decision-making loop in which the same fallible human decisions we rely on in practice must also be used to guide task allocation. How can imperfect and potentially biased human decisions train an accurate allocation model? Our key insight is to exploit weak prior information on human-task similarity to bootstrap model training. We show that the use of such a weak prior can improve task allocation accuracy, even when human decision-makers are fallible and biased. We present both theoretical analysis and empirical evaluation over synthetic data and a social media toxicity detection task. Results demonstrate the efficacy of our approach.",0922-6389,1879-8314,978-1-64368-394-2; 978-1-64368-395-9,17-32, , 2nd International Conference on Hybrid Human-Artificial Intelligence (HHAI)2nd International Conference on Hybrid Human-Artificial Intelligence (HHAI) ,,detection#methodology,
2503,"**Title**Is ChatGPT beter than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech

**Abstract**Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.","Huang, Fan; Kwak, Haewoon; An, Jisun",,"An, Jisun/0000-0002-4353-8009; Kwak, Haewoon/0000-0003-1418-0834; Huang, Fan/0000-0002-3097-0484",Is ChatGPT beter than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech,,,10.1145/3543873.3587368 ,Proceedings Paper ,,"Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.",,,978-1-4503-9416-1,294-297, , 32nd World Wide Web Conference (WWW)32nd World Wide Web Conference (WWW) ,,detection#methodology,
2504,"**Title**From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models

**Abstract**Warning: content in this paper may be upsetting or offensive to some readers.Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second one, often hateful or provocative, to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, in the sentence we need to end the cosmopolitan experiment, the word cosmopolitan likely means worldly to many, but secretly means Jewish to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians' speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3's performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks of such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources for future research in modeling dogwhistles and mitigating their online harms.","Mendelsohn, Julia; Le Bras, Ronan; Choi, Yejin; Sap, Maarten",,,From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models,,, ,Proceedings Paper ,,"Warning: content in this paper may be upsetting or offensive to some readers.Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second one, often hateful or provocative, to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, in the sentence we need to end the cosmopolitan experiment, the word cosmopolitan likely means worldly to many, but secretly means Jewish to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians' speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3's performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks of such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources for future research in modeling dogwhistles and mitigating their online harms.",,,978-1-959429-72-2,15162-15180, , 61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL)61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL) ,,Gen_dataset#detection#evaluation#methodology,
2505,"**Title**Causal keyword driven reliable text classification with large language model feedback

**Abstract**Recent studies show Pre-trained Language Models (PLMs) tend to shortcut learning, reducing effectiveness with Out-Of-Distribution (OOD) samples, prompting research on the impact of shortcuts and robust causal features by interpretable methods for text classification. However, current approaches encounter two primary challenges. Firstly, black-box interpretable methods often yield incorrect causal keywords. Secondly, existing methods do not differentiate between shortcuts and causal keywords, often employing a unified approach to deal with them. To address the first challenge, we propose a framework that incorporates Large Language Model's feedback into the process of identifying shortcuts and causal keywords. Specifically, we transform causal feature extraction into a word-level binary labeling task with the aid of ChatGPT. For the second challenge, we introduce a multi-grained shortcut mitigation framework. This framework includes two auxiliary tasks aimed at addressing shortcuts and causal features separately: shortcut reconstruction and counterfactual contrastive learning. These tasks enhance PLMs at both the token and sample granularity levels, respectively. Experimental results show that the proposed method achieves an average performance improvement of more than 1% under the premise of four different language model as the backbones for sentiment classification and toxicity detection tasks on 8 datasets compared with the most recent baseline methods.","Song, Rui; Li, Yingji; Tian, Mingjie; Wang, Hanwen; Giunchiglia, Fausto; Xu, Hao",,"Li, Yingji/0000-0002-3575-1395; Giunchiglia, Fausto/0000-0002-5903-6150",Causal keyword driven reliable text classification with large language model feedback,62,2.0,10.1016/j.ipm.2024.103964 ,Article ,,"Recent studies show Pre-trained Language Models (PLMs) tend to shortcut learning, reducing effectiveness with Out-Of-Distribution (OOD) samples, prompting research on the impact of shortcuts and robust causal features by interpretable methods for text classification. However, current approaches encounter two primary challenges. Firstly, black-box interpretable methods often yield incorrect causal keywords. Secondly, existing methods do not differentiate between shortcuts and causal keywords, often employing a unified approach to deal with them. To address the first challenge, we propose a framework that incorporates Large Language Model's feedback into the process of identifying shortcuts and causal keywords. Specifically, we transform causal feature extraction into a word-level binary labeling task with the aid of ChatGPT. For the second challenge, we introduce a multi-grained shortcut mitigation framework. This framework includes two auxiliary tasks aimed at addressing shortcuts and causal features separately: shortcut reconstruction and counterfactual contrastive learning. These tasks enhance PLMs at both the token and sample granularity levels, respectively. Experimental results show that the proposed method achieves an average performance improvement of more than 1% under the premise of four different language model as the backbones for sentiment classification and toxicity detection tasks on 8 datasets compared with the most recent baseline methods.",0306-4573,1873-5371,,, ,  ,,detection#methodology,
2507,"**Title**Unveiling Toxic Tendencies of Small Language Models in Unconstrained Generation Tasks

**Abstract**The prevalence of toxicity online presents a significant challenge for platforms and publishers alike. Recent studies conducted on Small Language Models (SLMs) have identified the inherent toxicity that dwell in these models. In this work, we study and benchmark the extent to which SLMs can be prompted to generate toxic language. The following SLMs are evaluated for their toxicity levels: GPT-2 Large, Gemma-2B, Mistral-7B, Falcon-7B, and Llama 2-13B. We go a step closer to understanding the correlation between toxicity and the intrinsic parameters of the state-of-the-art SLMs. Next, we study the efficacy of a basic word-filtering approach to controlled text generation. Following this, we proceed to establish a mathematical ground for computing the weighted toxicity of continuations with respect to the toxicity of prompts by treating toxicity as a fuzzy metric. Finally, we extend our analysis to examine the unexpected toxicity levels of generated continuations when prompted with non-toxic inputs.","Chandra, Lakshay; Susan, Seba; Kumar, Dhruv; Kant, Krishan","Susan, Dr. Seba/KHY-0356-2024",,Unveiling Toxic Tendencies of Small Language Models in Unconstrained Generation Tasks,,,10.1109/CONECCT62155.2024.10677188 ,Proceedings Paper ,,"The prevalence of toxicity online presents a significant challenge for platforms and publishers alike. Recent studies conducted on Small Language Models (SLMs) have identified the inherent toxicity that dwell in these models. In this work, we study and benchmark the extent to which SLMs can be prompted to generate toxic language. The following SLMs are evaluated for their toxicity levels: GPT-2 Large, Gemma-2B, Mistral-7B, Falcon-7B, and Llama 2-13B. We go a step closer to understanding the correlation between toxicity and the intrinsic parameters of the state-of-the-art SLMs. Next, we study the efficacy of a basic word-filtering approach to controlled text generation. Following this, we proceed to establish a mathematical ground for computing the weighted toxicity of continuations with respect to the toxicity of prompts by treating toxicity as a fuzzy metric. Finally, we extend our analysis to examine the unexpected toxicity levels of generated continuations when prompted with non-toxic inputs.",2334-0940,,979-8-3503-8593-9; 979-8-3503-8592-2,, ," 10th IEEE International Conference on Electronics, Computing and Communication Technologies (IEEE CONECCT)10th IEEE International Conference on Electronics, Computing and Communication Technologies (IEEE CONECCT) ",,evaluation,
2510,"**Title**Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models

**Abstract**Pretrained large language models have become indispensable for solving various natural language processing (NLP) tasks. However, safely deploying them in real world applications is challenging because they generate toxic content. To address this challenge, we propose two novel pretraining data augmentation strategies that significantly reduce model toxicity without compromising its utility. Our two strategies are: (1) MEDA: adds raw toxicity score as meta-data to the pretraining samples, and (2) INST: adds instructions to those samples indicating their toxicity. Our results indicate that our best performing strategy (INST) substantially reduces the toxicity probability up to 61% while preserving the accuracy on five benchmark NLP tasks as well as improving AUC scores on four bias detection tasks by 1.3%. We also demonstrate the generalizability of our techniques by scaling the number of training samples and the number of model parameters.","Prabhumoye, Shrimai; Patwary, Mostofa; Shoeybi, Mohammad; Catanzaro, Bryan",,,Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models,,, ,Proceedings Paper ,,"Pretrained large language models have become indispensable for solving various natural language processing (NLP) tasks. However, safely deploying them in real world applications is challenging because they generate toxic content. To address this challenge, we propose two novel pretraining data augmentation strategies that significantly reduce model toxicity without compromising its utility. Our two strategies are: (1) MEDA: adds raw toxicity score as meta-data to the pretraining samples, and (2) INST: adds instructions to those samples indicating their toxicity. Our results indicate that our best performing strategy (INST) substantially reduces the toxicity probability up to 61% while preserving the accuracy on five benchmark NLP tasks as well as improving AUC scores on four bias detection tasks by 1.3%. We also demonstrate the generalizability of our techniques by scaling the number of training samples and the number of model parameters.",,,978-1-959429-44-9,2636-2651, , 17th Conference of the European-Chapter of the Association-for-Computational-Linguistics (EACL)17th Conference of the European-Chapter of the Association-for-Computational-Linguistics (EACL) ,,detox,
2512,"**Title**Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies

**Abstract**While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback-either produced by the LLM itself (self-correction) or some external system-are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.","Pan, Liangming; Saxon, Michael; Xu, Wenda; Nathani, Deepak; Wang, Xinyi; Wang, William Yang","Wang, Xinyi/IAM-0594-2023; Pan, Liangming/LIF-2753-2024; Nathani, Deepak/ABG-4715-2021",,Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies,12,,10.1162/tacl_a_00660 ,Article ,,"While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback-either produced by the LLM itself (self-correction) or some external system-are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.",,2307-387X,,484-506, ,  ,,survey,
2513,"**Title**Toward a Programmable Humanizing Artificial Intelligence Through Scalable Stance-Directed Architecture

**Abstract**The rise of harmful online content underscores the urgent need for artificial intelligence (AI) systems to effectively detect, filter, and foster safer and healthier communication. This article introduces a novel approach to mitigating toxic content generation propensities of large language models (LLMs) by fine-tuning them with a programmable stance-directed focus on core human values and the common good. We propose a streamlined keyword coding and processing pipeline that generates weakly labeled data to train AI models to avoid toxicity and champion civil discourse. We also developed a toxicity classifier and an aspect-based sentiment analysis model to assess and control the effectiveness of a humanizing AI model. We evaluate the proposed pipeline using a contentious real-world X (formerly Twitter) dataset on U.S. race relations. Our approach successfully curbs the toxic content generation propensity of an unrestricted LLM by a significant 85%.","Cetinkaya, Yusuf Mucahit; Lee, Yeonjung; Kulah, Emre; Toroslu, Ismail Hakki; Cowan, Michael A.; Davulcu, Hasan","etinkaya, Yusuf Mcahit/GXH-9957-2022; Klah, Emre/ABB-4969-2020; Toroslu, Ismail/Q-5390-2019","toroslu, ismail/0000-0002-4524-8232; Lee, Yeonjung/0000-0003-4048-1841; Davulcu, Hasan/0000-0001-5602-8270; Cetinkaya, Yusuf Mucahit/0000-0001-5338-750X; Kulah, Emre/0000-0003-0877-5487",Toward a Programmable Humanizing Artificial Intelligence Through Scalable Stance-Directed Architecture,28,5.0,10.1109/MIC.2024.3450090 ,Article ,,"The rise of harmful online content underscores the urgent need for artificial intelligence (AI) systems to effectively detect, filter, and foster safer and healthier communication. This article introduces a novel approach to mitigating toxic content generation propensities of large language models (LLMs) by fine-tuning them with a programmable stance-directed focus on core human values and the common good. We propose a streamlined keyword coding and processing pipeline that generates weakly labeled data to train AI models to avoid toxicity and champion civil discourse. We also developed a toxicity classifier and an aspect-based sentiment analysis model to assess and control the effectiveness of a humanizing AI model. We evaluate the proposed pipeline using a contentious real-world X (formerly Twitter) dataset on U.S. race relations. Our approach successfully curbs the toxic content generation propensity of an unrestricted LLM by a significant 85%.",1089-7801,1941-0131,,20-27, ,  ,,detox,
2518,"**Title**Quark: Controllable Text Generation with Reinforced [Un]learning

**Abstract**Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO [66], while relying only on standard language modeling primitives.","Lu, Ximing; Welleck, Sean; Hessel, Jack; Jiang, Liwei; Qin, Lianhui; West, Peter; Ammanabrolu, Prithviraj; Choi, Yejin","Lu, Ximing/LLL-7542-2024",,Quark: Controllable Text Generation with Reinforced [Un]learning,,, ,Proceedings Paper ,,"Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO [66], while relying only on standard language modeling primitives.",1049-5258,,978-1-7138-7108-8,, , 36th Conference on Neural Information Processing Systems (NeurIPS)36th Conference on Neural Information Processing Systems (NeurIPS) ,,detox,
2519,"**Title**Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models

**Abstract**Large language models produce human-like text that drives a growing number of applications. However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful. Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward. To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks. We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks. Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation.","Rauh, Maribeth; Mellor, John; Uesato, Jonathan; Huang, Po-Sen; Welbl, Johannes; Weidinger, Laura; Dathathri, Sumanth; Glaese, Amelia; Irving, Geoffrey; Gabriel, Iason; Isaac, William; Hendricks, Lisa Anne","Gabriel, Iason/IST-7093-2023","Gabriel, Iason/0000-0002-7552-4576",Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,,, ,Proceedings Paper ,,"Large language models produce human-like text that drives a growing number of applications. However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful. Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward. To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks. We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks. Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation.",1049-5258,,978-1-7138-7108-8,, , 36th Conference on Neural Information Processing Systems (NeurIPS)36th Conference on Neural Information Processing Systems (NeurIPS) ,,evaluation#methodology,
2520,"**Title**REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models

**Abstract**Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release REALTOXICITYPROMPTS, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using REALTOXICITYPROMPTS, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning bad words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et al., 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.","Gehman, Samuel; Gururangan, Suchin; Sap, Maarten; Choi, Yejin; Smith, Noah A.",,,REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models,,, ,Proceedings Paper ,,"Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release REALTOXICITYPROMPTS, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using REALTOXICITYPROMPTS, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning bad words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et al., 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",,,978-1-952148-90-3,, , Meeting of the Association-for-Computational-Linguistics (ACL-EMNLP)Meeting of the Association-for-Computational-Linguistics (ACL-EMNLP) ,,Gen_dataset#evaluation,
2524,"**Title**Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation

**Abstract**Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of expert PEM and anti-expert PEM. Remarkably, even antiexpert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of truthfulness and detoxification, we conduct extensive experiments on LLMs, encompassing additional abilities such as language modelling and mathematical reasoning. Our empirical results demonstrate that our approach effectively improves truthfulness and detoxification, while largely preserving the fundamental abilities of LLMs.","Hu, Xinshuo; Li, Dongfang; Hu, Baotian; Zheng, Zihao; Liu, Zhenyu; Zhang, Min","Hu, Baotian/AAA-4102-2022; Liu, Zhenyu/LBI-5068-2024; Li, Dongfang/IWY-0600-2023",,Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation,,, ,Proceedings Paper ,,"Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of expert PEM and anti-expert PEM. Remarkably, even antiexpert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of truthfulness and detoxification, we conduct extensive experiments on LLMs, encompassing additional abilities such as language modelling and mathematical reasoning. Our empirical results demonstrate that our approach effectively improves truthfulness and detoxification, while largely preserving the fundamental abilities of LLMs.",2159-5399,2374-3468,*****************,18252-18260, , 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence ,,detox,
2526,"**Title**Securing Applications of Large Language Models: A Shift-Left Approach

**Abstract**The emergence of large language models (LLMs) has brought forth remarkable capabilities in various domains, yet it also poses inherent risks to trustfulness, encompassing concerns such as toxicity, stereotype bias, adversarial robustness, ethics, privacy, and fairness. Particularly in sensitive applications like customer support chatbots, AI assistants, and digital information automation, which handle privacy-sensitive data, the adoption of generative pre-trained transformer (GPT) models is pervasive. However, ensuring robust security measures to mitigate potential security vulnerabilities is imperative. This paper advocates for a proactive approach termed security shift-left, which emphasizes integrating security measures early in the development lifecycle to bolster the security posture of LLM-based applications. Our proposed method leverages basic machine learning (ML) techniques and retrieval-augmented generation (RAG) to effectively address security concerns. We present empirical evidence validating the efficacy of our approach with one LLM-based security application designed for the detection of malicious intent, utilizing both open-source datasets and synthesized datasets. By adopting this security shift-left methodology, developers can confidently develop LLM-based applications with robust security protection, safeguarding against potential threats and vulnerabilities.","Lan, Qianlong; Kaul, Anuj; Das Pattanaik, Nishant Kumar; Pattanayak, Piyush; Pandurangan, Vinothini","Lan, Qianlong/M-4141-2019",,Securing Applications of Large Language Models: A Shift-Left Approach,,,10.1109/eIT60633.2024.10609922 ,Proceedings Paper ,,"The emergence of large language models (LLMs) has brought forth remarkable capabilities in various domains, yet it also poses inherent risks to trustfulness, encompassing concerns such as toxicity, stereotype bias, adversarial robustness, ethics, privacy, and fairness. Particularly in sensitive applications like customer support chatbots, AI assistants, and digital information automation, which handle privacy-sensitive data, the adoption of generative pre-trained transformer (GPT) models is pervasive. However, ensuring robust security measures to mitigate potential security vulnerabilities is imperative. This paper advocates for a proactive approach termed security shift-left, which emphasizes integrating security measures early in the development lifecycle to bolster the security posture of LLM-based applications. Our proposed method leverages basic machine learning (ML) techniques and retrieval-augmented generation (RAG) to effectively address security concerns. We present empirical evidence validating the efficacy of our approach with one LLM-based security application designed for the detection of malicious intent, utilizing both open-source datasets and synthesized datasets. By adopting this security shift-left methodology, developers can confidently develop LLM-based applications with robust security protection, safeguarding against potential threats and vulnerabilities.",2154-0357,,979-8-3503-3065-6; 979-8-3503-3064-9,378-379, , IEEE International Conference on Electro Information Technology (EIT)IEEE International Conference on Electro Information Technology (EIT) ,,detox#evaluation,
2528,"**Title**AI model disgorgement: Methods and choices

**Abstract**Over the past few years, machine learning models have significantly increased in size and complexity, especially in the area of generative AI such as large language models. These models require massive amounts of data and compute capacity to train, to the extent that concerns over the training data (such as protected or private content) cannot be practically addressed by retraining the model from scratch with the questionable data removed or altered. Furthermore, despite significant efforts and controls dedicated to ensuring that training corpora are properly curated and composed, the sheer volume required makes it infeasible to manually inspect each datum comprising a training corpus. One potential approach to training corpus data defects is model disgorgement, by which we broadly mean the elimination or reduction of not only any improperly used data, but also the effects of improperly used data on any component of an ML model. Model disgorgement techniques can be used to address a wide range of issues, such as reducing bias or toxicity, increasing fidelity, and ensuring responsible use of intellectual property. In this paper, we survey the landscape of model disgorgement methods and introduce a taxonomy of disgorgement techniques that are applicable to modern ML systems. In particular, we investigate the various meanings of removing the effects of data on the trained model in a way that does not require retraining from scratch.","Achille, Alessandro; Kearns, Michael; Klingenberg, Carson; Soatto, Stefano",,"Klingenberg, Carson/0009-0001-5386-0958",AI model disgorgement: Methods and choices,121,18.0,10.1073/pnas.2307304121 ,Article ,,"Over the past few years, machine learning models have significantly increased in size and complexity, especially in the area of generative AI such as large language models. These models require massive amounts of data and compute capacity to train, to the extent that concerns over the training data (such as protected or private content) cannot be practically addressed by retraining the model from scratch with the questionable data removed or altered. Furthermore, despite significant efforts and controls dedicated to ensuring that training corpora are properly curated and composed, the sheer volume required makes it infeasible to manually inspect each datum comprising a training corpus. One potential approach to training corpus data defects is model disgorgement, by which we broadly mean the elimination or reduction of not only any improperly used data, but also the effects of improperly used data on any component of an ML model. Model disgorgement techniques can be used to address a wide range of issues, such as reducing bias or toxicity, increasing fidelity, and ensuring responsible use of intellectual property. In this paper, we survey the landscape of model disgorgement methods and introduce a taxonomy of disgorgement techniques that are applicable to modern ML systems. In particular, we investigate the various meanings of removing the effects of data on the trained model in a way that does not require retraining from scratch.",0027-8424,1091-6490,,, ,  ,,detox,
2531,"**Title**On Second Thought, Lets Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning

**Abstract**Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zeroshot CoT reasoning in sensitive domains significantly increases a model ' s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.","Shaikh, Omar; Zhang, Hongxin; Held, William; Bernstein, Michael; Yang, Diyi",,"Bernstein, Michael/0000-0001-8020-9434","On Second Thought, Lets Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",,, ,Proceedings Paper ,,"Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zeroshot CoT reasoning in sensitive domains significantly increases a model ' s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.",,,978-1-959429-72-2,4454-4470, , 61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL)61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL) ,,evaluation,
2532,"**Title**DEE: Dual-Stage Explainable Evaluation Method for Text Generation

**Abstract**Automatic methods for evaluating machine-generated texts hold significant importance due to the expanding applications of generative systems. Conventional methods tend to grapple with a lack of explainability, issuing a solitary numerical score to signify the assessment outcome. Recent advancements have sought to mitigate this limitation by incorporating large language models (LLMs) to offer more detailed error analyses, yet their applicability remains constrained, particularly in industrial contexts where comprehensive error coverage and swift detection are paramount. To alleviate these challenges, we introduce DEE, a Dual-stage Explainable Evaluation method for estimating the quality of text generation. Built upon Llama 2, DEE follows a dual-stage principle guided by stage-specific instructions to perform efficient identification of errors in generated texts in the initial stage and subsequently delves into providing comprehensive diagnostic reports in the second stage. DEE is fine-tuned on our elaborately assembled dataset AntEval, which encompasses 15K instances from 4 real-world applications of Alipay that employ generative systems. The dataset concerns newly emerged issues like hallucination and toxicity, thereby broadening the scope of DEE's evaluation criteria. Experimental results affirm that DEE's superiority over existing evaluation methods, achieving significant improvements in both human correlation as well as efficiency.","Zhang, Shenyu; Li, Yu; Wu, Rui; Huang, Xiutian; Chen, Yongrui; Xu, Wenhao; Qi, Guilin","Chen, Yongrui/AAS-3725-2020",,DEE: Dual-Stage Explainable Evaluation Method for Text Generation,14856,,10.1007/978-981-97-5575-2_29 ,Proceedings Paper ,,"Automatic methods for evaluating machine-generated texts hold significant importance due to the expanding applications of generative systems. Conventional methods tend to grapple with a lack of explainability, issuing a solitary numerical score to signify the assessment outcome. Recent advancements have sought to mitigate this limitation by incorporating large language models (LLMs) to offer more detailed error analyses, yet their applicability remains constrained, particularly in industrial contexts where comprehensive error coverage and swift detection are paramount. To alleviate these challenges, we introduce DEE, a Dual-stage Explainable Evaluation method for estimating the quality of text generation. Built upon Llama 2, DEE follows a dual-stage principle guided by stage-specific instructions to perform efficient identification of errors in generated texts in the initial stage and subsequently delves into providing comprehensive diagnostic reports in the second stage. DEE is fine-tuned on our elaborately assembled dataset AntEval, which encompasses 15K instances from 4 real-world applications of Alipay that employ generative systems. The dataset concerns newly emerged issues like hallucination and toxicity, thereby broadening the scope of DEE's evaluation criteria. Experimental results affirm that DEE's superiority over existing evaluation methods, achieving significant improvements in both human correlation as well as efficiency.",0302-9743,1611-3349,978-981-97-5574-5; 978-981-97-5575-2,390-401, , 29th International Conference on Database Systems for Advanced Applications (DASFAA)29th International Conference on Database Systems for Advanced Applications (DASFAA) ,,Gen_dataset,
2534,"**Title**People who share encounters with racism are silenced online by humans and machines, but a guideline-reframing intervention holds promise

**Abstract**Are members of marginalized communities silenced on social media when they share personal experiences of racism? Here, we investigate the role of algorithms, humans, and platform guidelines in suppressing disclosures of racial discrimination. In a field study of actual posts from a neighborhood-based social media platform, we find that when users talk about their experiences as targets of racism, their posts are disproportionately flagged for removal as toxic by five widely used moderation algorithms from major online platforms, including the most recent large language models. We show that human users disproportionately flag these disclosures for removal as well. Next, in a follow-up experiment, we demonstrate that merely witnessing such suppression negatively influences how Black Americans view the community and their place in it. Finally, to address these challenges to equity and inclusion in online spaces, we introduce a mitigation strategy: a guideline-reframing intervention that is effective at reducing silencing behavior across the political spectrum.","Lee, Cinoo; Gligoric, Kristina; Kalluri, Pratyusha Ria; Harrington, Maggie; Durmus, Esin; Sanchez, Kiara L.; San, Nay; Tse, Danny; Zhao, Xuan; Hamedani, MarYam G.; Markus, Hazel Rose; Jurafsky, Dan; Eberhardt, Jennifer L.","Gligori, Kristina/AAL-5227-2021","Gligoric, Kristina/0000-0001-8726-740X; Lee, Cinoo/0009-0002-1305-4705; Hamedani, MarYam/0000-0001-6925-4947","People who share encounters with racism are silenced online by humans and machines, but a guideline-reframing intervention holds promise",121,38.0,10.1073/pnas.2322764121 ,Article ,,"Are members of marginalized communities silenced on social media when they share personal experiences of racism? Here, we investigate the role of algorithms, humans, and platform guidelines in suppressing disclosures of racial discrimination. In a field study of actual posts from a neighborhood-based social media platform, we find that when users talk about their experiences as targets of racism, their posts are disproportionately flagged for removal as toxic by five widely used moderation algorithms from major online platforms, including the most recent large language models. We show that human users disproportionately flag these disclosures for removal as well. Next, in a follow-up experiment, we demonstrate that merely witnessing such suppression negatively influences how Black Americans view the community and their place in it. Finally, to address these challenges to equity and inclusion in online spaces, we introduce a mitigation strategy: a guideline-reframing intervention that is effective at reducing silencing behavior across the political spectrum.",0027-8424,1091-6490,,, ,  ,,detection#evaluation,
2535,"**Title**GeDi: Generative Discriminator Guided Sequence Generation

**Abstract**While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. One promising approach to address this is to use discriminators to guide decoding from LMs, but existing methods for this are too slow to be useful in practice for many applications. We present GeDi as a significantly more efficient discriminator-based approach for guiding decoding. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than previous controllable generation methods. GeDi results in significantly faster generation speeds than the only previous method that achieved comparable controllability in our experiments. We also show that GeDi can make GPT-2 and GPT-3 significantly less toxic while maintaining linguistic fluency, without sacrificing significantly on generation speed. Lastly, we find training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword.","Krause, Ben; Gotmare, Akhilesh Deepak; McCann, Bryan; Keskar, Nitish Shirish; Joty, Shafiq; Socher, Richard; Rajani, Nazneen Fatema",,,GeDi: Generative Discriminator Guided Sequence Generation,,, ,Proceedings Paper ,,"While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. One promising approach to address this is to use discriminators to guide decoding from LMs, but existing methods for this are too slow to be useful in practice for many applications. We present GeDi as a significantly more efficient discriminator-based approach for guiding decoding. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than previous controllable generation methods. GeDi results in significantly faster generation speeds than the only previous method that achieved comparable controllability in our experiments. We also show that GeDi can make GPT-2 and GPT-3 significantly less toxic while maintaining linguistic fluency, without sacrificing significantly on generation speed. Lastly, we find training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword.",,,978-1-955917-10-0,4929-4952, , Meeting of the Association-for-Computational-Linguistics (ACL-EMNLP)Meeting of the Association-for-Computational-Linguistics (ACL-EMNLP) ,,detox,
2536,"**Title**Jill Watson: A Virtual Teaching Assistant Powered by ChatGPT

**Abstract**Conversational AI agents often require extensive datasets for training that are not publicly released, are limited to social chit-chat or handling a specific domain, and may not be easily extended to accommodate the latest advances in AI technologies. This paper introduces Jill Watson, a conversational Virtual Teaching Assistant (VTA) leveraging the capabilities of ChatGPT. Jill Watson based on ChatGPT requires no prior training and uses a modular design to allow the integration of new APIs using a skill-based architecture inspired by XiaoIce. Jill Watson is also well-suited for intelligent textbooks as it can process and converse using multiple large documents. We exclusively utilize publicly available resources for reproducibility and extensibility. Comparative analysis shows that our system outperforms the legacy knowledge-based Jill Watson as well as the OpenAI Assistants service. We employ many safety measures that reduce instances of hallucinations and toxicity. The paper also includes real-world examples from a classroom setting that demonstrate different features of Jill Watson and its effectiveness.","Taneja, Karan; Maiti, Pratyusha; Kakar, Sandeep; Guruprasad, Pranav; Rao, Sanjeev; Goel, Ashok K.",,"Maiti, Pratyusha/0009-0005-4469-837X",Jill Watson: A Virtual Teaching Assistant Powered by ChatGPT,14829,,10.1007/978-3-031-64302-6_23 ,Proceedings Paper ,,"Conversational AI agents often require extensive datasets for training that are not publicly released, are limited to social chit-chat or handling a specific domain, and may not be easily extended to accommodate the latest advances in AI technologies. This paper introduces Jill Watson, a conversational Virtual Teaching Assistant (VTA) leveraging the capabilities of ChatGPT. Jill Watson based on ChatGPT requires no prior training and uses a modular design to allow the integration of new APIs using a skill-based architecture inspired by XiaoIce. Jill Watson is also well-suited for intelligent textbooks as it can process and converse using multiple large documents. We exclusively utilize publicly available resources for reproducibility and extensibility. Comparative analysis shows that our system outperforms the legacy knowledge-based Jill Watson as well as the OpenAI Assistants service. We employ many safety measures that reduce instances of hallucinations and toxicity. The paper also includes real-world examples from a classroom setting that demonstrate different features of Jill Watson and its effectiveness.",2945-9133,1611-3349,978-3-031-64301-9; 978-3-031-64302-6,324-337, , 25th International Conference on Artificial Intelligence in Education (AIED)25th International Conference on Artificial Intelligence in Education (AIED) ,,detox,
2541,"**Title**Can You Label Less by Using Out-of-Domain Data? Active & Transfer Learning with Few-shot Instructions

**Abstract**Labeling social-media data for custom dimensions of toxicity and social bias is challenging and labor-intensive. Existing transfer and active learning approaches meant to reduce annotation effort require fine-tuning, which suffers from overfitting to noise and can cause domain shift with small sample sizes. In this work, we propose a novel Active Transfer Few-shot Instructions (ATF) approach which requires no fine-tuning. ATF leverages the internal linguistic knowledge of pretrained language models (PLMs) to facilitate the transfer of information from existing pre-labeled datasets (source-domain task) with minimum labeling effort on unlabeled target data (target-domain task). Our strategy can yield positive transfer achieving a mean AUC gain of 10.5% compared to no transfer with a large 22b parameter PLM. We further show that annotation of just a few target-domain samples via active learning can be beneficial for transfer, but the impact diminishes with more annotation effort (26% drop in gain between 100 and 2000 annotated examples). Finally, we find that not all transfer scenarios yield a positive gain, which seems related to the PLMs initial performance on the target-domain task.","Kocielnik, Rafal; Kangaslahti, Sara; Prabhumoye, Shrimai; Hari, Meena; Alvarez, R. Michael; Anandkumar, Anima",,,Can You Label Less by Using Out-of-Domain Data? Active & Transfer Learning with Few-shot Instructions,203,, ,Proceedings Paper ,,"Labeling social-media data for custom dimensions of toxicity and social bias is challenging and labor-intensive. Existing transfer and active learning approaches meant to reduce annotation effort require fine-tuning, which suffers from overfitting to noise and can cause domain shift with small sample sizes. In this work, we propose a novel Active Transfer Few-shot Instructions (ATF) approach which requires no fine-tuning. ATF leverages the internal linguistic knowledge of pretrained language models (PLMs) to facilitate the transfer of information from existing pre-labeled datasets (source-domain task) with minimum labeling effort on unlabeled target data (target-domain task). Our strategy can yield positive transfer achieving a mean AUC gain of 10.5% compared to no transfer with a large 22b parameter PLM. We further show that annotation of just a few target-domain samples via active learning can be beneficial for transfer, but the impact diminishes with more annotation effort (26% drop in gain between 100 and 2000 annotated examples). Finally, we find that not all transfer scenarios yield a positive gain, which seems related to the PLMs initial performance on the target-domain task.",2640-3498,,*****************,22-32, , Workshop on Transfer Learning for Natural Language ProcessingWorkshop on Transfer Learning for Natural Language Processing ,,detection,
2542,"**Title**HOT ChatGPT: ThePromiseofChatGPTinDetectingand Discriminating Hateful, Offensive, and Toxic Comments on Social Media

**Abstract**Harmful textual content is pervasive on social media, poisoning online communities and negatively impacting participation. A common approach to this issue is developing detection models that rely on human annotations. However, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. Generative AI models have the potential to understand and detect harmful textual content. We used ChatGPT to investigate this potential and compared its performance with MTurker annotations for three frequently discussed concepts related to harmful textual content on social media: Hateful, Offensive, and Toxic (HOT). We designed five prompts to interact with ChatGPT and conducted four experiments eliciting HOT classifications. Our results show that ChatGPT can achieve an accuracy of approximately 80% when compared to MTurker annotations. Specifically, the model displays a more consistent classification for non-HOT comments than HOT comments compared to human annotations. Our findings also suggest that ChatGPT classifications align with the provided HOT definitions. However, ChatGPT classifies hateful andoffensive as subsets of toxic. Moreover, the choice of prompts used to interact with ChatGPT impacts its performance. Based on these insights, our study provides several meaningful implications for employing ChatGPT to detect HOT content, particularly regarding the reliability and consistency of its performance, its understanding and reasoning of the HOT concept, and the impact of prompts on its performance. Overall, our study provides guidance on the potential of using generative AI models for moderating large volumes of user-generated textual content on social media.","Li, Lingyao; Fan, Lizhou; Atreja, Shubham; Hemphill, Libby","Hemphill, Libby/AAH-7062-2019","Atreja, Shubham/0000-0002-0056-3060; Fan, Lizhou/0000-0002-7962-9113; Hemphill, Libby/0000-0002-3793-7281","HOT ChatGPT: ThePromiseofChatGPTinDetectingand Discriminating Hateful, Offensive, and Toxic Comments on Social Media",18,2.0,10.1145/3643829 ,Article ,,"Harmful textual content is pervasive on social media, poisoning online communities and negatively impacting participation. A common approach to this issue is developing detection models that rely on human annotations. However, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. Generative AI models have the potential to understand and detect harmful textual content. We used ChatGPT to investigate this potential and compared its performance with MTurker annotations for three frequently discussed concepts related to harmful textual content on social media: Hateful, Offensive, and Toxic (HOT). We designed five prompts to interact with ChatGPT and conducted four experiments eliciting HOT classifications. Our results show that ChatGPT can achieve an accuracy of approximately 80% when compared to MTurker annotations. Specifically, the model displays a more consistent classification for non-HOT comments than HOT comments compared to human annotations. Our findings also suggest that ChatGPT classifications align with the provided HOT definitions. However, ChatGPT classifies hateful andoffensive as subsets of toxic. Moreover, the choice of prompts used to interact with ChatGPT impacts its performance. Based on these insights, our study provides several meaningful implications for employing ChatGPT to detect HOT content, particularly regarding the reliability and consistency of its performance, its understanding and reasoning of the HOT concept, and the impact of prompts on its performance. Overall, our study provides guidance on the potential of using generative AI models for moderating large volumes of user-generated textual content on social media.",1559-1131,1559-114X,,, ,  ,,detection,
2544,"**Title**ParaDetox: Detoxification with Parallel Data

**Abstract**We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxic-neutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task. We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources.We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluations. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems.","Logacheva, Varvara; Dementieva, Daryna; Ustyantsev, Sergey; Moskovskiy, Daniil; Dale, David; Krotova, Irina; Semenov, Nikita; Panchenko, Alexander","Panchenko, Alexander/AAQ-7808-2021",,ParaDetox: Detoxification with Parallel Data,,, ,Proceedings Paper ,,"We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxic-neutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task. We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources.We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluations. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems.",,,978-1-955917-21-6,6804-6818, , 60th Annual Meeting of the Association-for-Computational-Linguistics (ACL)60th Annual Meeting of the Association-for-Computational-Linguistics (ACL) ,,detox,
2545,"**Title**Overview of PAN 2024: Multi-author Writing Style Analysis, Multilingual Text Detoxification, Oppositional Thinking Analysis, and Generative AI Authorship Verification Condensed Lab Overview

**Abstract**The goal of the PAN lab is to advance the state of the art in text forensics and stylometry through an objective evaluation of new and established methods on new benchmark datasets. IN 2024, we organized four shared tasks: (1) multi-author writing style analysis, which we continue from 2023; (2) multilingual text detoxification, a new task that aims to re-formulate text in a non-toxic way for multiple languages; (3) oppositional thinking analysis, a new task that aims to discriminate critical thinking from conspiracy narratives and identify their core actors; and (4) generative AI authorship verification, which formulates the detection of AI-generated text as an authorship problem. PAN 2024 concluded as one of our most successful editions with 74 notebook papers by 147 participating teams.","Ayele, Abinew Ali; Babakov, Nikolay; Bevendorff, Janek; Casals, Xavier Bonet; Chulvi, Berta; Dementieva, Daryna; Elnagar, Ashaf; Freitag, Dayne; Froebe, Maik; Korencic, Damir; Mayerl, Maximilian; Moskovskiy, Daniil; Mukherjee, Animesh; Panchenko, Alexander; Potthast, Martin; Rangel, Francisco; Rizwan, Naquee; Rosso, Paolo; Schneider, Florian; Smirnova, Alisa; Stamatatos, Efstathios; Stakovskii, Elisei; Stein, Benno; Taule, Mariona; Ustalov, Dmitry; Wang, Xintong; Wiegmann, Matti; Yimam, Seid Muhie; Zangerle, Eva","Panchenko, Alexander/T-7560-2017; Zangerle, Eva/AAB-3833-2020; Chulvi, Berta/HJY-1145-2023; Stamatatos, Efstathios/F-2927-2012; Ustalov, Dmitry/P-6307-2014",,"Overview of PAN 2024: Multi-author Writing Style Analysis, Multilingual Text Detoxification, Oppositional Thinking Analysis, and Generative AI Authorship Verification Condensed Lab Overview",14959,,10.1007/978-3-031-71908-0_11 ,Proceedings Paper ,,"The goal of the PAN lab is to advance the state of the art in text forensics and stylometry through an objective evaluation of new and established methods on new benchmark datasets. IN 2024, we organized four shared tasks: (1) multi-author writing style analysis, which we continue from 2023; (2) multilingual text detoxification, a new task that aims to re-formulate text in a non-toxic way for multiple languages; (3) oppositional thinking analysis, a new task that aims to discriminate critical thinking from conspiracy narratives and identify their core actors; and (4) generative AI authorship verification, which formulates the detection of AI-generated text as an authorship problem. PAN 2024 concluded as one of our most successful editions with 74 notebook papers by 147 participating teams.",0302-9743,1611-3349,978-3-031-71907-3; 978-3-031-71908-0,231-259, , 15th International Conference of the Cross-Language-Evaluation-Forum-Association (CLEF)15th International Conference of the Cross-Language-Evaluation-Forum-Association (CLEF) ,,detox,
2548,"**Title**Studying the Role of Named Entities for Content Preservation in Text Style Transfer

**Abstract**Text style transfer techniques are gaining popularity in Natural Language Processing, finding various applications such as text detoxification, sentiment, or formality transfer. However, the majority of the existing approaches were tested on such domains as online communications on public platforms, music, or entertainment yet none of them were applied to the domains which are typical for task-oriented production systems, such as personal plans arrangements (e.g. booking of flights or reserving a table in a restaurant). We fill this gap by studying formality transfer in this domain.We noted that, the texts in this domain are full of named entities, which are very important for keeping the original sense of the text. Indeed, if for example, someone communicates destination city of a flight is must not be altered. Thus, we concentrate on the role of named entities in content preservation for formality text style transfer.We collect a new dataset for the evaluation of content similarity measures in text style transfer. It is taken from a corpus of task-oriented dialogues and contains many important entities related to realistic requests that make this dataset particularly useful for testing style transfer models before using them in production. Besides, we perform an error analysis of a pre-trained formality transfer model and introduce a simple technique to use information about named entities to enhance the performance of baseline content similarity measures used in text style transfer.","Babakov, Nikolay; Dale, David; Logacheva, Varvara; Krotova, Irina; Panchenko, Alexander","Panchenko, Alexander/AAQ-7808-2021","Babakov, Nikolay/0000-0002-2568-6702",Studying the Role of Named Entities for Content Preservation in Text Style Transfer,13286,,10.1007/978-3-031-08473-7_40 ,Proceedings Paper ,,"Text style transfer techniques are gaining popularity in Natural Language Processing, finding various applications such as text detoxification, sentiment, or formality transfer. However, the majority of the existing approaches were tested on such domains as online communications on public platforms, music, or entertainment yet none of them were applied to the domains which are typical for task-oriented production systems, such as personal plans arrangements (e.g. booking of flights or reserving a table in a restaurant). We fill this gap by studying formality transfer in this domain.We noted that, the texts in this domain are full of named entities, which are very important for keeping the original sense of the text. Indeed, if for example, someone communicates destination city of a flight is must not be altered. Thus, we concentrate on the role of named entities in content preservation for formality text style transfer.We collect a new dataset for the evaluation of content similarity measures in text style transfer. It is taken from a corpus of task-oriented dialogues and contains many important entities related to realistic requests that make this dataset particularly useful for testing style transfer models before using them in production. Besides, we perform an error analysis of a pre-trained formality transfer model and introduce a simple technique to use information about named entities to enhance the performance of baseline content similarity measures used in text style transfer.",0302-9743,1611-3349,978-3-031-08473-7; 978-3-031-08472-0,437-448, , 27th International Conference on Applications of Natural Language to Information Systems (NLDB)27th International Conference on Applications of Natural Language to Information Systems (NLDB) ,,detox,
2549,"**Title**Fine-Grained Human Feedback Gives Better Rewards for Language Model Training

**Abstract**Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF)-where human preference judgments on LM outputs are transformed into a learning signal-has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce FINE-GRAINED RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.","Wu, Zeqiu; Hu, Yushi; Shi, Weijia; Dziri, Nouha; Suhr, Alane; Ammanabrolu, Prithviraj; Smith, Noah A.; Ostendorf, Mari; Hajishirzi, Hannaneh",,,Fine-Grained Human Feedback Gives Better Rewards for Language Model Training,,, ,Proceedings Paper ,,"Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF)-where human preference judgments on LM outputs are transformed into a learning signal-has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce FINE-GRAINED RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.",1049-5258,,*****************,, , 37th Conference on Neural Information Processing Systems (NeurIPS)37th Conference on Neural Information Processing Systems (NeurIPS) ,,detox,
2564,"**Title**Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech

**Abstract**Tackling online hatred using informed textual responses - called counter narratives - has been brought under the spotlight recently. Accordingly, a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading. Still, current neural approaches tend to produce generic/repetitive responses and lack grounded and up-to-date evidence such as facts, statistics, or examples. Moreover, these models can create plausible but not necessarily true arguments. In this paper we present the first complete knowledge-bound counter narrative generation pipeline, grounded in an external knowledge repository that can provide more informative content to fight online hatred. Together with our approach, we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in-domain and cross-domain settings.","Chung, Yi-Ling; Tekiroglu, Serra Sinem; Guerini, Marco",,"Tekiroglu, Serra Sinem/0000-0001-7229-7649; Guerini, Marco/0000-0003-1582-6617",Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech,,, ,Proceedings Paper ,,"Tackling online hatred using informed textual responses - called counter narratives - has been brought under the spotlight recently. Accordingly, a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading. Still, current neural approaches tend to produce generic/repetitive responses and lack grounded and up-to-date evidence such as facts, statistics, or examples. Moreover, these models can create plausible but not necessarily true arguments. In this paper we present the first complete knowledge-bound counter narrative generation pipeline, grounded in an external knowledge repository that can provide more informative content to fight online hatred. Together with our approach, we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in-domain and cross-domain settings.",,,978-1-954085-54-1,899-914, , Joint Conference of 59th Annual Meeting of the Association-for-Computational-Linguistics (ACL) / 11th International Joint Conference on Natural Language Processing (IJCNLP) / 6th Workshop on Representation Learning for NLP (RepL4NLP)Joint Conference of 59th Annual Meeting of the Association-for-Computational-Linguistics (ACL) / 11th International Joint Conference on Natural Language Processing (IJCNLP) / 6th Workshop on Representation Learning for NLP (RepL4NLP) ,,detox,
2566,"**Title**Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech

**Abstract**Undermining the impact of hateful content with informed and non-aggressive responses, called counter narratives, has emerged as a possible solution for having healthier online communities. Thus, some NLP studies have started addressing the task of counter narrative generation. Although such studies have made an effort to build hate speech / counter narrative (HS/CN) datasets for neural generation, they fall short in reaching either highquality and/or high-quantity. In this paper, we propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and/or post-edit. Our experiments comprised several loops including dynamic variations. Results show that the methodology is scalable and facilitates diverse, novel, and cost-effective data collection. To our knowledge, the resulting dataset is the only expertbased multi-target HS/CN dataset available to the community.","Fanton, Margherita; Bonaldi, Helena; Tekiroglu, Serra Sinem; Guerini, Marco",,"Guerini, Marco/0000-0003-1582-6617; Fanton, Nicola/0000-0002-8953-6148",Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech,,, ,Proceedings Paper ,,"Undermining the impact of hateful content with informed and non-aggressive responses, called counter narratives, has emerged as a possible solution for having healthier online communities. Thus, some NLP studies have started addressing the task of counter narrative generation. Although such studies have made an effort to build hate speech / counter narrative (HS/CN) datasets for neural generation, they fall short in reaching either highquality and/or high-quantity. In this paper, we propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and/or post-edit. Our experiments comprised several loops including dynamic variations. Results show that the methodology is scalable and facilitates diverse, novel, and cost-effective data collection. To our knowledge, the resulting dataset is the only expertbased multi-target HS/CN dataset available to the community.",,,978-1-954085-52-7,3226-3240, , Joint Conference of 59th Annual Meeting of the Association-for-Computational-Linguistics (ACL) / 11th International Joint Conference on Natural Language Processing (IJCNLP) / 6th Workshop on Representation Learning for NLP (RepL4NLP)Joint Conference of 59th Annual Meeting of the Association-for-Computational-Linguistics (ACL) / 11th International Joint Conference on Natural Language Processing (IJCNLP) / 6th Workshop on Representation Learning for NLP (RepL4NLP) ,,detox#methodology,
2570,"**Title**Memory-enhanced text style transfer with dynamic style learning and calibration

**Abstract**Text style transfer aims to rephrase a sentence to match the desired style while retaining the original content. As a controllable text generation task, mainstream approaches use content-independent style embedding as control variables to guide stylistic generation. Nonetheless, stylistic properties are context-sensitive even under the same style. For example, delicious and helpful convey positive sentiments, although they are more likely to describe food and people, respectively. Therefore, desired style signals must vary with the content. To this end, we propose a memory-enhanced transfer method, which learns fine-grained style representation concerning content to assist transfer. Rather than employing static style embedding or latent variables, our method abstracts linguistic characteristics from training corpora and memorizes subdivided content with the corresponding style representations. The style signal is dynamically retrieved from memory using the content as a query, providing a more expressive and flexible latent style space. To address the imbalance between quantity and quality in different content, we further introduce a calibration method to augment memory construction by modeling the relationship between candidate styles. Experimental results obtained using three benchmark datasets confirm the superior performance of our model compared to competitive approaches. The evaluation metrics and case study also indicate that our model can generate diverse stylistic phrases matching context.","Lin, Fuqiang; Song, Yiping; Tian, Zhiliang; Chen, Wangqun; Dong, Diwen; Liu, Bo",,,Memory-enhanced text style transfer with dynamic style learning and calibration,67,4.0,10.1007/s11432-022-3726-0 ,Article ,,"Text style transfer aims to rephrase a sentence to match the desired style while retaining the original content. As a controllable text generation task, mainstream approaches use content-independent style embedding as control variables to guide stylistic generation. Nonetheless, stylistic properties are context-sensitive even under the same style. For example, delicious and helpful convey positive sentiments, although they are more likely to describe food and people, respectively. Therefore, desired style signals must vary with the content. To this end, we propose a memory-enhanced transfer method, which learns fine-grained style representation concerning content to assist transfer. Rather than employing static style embedding or latent variables, our method abstracts linguistic characteristics from training corpora and memorizes subdivided content with the corresponding style representations. The style signal is dynamically retrieved from memory using the content as a query, providing a more expressive and flexible latent style space. To address the imbalance between quantity and quality in different content, we further introduce a calibration method to augment memory construction by modeling the relationship between candidate styles. Experimental results obtained using three benchmark datasets confirm the superior performance of our model compared to competitive approaches. The evaluation metrics and case study also indicate that our model can generate diverse stylistic phrases matching context.",1674-733X,1869-1919,,, ,  ,,detox,
2586,"**Title**Transformers and meta-tokenization in sentiment analysis for software engineering

**Abstract**Sentiment analysis has been used to study aspects of software engineering, such as issue resolution, toxicity, and self-admitted technical debt. To address the peculiarities of software engineering texts, sentiment analysis tools often consider the specific technical lingo practitioners use. To further improve the application of sentiment analysis, there have been two recommendations: Using pre-trained transformer models to classify sentiment and replacing non-natural language elements with meta-tokens. In this work, we benchmark five different sentiment analysis tools (two pre-trained transformer models and three machine learning tools) on 2 gold-standard sentiment analysis datasets. We find that pre-trained transformers outperform the best machine learning tool on only one of the two datasets, and that even on that dataset the performance difference is a few percentage points. Therefore, we recommend that software engineering researchers should not just consider predictive performance when selecting a sentiment analysis tool because the best-performing sentiment analysis tools perform very similarly to each other (within 4 percentage points). Meanwhile, we find that meta-tokenization does not improve the predictive performance of sentiment analysis tools. Both of our findings can be used by software engineering researchers who seek to apply sentiment analysis tools to software engineering data.","Cassee, Nathan; Agaronian, Andrei; Constantinou, Eleni; Novielli, Nicole; Serebrenik, Alexander","Novielli, Nicole/Y-9583-2019","Constantinou, Eleni/0000-0002-4242-2581; Novielli, Nicole/0000-0003-1160-2608",Transformers and meta-tokenization in sentiment analysis for software engineering,29,4.0,10.1007/s10664-024-10468-2 ,Article ,,"Sentiment analysis has been used to study aspects of software engineering, such as issue resolution, toxicity, and self-admitted technical debt. To address the peculiarities of software engineering texts, sentiment analysis tools often consider the specific technical lingo practitioners use. To further improve the application of sentiment analysis, there have been two recommendations: Using pre-trained transformer models to classify sentiment and replacing non-natural language elements with meta-tokens. In this work, we benchmark five different sentiment analysis tools (two pre-trained transformer models and three machine learning tools) on 2 gold-standard sentiment analysis datasets. We find that pre-trained transformers outperform the best machine learning tool on only one of the two datasets, and that even on that dataset the performance difference is a few percentage points. Therefore, we recommend that software engineering researchers should not just consider predictive performance when selecting a sentiment analysis tool because the best-performing sentiment analysis tools perform very similarly to each other (within 4 percentage points). Meanwhile, we find that meta-tokenization does not improve the predictive performance of sentiment analysis tools. Both of our findings can be used by software engineering researchers who seek to apply sentiment analysis tools to software engineering data.",1382-3256,1573-7616,,, ,  ,,detection,
2591,"**Title**Abusive Speech Detection and Politeness Transfer

**Abstract**In the recent times of lockdown, the usage of all kinds of online platforms like Twitter, Facebook, YouTube and Reddit have increased by quite an extent. In addition to using these platforms for creating and sharing positive and inspiring content, a lot of hate and anger comments also seem to be prevalent in them. These problems are tackled by first detecting these forms of hate speech in textual data, then imparting politeness to the hateful comments while preserving the meaning conveyed. For the first phase of abusive speech detection, Baseline models like Logistic Regression, Naive Bayes, SVM, Random Forest and Decision Tree were trained and analyzed. Next, state-of-the-art models like LSTMs, Bi-LSTMs and Transformers were trained for classification of text. Word vectorization models like BOW and TF-IDF and also GloVe embeddings were used and evaluated on the models. It was found that Logistic Regression (with BOW), SVM (with TFIDF) and LSTMs were better performing than others in their categories. A hybrid model of the best performing classifiers was finally used. The next phase of politeness transfer to the abusive text was explored using BERT's language model and its bidirectional property of understanding context to reduce the average toxicity of input sentences.","Preetham, K.; Arumugham, D. Arun; Kumar, M. Yogesh; Begum, B. Shameedha","Begum, B.Shameedha/AAY-1640-2021",,Abusive Speech Detection and Politeness Transfer,13102,,10.1007/978-3-031-12700-7_9 ,Proceedings Paper ,,"In the recent times of lockdown, the usage of all kinds of online platforms like Twitter, Facebook, YouTube and Reddit have increased by quite an extent. In addition to using these platforms for creating and sharing positive and inspiring content, a lot of hate and anger comments also seem to be prevalent in them. These problems are tackled by first detecting these forms of hate speech in textual data, then imparting politeness to the hateful comments while preserving the meaning conveyed. For the first phase of abusive speech detection, Baseline models like Logistic Regression, Naive Bayes, SVM, Random Forest and Decision Tree were trained and analyzed. Next, state-of-the-art models like LSTMs, Bi-LSTMs and Transformers were trained for classification of text. Word vectorization models like BOW and TF-IDF and also GloVe embeddings were used and evaluated on the models. It was found that Logistic Regression (with BOW), SVM (with TFIDF) and LSTMs were better performing than others in their categories. A hybrid model of the best performing classifiers was finally used. The next phase of politeness transfer to the abusive text was explored using BERT's language model and its bidirectional property of understanding context to reduce the average toxicity of input sentences.",0302-9743,1611-3349,978-3-031-12699-4; 978-3-031-12700-7,81-90, , 9th International Conference on Pattern Recognition and Machine Intelligence (PReMI)9th International Conference on Pattern Recognition and Machine Intelligence (PReMI) ,,detox,
2610,"**Title**AStarTwice at SemEval-2021 Task 5: Toxic Span Detection using RoBERTa-CRF, Domain Specific Pre-Training and Self-Training

**Abstract**This paper describes our contribution to SemEval-2021 Task 5: Toxic Spans Detection. Our solution is built upon RoBERTa language model and Conditional Random Fields (CRF). We pre-trained RoBERTa on Civil Comments dataset, enabling it to create better contextual representation for this task. We also employed the semi-supervised learning technique of selftraining, which allowed us to extend our training dataset. In addition to these, we also identified some pre-processing steps that significantly improved our F1 score. Our proposed system achieved a rank of 41 with an F1 score of 66.16%.","Suman, Thakur Ashutosh; Jain, Abhinav",,,"AStarTwice at SemEval-2021 Task 5: Toxic Span Detection using RoBERTa-CRF, Domain Specific Pre-Training and Self-Training",,, ,Proceedings Paper ,,"This paper describes our contribution to SemEval-2021 Task 5: Toxic Spans Detection. Our solution is built upon RoBERTa language model and Conditional Random Fields (CRF). We pre-trained RoBERTa on Civil Comments dataset, enabling it to create better contextual representation for this task. We also employed the semi-supervised learning technique of selftraining, which allowed us to extend our training dataset. In addition to these, we also identified some pre-processing steps that significantly improved our F1 score. Our proposed system achieved a rank of 41 with an F1 score of 66.16%.",,,978-1-954085-70-1,875-880, , 15th International Workshops on Semantic Evaluation (SemEval)15th International Workshops on Semantic Evaluation (SemEval) ,,detection,
2611,"**Title**LZ1904 at SemEval-2021 Task 5: Bi-LSTM-CRF for Toxic Span Detection using PretrainedWord Embedding

**Abstract**Recurrent Neural Networks (RNN) have been widely used in various Natural Language Processing (NLP) tasks such as text classification, sequence tagging, and machine translation. Long Short Term Memory (LSTM), a special unit of RNN, has the advantage of memorizing past and even future information in a sentence (especially for bidirectional LSTM). In the shared task of detecting toxic spans in texts, we first apply pretrained word embedding (GloVe) to generate the word vectors after tokenization. Then we construct Bidirectional Long Short Term Memory-Conditional Random Field (Bi-LSTM-CRF) model by Baidu research to predict whether each word in the sentence is toxic or not. We tune hyperparameters of dropout rate, number of LSTM units, embedding size with 10 epochs and choose the epoch with best validation recall. Our model achieves an F1 score of 66.99% on test dataset.","Zou, Liang; Li, Wen",,,LZ1904 at SemEval-2021 Task 5: Bi-LSTM-CRF for Toxic Span Detection using PretrainedWord Embedding,,, ,Proceedings Paper ,,"Recurrent Neural Networks (RNN) have been widely used in various Natural Language Processing (NLP) tasks such as text classification, sequence tagging, and machine translation. Long Short Term Memory (LSTM), a special unit of RNN, has the advantage of memorizing past and even future information in a sentence (especially for bidirectional LSTM). In the shared task of detecting toxic spans in texts, we first apply pretrained word embedding (GloVe) to generate the word vectors after tokenization. Then we construct Bidirectional Long Short Term Memory-Conditional Random Field (Bi-LSTM-CRF) model by Baidu research to predict whether each word in the sentence is toxic or not. We tune hyperparameters of dropout rate, number of LSTM units, embedding size with 10 epochs and choose the epoch with best validation recall. Our model achieves an F1 score of 66.99% on test dataset.",,,978-1-954085-70-1,1009-1014, , 15th International Workshops on Semantic Evaluation (SemEval)15th International Workshops on Semantic Evaluation (SemEval) ,,detection,
2613,"**Title**Four Types of Toxic People: Characterizing Online Users' Toxicity over Time

**Abstract**Identifying types of online users' toxic behavior reveals important insights from social media interactions, including whether a user becomes radicalized (more toxic) or pacified (less toxic) over time. In this research, we design two metrics to identify toxic user types: F score that captures the changes in a user's toxicity, and G score that captures the direction of the shift taking place in the user's toxicity pattern. We apply these metrics to a dataset of 4M user comments from Reddit by defining four toxic user types based on the toxicity scores of a user's comments: (a) Steady Users whose toxicity scores are steady over time, (b) Fickle-Minded Users that switch between toxic and non-toxic commenting, (c) Pacified Users whose commenting becomes less toxic in time, and (d) Radicalized Users that become gradually toxic. Findings from the Reddit dataset indicate that fickle-minded users form the largest group (31.2%), followed by pacified (25.8%), radicalized (25.4%), and steadily toxic users (17.6%). The results suggest that the most typical behavior type of toxicity is switching between toxic and non-toxic commenting. This research has implications for preserving the user-friendliness of online communities by identifying continuously toxic users and users in danger of becoming radicalized (in terms of their toxic behavior), and designing interventions to mitigate these behavior types. Using the metrics we have defined, identifying these user types becomes possible. More research is needed to understand why these patterns take place and how they could be mitigated.","Mall, Raghvendra; Nagpal, Mridul; Salminen, Joni; Almerekhi, Hind; Jung, Soon-gyo; Jansen, Bernard J.","Almerekhi, Hind/ITU-8218-2023; Mall, Raghvendra/M-7132-2013",,Four Types of Toxic People: Characterizing Online Users' Toxicity over Time,,,10.1145/3419249.3420142 ,Proceedings Paper ,,"Identifying types of online users' toxic behavior reveals important insights from social media interactions, including whether a user becomes radicalized (more toxic) or pacified (less toxic) over time. In this research, we design two metrics to identify toxic user types: F score that captures the changes in a user's toxicity, and G score that captures the direction of the shift taking place in the user's toxicity pattern. We apply these metrics to a dataset of 4M user comments from Reddit by defining four toxic user types based on the toxicity scores of a user's comments: (a) Steady Users whose toxicity scores are steady over time, (b) Fickle-Minded Users that switch between toxic and non-toxic commenting, (c) Pacified Users whose commenting becomes less toxic in time, and (d) Radicalized Users that become gradually toxic. Findings from the Reddit dataset indicate that fickle-minded users form the largest group (31.2%), followed by pacified (25.8%), radicalized (25.4%), and steadily toxic users (17.6%). The results suggest that the most typical behavior type of toxicity is switching between toxic and non-toxic commenting. This research has implications for preserving the user-friendliness of online communities by identifying continuously toxic users and users in danger of becoming radicalized (in terms of their toxic behavior), and designing interventions to mitigate these behavior types. Using the metrics we have defined, identifying these user types becomes possible. More research is needed to understand why these patterns take place and how they could be mitigated.",,,978-1-4503-7579-5,, , 11th Nordic Conference on Human-Computer Interaction (NordiCHI)11th Nordic Conference on Human-Computer Interaction (NordiCHI) ,,detection#methodology,
2614,"**Title**Toxicity in Online Games: The Prevalence and Eficacy of Coping Strategies

**Abstract**Toxicity is pervasive in online multiplayer games, exposing players to disruptive and harmful behaviours. Players employ various approaches to cope with exposure to toxicity; however, game designers and researchers lack guidance on how to implement coping support within games. In this paper, we first conduct a formative study to collect a comprehensive list of coping approaches from toxicity literature and use affinity mapping to identify overarching game-based coping strategies. Then, we report findings from a survey (n = 85) on players' experiences with toxicity, how they employ the identified coping strategies, how games support coping, and their general coping styles. Our paper contributes a framework for coping strategies to deal with game-based toxicity and provides insights into the prevalence of these strategies among players and factors that affect their usage and effectiveness. These findings can be used to guide better in-game tools that help players mitigate the harm caused by toxicity.","Frommel, Julian; Mandryk, Regan L.","Frommel, Julian/JVM-8038-2024; Mandryk, Regan/HZK-7531-2023",,Toxicity in Online Games: The Prevalence and Eficacy of Coping Strategies,,,10.1145/3613904.3642523 ,Proceedings Paper ,,"Toxicity is pervasive in online multiplayer games, exposing players to disruptive and harmful behaviours. Players employ various approaches to cope with exposure to toxicity; however, game designers and researchers lack guidance on how to implement coping support within games. In this paper, we first conduct a formative study to collect a comprehensive list of coping approaches from toxicity literature and use affinity mapping to identify overarching game-based coping strategies. Then, we report findings from a survey (n = 85) on players' experiences with toxicity, how they employ the identified coping strategies, how games support coping, and their general coping styles. Our paper contributes a framework for coping strategies to deal with game-based toxicity and provides insights into the prevalence of these strategies among players and factors that affect their usage and effectiveness. These findings can be used to guide better in-game tools that help players mitigate the harm caused by toxicity.",,,979-8-4007-0330-0,, , ACM CHI Conference on Human Factors in Computing Sytems (CHI)ACM CHI Conference on Human Factors in Computing Sytems (CHI) ,,detection,
2616,"**Title**Analysing the Spread of Toxicity on Twitter

**Abstract**The spread of hate speech on social media platforms has become a rising concern in recent years. Understanding the spread of hate is crucial for mitigating its harmful effects and fostering a healthier online environment. In this paper, we propose a new model to capture the evolution of toxicity in a network - if a tweet with a certain toxicity (hatefulness) is posted, how much toxic a social network will become after a given number of rounds. We compute a toxicity score for each tweet, indicating the extent of the hatefulness of that tweet.Toxicity spread has not been adequately addressed in the existing literature. The two popular paradigms for modelling information spread, namely the Susceptible-Infected-Recovered (SIR) and its variants, as well as the spreading-activation models (SPA), are not suitable for modelling toxicity spread. The first paradigm employs a threshold and categorizes tweets as either toxic or non-toxic, while the second paradigm treats hate as energy and applies energy-conversion principles to model its propagation. Through analysis of a Twitter dataset consisting of 19.58 million tweets, we observe that the total toxicity, as well as the average toxicity of original tweets and retweets in the network, does not remain constant but rather increases over time.In this paper, we propose a new method for toxicity spread. First, we categorize users into three distinct groups: Amplifiers, Attenuators, and Copycats. These categories are assigned based on the exchange of toxicity by a user, with Amplifiers sending out more toxicity than they receive, Attenuators experiencing a higher influx of toxicity compared to what they generate, and Copycats simply mirroring the hate they receive. We perform extensive experimentation on Barabasi-Albert (BA) graphs, as well as subgraphs extracted from the Twitter dataset. Our model is able to replicate the patterns of toxicity.","Vaidya, Aatman; Nagar, Seema; Nanavati, Amit A.",,"Nanavati, Amit A./0000-0002-4131-9865; Nagar, Seema/0000-0003-1930-7101",Analysing the Spread of Toxicity on Twitter,,,10.1145/3632410.3632436 ,Proceedings Paper ,,"The spread of hate speech on social media platforms has become a rising concern in recent years. Understanding the spread of hate is crucial for mitigating its harmful effects and fostering a healthier online environment. In this paper, we propose a new model to capture the evolution of toxicity in a network - if a tweet with a certain toxicity (hatefulness) is posted, how much toxic a social network will become after a given number of rounds. We compute a toxicity score for each tweet, indicating the extent of the hatefulness of that tweet.Toxicity spread has not been adequately addressed in the existing literature. The two popular paradigms for modelling information spread, namely the Susceptible-Infected-Recovered (SIR) and its variants, as well as the spreading-activation models (SPA), are not suitable for modelling toxicity spread. The first paradigm employs a threshold and categorizes tweets as either toxic or non-toxic, while the second paradigm treats hate as energy and applies energy-conversion principles to model its propagation. Through analysis of a Twitter dataset consisting of 19.58 million tweets, we observe that the total toxicity, as well as the average toxicity of original tweets and retweets in the network, does not remain constant but rather increases over time.In this paper, we propose a new method for toxicity spread. First, we categorize users into three distinct groups: Amplifiers, Attenuators, and Copycats. These categories are assigned based on the exchange of toxicity by a user, with Amplifiers sending out more toxicity than they receive, Attenuators experiencing a higher influx of toxicity compared to what they generate, and Copycats simply mirroring the hate they receive. We perform extensive experimentation on Barabasi-Albert (BA) graphs, as well as subgraphs extracted from the Twitter dataset. Our model is able to replicate the patterns of toxicity.",,,979-8-4007-1634-8,118-126, , 7th ACM India Joint International Conference on Data Science and Management of Data (CODS-COMAD) / 11th ACM IKDD CODS Conference / 29th COMAD Conference7th ACM India Joint International Conference on Data Science and Management of Data (CODS-COMAD) / 11th ACM IKDD CODS Conference / 29th COMAD Conference ,,methodology,
2626,"**Title**Technological Solutions to Online Toxicity: Potential and Pitfalls

**Abstract**Social media platforms present a perplexing duality, acting at once as sites to build community and a sense of belonging, while also giving rise to misinformation, facilitating and intensifying disinformation campaigns and perpetuating existing patterns of discrimination from the physical world. The first-step platforms take in mitigating the harmful side of social media involves identifying and managing toxic content. Users produce an enormous volume of posts which must be evaluated very quickly. This is an application context that requires machine-learning (ML) tools, but as we detail in this article, ML approaches rely on human annotators, analysts, and moderators. Our review of existing methods and potential improvements indicates that neither humans nor ML can be removed from this process in the near future. However, we see room for improvement in the working conditions of these human workers.","Bodaghi, Arezo; Fung, Benjamin C. M.; Schmitt, Ketra A.","Schmitt, Ketra/GWQ-5329-2022","bodaghi, Arezo/0000-0002-7450-0687; Schmitt, Ketra/0000-0002-4260-6209",Technological Solutions to Online Toxicity: Potential and Pitfalls,42,4.0,10.1109/MTS.2023.3340235 ,Article ,,"Social media platforms present a perplexing duality, acting at once as sites to build community and a sense of belonging, while also giving rise to misinformation, facilitating and intensifying disinformation campaigns and perpetuating existing patterns of discrimination from the physical world. The first-step platforms take in mitigating the harmful side of social media involves identifying and managing toxic content. Users produce an enormous volume of posts which must be evaluated very quickly. This is an application context that requires machine-learning (ML) tools, but as we detail in this article, ML approaches rely on human annotators, analysts, and moderators. Our review of existing methods and potential improvements indicates that neither humans nor ML can be removed from this process in the near future. However, we see room for improvement in the working conditions of these human workers.",0278-0097,1937-416X,,57-65, ,  ,,evaluation,
2647,"**Title**Challenges in moderating disruptive player behavior in online competitive action games

**Abstract**Online competitive action games are a very popular form of entertainment. While most are respectfully enjoyed by millions of players, a small group of players engages in disruptive behavior, such as cheating and hate speech. Identifying and subsequently moderating these toxic players is a challenging task. Previous research has only studied specific aspects of this problem using curated data and with limited access to real-world moderation practices. In contrast, our work offers a unique and holistic view of the universal challenges of moderating disruptive behavior in online systems. We combine an analysis of a large dataset from a popular online competitive first-person action title (Call of Duty (R): Modern Warfare (R) II) with insights from stakeholders involved in moderation. We identify six universal challenges related to handling disruptive behaviors in such games. We discuss challenges omitted by prior work, such as handling high-volume imbalanced data or ensuring the comfort of human moderators. We also offer a discussion of possible technical, design, and policy approaches to mitigating these challenges.","Kocielnik, Rafal; Li, Zhuofang; Kann, Claudia; Sambrano, Deshawn; Morrier, Jacob; Linegar, Mitchell; Taylor, Carly; Kim, Min; Naqvie, Nabiha; Soltani, Feri; Dehpanah, Arman; Cahill, Grant; Anandkumar, Animashree; Alvarez, R. Michael","Linegar, Mitchell/KHY-6717-2024",,Challenges in moderating disruptive player behavior in online competitive action games,6,,10.3389/fcomp.2024.1283735 ,Article ,,"Online competitive action games are a very popular form of entertainment. While most are respectfully enjoyed by millions of players, a small group of players engages in disruptive behavior, such as cheating and hate speech. Identifying and subsequently moderating these toxic players is a challenging task. Previous research has only studied specific aspects of this problem using curated data and with limited access to real-world moderation practices. In contrast, our work offers a unique and holistic view of the universal challenges of moderating disruptive behavior in online systems. We combine an analysis of a large dataset from a popular online competitive first-person action title (Call of Duty (R): Modern Warfare (R) II) with insights from stakeholders involved in moderation. We identify six universal challenges related to handling disruptive behaviors in such games. We discuss challenges omitted by prior work, such as handling high-volume imbalanced data or ensuring the comfort of human moderators. We also offer a discussion of possible technical, design, and policy approaches to mitigating these challenges.",,2624-9898,,, ,  ,,Gen_dataset#detection#methodology,
2650,"**Title**Can Hallucination Reduction in LLMs Improve Online Sexism Detection?

**Abstract**Online sexism is a pervasive problem with a significant impact on the targeted individuals and social inequalities. Automated tools are now widely used to identify sexist content at scale, but most of these tools do not provide any further explanations beyond generic categories such as 'toxicity', 'abuse' or 'sexism'. This paper explores the impact of hallucination reduction in LLMs on enhancing sexism detection across three different levels: binary sexism, four-categories of sexism, and fine-grained vectors, with a focus on explainability in sexism detection. We have successfully applied Neural Path Hunter (NPH) to GPT-2, with the purpose of teaching the model to hallucinate less. We have used hallucination-reduced GPT-2, achieving accuracy rates of 83.2% for binary detection, 52.2% for four-categories classification and 38.0% for the 11-vectors fine-grained classification, respectively. The results indicate that: i) While the model performances may slightly lag behind the baseline models, hallucination-reducing methods have the potential to significantly influence LLM performance across various applications, beyond just dialogue-response systems. Additionally, this method could potentially mitigate model bias and improve generalization capabilities, based upon the dataset quality and the selected hallucination reduction technique.","Ding, Leyuan; Rajapaksha, Praboda; Aung Kaung Myat; Farahbakhsh, Reza; Crespi, Noel","Crespi, Noel/ABE-7052-2020; Rajapaksha, Praboda/GLT-9388-2022",,Can Hallucination Reduction in LLMs Improve Online Sexism Detection?,1065,,10.1007/978-3-031-66329-1_40 ,Proceedings Paper ,,"Online sexism is a pervasive problem with a significant impact on the targeted individuals and social inequalities. Automated tools are now widely used to identify sexist content at scale, but most of these tools do not provide any further explanations beyond generic categories such as 'toxicity', 'abuse' or 'sexism'. This paper explores the impact of hallucination reduction in LLMs on enhancing sexism detection across three different levels: binary sexism, four-categories of sexism, and fine-grained vectors, with a focus on explainability in sexism detection. We have successfully applied Neural Path Hunter (NPH) to GPT-2, with the purpose of teaching the model to hallucinate less. We have used hallucination-reduced GPT-2, achieving accuracy rates of 83.2% for binary detection, 52.2% for four-categories classification and 38.0% for the 11-vectors fine-grained classification, respectively. The results indicate that: i) While the model performances may slightly lag behind the baseline models, hallucination-reducing methods have the potential to significantly influence LLM performance across various applications, beyond just dialogue-response systems. Additionally, this method could potentially mitigate model bias and improve generalization capabilities, based upon the dataset quality and the selected hallucination reduction technique.",2367-3370,2367-3389,978-3-031-66328-4; 978-3-031-66329-1,625-638, , Intelligent Systems ConferenceIntelligent Systems Conference ,,detection,
2661,"**Title**Bystander Detection: Automatic Labeling Techniques using Feature Selection and Machine Learning

**Abstract**A hostile or aggressive behavior on an online platform by an individual or a group of people is termed as cyberbullying. A bystander is the one who sees or knows about such incidences of cyberbullying. A defender who intervenes can mitigate the impact of bullying, an instigator who accomplices the bully, can add to the victim's suffering, and an impartial onlooker who remains neutral and observes the scenario without getting engaged. Studying the behavior of Bystanders role can help in shaping the scale and progression of bullying incidents. However, the lack of data hinders the research in this area. Recently, a dataset, CYBY23, of Twitter threads having main tweets and the replies of Bystanders was published on Kaggle in Oct 2023. The dataset has extracted features related to toxicity and sensitivity of the main tweets and reply tweets. The authors have got manual annotators to assign the labels of Bystanders' roles. Manually labeling bystanders' roles is a labor-intensive task which eventually raises the need to have an automatic labeling technique for identifying the Bystander role. In this work, we aim to suggest a machine-learning model with high efficiency for the automatic labeling of Bystanders. Initially, the dataset was re-sampled using SMOTE to make it a balanced dataset. Next, we experimented with 12 models using various feature engineering techniques. Best features were selected for further experimentation by removing highly correlated and less relevant features. The models were evaluated on the metrics of accuracy, precision, recall, and F1 score. We found that the Random Forest Classifier (RFC) model with a certain set of features is the highest scorer among all 12 models. The RFC model was further tested against various splits of training and test sets. The highest results were achieved using a training set of 85% and a test set of 15%, having 78.83% accuracy, 81.79% precision, 74.83% recall, and 79.45% F1 score. Automatic labeling proposed in this work, will help in scaling the dataset which will be useful for further studies related to cyberbullying.","Gupta, Anamika; Thakkar, Khushboo; Bhasin, Veenu; Tiwari, Aman; Mathur, Vibhor",,,Bystander Detection: Automatic Labeling Techniques using Feature Selection and Machine Learning,15,1.0, ,Article ,,"A hostile or aggressive behavior on an online platform by an individual or a group of people is termed as cyberbullying. A bystander is the one who sees or knows about such incidences of cyberbullying. A defender who intervenes can mitigate the impact of bullying, an instigator who accomplices the bully, can add to the victim's suffering, and an impartial onlooker who remains neutral and observes the scenario without getting engaged. Studying the behavior of Bystanders role can help in shaping the scale and progression of bullying incidents. However, the lack of data hinders the research in this area. Recently, a dataset, CYBY23, of Twitter threads having main tweets and the replies of Bystanders was published on Kaggle in Oct 2023. The dataset has extracted features related to toxicity and sensitivity of the main tweets and reply tweets. The authors have got manual annotators to assign the labels of Bystanders' roles. Manually labeling bystanders' roles is a labor-intensive task which eventually raises the need to have an automatic labeling technique for identifying the Bystander role. In this work, we aim to suggest a machine-learning model with high efficiency for the automatic labeling of Bystanders. Initially, the dataset was re-sampled using SMOTE to make it a balanced dataset. Next, we experimented with 12 models using various feature engineering techniques. Best features were selected for further experimentation by removing highly correlated and less relevant features. The models were evaluated on the metrics of accuracy, precision, recall, and F1 score. We found that the Random Forest Classifier (RFC) model with a certain set of features is the highest scorer among all 12 models. The RFC model was further tested against various splits of training and test sets. The highest results were achieved using a training set of 85% and a test set of 15%, having 78.83% accuracy, 81.79% precision, 74.83% recall, and 79.45% F1 score. Automatic labeling proposed in this work, will help in scaling the dataset which will be useful for further studies related to cyberbullying.",2158-107X,2156-5570,,1135-1143, ,  ,,detection,
2674,"**Title**Anatomy of Hate Speech Datasets: Composition Analysis and Cross-dataset Classification

**Abstract**Manifestations of hate speech in different scenarios are increasingly frequent on social platforms. In this context, there is a large number of works that propose solutions for identifying this type of content in these environments. Most efforts to automatically detect hate speech follow the same process of supervised learning, using annotators to label a predefined set of messages, which are, in turn, used to train classifiers. However, annotators can create labels for different classification tasks, with divergent definitions of hate speech, binary or multi-label schemes, and various methodologies for collecting data. In this context, we examine the principal publicly available datasets for hate speech research. We investigate the types of hate speech (e.g., ethnicity, religion, sexual orientation) present in their composition, explore their content beyond the labels, and use cross-dataset classification to examine the use of the labeled data beyond its original work. Our results reveal interesting insights toward a better understanding of the hate speech phenomenon and improving its detection on social platforms.Warning. This paper contains offensive words and tweet examples.","Guimaraes, Samuel; Kakizaki, Gabriel; Melo, Philipe; Silva, Marcio; Murai, Fabricio; Reis, Julio C. S.; Benevenuto, Fabricio",,,Anatomy of Hate Speech Datasets: Composition Analysis and Cross-dataset Classification,,,10.1145/3603163.3609158 ,Proceedings Paper ,,"Manifestations of hate speech in different scenarios are increasingly frequent on social platforms. In this context, there is a large number of works that propose solutions for identifying this type of content in these environments. Most efforts to automatically detect hate speech follow the same process of supervised learning, using annotators to label a predefined set of messages, which are, in turn, used to train classifiers. However, annotators can create labels for different classification tasks, with divergent definitions of hate speech, binary or multi-label schemes, and various methodologies for collecting data. In this context, we examine the principal publicly available datasets for hate speech research. We investigate the types of hate speech (e.g., ethnicity, religion, sexual orientation) present in their composition, explore their content beyond the labels, and use cross-dataset classification to examine the use of the labeled data beyond its original work. Our results reveal interesting insights toward a better understanding of the hate speech phenomenon and improving its detection on social platforms.Warning. This paper contains offensive words and tweet examples.",,,979-8-4007-0232-7,, , 34th ACM Conference on Hypertext and Social Media (HT)34th ACM Conference on Hypertext and Social Media (HT) ,,evaluation#methodology,
2676,"**Title**Improving Hate Speech Detection Using Double-Layers Hybrid CNN-RNN Model on Imbalanced Dataset

**Abstract**Hate speech detection is crucial in curbing online toxicity and fostering a safer digital environment. Previous research has proposed the use of a hybrid CNN-RNN model for this purpose. This study aims to improve the performance of the hybrid CNN-RNN method by using a double-layer approach to address imbalanced datasets. The novelty lies in using double layers of hybrid CNN-RNN to enhance hate speech detection accuracy. This research also employed an oversampling technique alongside the double-layer model. The process included preprocessing, feature extraction, training tuning, testing, and performance evaluation. The results demonstrated that the double-layer hybrid CNN-RNN model achieved an accuracy of 0.827, a precision of 0.797, a recall of 0.759, and an F1 score of 0.883, with imbalanced data. Meanwhile, balanced data yielded a higher accuracy of 0.908, a precision of 0.943, a recall of 0.894, and an F1 score of 0.914. Moreover, the proposed model outperformed the hybrid CNN-RNN with an imbalanced dataset, generating an accuracy of 0.752, a precision of 0.797, a recall of 0.559, and an F1 score of 0.657. Dropout and early stopping techniques addressed overfitting in complex models and large datasets. This research has advanced hate speech detection methodologies by demonstrating the effectiveness of a double-layer hybrid CNN-RNN model, especially for imbalanced data. It underscores the importance of addressing imbalanced datasets for improved model accuracy. Future work could explore alternative data augmentation techniques or compare the proposed model with other architectures.","Riyadi, Slamet; Andriyani, Annisa Divayu; Sulaiman, Siti Noraini","Sulaiman, Siti Noraini/J-2316-2019",,Improving Hate Speech Detection Using Double-Layers Hybrid CNN-RNN Model on Imbalanced Dataset,12,,10.1109/ACCESS.2024.3487433 ,Article ,,"Hate speech detection is crucial in curbing online toxicity and fostering a safer digital environment. Previous research has proposed the use of a hybrid CNN-RNN model for this purpose. This study aims to improve the performance of the hybrid CNN-RNN method by using a double-layer approach to address imbalanced datasets. The novelty lies in using double layers of hybrid CNN-RNN to enhance hate speech detection accuracy. This research also employed an oversampling technique alongside the double-layer model. The process included preprocessing, feature extraction, training tuning, testing, and performance evaluation. The results demonstrated that the double-layer hybrid CNN-RNN model achieved an accuracy of 0.827, a precision of 0.797, a recall of 0.759, and an F1 score of 0.883, with imbalanced data. Meanwhile, balanced data yielded a higher accuracy of 0.908, a precision of 0.943, a recall of 0.894, and an F1 score of 0.914. Moreover, the proposed model outperformed the hybrid CNN-RNN with an imbalanced dataset, generating an accuracy of 0.752, a precision of 0.797, a recall of 0.559, and an F1 score of 0.657. Dropout and early stopping techniques addressed overfitting in complex models and large datasets. This research has advanced hate speech detection methodologies by demonstrating the effectiveness of a double-layer hybrid CNN-RNN model, especially for imbalanced data. It underscores the importance of addressing imbalanced datasets for improved model accuracy. Future work could explore alternative data augmentation techniques or compare the proposed model with other architectures.",2169-3536,,,159660-159668, ,  ,,detection,
2677,"**Title**Understanding and identifying the use of emotes in toxic chat on Twitch

**Abstract**The latest advances in NLP (natural language processing) have led to the launch of the much needed machine- driven toxic chat detection. Nevertheless, people continuously find new forms of hateful expressions that are easily identified by humans, but not by machines. One such common expression is the mix of text and emotes, a type of visual toxic chat that is increasingly used to evade algorithmic moderation and a trend that is an understudied aspect of the problem of online toxicity. This research analyzes chat conversations from the popular streaming platform Twitch to understand the varied types of visual toxic chat. Emotes were sometimes used to replace a letter, seek attention, or for emotional expression. We created a labeled dataset that contains 29,721 cases of emotes replacing letters. Based on the dataset, we built a neural network classifier and identified visual toxic chat that would otherwise be undetected through traditional methods and caught an additional 1.3% examples of toxic chat out of 15 million chat utterances.","Kim, Jaeheon; Wohn, Donghee Yvette; Cha, Meeyoung","Cha, Meeyoung/B-6925-2011; Cha, Meeyoung/KOD-4491-2024","Cha, Meeyoung/0000-0003-4085-9648",Understanding and identifying the use of emotes in toxic chat on Twitch,27,,10.1016/j.osnem.2021.100180 ,Article ,,"The latest advances in NLP (natural language processing) have led to the launch of the much needed machine- driven toxic chat detection. Nevertheless, people continuously find new forms of hateful expressions that are easily identified by humans, but not by machines. One such common expression is the mix of text and emotes, a type of visual toxic chat that is increasingly used to evade algorithmic moderation and a trend that is an understudied aspect of the problem of online toxicity. This research analyzes chat conversations from the popular streaming platform Twitch to understand the varied types of visual toxic chat. Emotes were sometimes used to replace a letter, seek attention, or for emotional expression. We created a labeled dataset that contains 29,721 cases of emotes replacing letters. Based on the dataset, we built a neural network classifier and identified visual toxic chat that would otherwise be undetected through traditional methods and caught an additional 1.3% examples of toxic chat out of 15 million chat utterances.",,2468-6964,,, ,  ,,Gen_dataset#detection,
2678,"**Title**Accuracy and Fairness for Web-Based Content Analysis under Temporal Shifts and Delayed Labeling

**Abstract**Web-based content analysis tasks, such as labeling toxicity, misinformation, or spam often rely on machine learning models to achieve cost and scale efficiencies. As these models impact real human lives, ensuring accuracy and fairness of such models is critical. However, maintaining the performance of these models over time can be challenging due to the temporal shifts in the application context and the sub-populations represented. Furthermore, there is often a delay in obtaining human expert labels for the raw data, which hinders the timely adaptation and safe deployment of the models. To overcome these challenges, we propose a novel approach that anticipates future distributions of data, especially in settings where unlabeled data becomes available earlier than the labels to estimate the future distribution of labels per sub-population and adapt the model preemptively. We evaluate our approach using multiple temporally-shifting datasets and consider bias based on racial, political, and demographic identities. We find that the proposed approach yields promising performance with respect to both accuracy and fairness. Our paper contributes to the web science literature by proposing a novel method for enhancing the quality and equity of web-based content analysis using machine learning. Experimental code and datasets are publicly available at https://github.com/Behavioral-Informatics-Lab/FAIRCAST.","Almuzaini, Abdulaziz A.; Pennock, David M.; Singh, Vivek K.",,"Singh, Vivek Kumar/0000-0002-8194-2336; Pennock, David/0000-0003-0522-4815",Accuracy and Fairness for Web-Based Content Analysis under Temporal Shifts and Delayed Labeling,,,10.1145/3614419.3644028 ,Proceedings Paper ,,"Web-based content analysis tasks, such as labeling toxicity, misinformation, or spam often rely on machine learning models to achieve cost and scale efficiencies. As these models impact real human lives, ensuring accuracy and fairness of such models is critical. However, maintaining the performance of these models over time can be challenging due to the temporal shifts in the application context and the sub-populations represented. Furthermore, there is often a delay in obtaining human expert labels for the raw data, which hinders the timely adaptation and safe deployment of the models. To overcome these challenges, we propose a novel approach that anticipates future distributions of data, especially in settings where unlabeled data becomes available earlier than the labels to estimate the future distribution of labels per sub-population and adapt the model preemptively. We evaluate our approach using multiple temporally-shifting datasets and consider bias based on racial, political, and demographic identities. We find that the proposed approach yields promising performance with respect to both accuracy and fairness. Our paper contributes to the web science literature by proposing a novel method for enhancing the quality and equity of web-based content analysis using machine learning. Experimental code and datasets are publicly available at https://github.com/Behavioral-Informatics-Lab/FAIRCAST.",,,979-8-4007-0334-8,268-278, ," 16th ACM Web Science Conference (WebScience) - Reflecting on the Web, AI, and Society16th ACM Web Science Conference (WebScience) - Reflecting on the Web, AI, and Society ",,detection#methodology,
2680,"**Title**Persistent interaction patterns across social media platforms and over time

**Abstract**Growing concern surrounds the impact of social media platforms on public discourse(1-4) and their influence on social dynamics(5-9), especially in the context of toxicity(10-12). Here, to better understand these phenomena, we use a comparative approach to isolate human behavioural patterns across multiple social media platforms. In particular, we analyse conversations in different online communities, focusing on identifying consistent patterns of toxic content. Drawing from an extensive dataset that spans eight platforms over 34 years-from Usenet to contemporary social media-our findings show consistent conversation patterns and user behaviour, irrespective of the platform, topic or time. Notably, although long conversations consistently exhibit higher toxicity, toxic language does not invariably discourage people from participating in a conversation, and toxicity does not necessarily escalate as discussions evolve. Our analysis suggests that debates and contrasting sentiments among users significantly contribute to more intense and hostile discussions. Moreover, the persistence of these patterns across three decades, despite changes in platforms and societal norms, underscores the pivotal role of human behaviour in shaping online discourse.","Avalle, Michele; Di Marco, Niccolo; Etta, Gabriele; Sangiorgio, Emanuele; Alipour, Shayan; Bonetti, Anita; Alvisi, Lorenzo; Scala, Antonio; Baronchelli, Andrea; Cinelli, Matteo; Quattrociocchi, Walter","Cinelli, Matteo/AFO-0408-2022; Alipour, Shayan/KGM-2672-2024; Scala, Antonio/A-2098-2012; Avalle, Michele/KVA-6179-2024; Baronchelli, Andrea/F-9550-2016","Di Marco, Niccolo/0000-0003-4335-7328; Alvisi, Lorenzo/0009-0007-4222-348X; Scala, Antonio/0000-0002-3414-2686; Sangiorgio, Emanuele/0009-0003-1024-3735; Avalle, Michele/0009-0007-4934-2326; Baronchelli, Andrea/0000-0002-0255-0829",Persistent interaction patterns across social media platforms and over time,628,8008.0,10.1038/s41586-024-07229-y ,Article ,,"Growing concern surrounds the impact of social media platforms on public discourse(1-4) and their influence on social dynamics(5-9), especially in the context of toxicity(10-12). Here, to better understand these phenomena, we use a comparative approach to isolate human behavioural patterns across multiple social media platforms. In particular, we analyse conversations in different online communities, focusing on identifying consistent patterns of toxic content. Drawing from an extensive dataset that spans eight platforms over 34 years-from Usenet to contemporary social media-our findings show consistent conversation patterns and user behaviour, irrespective of the platform, topic or time. Notably, although long conversations consistently exhibit higher toxicity, toxic language does not invariably discourage people from participating in a conversation, and toxicity does not necessarily escalate as discussions evolve. Our analysis suggests that debates and contrasting sentiments among users significantly contribute to more intense and hostile discussions. Moreover, the persistence of these patterns across three decades, despite changes in platforms and societal norms, underscores the pivotal role of human behaviour in shaping online discourse.",0028-0836,1476-4687,,, ,  ,,Gen_dataset,
2685,"**Title**AlexU-BackTranslation-TL at SemEval-2020 Task 12: Improving Offensive Language Detection using Data Augmentation and Transfer Learning

**Abstract**Social media platforms, online news commenting spaces, and many other public forums have become widely known for issues of abusive behavior such as cyber-bullying and personal attacks. In this paper, we use the annotated tweets of Offensive Language Identification Dataset (OLID) to train three levels of deep learning classifiers to solve the three sub-tasks associated with the dataset. Sub-task A is to determine if the tweet is toxic or not. Then, for offensive tweets, sub-task B requires determining whether the toxicity is targeted. Finally, for sub-task C, we predict the target of the offense; i.e. a group, individual or other entity. In our solution, we tackle the problem of class imbalance in the dataset by using back translation for data augmentation and utilizing fine-tuned BERT model in an ensemble of deep learning classifiers. We used this solution to participate in the three English sub-tasks of SemEval-2020 task 12. The proposed solution achieved 0:91393, 0:6300 and 0:57607 macro F1-average in sub-tasks A, B and C respectively. We achieved the 8th, 14th and 21st places for sub-tasks A, B and C respectively.","Ibrahim, Mai; Torki, Marwan; El-Makky, Nagwa","Torki, Marwan/ACE-3852-2022","Torki, Marwan/0000-0002-6149-1718",AlexU-BackTranslation-TL at SemEval-2020 Task 12: Improving Offensive Language Detection using Data Augmentation and Transfer Learning,,, ,Proceedings Paper ,,"Social media platforms, online news commenting spaces, and many other public forums have become widely known for issues of abusive behavior such as cyber-bullying and personal attacks. In this paper, we use the annotated tweets of Offensive Language Identification Dataset (OLID) to train three levels of deep learning classifiers to solve the three sub-tasks associated with the dataset. Sub-task A is to determine if the tweet is toxic or not. Then, for offensive tweets, sub-task B requires determining whether the toxicity is targeted. Finally, for sub-task C, we predict the target of the offense; i.e. a group, individual or other entity. In our solution, we tackle the problem of class imbalance in the dataset by using back translation for data augmentation and utilizing fine-tuned BERT model in an ensemble of deep learning classifiers. We used this solution to participate in the three English sub-tasks of SemEval-2020 task 12. The proposed solution achieved 0:91393, 0:6300 and 0:57607 macro F1-average in sub-tasks A, B and C respectively. We achieved the 8th, 14th and 21st places for sub-tasks A, B and C respectively.",,,978-1-952148-31-6,1881-1890, , 14th International Workshops on Semantic Evaluation (SemEval)14th International Workshops on Semantic Evaluation (SemEval) ,,detection,
2698,"**Title**Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers

**Abstract**To address the problem of NLP classifiers learning spurious correlations between training features and target labels, a common approach is to make the model's predictions invariant to these features. However, this can be counterproductive when the features have a non-zero causal effect on the target label and thus are important for prediction. Therefore, using methods from the causal inference literature, we propose an algorithm to regularize the learnt effect of the features on the model's prediction to the estimated effect of feature on label. This results in an automated augmentation method that leverages the estimated effect of a feature to appropriately change the labels for new augmented inputs. On toxicity and IMDB review datasets, the proposed algorithm minimises spurious correlations and improves the minority group (i.e., samples breaking spurious correlations) accuracy, while also improving the total accuracy compared to standard training. (1)","Bansal, Parikshit; Sharma, Amit","Sharma, Amit/JEF-9067-2023",,Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers,,, ,Proceedings Paper ,,"To address the problem of NLP classifiers learning spurious correlations between training features and target labels, a common approach is to make the model's predictions invariant to these features. However, this can be counterproductive when the features have a non-zero causal effect on the target label and thus are important for prediction. Therefore, using methods from the causal inference literature, we propose an algorithm to regularize the learnt effect of the features on the model's prediction to the estimated effect of feature on label. This results in an automated augmentation method that leverages the estimated effect of a feature to appropriately change the labels for new augmented inputs. On toxicity and IMDB review datasets, the proposed algorithm minimises spurious correlations and improves the minority group (i.e., samples breaking spurious correlations) accuracy, while also improving the total accuracy compared to standard training. (1)",,,978-1-959429-72-2,2271-2287, , 61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL)61st Annual Meeting of the the Association-for-Computational-Linguistics (ACL) ,,detection,
2700,"**Title**A context-aware attention and graph neural network-based multimodal framework for misogyny detection

**Abstract**A substantial portion of offensive content on social media is directed towards women. Since the approaches for general offensive content detection face a challenge in detecting misogynistic content, it requires solutions tailored to address offensive content against women. To this end, we propose a novel multimodal framework for the detection of misogynistic and sexist content. The framework comprises three modules: the Multimodal Attention module (MANM), the Graph-based Feature Reconstruction Module (GFRM), and the Content-specific Features Learning Module (CFLM). The MANM employs adaptive gating-based multimodal context-aware attention, enabling the model to focus on relevant visual and textual information and generating contextually relevant features. The GFRM module utilizes graphs to refine features within individual modalities, while the CFLM focuses on learning text and image-specific features such as toxicity features and caption features. Additionally, we curate a set of misogynous lexicons to compute the misogyny-specific lexicon score from the text. We apply test-time augmentation in feature space to better generalize the predictions on diverse inputs. The performance of the proposed approach has been evaluated on two multimodal datasets, MAMI, and MMHS150K, with 11,000 and 13,494 samples, respectively. The proposed method demonstrates an average improvement of 11.87% and 10.82% in macro-F1 over existing multimodal methods on the MAMI and MMHS150K datasets, respectively.","Rehman, Mohammad Zia Ur; Zahoor, Sufyaan; Manzoor, Areeb; Maqbool, Musharaf; Kumar, Nagendra",,", Mohammad Zia Ur Rehman/0000-0001-6374-8102; Kumar, Nagendra/0000-0003-4644-3168",A context-aware attention and graph neural network-based multimodal framework for misogyny detection,62,1.0,10.1016/j.ipm.2024.103895 ,Article ,,"A substantial portion of offensive content on social media is directed towards women. Since the approaches for general offensive content detection face a challenge in detecting misogynistic content, it requires solutions tailored to address offensive content against women. To this end, we propose a novel multimodal framework for the detection of misogynistic and sexist content. The framework comprises three modules: the Multimodal Attention module (MANM), the Graph-based Feature Reconstruction Module (GFRM), and the Content-specific Features Learning Module (CFLM). The MANM employs adaptive gating-based multimodal context-aware attention, enabling the model to focus on relevant visual and textual information and generating contextually relevant features. The GFRM module utilizes graphs to refine features within individual modalities, while the CFLM focuses on learning text and image-specific features such as toxicity features and caption features. Additionally, we curate a set of misogynous lexicons to compute the misogyny-specific lexicon score from the text. We apply test-time augmentation in feature space to better generalize the predictions on diverse inputs. The performance of the proposed approach has been evaluated on two multimodal datasets, MAMI, and MMHS150K, with 11,000 and 13,494 samples, respectively. The proposed method demonstrates an average improvement of 11.87% and 10.82% in macro-F1 over existing multimodal methods on the MAMI and MMHS150K datasets, respectively.",0306-4573,1873-5371,,, ,  ,,detection#methodology,
2718,"**Title**Cyber democracy in the digital age: Characterizing hate networks in the 2022 US midterm elections

**Abstract**Social media has become integral to societal discourse and play a role in shaping public engagement, particularly in democratic electoral processes. This paper addresses the pressing issue of hate speech on social media during the 2022 US midterm elections. Unlike previous research, which often relies on limited datasets and classic methodologies, we leverage Open Source Intelligence (OSINT) and Natural Language Processing (NLP) techniques to analyze Twitter data through advanced models of entity recognition, sentiment analysis, and community extraction, having persistence in Knowledge Graphs for consuming the intelligence efficiently. Results indicate that in the US midterm elections 2022, Arizona was the state that provided more content (507,551 tweets) related to a Chief Electoral Official, with 31.58% of them identified in the most aggressive cluster due to its mean attribute values of attack on commenter(0.7), inflammatory(similar to 0.3), attack on author(similar to 0.2), and toxicity(similar to 0.2). The name entity recognition model also identified an association between those aggressive tweets and the previous 2020 US Presidential campaign, characterized by attacks on election officials based on conspiracy theories campaigns. Knowledge graphs contributed to understanding the concentration of attacks and connectivity between topics commonly mentioned in hate speech content. Thus, our results offer detailed insights into the actors and dynamics of online harassment in electoral contexts, illuminating the challenges posed by harassment and proposing preventive mechanisms applicable to diverse electoral processes worldwide.","Rozo, Andres Zapata; Campo-Archbold, Alejandra; Diaz-Lopez, Daniel; Gray, Ian; Pastor-Galindo, Javier; Nespoli, Pantaleone; Marmol, Felix Gomez; McCoy, Damon","Mrmol, Flix/A-7505-2016","Diaz-Lopez, Daniel/0000-0001-7244-2631; Campo-Archbold, Alejandra/0000-0002-2574-8184",Cyber democracy in the digital age: Characterizing hate networks in the 2022 US midterm elections,110,,10.1016/j.inffus.2024.102459 ,Article ,,"Social media has become integral to societal discourse and play a role in shaping public engagement, particularly in democratic electoral processes. This paper addresses the pressing issue of hate speech on social media during the 2022 US midterm elections. Unlike previous research, which often relies on limited datasets and classic methodologies, we leverage Open Source Intelligence (OSINT) and Natural Language Processing (NLP) techniques to analyze Twitter data through advanced models of entity recognition, sentiment analysis, and community extraction, having persistence in Knowledge Graphs for consuming the intelligence efficiently. Results indicate that in the US midterm elections 2022, Arizona was the state that provided more content (507,551 tweets) related to a Chief Electoral Official, with 31.58% of them identified in the most aggressive cluster due to its mean attribute values of attack on commenter(0.7), inflammatory(similar to 0.3), attack on author(similar to 0.2), and toxicity(similar to 0.2). The name entity recognition model also identified an association between those aggressive tweets and the previous 2020 US Presidential campaign, characterized by attacks on election officials based on conspiracy theories campaigns. Knowledge graphs contributed to understanding the concentration of attacks and connectivity between topics commonly mentioned in hate speech content. Thus, our results offer detailed insights into the actors and dynamics of online harassment in electoral contexts, illuminating the challenges posed by harassment and proposing preventive mechanisms applicable to diverse electoral processes worldwide.",1566-2535,1872-6305,,, ,  ,,Gen_dataset,
2742,"**Title**Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models

**Abstract**Toxic content detection is crucial for online services to remove inappropriate content that violates community standards. To automate the detection process, prior works have proposed varieties of machine learning (ML) approaches to train Language Models (LMs) for toxic content detection. However, both their accuracy and transferability across datasets are limited. Recently, Large Language Models (LLMs) have shown promise in toxic content detection due to their superior zero-shot and few-shot in-context learning ability as well as broad transferability on ML tasks. However, efficiently designing prompts for LLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder their deployments in production. To address these challenges, in this work, we propose BD-LLM, a novel and efficient approach to Bootstrapping and Distilling LLMs for toxic content detection. Specifically, we design a novel prompting method named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection performance and extract high-quality rationales. DToT can automatically select more fine-grained context to re-prompt LLMs when their responses lack confidence. Additionally, we use the rationales extracted via DToT to fine-tune student LMs. Our experimental results on various datasets demonstrate that DToT can improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs fine-tuned with rationales extracted via DToT outperform baselines on all datasets with up to 16.9% accuracy improvement, while being more than 60x smaller than conventional LLMs. Finally, we observe that student LMs fine-tuned with rationales exhibit better cross-dataset transferability.","Zhang, Jiang; Wu, Qiong; Xu, Yiming; Cao, Cheng; Du, Zheng; Psounis, Konstantinos",,,Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models,,, ,Proceedings Paper ,,"Toxic content detection is crucial for online services to remove inappropriate content that violates community standards. To automate the detection process, prior works have proposed varieties of machine learning (ML) approaches to train Language Models (LMs) for toxic content detection. However, both their accuracy and transferability across datasets are limited. Recently, Large Language Models (LLMs) have shown promise in toxic content detection due to their superior zero-shot and few-shot in-context learning ability as well as broad transferability on ML tasks. However, efficiently designing prompts for LLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder their deployments in production. To address these challenges, in this work, we propose BD-LLM, a novel and efficient approach to Bootstrapping and Distilling LLMs for toxic content detection. Specifically, we design a novel prompting method named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection performance and extract high-quality rationales. DToT can automatically select more fine-grained context to re-prompt LLMs when their responses lack confidence. Additionally, we use the rationales extracted via DToT to fine-tune student LMs. Our experimental results on various datasets demonstrate that DToT can improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs fine-tuned with rationales extracted via DToT outperform baselines on all datasets with up to 16.9% accuracy improvement, while being more than 60x smaller than conventional LLMs. Finally, we observe that student LMs fine-tuned with rationales exhibit better cross-dataset transferability.",2159-5399,2374-3468,*****************,21779-21787, , 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence ,,detection#methodology,
2745,"**Title**Violent Speech Detection in Educational Environments

**Abstract**Nowadays, social networks allow people to interact by exchanging messages, publishing public or private photos or videos. But sometimes they become a space of toxic language to criticise, insult, hate, and attack. In this context, researchers promote a strong intention to study, analyse and detect hate speech. By automating its detection, the spread of anxiety and the rise of hateful content can be limited, especially among children in the online schools. However, with the absence of online database of vulgar English speech by Students in schools, detecting violent speech becomes a difficult task. In this paper, we propose a new dataset-based framework for the detection of students violent speech using natural language processing and learning techniques. This focuses on a proposed Students's Violent Speech (SVS) dataset with 7056 tagged tweets. The dataset is collected and pre-processed to be analyzed to show the performance and accuracy of the proposed model.","Chnini, Manel; Fredj, Nissaf; BenSaid, Fatma; Kacem, Yessine Hadj","kacem, Yessine/C-3434-2014",,Violent Speech Detection in Educational Environments,,,10.1109/AICCSA59173.2023.10479330 ,Proceedings Paper ,,"Nowadays, social networks allow people to interact by exchanging messages, publishing public or private photos or videos. But sometimes they become a space of toxic language to criticise, insult, hate, and attack. In this context, researchers promote a strong intention to study, analyse and detect hate speech. By automating its detection, the spread of anxiety and the rise of hateful content can be limited, especially among children in the online schools. However, with the absence of online database of vulgar English speech by Students in schools, detecting violent speech becomes a difficult task. In this paper, we propose a new dataset-based framework for the detection of students violent speech using natural language processing and learning techniques. This focuses on a proposed Students's Violent Speech (SVS) dataset with 7056 tagged tweets. The dataset is collected and pre-processed to be analyzed to show the performance and accuracy of the proposed model.",2161-5322,,979-8-3503-1943-9,, , 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA)20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA) ,,Gen_dataset,
2747,"**Title**A Generalizable Context-Aware Deep Learning Model for Abusive Language Detection

**Abstract**The proliferation of abusive language and hate speech in online content has become a pressing societal concern, necessitating effective detection methods. Recent years have witnessed a surge in datasets and computational methods for detecting abusive language, reflecting the growing interest in combating online abuse. Deep learning, in particular, has emerged as a powerful tool for addressing this pervasive issue. This paper presents a novel context-aware, attention-based Bidirectional Long Short-Term Memory (Bi-LSTM) model that relies exclusively on textual features. The model is designed for robust detection of abusive language. The proposed model integrates a domain-specific language model, HateBERT, with stacked Bi-LSTM and attention mechanism to enhance the processing capabilities of neural networks, enabling nuanced understanding of abusive language patterns. The versatility of the model is demonstrated through experiments on diverse abuse categories, showcasing its ability to effectively classify various types of abuse. The paper compares the model with existing state-of-the-art approaches and the findings underscore the potential of deep learning-based models in addressing the pervasive issue of online abusive behavior.","Kia, Mahsa Abazari; Samiee, Dorsa; Pournajar, Nasrin",,,A Generalizable Context-Aware Deep Learning Model for Abusive Language Detection,15022,,10.1007/978-3-031-72350-6_4 ,Proceedings Paper ,,"The proliferation of abusive language and hate speech in online content has become a pressing societal concern, necessitating effective detection methods. Recent years have witnessed a surge in datasets and computational methods for detecting abusive language, reflecting the growing interest in combating online abuse. Deep learning, in particular, has emerged as a powerful tool for addressing this pervasive issue. This paper presents a novel context-aware, attention-based Bidirectional Long Short-Term Memory (Bi-LSTM) model that relies exclusively on textual features. The model is designed for robust detection of abusive language. The proposed model integrates a domain-specific language model, HateBERT, with stacked Bi-LSTM and attention mechanism to enhance the processing capabilities of neural networks, enabling nuanced understanding of abusive language patterns. The versatility of the model is demonstrated through experiments on diverse abuse categories, showcasing its ability to effectively classify various types of abuse. The paper compares the model with existing state-of-the-art approaches and the findings underscore the potential of deep learning-based models in addressing the pervasive issue of online abusive behavior.",0302-9743,1611-3349,978-3-031-72349-0; 978-3-031-72350-6,49-63, , 33rd International Conference on Artificial Neural Networks and Machine Learning (ICANN)33rd International Conference on Artificial Neural Networks and Machine Learning (ICANN) ,,detection#methodology,
2750,"**Title**S-NLP at SemEval-2021 Task 5: An Analysis of Dual Networks for Sequence Tagging

**Abstract**The SemEval 2021 task 5: Toxic Spans Detection is a task of identifying considered-toxic spans in text, which provides a valuable, automatic tool for moderating online contents. This paper represents the second-place method for the task, an ensemble of two approaches. While one approach relies on combining different embedding methods to extract diverse semantic and syntactic representations of words in context; the other utilizes extra data with a slightly customized Self-training, a semisupervised learning technique, for sequence tagging problems. Both of our architectures take advantage of a strong language model, which was fine-tuned on a toxic classification task. Although experimental evidence indicates higher effectiveness of the first approach than the second one, combining them leads to our best results of 70.77 F1-score on the test dataset.",Viet Anh Nguyen; Tam Minh Nguyen; Huy Quang Dao; Quang Huu Pham,,,S-NLP at SemEval-2021 Task 5: An Analysis of Dual Networks for Sequence Tagging,,, ,Proceedings Paper ,,"The SemEval 2021 task 5: Toxic Spans Detection is a task of identifying considered-toxic spans in text, which provides a valuable, automatic tool for moderating online contents. This paper represents the second-place method for the task, an ensemble of two approaches. While one approach relies on combining different embedding methods to extract diverse semantic and syntactic representations of words in context; the other utilizes extra data with a slightly customized Self-training, a semisupervised learning technique, for sequence tagging problems. Both of our architectures take advantage of a strong language model, which was fine-tuned on a toxic classification task. Although experimental evidence indicates higher effectiveness of the first approach than the second one, combining them leads to our best results of 70.77 F1-score on the test dataset.",,,978-1-954085-70-1,888-897, , 15th International Workshops on Semantic Evaluation (SemEval)15th International Workshops on Semantic Evaluation (SemEval) ,,detection,
2751,"**Title**Attention-Based Bi-LSTM Network for Abusive Language Detection

**Abstract**Prevention of abusive content in social media platforms has received a lot of attention in recent years. The problem is still a challenge due to the nuances present in the content posted and the tactics of users to pass through the abusive detection algorithms employed by the social media giants. This paper presents a robust deep learning model that takes the advantage of both character and word representations, to address the challenges associated with the text content of a social media post for detecting abusive content. The proposed model uses character CNN to capture morphological information and learns the important features to distinguish abusive content with the help of an Attention-based Bi-LSTM network. In addition, it uses the pooling layer to avoid spatial translations. We confined our model to only the text content of the social media posts due to the limitations of collecting user characteristics for existing datasets and to honour the privacy concerns of users. Our model is dataset agnostic, as revealed by the empirical results on the data sets of multiple social media platforms. It outperformed all previous methods for abusive language detection with text-only features.","Nelatoori, Kiran Babu; Kommanti, Hima Bindu","K, H/JEZ-2854-2023","K, Himabindu/0000-0001-8958-4222; Nelatoori, Kiran Babu/0000-0002-9425-4057",Attention-Based Bi-LSTM Network for Abusive Language Detection,69,11.0,10.1080/03772063.2022.2034534 ,Article ,,"Prevention of abusive content in social media platforms has received a lot of attention in recent years. The problem is still a challenge due to the nuances present in the content posted and the tactics of users to pass through the abusive detection algorithms employed by the social media giants. This paper presents a robust deep learning model that takes the advantage of both character and word representations, to address the challenges associated with the text content of a social media post for detecting abusive content. The proposed model uses character CNN to capture morphological information and learns the important features to distinguish abusive content with the help of an Attention-based Bi-LSTM network. In addition, it uses the pooling layer to avoid spatial translations. We confined our model to only the text content of the social media posts due to the limitations of collecting user characteristics for existing datasets and to honour the privacy concerns of users. Our model is dataset agnostic, as revealed by the empirical results on the data sets of multiple social media platforms. It outperformed all previous methods for abusive language detection with text-only features.",0377-2063,0974-780X,,7884-7892, ,  ,,detection#methodology,
2752,"**Title**Polarized Opinion Detection Improves the Detection of Toxic Language

**Abstract**Distance from unimodality (DFU) has been found to correlate well with human judgment for the assessment of polarized opinions. However, its un-normalized nature makes it less intuitive and somewhat difficult to exploit in machine learning (e.g., as a supervised signal). In this work a normalized version of this measure, called nDFU, is proposed that leads to better assessment of the degree of polarization. Then, we propose a methodology for K-class text classification, based on nDFU, that exploits polarized texts in the dataset. Such polarized instances are assigned to a separate K+1 class, so that a K+1-class classifier is trained. An empirical analysis on three datasets for abusive language detection, shows that nDFU can be used to model polarized annotations and prevent them from harming the classification performance. Finally, we further exploit nDFU to specify conditions that could explain polarization given a dimension and present text examples that polarized the annotators when the dimension was gender and race. Our code is available at https://github.com/ipavlopoulos/ndfu.","Pavlopoulos, John; Likas, Aristidis","Pavlopoulos, John/MEO-1328-2025",,Polarized Opinion Detection Improves the Detection of Toxic Language,,, ,Proceedings Paper ,,"Distance from unimodality (DFU) has been found to correlate well with human judgment for the assessment of polarized opinions. However, its un-normalized nature makes it less intuitive and somewhat difficult to exploit in machine learning (e.g., as a supervised signal). In this work a normalized version of this measure, called nDFU, is proposed that leads to better assessment of the degree of polarization. Then, we propose a methodology for K-class text classification, based on nDFU, that exploits polarized texts in the dataset. Such polarized instances are assigned to a separate K+1 class, so that a K+1-class classifier is trained. An empirical analysis on three datasets for abusive language detection, shows that nDFU can be used to model polarized annotations and prevent them from harming the classification performance. Finally, we further exploit nDFU to specify conditions that could explain polarization given a dimension and present text examples that polarized the annotators when the dimension was gender and race. Our code is available at https://github.com/ipavlopoulos/ndfu.",,,979-8-89176-088-2,1946-1958, , 18th Conference of the European-Chapter of the Association-for-Computational-Linguistics (EACL)18th Conference of the European-Chapter of the Association-for-Computational-Linguistics (EACL) ,,methodology,
2754,"**Title**Towards safer online communities: Deep learning and explainable AI for hate speech detection and classification

**Abstract**The internet and social media facilitate widespread idea sharing but also contribute to cybercrimes and harmful behaviors, notably the dissemination of abusive and hateful speech, which poses a significant threat to societal cohesion. Hence, prompt and accurate detection of such harmful content is crucial. To address this issue, our study introduces a fully automated end-toend model for hate speech detection and classification using Natural Language Processing and Deep Learning techniques. The proposed architecture comprising embedding, Convolutional, bidirectional Recurrent Neural Network, and bidirectional Long Short Term Memory layers, achieved the highest accuracy of 98.5%. Additionally, we employ explainable AI techniques, such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), to gain insights into the performance of the proposed framework. This comprehensive approach meets the pressing demand for swift and precise detection and categorization of harmful online content.","Kibriya, Hareem; Siddiqa, Ayesha; Khan, Wazir Zada; Khan, Muhammad Khurram","KHAN, MUHAMMAD KHURRAM/E-4836-2014; Siddiqa, Ayesha/GXW-1936-2022; Khan, Wazir Zada/G-8580-2015",", Hareem Kibriya/0009-0003-1525-226X; Siddiqa, Ayesha/0000-0003-0780-6376; Khan, Wazir Zada/0000-0003-0819-4236",Towards safer online communities: Deep learning and explainable AI for hate speech detection and classification,116,,10.1016/j.compeleceng.2024.109153 ,Article ,,"The internet and social media facilitate widespread idea sharing but also contribute to cybercrimes and harmful behaviors, notably the dissemination of abusive and hateful speech, which poses a significant threat to societal cohesion. Hence, prompt and accurate detection of such harmful content is crucial. To address this issue, our study introduces a fully automated end-toend model for hate speech detection and classification using Natural Language Processing and Deep Learning techniques. The proposed architecture comprising embedding, Convolutional, bidirectional Recurrent Neural Network, and bidirectional Long Short Term Memory layers, achieved the highest accuracy of 98.5%. Additionally, we employ explainable AI techniques, such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), to gain insights into the performance of the proposed framework. This comprehensive approach meets the pressing demand for swift and precise detection and categorization of harmful online content.",0045-7906,1879-0755,,, ,  ,,detection,
2755,"**Title**Inappropriate Text Detection and Rephrasing Using NLP

**Abstract**The impact of offensive language on public and professional discourse highlights the need for efficient mitigating measures. Cutting-edge computational linguistic techniques were used to identify and treat such language in a novel way. A two-pronged mechanism is used when hazardous content is found: offending terminology is either removed or put through Natural Language Pre-processing, producing rephrased information that maintains the original meaning of the text. Additionally, this work uses two freely accessible datasets for text categorization. The technique is unique, because during the rephrasing stage, we consider the incorrect words to get their synonyms, and we choose to fit for replacement in the phrase. Classification best accuracy we have achieved of about 95%. The method is comprehensive and aims to create a setting that encourages courteous and peaceful discussion while maintaining semantic integrity. This research provides a sophisticated approach to fostering meaningful relationships in both public and professional contexts by fully addressing incorrect language.","Jain, Sanyam; Tripathy, B. K.","Tripathy, Bata/AAO-7494-2020",,Inappropriate Text Detection and Rephrasing Using NLP,2030,,10.1007/978-3-031-53731-8_21 ,Proceedings Paper ,,"The impact of offensive language on public and professional discourse highlights the need for efficient mitigating measures. Cutting-edge computational linguistic techniques were used to identify and treat such language in a novel way. A two-pronged mechanism is used when hazardous content is found: offending terminology is either removed or put through Natural Language Pre-processing, producing rephrased information that maintains the original meaning of the text. Additionally, this work uses two freely accessible datasets for text categorization. The technique is unique, because during the rephrasing stage, we consider the incorrect words to get their synonyms, and we choose to fit for replacement in the phrase. Classification best accuracy we have achieved of about 95%. The method is comprehensive and aims to create a setting that encourages courteous and peaceful discussion while maintaining semantic integrity. This research provides a sophisticated approach to fostering meaningful relationships in both public and professional contexts by fully addressing incorrect language.",1865-0929,1865-0937,978-3-031-53730-1; 978-3-031-53731-8,261-273, , 5th International Conference on Soft Computing and its Engineering Applications (IcSoftComp)5th International Conference on Soft Computing and its Engineering Applications (IcSoftComp) ,,detox,
2760,"**Title**Robustness of models addressing Information Disorder: A comprehensive review and benchmarking study

**Abstract**Machine learning and deep learning models are increasingly susceptible to adversarial attacks, particularly in critical areas like cybersecurity and Information Disorder. This study provides a comprehensive evaluation of model Robustness against such attacks across key tasks well -assessed in Information Disorder literature: Toxic Speech Detection, Sentiment Analysis, Propaganda Detection, and Hate Speech Detection. Rigorous experiments conducted across 13 models and 12 diverse datasets highlight significant vulnerabilities. The methodological framework implements adversarial attacks that strategically manipulates model inputs based on keyword significance, identified using the LIME method, an advanced explainable AI technique. The evaluation measures Robustness primarily through accuracy of the models and attack success rates. The experiments reveal that current models display inconsistent resistance to adversarial manipulations, underscoring an urgent need for developing more sophisticated defensive strategies. The study sheds light on the critical weaknesses in existing models and charts a course for future research to fortify AI resilience against evolving cyber threats. The findings advocate for a paradigm shift in model training and development to prioritize adversarial Robustness, ensuring that AI systems are equipped to handle real -world adversarial scenarios effectively.","Fenza, Giuseppe; Loia, Vincenzo; Stanzione, Claudio; Di Gisi, Maria",,"Di Gisi, Maria/0009-0003-5434-5426; FENZA, GIUSEPPE/0000-0002-4736-0113; stanzione, claudio/0000-0003-0158-3132",Robustness of models addressing Information Disorder: A comprehensive review and benchmarking study,596,,10.1016/j.neucom.2024.127951 ,Article ,,"Machine learning and deep learning models are increasingly susceptible to adversarial attacks, particularly in critical areas like cybersecurity and Information Disorder. This study provides a comprehensive evaluation of model Robustness against such attacks across key tasks well -assessed in Information Disorder literature: Toxic Speech Detection, Sentiment Analysis, Propaganda Detection, and Hate Speech Detection. Rigorous experiments conducted across 13 models and 12 diverse datasets highlight significant vulnerabilities. The methodological framework implements adversarial attacks that strategically manipulates model inputs based on keyword significance, identified using the LIME method, an advanced explainable AI technique. The evaluation measures Robustness primarily through accuracy of the models and attack success rates. The experiments reveal that current models display inconsistent resistance to adversarial manipulations, underscoring an urgent need for developing more sophisticated defensive strategies. The study sheds light on the critical weaknesses in existing models and charts a course for future research to fortify AI resilience against evolving cyber threats. The findings advocate for a paradigm shift in model training and development to prioritize adversarial Robustness, ensuring that AI systems are equipped to handle real -world adversarial scenarios effectively.",0925-2312,1872-8286,,, ,  ,,survey,
2763,"**Title**Beyond Binary Classification: A Fine-Grained Safety Dataset for Large Language Models

**Abstract**Large Language Models (LLMs) excel in interactive chat scenarios due to their advanced conversational abilities. However, their training process invariably exposes them to a diverse range of harmful or toxic content, posing significant challenges in ensuring that LLM responses align with human ethical values. Consequently, the detection and quantification of adverse content remains a paramount issue in contemporary research. In this paper, we introduce the SAFE dataset, a novel resource designed to advance safety assessment research in LLMs. Our dataset extends beyond the binary categorization of content into safe and unsafe. Drawing upon human interpretations of safety, we further delineate unsafe content into six granular categories: Sensitivity, Harmfulness, Falsehood, Information Corruption, Unnaturalness, and Deviation from Instructions. This refined classification aims to enhance LLMs' ability to discern unsafe data more accurately. In total, we have created a dataset comprising 52,340 instruction-response pairs, each annotated with safety meta-tags. Additionally, we have compiled expert comparative assessments for these indicators. We developed a multi-expert rating model trained on the SAFE dataset, designed to evaluate the responses of LLMs across various dimensions. This approach highlights the potential of our dataset in the realm of safety assessment for LLMs. The model's capability to provide multi-faceted evaluations reflects an advanced understanding of the nuanced requirements in LLM response assessment. We believe this dataset represents a valuable resource for the community, contributing to the safe development and deployment of LLMs. Our findings and resources are poised to fuel future research endeavors in this domain.","Yu, Jia; Li, Long; Lan, Zhenzhong",,,Beyond Binary Classification: A Fine-Grained Safety Dataset for Large Language Models,12,,10.1109/ACCESS.2024.3393245 ,Article ,,"Large Language Models (LLMs) excel in interactive chat scenarios due to their advanced conversational abilities. However, their training process invariably exposes them to a diverse range of harmful or toxic content, posing significant challenges in ensuring that LLM responses align with human ethical values. Consequently, the detection and quantification of adverse content remains a paramount issue in contemporary research. In this paper, we introduce the SAFE dataset, a novel resource designed to advance safety assessment research in LLMs. Our dataset extends beyond the binary categorization of content into safe and unsafe. Drawing upon human interpretations of safety, we further delineate unsafe content into six granular categories: Sensitivity, Harmfulness, Falsehood, Information Corruption, Unnaturalness, and Deviation from Instructions. This refined classification aims to enhance LLMs' ability to discern unsafe data more accurately. In total, we have created a dataset comprising 52,340 instruction-response pairs, each annotated with safety meta-tags. Additionally, we have compiled expert comparative assessments for these indicators. We developed a multi-expert rating model trained on the SAFE dataset, designed to evaluate the responses of LLMs across various dimensions. This approach highlights the potential of our dataset in the realm of safety assessment for LLMs. The model's capability to provide multi-faceted evaluations reflects an advanced understanding of the nuanced requirements in LLM response assessment. We believe this dataset represents a valuable resource for the community, contributing to the safe development and deployment of LLMs. Our findings and resources are poised to fuel future research endeavors in this domain.",2169-3536,,,64717-64726, ,  ,,Gen_dataset,
2766,"**Title**How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models

**Abstract**Abstract:Language is a deep-rooted means of perpetration of stereotypes and discrimination. Large Language Models (LLMs), now a pervasive technology in our everyday lives, can cause extensive harm when prone to generating toxic responses. The standard way to address this issue is to align the LLM, which, however, dampens the issue without constituting a definitive solution. Therefore, testing LLM even after alignment efforts remains crucial for detecting any residual deviations with respect to ethical standards. We present EvoTox, an automated testing framework for LLMs' inclination to toxicity, providing a way to quantitatively assess how much LLMs can be pushed towards toxic responses even in the presence of alignment. The framework adopts an iterative evolution strategy that exploits the interplay between two LLMs, the System Under Test (SUT) and the Prompt Generator steering SUT responses toward higher toxicity. The toxicity level is assessed by an automated oracle based on an existing toxicity classifier. We conduct a quantitative and qualitative empirical evaluation using four state-of-the-art LLMs as evaluation subjects having increasing complexity (7-13 billion parameters). Our quantitative evaluation assesses the cost-effectiveness of four alternative versions of EvoTox against existing baseline methods, based on random search, curated datasets of toxic prompts, and adversarial attacks. Our qualitative assessment engages human evaluators to rate the fluency of the generated prompts and the perceived toxicity of the responses collected during the testing sessions. Results indicate that the effectiveness, in terms of detected toxicity level, is significantly higher than the selected baseline methods (effect size up to 1.0 against random search and up to 0.99 against adversarial attacks). Furthermore, EvoTox yields a limited cost overhead (from 22% to 35% on average).","Corbo S,Bancale L,Gennaro V,Lestingi L,Scotti V,Camilli M",,,How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models,abs/2501.01741,,10.48550/ARXIV.2501.01741 , Journal Article,,"Abstract:Language is a deep-rooted means of perpetration of stereotypes and discrimination. Large Language Models (LLMs), now a pervasive technology in our everyday lives, can cause extensive harm when prone to generating toxic responses. The standard way to address this issue is to align the LLM, which, however, dampens the issue without constituting a definitive solution. Therefore, testing LLM even after alignment efforts remains crucial for detecting any residual deviations with respect to ethical standards. We present EvoTox, an automated testing framework for LLMs' inclination to toxicity, providing a way to quantitatively assess how much LLMs can be pushed towards toxic responses even in the presence of alignment. The framework adopts an iterative evolution strategy that exploits the interplay between two LLMs, the System Under Test (SUT) and the Prompt Generator steering SUT responses toward higher toxicity. The toxicity level is assessed by an automated oracle based on an existing toxicity classifier. We conduct a quantitative and qualitative empirical evaluation using four state-of-the-art LLMs as evaluation subjects having increasing complexity (7-13 billion parameters). Our quantitative evaluation assesses the cost-effectiveness of four alternative versions of EvoTox against existing baseline methods, based on random search, curated datasets of toxic prompts, and adversarial attacks. Our qualitative assessment engages human evaluators to rate the fluency of the generated prompts and the perceived toxicity of the responses collected during the testing sessions. Results indicate that the effectiveness, in terms of detected toxicity level, is significantly higher than the selected baseline methods (effect size up to 1.0 against random search and up to 0.99 against adversarial attacks). Furthermore, EvoTox yields a limited cost overhead (from 22% to 35% on average).",,,,, CoRR,  ,,evaluation,
2767,"**Title**Realistic Evaluation of Toxicity in Large Language Models

**Abstract**Large language models (LLMs) have become
     integral to our professional workflows and daily
     lives. Nevertheless, these machine companions
    of ours have a critical flaw: the huge amount of
    data which endows them with vast and diverse
    knowledge, also exposes them to the inevitable
    toxicity and bias. While most LLMs incorpo-
     rate defense mechanisms to prevent the gener-
    ation of harmful content, these safeguards can
    be easily bypassed with minimal prompt engi-
    neering. In this paper, we introduce the new
    Thoroughly Engineered Toxicity (TET) dataset,
    comprising manually crafted prompts designed
    to nullify the protective layers of such models.
    Through extensive evaluations, we demonstrate
    the pivotal role of TET in providing a rigorous
    benchmark for evaluation of toxicity awareness
     in several popular LLMs: it highlights the toxic-
     ity in the LLMs that might remain hidden when
    using normal prompts, thus revealing subtler
    issues in their behavior.

1  Introduction

Large language models (LLMs), or any other sys-
tem achieving such widespread popularity, necessi-
tate a meticulous evaluation of safety to ensure their
positive impact on the world. Numerous safety as-
sessments (Chang et al., 2023; Mukherjee et al.,
2023; Wang et al., 2023b; Zhuo et al., 2023) have
been conducted, each employing diverse strategies,
safety definitions, and prompts.
  However, these evaluations and the datasets
they employ have a significant drawback: they of-
ten rely on unnatural prompting methods, which
does not represent how people interact with chat
models in real-life scenarios. For instance, Real-
ToxicityPrompts (Gehman et al., 2020) is a no-
table dataset designed for toxicity testing of Large
Language Models, comprising 100,000 sentences

    Equal contribution.
    Corresponding author.sourced from the OpenWebTextCorpus (Gokaslan
and Cohen, 2019). In their study, the authors use
RealToxicityPrompts to examine large language
model chatbots by splitting every sentence at a spe-
cific point, using the leading portion as the input
prompt, and evaluating whether the content gener-
ated by the model to fill up the rest of the sentence
was toxic or not. Another noteworthy dataset is
ToxiGen (Hartvigsen et al., 2022), which consists
of 274,186 sentences generated by GPT-3 (Brown
et al., 2020). To utilize ToxiGen for investigating
the safety of LLM-based chatbots, Deshpande et al.
(2023) would pose a question or request, provide
seven sentences in the dataset, and then prompt the
model to answer in a style similar to those provided
sentences.
  To address the unrealistic nature of the current
toxic dataset benchmark for large language models,
we introduce the Thoroughly Engineered Toxicity
(TET) dataset, comprising 2546 prompts filtered
from over 1 million real-world interactions with
25 different Large Language Models compiled in
the chat-lmsys-1M dataset (Zheng et al., 2023).
Collected from 210K unique IP addresses in the
wild on the Vicuna demo and Chatbot Arena web-
site1, this dataset presents a repository of realistic
prompts that people commonly use to engage with
LLMs in real-world contexts.
  Besides the challenge of being distant from real-
world usage, another well-known issue in evalu-
ating LLMs involves their susceptibility to jail-
break prompts, where prompt engineering can pro-
foundly alter these models behavior (Liu et al.,
2023). This vulnerability implies that individuals
with harmful intentions could potentially exploit
prompt engineering techniques, turning LLMs into
powerful tools for malicious purposes and caus-
ing them to generate toxicity and harmful content
that may go undetected during evaluation. This

    1https://chat.lmsys.org1038","Luong T,Le TT,Ngo L,Nguyen T",,,Realistic Evaluation of Toxicity in Large Language Models,,,10.18653/V1/2024.FINDINGS-ACL.61 , Conference Paper,,"Large language models (LLMs) have become
     integral to our professional workflows and daily
     lives. Nevertheless, these machine companions
    of ours have a critical flaw: the huge amount of
    data which endows them with vast and diverse
    knowledge, also exposes them to the inevitable
    toxicity and bias. While most LLMs incorpo-
     rate defense mechanisms to prevent the gener-
    ation of harmful content, these safeguards can
    be easily bypassed with minimal prompt engi-
    neering. In this paper, we introduce the new
    Thoroughly Engineered Toxicity (TET) dataset,
    comprising manually crafted prompts designed
    to nullify the protective layers of such models.
    Through extensive evaluations, we demonstrate
    the pivotal role of TET in providing a rigorous
    benchmark for evaluation of toxicity awareness
     in several popular LLMs: it highlights the toxic-
     ity in the LLMs that might remain hidden when
    using normal prompts, thus revealing subtler
    issues in their behavior.

1  Introduction

Large language models (LLMs), or any other sys-
tem achieving such widespread popularity, necessi-
tate a meticulous evaluation of safety to ensure their
positive impact on the world. Numerous safety as-
sessments (Chang et al., 2023; Mukherjee et al.,
2023; Wang et al., 2023b; Zhuo et al., 2023) have
been conducted, each employing diverse strategies,
safety definitions, and prompts.
  However, these evaluations and the datasets
they employ have a significant drawback: they of-
ten rely on unnatural prompting methods, which
does not represent how people interact with chat
models in real-life scenarios. For instance, Real-
ToxicityPrompts (Gehman et al., 2020) is a no-
table dataset designed for toxicity testing of Large
Language Models, comprising 100,000 sentences

    Equal contribution.
    Corresponding author.sourced from the OpenWebTextCorpus (Gokaslan
and Cohen, 2019). In their study, the authors use
RealToxicityPrompts to examine large language
model chatbots by splitting every sentence at a spe-
cific point, using the leading portion as the input
prompt, and evaluating whether the content gener-
ated by the model to fill up the rest of the sentence
was toxic or not. Another noteworthy dataset is
ToxiGen (Hartvigsen et al., 2022), which consists
of 274,186 sentences generated by GPT-3 (Brown
et al., 2020). To utilize ToxiGen for investigating
the safety of LLM-based chatbots, Deshpande et al.
(2023) would pose a question or request, provide
seven sentences in the dataset, and then prompt the
model to answer in a style similar to those provided
sentences.
  To address the unrealistic nature of the current
toxic dataset benchmark for large language models,
we introduce the Thoroughly Engineered Toxicity
(TET) dataset, comprising 2546 prompts filtered
from over 1 million real-world interactions with
25 different Large Language Models compiled in
the chat-lmsys-1M dataset (Zheng et al., 2023).
Collected from 210K unique IP addresses in the
wild on the Vicuna demo and Chatbot Arena web-
site1, this dataset presents a repository of realistic
prompts that people commonly use to engage with
LLMs in real-world contexts.
  Besides the challenge of being distant from real-
world usage, another well-known issue in evalu-
ating LLMs involves their susceptibility to jail-
break prompts, where prompt engineering can pro-
foundly alter these models behavior (Liu et al.,
2023). This vulnerability implies that individuals
with harmful intentions could potentially exploit
prompt engineering techniques, turning LLMs into
powerful tools for malicious purposes and caus-
ing them to generate toxicity and harmful content
that may go undetected during evaluation. This

    1https://chat.lmsys.org1038",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024  ",,Gen_dataset#evaluation,
2768,"**Title**Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias

**Abstract**The common toxicity and societal bias in
    contents generated by large language mod-
     els (LLMs) necessitate strategies to reduce
    harm. Present solutions often demand white-
   box access to the model or substantial train-
     ing, which is impractical for cutting-edge com-
    mercial LLMs. Moreover, prevailing prompt-
    ing methods depend on external tool feed-
    back and fail to simultaneously lessen toxi-
    city and bias.  Motivated by social psychol-
   ogy principles, we propose a novel strategy
   named perspective-taking prompting (PET)
    that inspires LLMs to integrate diverse hu-
   man perspectives and self-regulate their re-
    sponses. This self-correction mechanism can
     significantly diminish toxicity (up to 89%) and
    bias (up to 73%) in LLMs responses. Rigor-
    ous evaluations and ablation studies are con-
    ducted on two commercial LLMs (ChatGPT
    and GLM) and three open-source LLMs, reveal-
    ing PETs superiority in producing less harmful
    responses, outperforming five strong baselines.

  Words kill, words give life; theyre either poison
or fruityou choose.
                   ~ Proverbs 18:21 (MSG)

1  Introduction
Large language models (LLMs; OpenAI et al. 2023;
Chowdhery et al. 2023; Touvron et al. 2023; Chiang
et al. 2023) excel in numerous NLP tasks, enhanc-
ing the efficiency of our work and life (Kasneci
et al., 2023; Kung et al., 2023). Meanwhile, recent
research pointed out that LLMs inevitably give ob-
jectionable responses, as they are pre-trained on
a vast amount of unsanitized web text (Gehman
et al., 2020). For instance, LLMs could output
toxic content with harmful attributes (e.g., rude,
disrespectful, insulting sentences) (Gehman et al.,
2020). They may also generate content with so-
cietal bias (Sheng et al., 2021b), which exhibits

   *Corresponding author.Figure 1: Shortcomings and limitations in current mea-
sures on reducing toxicity and bias.

stereotypes towards particular demographic groups,
e.g., Asians are good at math.).  It remains an
ongoing endeavor to make LLMs deliver harmless
and unbiased content (Gabriel, 2020; Bai et al.,
2022a; Liu et al., 2023; Shen et al., 2023).
  While many efforts have been devoted to alle-
viating toxicity and bias (Weidinger et al., 2021;
Mehrabi et al., 2021), existing measures exhibit
two shortcomings when applied to state-of-the-
art commercial LLMs, e.g., GPT-4 (OpenAI et al.,
2023). (1) Impractical requirement of white-box ac-
cess. Many solutions require access to the models
internal representations (Leong et al., 2023) or con-
trol decoding processes (Krause et al., 2021; Liu
et al., 2021), which is impossible to deploy on
commercial LLMs that only reveal limited log-
its.  (2) Huge training cost. Some solutions re-
quire domain-specific training, which is very cost-
prohibitive (Gururangan et al., 2020). While they
may work for older models like GPT-2, it is diffi-
cult to extend them to up-to-date LLMs (Gou et al.,
2023), which have significantly distinct behaviors
and features (c.f. Table 1).
  Driven by these issues, in this study, we con-
centrate on the black-box scenario. However, we
notice two limitations of existing measures. (1)
Single-issue focus. One issue is their focus on
addressing a single type of problematic behavior
while neglecting the need for concurrent adjust-
ments across various problematic attributes. More
seriously, Yang et al. (2022) point out some detoxi-
fication techniques (Liu et al., 2021) may inadver-8341Commercial LLMs





    Without
  External Tools             Two Shortcomings
     White-box access          Huge training cost

Unable to deploy on commercial LLMs      Results are hard to reproduce
             Two Limitations
      Single-issue focus            External tool reliance

    Inadvertently exacerbate bias     Complicates deployment and adaptation","Xu R,Zhou Z,Zhang T,Qi Z,Yao S,Xu K,Xu W,Qiu H",,,Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias,,, , Conference Paper,,"The common toxicity and societal bias in
    contents generated by large language mod-
     els (LLMs) necessitate strategies to reduce
    harm. Present solutions often demand white-
   box access to the model or substantial train-
     ing, which is impractical for cutting-edge com-
    mercial LLMs. Moreover, prevailing prompt-
    ing methods depend on external tool feed-
    back and fail to simultaneously lessen toxi-
    city and bias.  Motivated by social psychol-
   ogy principles, we propose a novel strategy
   named perspective-taking prompting (PET)
    that inspires LLMs to integrate diverse hu-
   man perspectives and self-regulate their re-
    sponses. This self-correction mechanism can
     significantly diminish toxicity (up to 89%) and
    bias (up to 73%) in LLMs responses. Rigor-
    ous evaluations and ablation studies are con-
    ducted on two commercial LLMs (ChatGPT
    and GLM) and three open-source LLMs, reveal-
    ing PETs superiority in producing less harmful
    responses, outperforming five strong baselines.

  Words kill, words give life; theyre either poison
or fruityou choose.
                   ~ Proverbs 18:21 (MSG)

1  Introduction
Large language models (LLMs; OpenAI et al. 2023;
Chowdhery et al. 2023; Touvron et al. 2023; Chiang
et al. 2023) excel in numerous NLP tasks, enhanc-
ing the efficiency of our work and life (Kasneci
et al., 2023; Kung et al., 2023). Meanwhile, recent
research pointed out that LLMs inevitably give ob-
jectionable responses, as they are pre-trained on
a vast amount of unsanitized web text (Gehman
et al., 2020). For instance, LLMs could output
toxic content with harmful attributes (e.g., rude,
disrespectful, insulting sentences) (Gehman et al.,
2020). They may also generate content with so-
cietal bias (Sheng et al., 2021b), which exhibits

   *Corresponding author.Figure 1: Shortcomings and limitations in current mea-
sures on reducing toxicity and bias.

stereotypes towards particular demographic groups,
e.g., Asians are good at math.).  It remains an
ongoing endeavor to make LLMs deliver harmless
and unbiased content (Gabriel, 2020; Bai et al.,
2022a; Liu et al., 2023; Shen et al., 2023).
  While many efforts have been devoted to alle-
viating toxicity and bias (Weidinger et al., 2021;
Mehrabi et al., 2021), existing measures exhibit
two shortcomings when applied to state-of-the-
art commercial LLMs, e.g., GPT-4 (OpenAI et al.,
2023). (1) Impractical requirement of white-box ac-
cess. Many solutions require access to the models
internal representations (Leong et al., 2023) or con-
trol decoding processes (Krause et al., 2021; Liu
et al., 2021), which is impossible to deploy on
commercial LLMs that only reveal limited log-
its.  (2) Huge training cost. Some solutions re-
quire domain-specific training, which is very cost-
prohibitive (Gururangan et al., 2020). While they
may work for older models like GPT-2, it is diffi-
cult to extend them to up-to-date LLMs (Gou et al.,
2023), which have significantly distinct behaviors
and features (c.f. Table 1).
  Driven by these issues, in this study, we con-
centrate on the black-box scenario. However, we
notice two limitations of existing measures. (1)
Single-issue focus. One issue is their focus on
addressing a single type of problematic behavior
while neglecting the need for concurrent adjust-
ments across various problematic attributes. More
seriously, Yang et al. (2022) point out some detoxi-
fication techniques (Liu et al., 2021) may inadver-8341Commercial LLMs





    Without
  External Tools             Two Shortcomings
     White-box access          Huge training cost

Unable to deploy on commercial LLMs      Results are hard to reproduce
             Two Limitations
      Single-issue focus            External tool reliance

    Inadvertently exacerbate bias     Complicates deployment and adaptation",,,,,Association for Computational Linguistics ,"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024  ",,detox,
2769,"**Title**Down the Toxicity Rabbit Hole: A Framework to Bias Audit Large Language Models with Key Emphasis on Racism, Antisemitism, and Misogyny

**Abstract**Abstract:This paper makes three contributions. First, it presents a generalizable, novel framework dubbed \textit{toxicity rabbit hole} that iteratively elicits toxic content from a wide suite of large language models. Spanning a set of 1,266 identity groups, we first conduct a bias audit of \texttt{PaLM 2} guardrails presenting key insights. Next, we report generalizability across several other models. Through the elicited toxic content, we present a broad analysis with a key emphasis on racism, antisemitism, misogyny, Islamophobia, homophobia, and transphobia. Finally, driven by concrete examples, we discuss potential ramifications.","Dutta A,Khorramrouz A,Dutta S,KhudaBukhsh AR",,,"Down the Toxicity Rabbit Hole: A Framework to Bias Audit Large Language Models with Key Emphasis on Racism, Antisemitism, and Misogyny",,, , Conference Paper,,"Abstract:This paper makes three contributions. First, it presents a generalizable, novel framework dubbed \textit{toxicity rabbit hole} that iteratively elicits toxic content from a wide suite of large language models. Spanning a set of 1,266 identity groups, we first conduct a bias audit of \texttt{PaLM 2} guardrails presenting key insights. Next, we report generalizability across several other models. Through the elicited toxic content, we present a broad analysis with a key emphasis on racism, antisemitism, misogyny, Islamophobia, homophobia, and transphobia. Finally, driven by concrete examples, we discuss potential ramifications.",,,,,ijcai.org ,"Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 2024, Jeju, South Korea, August 3-9, 2024  ",,evaluation,
2770,"**Title**Efficient Detection of Toxic Prompts in Large Language Models

**Abstract**Abstract:Large language models (LLMs) like ChatGPT and Gemini have significantly advanced natural language processing, enabling various applications such as chatbots and automated content generation. However, these models can be exploited by malicious individuals who craft toxic prompts to elicit harmful or unethical responses. These individuals often employ jailbreaking techniques to bypass safety mechanisms, highlighting the need for robust toxic prompt detection methods. Existing detection techniques, both blackbox and whitebox, face challenges related to the diversity of toxic prompts, scalability, and computational efficiency. In response, we propose ToxicDetector, a lightweight greybox method designed to efficiently detect toxic prompts in LLMs. ToxicDetector leverages LLMs to create toxic concept prompts, uses embedding vectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP) classifier for prompt classification. Our evaluation on various versions of the LLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector achieves a high accuracy of 96.39\% and a low false positive rate of 2.00\%, outperforming state-of-the-art methods. Additionally, ToxicDetector's processing time of 0.0780 seconds per prompt makes it highly suitable for real-time applications. ToxicDetector achieves high accuracy, efficiency, and scalability, making it a practical method for toxic prompt detection in LLMs.","Liu Y,Yu J,Sun H,Shi L,Deng G,Chen Y,Liu Y",,,Efficient Detection of Toxic Prompts in Large Language Models,,,10.1145/3691620.3695018 , Conference Paper,,"Abstract:Large language models (LLMs) like ChatGPT and Gemini have significantly advanced natural language processing, enabling various applications such as chatbots and automated content generation. However, these models can be exploited by malicious individuals who craft toxic prompts to elicit harmful or unethical responses. These individuals often employ jailbreaking techniques to bypass safety mechanisms, highlighting the need for robust toxic prompt detection methods. Existing detection techniques, both blackbox and whitebox, face challenges related to the diversity of toxic prompts, scalability, and computational efficiency. In response, we propose ToxicDetector, a lightweight greybox method designed to efficiently detect toxic prompts in LLMs. ToxicDetector leverages LLMs to create toxic concept prompts, uses embedding vectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP) classifier for prompt classification. Our evaluation on various versions of the LLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector achieves a high accuracy of 96.39\% and a low false positive rate of 2.00\%, outperforming state-of-the-art methods. Additionally, ToxicDetector's processing time of 0.0780 seconds per prompt makes it highly suitable for real-time applications. ToxicDetector achieves high accuracy, efficiency, and scalability, making it a practical method for toxic prompt detection in LLMs.",,,,,ACM ,"Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, ASE 2024, Sacramento, CA, USA, October 27 - November 1, 2024  ",,detection#methodology,
2771,No abstract available,"Taveekitworachai P,Abdullah F,Gursesli MC,Lanat A,Guazzini A,Thawonmas R",,,Prompt Evolution Through Examples for Large Language Models-A Case Study in Game Comment Toxicity Classification,,,10.1109/METROIND4.0IOT61288.2024.10584130 , Conference Paper,,,,,,,IEEE ,"IEEE International Workshop on Metrology for Industry 4.0 and IoT, MetroInd4.0 & IoT 2024, Firenze, Italy, May 29-31, 2024  ",,,
2773,"**Title**Eliciting and Measuring Toxic Bias in Human-to-Machine Interactions in Large Language Models

**Abstract**Merit is a central pillar of liberal epistemology, humanism, and democracy. The scientific enterprise, built on merit, has proven effective in generating scientific and technological advances, reducing suffering, narrowing social gaps, and improving the quality of life globally. This perspective documents the ongoing attempts to undermine the core principles of liberal epistemology and to replace merit with non-scientific, politically motivated criteria. We explain the philosophical origins of this conflict, document the intrusion of ideology into our scientific institutions, discuss the perils of abandoning merit, and offer an alternative, human-centered approach to address existing social inequalities.","Stein K,Harvey A,Lopez A,Taj U,Watkins S,Watkins LA",,,Eliciting and Measuring Toxic Bias in Human-to-Machine Interactions in Large Language Models,,,10.1109/UEMCON62879.2024.10754689 , Conference Paper,,"Merit is a central pillar of liberal epistemology, humanism, and democracy. The scientific enterprise, built on merit, has proven effective in generating scientific and technological advances, reducing suffering, narrowing social gaps, and improving the quality of life globally. This perspective documents the ongoing attempts to undermine the core principles of liberal epistemology and to replace merit with non-scientific, politically motivated criteria. We explain the philosophical origins of this conflict, document the intrusion of ideology into our scientific institutions, discuss the perils of abandoning merit, and offer an alternative, human-centered approach to address existing social inequalities.",,,,,IEEE ,"15th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference, UEMCON 2024, Yorktown Heights, NY, USA, October 17-19, 2024  ",,methodology,
2774,"**Title**Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias

**Abstract**Abstract:We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in Large Language Models. The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s. We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic. We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale.","Khan A,Saravanan P,Venkatesan SK",,,Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias,abs/2402.07166,,10.48550/ARXIV.2402.07166 , Journal Article,,"Abstract:We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in Large Language Models. The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s. We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic. We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale.",,,,, CoRR,  ,,evaluation,
2775,"**Title**PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models

**Abstract**Abstract:Recent advances in large language models (LLMs) have led to their extensive global deployment, and ensuring their safety calls for comprehensive and multilingual toxicity evaluations. However, existing toxicity benchmarks are overwhelmingly focused on English, posing serious risks to deploying LLMs in other languages. We address this by introducing PolygloToxicityPrompts (PTP), the first large-scale multilingual toxicity evaluation benchmark of 425K naturally occurring prompts spanning 17 languages. We overcome the scarcity of naturally occurring toxicity in web-text and ensure coverage across languages with varying resources by automatically scraping over 100M web-text documents. Using PTP, we investigate research questions to study the impact of model size, prompt language, and instruction and preference-tuning methods on toxicity by benchmarking over 60 LLMs. Notably, we find that toxicity increases as language resources decrease or model size increases. Although instruction- and preference-tuning reduce toxicity, the choice of preference-tuning method does not have any significant impact. Our findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research.","Jain D,Kumar P,Gehman S,Zhou X,Hartvigsen T,Sap M",,,PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models,abs/2405.09373,,10.48550/ARXIV.2405.09373 , Journal Article,,"Abstract:Recent advances in large language models (LLMs) have led to their extensive global deployment, and ensuring their safety calls for comprehensive and multilingual toxicity evaluations. However, existing toxicity benchmarks are overwhelmingly focused on English, posing serious risks to deploying LLMs in other languages. We address this by introducing PolygloToxicityPrompts (PTP), the first large-scale multilingual toxicity evaluation benchmark of 425K naturally occurring prompts spanning 17 languages. We overcome the scarcity of naturally occurring toxicity in web-text and ensure coverage across languages with varying resources by automatically scraping over 100M web-text documents. Using PTP, we investigate research questions to study the impact of model size, prompt language, and instruction and preference-tuning methods on toxicity by benchmarking over 60 LLMs. Notably, we find that toxicity increases as language resources decrease or model size increases. Although instruction- and preference-tuning reduce toxicity, the choice of preference-tuning method does not have any significant impact. Our findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research.",,,,, CoRR,  ,,evaluation,
2776,"**Title**Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness

**Abstract**Large language models (LLMs) have become
     integral to our professional workflows and daily
     lives. Nevertheless, these machine companions
    of ours have a critical flaw: the huge amount of
    data which endows them with vast and diverse
    knowledge, also exposes them to the inevitable
    toxicity and bias. While most LLMs incorpo-
     rate defense mechanisms to prevent the gener-
    ation of harmful content, these safeguards can
    be easily bypassed with minimal prompt engi-
    neering. In this paper, we introduce the new
    Thoroughly Engineered Toxicity (TET) dataset,
    comprising manually crafted prompts designed
    to nullify the protective layers of such models.
    Through extensive evaluations, we demonstrate
    the pivotal role of TET in providing a rigorous
    benchmark for evaluation of toxicity awareness
     in several popular LLMs: it highlights the toxic-
     ity in the LLMs that might remain hidden when
    using normal prompts, thus revealing subtler
    issues in their behavior.

1  Introduction

Large language models (LLMs), or any other sys-
tem achieving such widespread popularity, necessi-
tate a meticulous evaluation of safety to ensure their
positive impact on the world. Numerous safety as-
sessments (Chang et al., 2023; Mukherjee et al.,
2023; Wang et al., 2023b; Zhuo et al., 2023) have
been conducted, each employing diverse strategies,
safety definitions, and prompts.
  However, these evaluations and the datasets
they employ have a significant drawback: they of-
ten rely on unnatural prompting methods, which
does not represent how people interact with chat
models in real-life scenarios. For instance, Real-
ToxicityPrompts (Gehman et al., 2020) is a no-
table dataset designed for toxicity testing of Large
Language Models, comprising 100,000 sentences

    Equal contribution.
    Corresponding author.sourced from the OpenWebTextCorpus (Gokaslan
and Cohen, 2019). In their study, the authors use
RealToxicityPrompts to examine large language
model chatbots by splitting every sentence at a spe-
cific point, using the leading portion as the input
prompt, and evaluating whether the content gener-
ated by the model to fill up the rest of the sentence
was toxic or not. Another noteworthy dataset is
ToxiGen (Hartvigsen et al., 2022), which consists
of 274,186 sentences generated by GPT-3 (Brown
et al., 2020). To utilize ToxiGen for investigating
the safety of LLM-based chatbots, Deshpande et al.
(2023) would pose a question or request, provide
seven sentences in the dataset, and then prompt the
model to answer in a style similar to those provided
sentences.
  To address the unrealistic nature of the current
toxic dataset benchmark for large language models,
we introduce the Thoroughly Engineered Toxicity
(TET) dataset, comprising 2546 prompts filtered
from over 1 million real-world interactions with
25 different Large Language Models compiled in
the chat-lmsys-1M dataset (Zheng et al., 2023).
Collected from 210K unique IP addresses in the
wild on the Vicuna demo and Chatbot Arena web-
site1, this dataset presents a repository of realistic
prompts that people commonly use to engage with
LLMs in real-world contexts.
  Besides the challenge of being distant from real-
world usage, another well-known issue in evalu-
ating LLMs involves their susceptibility to jail-
break prompts, where prompt engineering can pro-
foundly alter these models behavior (Liu et al.,
2023). This vulnerability implies that individuals
with harmful intentions could potentially exploit
prompt engineering techniques, turning LLMs into
powerful tools for malicious purposes and caus-
ing them to generate toxicity and harmful content
that may go undetected during evaluation. This

    1https://chat.lmsys.org1038",Wang W,,,"Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness",abs/2409.00551,,10.48550/ARXIV.2409.00551 , Journal Article,,"Large language models (LLMs) have become
     integral to our professional workflows and daily
     lives. Nevertheless, these machine companions
    of ours have a critical flaw: the huge amount of
    data which endows them with vast and diverse
    knowledge, also exposes them to the inevitable
    toxicity and bias. While most LLMs incorpo-
     rate defense mechanisms to prevent the gener-
    ation of harmful content, these safeguards can
    be easily bypassed with minimal prompt engi-
    neering. In this paper, we introduce the new
    Thoroughly Engineered Toxicity (TET) dataset,
    comprising manually crafted prompts designed
    to nullify the protective layers of such models.
    Through extensive evaluations, we demonstrate
    the pivotal role of TET in providing a rigorous
    benchmark for evaluation of toxicity awareness
     in several popular LLMs: it highlights the toxic-
     ity in the LLMs that might remain hidden when
    using normal prompts, thus revealing subtler
    issues in their behavior.

1  Introduction

Large language models (LLMs), or any other sys-
tem achieving such widespread popularity, necessi-
tate a meticulous evaluation of safety to ensure their
positive impact on the world. Numerous safety as-
sessments (Chang et al., 2023; Mukherjee et al.,
2023; Wang et al., 2023b; Zhuo et al., 2023) have
been conducted, each employing diverse strategies,
safety definitions, and prompts.
  However, these evaluations and the datasets
they employ have a significant drawback: they of-
ten rely on unnatural prompting methods, which
does not represent how people interact with chat
models in real-life scenarios. For instance, Real-
ToxicityPrompts (Gehman et al., 2020) is a no-
table dataset designed for toxicity testing of Large
Language Models, comprising 100,000 sentences

    Equal contribution.
    Corresponding author.sourced from the OpenWebTextCorpus (Gokaslan
and Cohen, 2019). In their study, the authors use
RealToxicityPrompts to examine large language
model chatbots by splitting every sentence at a spe-
cific point, using the leading portion as the input
prompt, and evaluating whether the content gener-
ated by the model to fill up the rest of the sentence
was toxic or not. Another noteworthy dataset is
ToxiGen (Hartvigsen et al., 2022), which consists
of 274,186 sentences generated by GPT-3 (Brown
et al., 2020). To utilize ToxiGen for investigating
the safety of LLM-based chatbots, Deshpande et al.
(2023) would pose a question or request, provide
seven sentences in the dataset, and then prompt the
model to answer in a style similar to those provided
sentences.
  To address the unrealistic nature of the current
toxic dataset benchmark for large language models,
we introduce the Thoroughly Engineered Toxicity
(TET) dataset, comprising 2546 prompts filtered
from over 1 million real-world interactions with
25 different Large Language Models compiled in
the chat-lmsys-1M dataset (Zheng et al., 2023).
Collected from 210K unique IP addresses in the
wild on the Vicuna demo and Chatbot Arena web-
site1, this dataset presents a repository of realistic
prompts that people commonly use to engage with
LLMs in real-world contexts.
  Besides the challenge of being distant from real-
world usage, another well-known issue in evalu-
ating LLMs involves their susceptibility to jail-
break prompts, where prompt engineering can pro-
foundly alter these models behavior (Liu et al.,
2023). This vulnerability implies that individuals
with harmful intentions could potentially exploit
prompt engineering techniques, turning LLMs into
powerful tools for malicious purposes and caus-
ing them to generate toxicity and harmful content
that may go undetected during evaluation. This

    1https://chat.lmsys.org1038",,,,, CoRR,  ,,evaluation,
2777,"**Title**How Toxicity Classifiers and Large Language Models Respond to Ableism

**Abstract**Abstract.
People with disabilities (PwD) regularly encounter ableist hate and microaggressions online. While online platforms use machine learning models to moderate online harm, there is little research investigating how these models interact with ableism. In this paper, we curated a dataset of 100 social media comments targeted towards PwD, and recruited 160 participants to rate and explain how toxic and ableist these comments were. We then prompted state-of-the art toxicity classifiers (TCs) and large language models (LLMs) to rate and explain the harm. Our analysis revealed that TCs and LLMs rated toxicity significantly lower than PwD, but LLMs rated ableism generally on par with PwD. However, ableism explanations by LLMs overlooked emotional harm, and lacked specificity and acknowledgement of context, important facets of PwD explanations. Going forward, we discuss challenges in designing disability-aware toxicity classifiers, and advocate for the shift from ableism detection to ableism interpretation and explanation.","Phutane M,Seelam A,Vashistha A",,,How Toxicity Classifiers and Large Language Models Respond to Ableism,abs/2410.03448,,10.48550/ARXIV.2410.03448 , Journal Article,,"Abstract.
People with disabilities (PwD) regularly encounter ableist hate and microaggressions online. While online platforms use machine learning models to moderate online harm, there is little research investigating how these models interact with ableism. In this paper, we curated a dataset of 100 social media comments targeted towards PwD, and recruited 160 participants to rate and explain how toxic and ableist these comments were. We then prompted state-of-the art toxicity classifiers (TCs) and large language models (LLMs) to rate and explain the harm. Our analysis revealed that TCs and LLMs rated toxicity significantly lower than PwD, but LLMs rated ableism generally on par with PwD. However, ableism explanations by LLMs overlooked emotional harm, and lacked specificity and acknowledgement of context, important facets of PwD explanations. Going forward, we discuss challenges in designing disability-aware toxicity classifiers, and advocate for the shift from ableism detection to ableism interpretation and explanation.",,,,, CoRR,  ,,evaluation,
2778,"**Title**Toxic Subword Pruning for Dialogue Response Generation on Large Language Models

**Abstract**Abstract:How to defend large language models (LLMs) from generating toxic content is an important research area. Yet, most research focused on various model training techniques to remediate LLMs by updating their weights. A typical related research area is safety alignment. This however is often costly and tedious and can expose the model to even more problems such as catastrophic forgetting if the trainings are not carefully handled by experienced NLP practitioners. We thus propose a simple yet effective and novel algorithm, namely \textbf{Tox}ic Subword \textbf{Prun}ing (ToxPrune) to prune the subword contained by the toxic words from BPE in trained LLMs. In contrast to the previous work that demonstrates pruning BPE tokens as harmful to the task of machine translation, we surprisingly found its usefulness in preventing toxic content from being generated on LLMs. Fortunately, our findings suggest that ToxPrune simultaneously improves the toxic language model NSFW-3B on the task of dialogue response generation obviously. We surprisingly found that ToxPrune can even obviously improve official Llama-3.1-6B in the metric of dialogue diversity. Extensive automatic results and human evaluation indicate that ToxPrune could be helpful for both remediating toxic LLMs and improving non-toxic LLMs on the task of dialogue response generation.\footnote{We plan to release the resources to facilitate future work.}","Lu H,Lam W",,,Toxic Subword Pruning for Dialogue Response Generation on Large Language Models,abs/2410.04155,,10.48550/ARXIV.2410.04155 , Journal Article,,"Abstract:How to defend large language models (LLMs) from generating toxic content is an important research area. Yet, most research focused on various model training techniques to remediate LLMs by updating their weights. A typical related research area is safety alignment. This however is often costly and tedious and can expose the model to even more problems such as catastrophic forgetting if the trainings are not carefully handled by experienced NLP practitioners. We thus propose a simple yet effective and novel algorithm, namely \textbf{Tox}ic Subword \textbf{Prun}ing (ToxPrune) to prune the subword contained by the toxic words from BPE in trained LLMs. In contrast to the previous work that demonstrates pruning BPE tokens as harmful to the task of machine translation, we surprisingly found its usefulness in preventing toxic content from being generated on LLMs. Fortunately, our findings suggest that ToxPrune simultaneously improves the toxic language model NSFW-3B on the task of dialogue response generation obviously. We surprisingly found that ToxPrune can even obviously improve official Llama-3.1-6B in the metric of dialogue diversity. Extensive automatic results and human evaluation indicate that ToxPrune could be helpful for both remediating toxic LLMs and improving non-toxic LLMs on the task of dialogue response generation.\footnote{We plan to release the resources to facilitate future work.}",,,,, CoRR,  ,,detox,
2779,"**Title**Leveraging Large Language Models and Topic Modeling for Toxicity Classification

**Abstract**Abstract:Content moderation and toxicity classification represent critical tasks with significant social implications. However, studies have shown that major classification models exhibit tendencies to magnify or reduce biases and potentially overlook or disadvantage certain marginalized groups within their classification processes. Researchers suggest that the positionality of annotators influences the gold standard labels in which the models learned from propagate annotators' bias. To further investigate the impact of annotator positionality, we delve into fine-tuning BERTweet and HateBERT on the dataset while using topic-modeling strategies for content moderation. The results indicate that fine-tuning the models on specific topics results in a notable improvement in the F1 score of the models when compared to the predictions generated by other prominent classification models such as GPT-4, PerspectiveAPI, and RewireAPI. These findings further reveal that the state-of-the-art large language models exhibit significant limitations in accurately detecting and interpreting text toxicity contrasted with earlier methodologies. Code is available at this https URL.","Oskouie HE,Chance C,Huang C,Capetz M,Eyeson E,Sarrafzadeh M",,,Leveraging Large Language Models and Topic Modeling for Toxicity Classification,abs/2411.17876,,10.48550/ARXIV.2411.17876 , Journal Article,,"Abstract:Content moderation and toxicity classification represent critical tasks with significant social implications. However, studies have shown that major classification models exhibit tendencies to magnify or reduce biases and potentially overlook or disadvantage certain marginalized groups within their classification processes. Researchers suggest that the positionality of annotators influences the gold standard labels in which the models learned from propagate annotators' bias. To further investigate the impact of annotator positionality, we delve into fine-tuning BERTweet and HateBERT on the dataset while using topic-modeling strategies for content moderation. The results indicate that fine-tuning the models on specific topics results in a notable improvement in the F1 score of the models when compared to the predictions generated by other prominent classification models such as GPT-4, PerspectiveAPI, and RewireAPI. These findings further reveal that the state-of-the-art large language models exhibit significant limitations in accurately detecting and interpreting text toxicity contrasted with earlier methodologies. Code is available at this https URL.",,,,, CoRR,  ,,detection,
2780,"**Title**Hierarchical Adversarial Correction to Mitigate Identity Term Bias in Toxicity Detection

**Abstract**Corpora that are the fundament for toxicity de-
    tection contain such expressions typically di-
    rected against a target individual or group, e.g.,
    people of a specific gender or ethnicity. Prior
   work has shown that the target identity mention
    can constitute a confounding variable. As an ex-
    ample, a model might learn that Christians are
    always mentioned in the context of hate speech.
    This misguided focus can lead to a limited gen-
    eralization to newly emerging targets that are
    not found in the training data. In this paper, we
    hypothesize and subsequently show that this is-
    sue can be mitigated by considering targets on
    different levels of specificity. We distinguish
     levels of (1) the existence of a target, (2) a class
     (e.g., that the target is a religious group), or
    (3) a specific target group (e.g., Christians or
    Muslims). We define a target label hierarchy
    based on these three levels and then exploit this
    hierarchy in an adversarial correction for the
    lowest level (i.e. (3)) while maintaining some
    basic target features. This approach does not
    lower the toxicity detection performance but
    increases the generalization to targets not being
    available at training time.

1  Introduction

The EU Code of conduct on countering illegal hate
speech online relies on the definition of hate speech
as all conduct publicly inciting to violence or ha-
tred directed against a group of persons or a mem-
ber of such a group defined by reference to race,
colour, religion, descent or national or ethnic ori-
gin.1,2 This definition points out the role of the
target in hate speech, which is one form of toxicity
in text, next to other offensive language (Leite et al.,
2020). Targets as a constituting element already

   1This paper contains some examples of toxicity. This is
strictly for the purpose of explaining subtleties of the phe-
nomenon that are important for this research. Please be aware
that this content could be offensive and cause you distress.
   2https://ec.europa.eu/newsroom/just/document.
cfm?doc_id=42985received some attention in previous work (Silva
et al., 2016; Lemmens et al., 2021, i.a.).
  Hate speech expressions vary a lot, from ex-
plicit formulations to more implicit, and sometimes
even intentionally cryptic references, to bypass
automatic filters. This is an issue, because data
collection procedures can never be entirely fair 
they suffer from being focused on specific time
frames, topics, and therefore also targets (Dixon
et al., 2018). The working hypothesis in our pa-
per follows Waseem and Hovy (2016), Talat et al.
(2018) and Davidson et al. (2019) who have shown
that models learn regularly occurring target terms
as features of toxicity, because corpora developed
for annotation and training might mention poten-
tial targets predominantly in a toxic context. For
toxicity directed against less frequently mentioned
targets or where identity terms are not explicitely
mentioned (e.g., Examples #8 and #9 in Table 1), a
biased model is more apt to not detect toxicity.
  We aim at improving on this situation and pro-
pose to perform adversarial correction of toxicity
classifiers with regard to target identities.  This
leads to a challenge: How specific should the target
mention that we correct for be? Correcting for spe-
cific targets might lead to a sparsity problem while
correcting for the occurrence in a binary fashion
might not provide sufficiently specific informationtarget
occurrence?     yestoxicity?

         yesclass?identity?   Figure 1: Example for toxicity and hierarchical identity
   classification. We study if debiasing for the identity
   prediction on various levels of specificity (Occurrence
   O, Class C and Identity I) improves the robustness of
   the toxicity classification.

35. . .    . . .
           religions



             muslimsI hate muslims.","Schfer J,Heid U,Klinger R",,,Hierarchical Adversarial Correction to Mitigate Identity Term Bias in Toxicity Detection,,, , Conference Paper,,"Corpora that are the fundament for toxicity de-
    tection contain such expressions typically di-
    rected against a target individual or group, e.g.,
    people of a specific gender or ethnicity. Prior
   work has shown that the target identity mention
    can constitute a confounding variable. As an ex-
    ample, a model might learn that Christians are
    always mentioned in the context of hate speech.
    This misguided focus can lead to a limited gen-
    eralization to newly emerging targets that are
    not found in the training data. In this paper, we
    hypothesize and subsequently show that this is-
    sue can be mitigated by considering targets on
    different levels of specificity. We distinguish
     levels of (1) the existence of a target, (2) a class
     (e.g., that the target is a religious group), or
    (3) a specific target group (e.g., Christians or
    Muslims). We define a target label hierarchy
    based on these three levels and then exploit this
    hierarchy in an adversarial correction for the
    lowest level (i.e. (3)) while maintaining some
    basic target features. This approach does not
    lower the toxicity detection performance but
    increases the generalization to targets not being
    available at training time.

1  Introduction

The EU Code of conduct on countering illegal hate
speech online relies on the definition of hate speech
as all conduct publicly inciting to violence or ha-
tred directed against a group of persons or a mem-
ber of such a group defined by reference to race,
colour, religion, descent or national or ethnic ori-
gin.1,2 This definition points out the role of the
target in hate speech, which is one form of toxicity
in text, next to other offensive language (Leite et al.,
2020). Targets as a constituting element already

   1This paper contains some examples of toxicity. This is
strictly for the purpose of explaining subtleties of the phe-
nomenon that are important for this research. Please be aware
that this content could be offensive and cause you distress.
   2https://ec.europa.eu/newsroom/just/document.
cfm?doc_id=42985received some attention in previous work (Silva
et al., 2016; Lemmens et al., 2021, i.a.).
  Hate speech expressions vary a lot, from ex-
plicit formulations to more implicit, and sometimes
even intentionally cryptic references, to bypass
automatic filters. This is an issue, because data
collection procedures can never be entirely fair 
they suffer from being focused on specific time
frames, topics, and therefore also targets (Dixon
et al., 2018). The working hypothesis in our pa-
per follows Waseem and Hovy (2016), Talat et al.
(2018) and Davidson et al. (2019) who have shown
that models learn regularly occurring target terms
as features of toxicity, because corpora developed
for annotation and training might mention poten-
tial targets predominantly in a toxic context. For
toxicity directed against less frequently mentioned
targets or where identity terms are not explicitely
mentioned (e.g., Examples #8 and #9 in Table 1), a
biased model is more apt to not detect toxicity.
  We aim at improving on this situation and pro-
pose to perform adversarial correction of toxicity
classifiers with regard to target identities.  This
leads to a challenge: How specific should the target
mention that we correct for be? Correcting for spe-
cific targets might lead to a sparsity problem while
correcting for the occurrence in a binary fashion
might not provide sufficiently specific informationtarget
occurrence?     yestoxicity?

         yesclass?identity?   Figure 1: Example for toxicity and hierarchical identity
   classification. We study if debiasing for the identity
   prediction on various levels of specificity (Occurrence
   O, Class C and Identity I) improves the robustness of
   the toxicity classification.

35. . .    . . .
           religions



             muslimsI hate muslims.",,,,,Association for Computational Linguistics ,"Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis, WASSA 2024, Bangkok, Thailand, August 15, 2024  ",,detection#methodology,
2781,"**Title**Implanting LLM's Knowledge via Reading Comprehension Tree for Toxicity Detection

**Abstract**Warning: This paper contains several toxic
and offensive statements.    Toxicity detection plays a crucial role in main-
    taining the peace of the society. Existing meth-
    ods can be roughly categorized as small lan-
    guage model (SLM) based and large language
    model (LLM) based. However, due to the limi-
     tation of SLMs on general knowledge and the
     potential embedded bias in LLMs despite their
    large amount of knowledge, it is not a good
    idea to detect toxicity only with either SLM or
  LLM based method.
    In this work, we propose to implant LLMs
    knowledge into SLM based methods such that
   we can stick to both types of models strengths.
   To this end, we develop a reading comprehen-
    sion (RC) tree to transfer knowledge between
   two models.  Specifically, we first construct
    the RC tree, from an extensive to intensive
    reading perspective, to capture the local and
    global information in the text. We then model
    samples encoded by SLM and knowledge ex-
    tracted from LLM as two distributions using
    the constructed RT tree. We finally transfer
    knowledge via optimal transportation between
    two distributions. Extensive experiments prove
    the effectiveness of our method on real-world
    and machine-generated datasets. Our code and
    data are available at https://github.com/
    khk-abc/toxic-detection.

1  Introduction

With the prevailing of online communication, toxic
content has been growing in recent years on the
websites. In addition, the malicious use of large
language models makes toxic language more com-
mon and difficult to detect. Toxic language may
result in serious hazards such as the promotion of
violent crimes and discrimination against marginal-
ized groups. Due to its harmfulness to individuals

    Corresponding author.Figure 1: Illustration of the shortcomings of SLM (here
RoBERTa) and LLM (here ChatGPT) based methods
for toxicity detection.

and society (Shakespeare, 2013), toxic language
has become a serious concern.
  The harm of toxic content can be prevented ei-
ther by pre-detection before online release or by
post-detection before spreading to a broader au-
dience, and thus the detection of toxic content
plays a crucial role in controlling toxicity. Existing
methods for this purpose can be roughly catego-
rized into small language model (SLM) based and
large language model (LLM) based types. The fine-
tuned SLM based methods (Antypas and Camacho-
Collados, 2023; He et al., 2023) are the mainstream.
Due to the limited knowledge, the performance of
these methods is not very satisfying. Recently, a
LLM based method (Zhang et al., 2023a) is pre-
sented for this task.  However, the potential of
LLMs has not been fully exploited. More impor-
tantly, LLMs have their own bias which may lead to
new safety issues (Liyanage and Ranaweera, 2023;
Leong et al., 2023; Zhang et al., 2024).
  As  Fig. 1  (a) and  (b) show,  neither  the
RoBERTa (Liu et al., 2019) based method nor Chat-
GPT (OpenAI, 2022) can make a correct prediction
about the given sample. To have a close look, we
let two language models paraphrase cockroaches
and scorpions by masking these two words for947","Kang H,Qian T",,,Implanting LLM's Knowledge via Reading Comprehension Tree for Toxicity Detection,,,10.18653/V1/2024.FINDINGS-ACL.56 , Conference Paper,,"Warning: This paper contains several toxic
and offensive statements.    Toxicity detection plays a crucial role in main-
    taining the peace of the society. Existing meth-
    ods can be roughly categorized as small lan-
    guage model (SLM) based and large language
    model (LLM) based. However, due to the limi-
     tation of SLMs on general knowledge and the
     potential embedded bias in LLMs despite their
    large amount of knowledge, it is not a good
    idea to detect toxicity only with either SLM or
  LLM based method.
    In this work, we propose to implant LLMs
    knowledge into SLM based methods such that
   we can stick to both types of models strengths.
   To this end, we develop a reading comprehen-
    sion (RC) tree to transfer knowledge between
   two models.  Specifically, we first construct
    the RC tree, from an extensive to intensive
    reading perspective, to capture the local and
    global information in the text. We then model
    samples encoded by SLM and knowledge ex-
    tracted from LLM as two distributions using
    the constructed RT tree. We finally transfer
    knowledge via optimal transportation between
    two distributions. Extensive experiments prove
    the effectiveness of our method on real-world
    and machine-generated datasets. Our code and
    data are available at https://github.com/
    khk-abc/toxic-detection.

1  Introduction

With the prevailing of online communication, toxic
content has been growing in recent years on the
websites. In addition, the malicious use of large
language models makes toxic language more com-
mon and difficult to detect. Toxic language may
result in serious hazards such as the promotion of
violent crimes and discrimination against marginal-
ized groups. Due to its harmfulness to individuals

    Corresponding author.Figure 1: Illustration of the shortcomings of SLM (here
RoBERTa) and LLM (here ChatGPT) based methods
for toxicity detection.

and society (Shakespeare, 2013), toxic language
has become a serious concern.
  The harm of toxic content can be prevented ei-
ther by pre-detection before online release or by
post-detection before spreading to a broader au-
dience, and thus the detection of toxic content
plays a crucial role in controlling toxicity. Existing
methods for this purpose can be roughly catego-
rized into small language model (SLM) based and
large language model (LLM) based types. The fine-
tuned SLM based methods (Antypas and Camacho-
Collados, 2023; He et al., 2023) are the mainstream.
Due to the limited knowledge, the performance of
these methods is not very satisfying. Recently, a
LLM based method (Zhang et al., 2023a) is pre-
sented for this task.  However, the potential of
LLMs has not been fully exploited. More impor-
tantly, LLMs have their own bias which may lead to
new safety issues (Liyanage and Ranaweera, 2023;
Leong et al., 2023; Zhang et al., 2024).
  As  Fig. 1  (a) and  (b) show,  neither  the
RoBERTa (Liu et al., 2019) based method nor Chat-
GPT (OpenAI, 2022) can make a correct prediction
about the given sample. To have a close look, we
let two language models paraphrase cockroaches
and scorpions by masking these two words for947",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024  ",,detection#methodology,
2783,"**Title**E\(^2\)T\(^2\): Emote Embedding for Twitch Toxicity Detection

**Abstract**AbstractThe Internet has become the medium of choice for socialization and communication. The rise of live streaming services has created countless online communities of varying sizes with their own jokes, references, slang, and other means of communication. One of the largest live streaming services is Twitch.tv or Twitch, where a unique culture of niche language and emote usage has developed. Emotes are custom-made images, or GIFs, used in chat with varying degrees of access influenced by channel and external site subscription status. Emotes render standard forms of English Natural Language Process- ing (NLP) for tasks such as detection of toxicity or cyberbullying ineffective on Twitch. In this paper, we propose a methodology and offer a largely-trained dataset for detecting emote-based toxicity on live streaming platforms such as Twitch.","Moosavi K,Martin E,Ahmad MA,Mashhadi A",,,E\(^2\)T\(^2\): Emote Embedding for Twitch Toxicity Detection,,,10.1145/3678884.3681840 , Conference Paper,,"AbstractThe Internet has become the medium of choice for socialization and communication. The rise of live streaming services has created countless online communities of varying sizes with their own jokes, references, slang, and other means of communication. One of the largest live streaming services is Twitch.tv or Twitch, where a unique culture of niche language and emote usage has developed. Emotes are custom-made images, or GIFs, used in chat with varying degrees of access influenced by channel and external site subscription status. Emotes render standard forms of English Natural Language Process- ing (NLP) for tasks such as detection of toxicity or cyberbullying ineffective on Twitch. In this paper, we propose a methodology and offer a largely-trained dataset for detecting emote-based toxicity on live streaming platforms such as Twitch.",,,,,ACM ,"Companion Publication of the 2024 Conference on Computer-Supported Cooperative Work and Social Computing, CSCW Companion 2024, San Jose, Costa Rica, November 9-13, 2024  ",,Gen_dataset#detection,
2784,"**Title**Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method

**Abstract**Extensive efforts in automated approaches for
    content moderation have been focused on devel-
    oping models to identify toxic, offensive, and
    hateful content with the aim of lightening the
    load for moderators. Yet, it remains uncertain
    whether improvements on those tasks have truly
    addressed moderators needs in accomplishing
     their work. In this paper, we surface gaps be-
    tween past research efforts that have aimed to
    provide automation for aspects of content mod-
     eration and the needs of volunteer content mod-
     erators, regarding identifying violations of var-
    ious moderation rules. To do so, we conduct
    a model review on Hugging Face to reveal the
     availability of models to cover various modera-
    tion rules and guidelines from three exemplar
    forums. We further put state-of-the-art LLMs
    to the test, evaluating how well these models
    perform in flagging violations of platform rules
    from one particular forum. Finally, we conduct
    a user survey study with volunteer moderators
     to gain insight into their perspectives on useful
    moderation models. Overall, we observe a non-
     trivial gap, as missing developed models and
   LLMs exhibit moderate to low performance
   on a significant portion of the rules. Modera-
     tors reports provide guides for future work on
    developing moderation assistant models.
1  Introduction
Content moderation guards online forums against
hostility and extremism while maintaining commu-
nity norms, ensuring the forums remain healthy
and open to all participants. While many platforms
pay for this service, others, such as Reddit, Discord,
Facebook, and Twitch, use a hybrid model, relying
on the labor of volunteers. Yet, behind the screen,
being a volunteer content moderator is time- and
emotionally-draining work. Moderators frequently
deal with abusive language, sensitive posts, and
unpleasant interactions with users (Seering et al.,
2019; Gilbert, 2020; Dosono and Semaan, 2019;
Wohn, 2019; Jiang et al., 2019), often doing thiswork in addition to full-time jobs. To support these
volunteers, efforts have been made to develop mod-
els, such as Google Perspective API1 and OpenAI
undesired content detection (Markov et al., 2023),
that can automatically identify content for removal
in order to alleviate moderators workload.
  Although these systems have shown great suc-
cess in detecting undesired content, they primar-
ily focus on toxic content. Yet, content moderation
encompasses more than toxicity detection,2 partic-
ularly in platforms that leverage volunteer modera-
tion within smaller communities hosted by the site.
For example, Reddit is a platform consisting of var-
ious communities, known as subreddits, focused
on a diverse set of topics, and each subreddit has
its own moderation rules. Fiesler et al. (2018) con-
ducted a study to explore various subreddit rules,
consolidating similar ones, and arrived at 25 dis-
tinct rule types. Hence, in order to support moder-
ators in detecting potential rule-violating content,
content moderation tools need to support much
more than just toxicity detection.
  In this paper, we aim to assess to what extent
current natural language processing (NLP) models
can serve the wide spectrum of moderation rules
so that they can be helpful in assisting moderators.
First, to understand the functions previous auto-
mated content moderation models have focused on,
we conduct a model review on Hugging Face (HF)
with rules from three subreddits as exemplars. This
allows us to gauge the progress of past model devel-
opments in covering various moderation rules. We
use model review as opposed to the more common
literature review to gain a technical understanding
of the existing models functions. In addition to
examining models that are built to handle specific
tasks, we also assess so-called general-purposed
large language models (LLMs) capability in cov-

   1https://perspectiveapi.com/
   2We use toxicity detection as an umbrella term for hate
speech detection, incivility detection, etc.3567","Cao YT,Domingo LF,Gilbert SA,Mazurek ML,Shilton K,Iii HD",,,Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method,,, , Conference Paper,,"Extensive efforts in automated approaches for
    content moderation have been focused on devel-
    oping models to identify toxic, offensive, and
    hateful content with the aim of lightening the
    load for moderators. Yet, it remains uncertain
    whether improvements on those tasks have truly
    addressed moderators needs in accomplishing
     their work. In this paper, we surface gaps be-
    tween past research efforts that have aimed to
    provide automation for aspects of content mod-
     eration and the needs of volunteer content mod-
     erators, regarding identifying violations of var-
    ious moderation rules. To do so, we conduct
    a model review on Hugging Face to reveal the
     availability of models to cover various modera-
    tion rules and guidelines from three exemplar
    forums. We further put state-of-the-art LLMs
    to the test, evaluating how well these models
    perform in flagging violations of platform rules
    from one particular forum. Finally, we conduct
    a user survey study with volunteer moderators
     to gain insight into their perspectives on useful
    moderation models. Overall, we observe a non-
     trivial gap, as missing developed models and
   LLMs exhibit moderate to low performance
   on a significant portion of the rules. Modera-
     tors reports provide guides for future work on
    developing moderation assistant models.
1  Introduction
Content moderation guards online forums against
hostility and extremism while maintaining commu-
nity norms, ensuring the forums remain healthy
and open to all participants. While many platforms
pay for this service, others, such as Reddit, Discord,
Facebook, and Twitch, use a hybrid model, relying
on the labor of volunteers. Yet, behind the screen,
being a volunteer content moderator is time- and
emotionally-draining work. Moderators frequently
deal with abusive language, sensitive posts, and
unpleasant interactions with users (Seering et al.,
2019; Gilbert, 2020; Dosono and Semaan, 2019;
Wohn, 2019; Jiang et al., 2019), often doing thiswork in addition to full-time jobs. To support these
volunteers, efforts have been made to develop mod-
els, such as Google Perspective API1 and OpenAI
undesired content detection (Markov et al., 2023),
that can automatically identify content for removal
in order to alleviate moderators workload.
  Although these systems have shown great suc-
cess in detecting undesired content, they primar-
ily focus on toxic content. Yet, content moderation
encompasses more than toxicity detection,2 partic-
ularly in platforms that leverage volunteer modera-
tion within smaller communities hosted by the site.
For example, Reddit is a platform consisting of var-
ious communities, known as subreddits, focused
on a diverse set of topics, and each subreddit has
its own moderation rules. Fiesler et al. (2018) con-
ducted a study to explore various subreddit rules,
consolidating similar ones, and arrived at 25 dis-
tinct rule types. Hence, in order to support moder-
ators in detecting potential rule-violating content,
content moderation tools need to support much
more than just toxicity detection.
  In this paper, we aim to assess to what extent
current natural language processing (NLP) models
can serve the wide spectrum of moderation rules
so that they can be helpful in assisting moderators.
First, to understand the functions previous auto-
mated content moderation models have focused on,
we conduct a model review on Hugging Face (HF)
with rules from three subreddits as exemplars. This
allows us to gauge the progress of past model devel-
opments in covering various moderation rules. We
use model review as opposed to the more common
literature review to gain a technical understanding
of the existing models functions. In addition to
examining models that are built to handle specific
tasks, we also assess so-called general-purposed
large language models (LLMs) capability in cov-

   1https://perspectiveapi.com/
   2We use toxicity detection as an umbrella term for hate
speech detection, incivility detection, etc.3567",,,,,Association for Computational Linguistics ,"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024  ",,detection,
2786,"**Title**Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation

**Abstract**Abstract:Large Language Models (LLMs) drive current AI breakthroughs despite very little being known about their internal representations. In this work, we propose to shed the light on LLMs inner mechanisms through the lens of geometry. In particular, we develop in closed form $(i)$ the intrinsic dimension in which the Multi-Head Attention embeddings are constrained to exist and $(ii)$ the partition and per-region affine mappings of the feedforward (MLP) network of LLMs' layers. Our theoretical findings further enable the design of novel principled solutions applicable to state-of-the-art LLMs. First, we show that, through our geometric understanding, we can bypass LLMs' RLHF protection by controlling the embedding's intrinsic dimension through informed prompt manipulation. Second, we derive interpretable geometrical features that can be extracted from any (pre-trained) LLM, providing a rich abstract representation of their inputs. We observe that these features are sufficient to help solve toxicity detection, and even allow the identification of various types of toxicity. Our results demonstrate how, even in large-scale regimes, exact theoretical results can answer practical questions in LLMs. Code: this https URL","Balestriero R,Cosentino R,Shekkizhar S",,,Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation,,, , Conference Paper,,"Abstract:Large Language Models (LLMs) drive current AI breakthroughs despite very little being known about their internal representations. In this work, we propose to shed the light on LLMs inner mechanisms through the lens of geometry. In particular, we develop in closed form $(i)$ the intrinsic dimension in which the Multi-Head Attention embeddings are constrained to exist and $(ii)$ the partition and per-region affine mappings of the feedforward (MLP) network of LLMs' layers. Our theoretical findings further enable the design of novel principled solutions applicable to state-of-the-art LLMs. First, we show that, through our geometric understanding, we can bypass LLMs' RLHF protection by controlling the embedding's intrinsic dimension through informed prompt manipulation. Second, we derive interpretable geometrical features that can be extracted from any (pre-trained) LLM, providing a rich abstract representation of their inputs. We observe that these features are sufficient to help solve toxicity detection, and even allow the identification of various types of toxicity. Our results demonstrate how, even in large-scale regimes, exact theoretical results can answer practical questions in LLMs. Code: this https URL",,,,,OpenReview.net ,"Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024  ",,detox,
2787,"**Title**Toxicity Detection for Free

**Abstract**Abstract:Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we introduce Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found we can distinguish between benign and toxic prompts from the distribution of the first response token's logits. Using this idea, we build a robust detector of toxic prompts using a sparse logistic regression model on the first response token logits. Our scheme outperforms SOTA detectors under multiple metrics.","Hu Z,Piet J,Zhao G,Jiao J,Wagner DA",,,Toxicity Detection for Free,,, , Conference Paper,,"Abstract:Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we introduce Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found we can distinguish between benign and toxic prompts from the distribution of the first response token's logits. Using this idea, we build a robust detector of toxic prompts using a sparse logistic regression model on the first response token logits. Our scheme outperforms SOTA detectors under multiple metrics.",,,,, ,"Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024  ",,detection#methodology,
2788,"**Title**A Critical Reflection on the Use of Toxicity Detection Algorithms in Proactive Content Moderation Systems

**Abstract**Abstract:Toxicity detection algorithms, originally designed with reactive content moderation in mind, are increasingly being deployed into proactive end-user interventions to moderate content. Through a socio-technical lens and focusing on contexts in which they are applied, we explore the use of these algorithms in proactive moderation systems. Placing a toxicity detection algorithm in an imagined virtual mobile keyboard, we critically explore how such algorithms could be used to proactively reduce the sending of toxic content. We present findings from design workshops conducted with four distinct stakeholder groups and find concerns around how contextual complexities may exasperate inequalities around content moderation processes. Whilst only specific user groups are likely to directly benefit from these interventions, we highlight the potential for other groups to misuse them to circumvent detection, validate and gamify hate, and manipulate algorithmic models to exasperate harm.","Warner M,Strohmayer A,Higgs M,Coventry LM",,,A Critical Reflection on the Use of Toxicity Detection Algorithms in Proactive Content Moderation Systems,abs/2401.10629,,10.48550/ARXIV.2401.10629 , Journal Article,,"Abstract:Toxicity detection algorithms, originally designed with reactive content moderation in mind, are increasingly being deployed into proactive end-user interventions to moderate content. Through a socio-technical lens and focusing on contexts in which they are applied, we explore the use of these algorithms in proactive moderation systems. Placing a toxicity detection algorithm in an imagined virtual mobile keyboard, we critically explore how such algorithms could be used to proactively reduce the sending of toxic content. We present findings from design workshops conducted with four distinct stakeholder groups and find concerns around how contextual complexities may exasperate inequalities around content moderation processes. Whilst only specific user groups are likely to directly benefit from these interventions, we highlight the potential for other groups to misuse them to circumvent detection, validate and gamify hate, and manipulate algorithmic models to exasperate harm.",,,,, CoRR,  ,,detection,
2791,"**Title**Challenges for Real-Time Toxicity Detection in Online Games

**Abstract**AbstractReal-time toxicity detection in online environments poses a significant challenge, due to the increasing prevalence of social media and gaming platforms. We introduce ToxBuster, a simple and scalable model that reliably detects toxic content in real-time for a line of chat by including chat history and metadata. ToxBuster consistently outperforms conventional toxicity models across popular multiplayer games, including Rainbow Six Siege, For Honor, and DOTA 2. We conduct an ablation study to assess the importance of each model component and explore ToxBusters transferability across the datasets. Furthermore, we showcase ToxBusters efficacy in post-game moderation, successfully flagging 82.1% of chat-reported players at a precision level of 90.0%. Additionally, we show how an additional 6% of unreported toxic players can be proactively moderated.","Ng LH,Lim AX,Yoder MM",,,Challenges for Real-Time Toxicity Detection in Online Games,abs/2407.04383,,10.48550/ARXIV.2407.04383 , Journal Article,,"AbstractReal-time toxicity detection in online environments poses a significant challenge, due to the increasing prevalence of social media and gaming platforms. We introduce ToxBuster, a simple and scalable model that reliably detects toxic content in real-time for a line of chat by including chat history and metadata. ToxBuster consistently outperforms conventional toxicity models across popular multiplayer games, including Rainbow Six Siege, For Honor, and DOTA 2. We conduct an ablation study to assess the importance of each model component and explore ToxBusters transferability across the datasets. Furthermore, we showcase ToxBusters efficacy in post-game moderation, successfully flagging 82.1% of chat-reported players at a precision level of 90.0%. Additionally, we show how an additional 6% of unreported toxic players can be proactively moderated.",,,,, CoRR,  ,,detection,
2792,"**Title**Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity

**Abstract**Abstract:We introduce a novel family of adversarial attacks that exploit the inability of language models to interpret ASCII art. To evaluate these attacks, we propose the ToxASCII benchmark and develop two custom ASCII art fonts: one leveraging special tokens and another using text-filled letter shapes. Our attacks achieve a perfect 1.0 Attack Success Rate across ten models, including OpenAI's o1-preview and LLaMA 3.1.
Warning: this paper contains examples of toxic language used for research purposes.","Berezin S,Farahbakhsh R,Crespi N",,,Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity,abs/2409.18708,,10.48550/ARXIV.2409.18708 , Journal Article,,"Abstract:We introduce a novel family of adversarial attacks that exploit the inability of language models to interpret ASCII art. To evaluate these attacks, we propose the ToxASCII benchmark and develop two custom ASCII art fonts: one leveraging special tokens and another using text-filled letter shapes. Our attacks achieve a perfect 1.0 Attack Success Rate across ten models, including OpenAI's o1-preview and LLaMA 3.1.
Warning: this paper contains examples of toxic language used for research purposes.",,,,, CoRR,  ,,methodology,
2793,"**Title**DeMod: A Holistic Tool with Explainable Detection and Personalized Modification for Toxicity Censorship

**Abstract**Abstract:Although there have been automated approaches and tools supporting toxicity censorship for social posts, most of them focus on detection. Toxicity censorship is a complex process, wherein detection is just an initial task and a user can have further needs such as rationale understanding and content modification. For this problem, we conduct a needfinding study to investigate people's diverse needs in toxicity censorship and then build a ChatGPT-based censorship tool named DeMod accordingly. DeMod is equipped with the features of explainable Detection and personalized Modification, providing fine-grained detection results, detailed explanations, and personalized modification suggestions. We also implemented the tool and recruited 35 Weibo users for evaluation. The results suggest DeMod's multiple strengths like the richness of functionality, the accuracy of censorship, and ease of use. Based on the findings, we further propose several insights into the design of content censorship systems.","Li Y,Zhang P,Gu H,Lu T,Qiao S,Shu Y,Shao Y,Gu N",,,DeMod: A Holistic Tool with Explainable Detection and Personalized Modification for Toxicity Censorship,abs/2411.01844,,10.48550/ARXIV.2411.01844 , Journal Article,,"Abstract:Although there have been automated approaches and tools supporting toxicity censorship for social posts, most of them focus on detection. Toxicity censorship is a complex process, wherein detection is just an initial task and a user can have further needs such as rationale understanding and content modification. For this problem, we conduct a needfinding study to investigate people's diverse needs in toxicity censorship and then build a ChatGPT-based censorship tool named DeMod accordingly. DeMod is equipped with the features of explainable Detection and personalized Modification, providing fine-grained detection results, detailed explanations, and personalized modification suggestions. We also implemented the tool and recruited 35 Weibo users for evaluation. The results suggest DeMod's multiple strengths like the richness of functionality, the accuracy of censorship, and ease of use. Based on the findings, we further propose several insights into the design of content censorship systems.",,,,, CoRR,  ,,detection#methodology,
2794,"**Title**A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement

**Abstract**Abstract:Content moderation typically combines the efforts of human moderators and machine learning models. However, these systems often rely on data where significant disagreement occurs during moderation, reflecting the subjective nature of toxicity perception. Rather than dismissing this disagreement as noise, we interpret it as a valuable signal that highlights the inherent ambiguity of the content,an insight missed when only the majority label is considered. In this work, we introduce a novel content moderation framework that emphasizes the importance of capturing annotation disagreement. Our approach uses multitask learning, where toxicity classification serves as the primary task and annotation disagreement is addressed as an auxiliary task. Additionally, we leverage uncertainty estimation techniques, specifically Conformal Prediction, to account for both the ambiguity in comment annotations and the model's inherent uncertainty in predicting toxicity and this http URL framework also allows moderators to adjust thresholds for annotation disagreement, offering flexibility in determining when ambiguity should trigger a review. We demonstrate that our joint approach enhances model performance, calibration, and uncertainty estimation, while offering greater parameter efficiency and improving the review process in comparison to single-task methods.","Villate-Castillo G,Ser J,Sanz B",,,A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement,abs/2411.04090,,10.48550/ARXIV.2411.04090 , Journal Article,,"Abstract:Content moderation typically combines the efforts of human moderators and machine learning models. However, these systems often rely on data where significant disagreement occurs during moderation, reflecting the subjective nature of toxicity perception. Rather than dismissing this disagreement as noise, we interpret it as a valuable signal that highlights the inherent ambiguity of the content,an insight missed when only the majority label is considered. In this work, we introduce a novel content moderation framework that emphasizes the importance of capturing annotation disagreement. Our approach uses multitask learning, where toxicity classification serves as the primary task and annotation disagreement is addressed as an auxiliary task. Additionally, we leverage uncertainty estimation techniques, specifically Conformal Prediction, to account for both the ambiguity in comment annotations and the model's inherent uncertainty in predicting toxicity and this http URL framework also allows moderators to adjust thresholds for annotation disagreement, offering flexibility in determining when ambiguity should trigger a review. We demonstrate that our joint approach enhances model performance, calibration, and uncertainty estimation, while offering greater parameter efficiency and improving the review process in comparison to single-task methods.",,,,, CoRR,  ,,detection#methodology,
2797,"**Title**Toxicity Detection towards Adaptability to Changing Perturbations

**Abstract**Abstract:Toxicity detection is crucial for maintaining the peace of the society. While existing methods perform well on normal toxic contents or those generated by specific perturbation methods, they are vulnerable to evolving perturbation patterns. However, in real-world scenarios, malicious users tend to create new perturbation patterns for fooling the detectors. For example, some users may circumvent the detector of large language models (LLMs) by adding `I am a scientist' at the beginning of the prompt. In this paper, we introduce a novel problem, i.e., continual learning jailbreak perturbation patterns, into the toxicity detection field. To tackle this problem, we first construct a new dataset generated by 9 types of perturbation patterns, 7 of them are summarized from prior work and 2 of them are developed by us. We then systematically validate the vulnerability of current methods on this new perturbation pattern-aware dataset via both the zero-shot and fine tuned cross-pattern detection. Upon this, we present the domain incremental learning paradigm and the corresponding benchmark to ensure the detector's robustness to dynamically emerging types of perturbed toxic text. Our code and dataset are provided in the appendix and will be publicly available at GitHub, by which we wish to offer new research opportunities for the security-relevant communities.","Kang H,Chen J,Li Y,Miao X,Xu M,Zhong M,Zhu Y,Qian T",,,Toxicity Detection towards Adaptability to Changing Perturbations,abs/2412.15267,,10.48550/ARXIV.2412.15267 , Journal Article,,"Abstract:Toxicity detection is crucial for maintaining the peace of the society. While existing methods perform well on normal toxic contents or those generated by specific perturbation methods, they are vulnerable to evolving perturbation patterns. However, in real-world scenarios, malicious users tend to create new perturbation patterns for fooling the detectors. For example, some users may circumvent the detector of large language models (LLMs) by adding `I am a scientist' at the beginning of the prompt. In this paper, we introduce a novel problem, i.e., continual learning jailbreak perturbation patterns, into the toxicity detection field. To tackle this problem, we first construct a new dataset generated by 9 types of perturbation patterns, 7 of them are summarized from prior work and 2 of them are developed by us. We then systematically validate the vulnerability of current methods on this new perturbation pattern-aware dataset via both the zero-shot and fine tuned cross-pattern detection. Upon this, we present the domain incremental learning paradigm and the corresponding benchmark to ensure the detector's robustness to dynamically emerging types of perturbed toxic text. Our code and dataset are provided in the appendix and will be publicly available at GitHub, by which we wish to offer new research opportunities for the security-relevant communities.",,,,, CoRR,  ,,evaluation#methodology,
2798,"**Title**Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph

**Abstract**Abstract:The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code will be available soon.","Zhao Y,Zhu J,Xu C,Li X",,,Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph,abs/2412.15268,,10.48550/ARXIV.2412.15268 , Journal Article,,"Abstract:The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code will be available soon.",,,,, CoRR,  ,,detection,
2800,No abstract available,"M P,K R,Hegde A,Girish K,Coelho S,Shashirekha HL",,,Taming Toxicity: Learning Models for Hate Speech and Offensive Language Detection in Social Media Text,3681,, , Conference Paper,,,,,,,CEUR-WS.org ,"Working Notes of FIRE 2023 - Forum for Information Retrieval Evaluation (FIRE-WN 2023), Goa, India, December 15-18, 2023  ",,detection,
2802,"**Title**Beyond plain toxic: building datasets for detection of flammable topics and inappropriate statements

**Abstract**Abstract:Toxicity on the Internet, such as hate speech, offenses towards particular users or groups of people, or the use of obscene words, is an acknowledged problem. However, there also exist other types of inappropriate messages which are usually not viewed as toxic, e.g. as they do not contain explicit offences. Such messages can contain covered toxicity or generalizations, incite harmful actions (crime, suicide, drug use), provoke ""heated"" discussions. Such messages are often related to particular sensitive topics, e.g. on politics, sexual minorities, social injustice which more often than other topics, e.g. cars or computing, yield toxic emotional reactions. At the same time, clearly not all messages within such flammable topics are inappropriate.
Towards this end, in this work, we present two text collections labelled according to binary notion of inapropriateness and a multinomial notion of sensitive topic. Assuming that the notion of inappropriateness is common among people of the same culture, we base our approach on human intuitive understanding of what is not acceptable and harmful. To objectivise the notion of inappropriateness, we define it in a data-driven way though crowdsourcing. Namely we run a large-scale annotation study asking workers if a given chatbot textual statement could harm reputation of a company created it. Acceptably high values of inter-annotator agreement suggest that the notion of inappropriateness exists and can be uniformly understood by different people. To define the notion of sensitive topics in an objective way we use on guidelines suggested commonly by specialists of legal and PR department of a large public company as potentially harmful.","Babakov N,Logacheva V,Panchenko A",,,Beyond plain toxic: building datasets for detection of flammable topics and inappropriate statements,58,2.0,10.1007/S10579-023-09682-Z , Journal Article,,"Abstract:Toxicity on the Internet, such as hate speech, offenses towards particular users or groups of people, or the use of obscene words, is an acknowledged problem. However, there also exist other types of inappropriate messages which are usually not viewed as toxic, e.g. as they do not contain explicit offences. Such messages can contain covered toxicity or generalizations, incite harmful actions (crime, suicide, drug use), provoke ""heated"" discussions. Such messages are often related to particular sensitive topics, e.g. on politics, sexual minorities, social injustice which more often than other topics, e.g. cars or computing, yield toxic emotional reactions. At the same time, clearly not all messages within such flammable topics are inappropriate.
Towards this end, in this work, we present two text collections labelled according to binary notion of inapropriateness and a multinomial notion of sensitive topic. Assuming that the notion of inappropriateness is common among people of the same culture, we base our approach on human intuitive understanding of what is not acceptable and harmful. To objectivise the notion of inappropriateness, we define it in a data-driven way though crowdsourcing. Namely we run a large-scale annotation study asking workers if a given chatbot textual statement could harm reputation of a company created it. Acceptably high values of inter-annotator agreement suggest that the notion of inappropriateness exists and can be uniformly understood by different people. To define the notion of sensitive topics in an objective way we use on guidelines suggested commonly by specialists of legal and PR department of a large public company as potentially harmful.",,,,, Lang. Resour. Evaluation,  ,,Gen_dataset#detection,
2810,"**Title**Leveraging deep learning for toxic comment detection in cursive languages

**Abstract**Comment sections of online news platforms are an essential space
to express opinions and discuss political topics. In contrast to other online
posts, news discussions are related to particular news articles, comments re-
fer to each other, and individual conversations emerge. However, the misuse
by spammers, haters, and trolls makes costly content moderation necessary.
Sentiment analysis can not only support moderation but also help to under-
stand the dynamics of online discussions. A subtask of content moderation
is the identication of toxic comments. To this end, we describe the con-
cept of toxicity and characterize its subclasses. Further, we present various
deep learning approaches, including datasets and architectures, tailored to
sentiment analysis in online discussions. One way to make these approaches
more comprehensible and trustworthy is ne-grained instead of binary com-
ment classication. On the downside, more classes require more training data.
Therefore, we propose to augment training data by using transfer learning.
We discuss real-world applications, such as semi-automated comment mod-
eration and troll detection. Finally, we outline future challenges and current
limitations in the light of most recent research publications.

Key words: Deep Learning; Natural Language Processing; User-generated
Content; Toxic Comment Classication; Hate Speech Detection;





Julian Risch (corresponding author)
Hasso Plattner Institute, University of Potsdam, Prof.-Dr.-Helmert-Str. 23, 14482
Potsdam, Germany e-mail: julian.risch@hpi.de phone: +49 331 5509 272

Ralf Krestel
Hasso Plattner Institute, University of Potsdam, Prof.-Dr.-Helmert-Str. 23, 14482
Potsdam, Germany e-mail: ralf.krestel@hpi.de","Shahid M,Umair M,Iqbal MA,Rashid M,Akram S,Zubair M",,,Leveraging deep learning for toxic comment detection in cursive languages,10,,10.7717/PEERJ-CS.2486 , Journal Article,,"Comment sections of online news platforms are an essential space
to express opinions and discuss political topics. In contrast to other online
posts, news discussions are related to particular news articles, comments re-
fer to each other, and individual conversations emerge. However, the misuse
by spammers, haters, and trolls makes costly content moderation necessary.
Sentiment analysis can not only support moderation but also help to under-
stand the dynamics of online discussions. A subtask of content moderation
is the identication of toxic comments. To this end, we describe the con-
cept of toxicity and characterize its subclasses. Further, we present various
deep learning approaches, including datasets and architectures, tailored to
sentiment analysis in online discussions. One way to make these approaches
more comprehensible and trustworthy is ne-grained instead of binary com-
ment classication. On the downside, more classes require more training data.
Therefore, we propose to augment training data by using transfer learning.
We discuss real-world applications, such as semi-automated comment mod-
eration and troll detection. Finally, we outline future challenges and current
limitations in the light of most recent research publications.

Key words: Deep Learning; Natural Language Processing; User-generated
Content; Toxic Comment Classication; Hate Speech Detection;





Julian Risch (corresponding author)
Hasso Plattner Institute, University of Potsdam, Prof.-Dr.-Helmert-Str. 23, 14482
Potsdam, Germany e-mail: julian.risch@hpi.de phone: +49 331 5509 272

Ralf Krestel
Hasso Plattner Institute, University of Potsdam, Prof.-Dr.-Helmert-Str. 23, 14482
Potsdam, Germany e-mail: ralf.krestel@hpi.de",,,,, PeerJ Comput. Sci.,  ,,detection,
2811,"**Title**Take Its Essence, Discard Its Dross! Debiasing for Toxic Language Detection via Counterfactual Causal Effect

**Abstract**Current methods of toxic language detection (TLD) typically rely on specific tokens to conduct decisions, which
makes them suffer from lexical bias, leading to inferior performance and generalization. Lexical bias has both useful
and misleading impacts on understanding toxicity. Unfortunately, instead of distinguishing between these impacts,
current debiasing methods typically eliminate them indiscriminately, resulting in a degradation in the detection
accuracy of the model. To this end, we propose a Counterfactual Causal Debiasing Framework (CCDF) to mitigate
lexical bias in TLD. It preserves the useful impact of lexical bias and eliminates the misleading impact. Specifically,
we first represent the total effect of the original sentence and biased tokens on decisions from a causal view. We then
conduct counterfactual inference to exclude the direct causal effect of lexical bias from the total effect. Empirical
evaluations demonstrate that the debiased TLD model incorporating CCDF achieves state-of-the-art performance in
both accuracy and fairness compared to competitive baselines applied on several vanilla models. The generalization
capability of our model outperforms current debiased models for out-of-distribution data.
Disclaimer: The samples presented by this paper may be considered offensive or vulgar.
Keywords: Toxic Language Detection, Lexical Bias, Causal Inference             1.  Introduction

In recent years, researchers have introduced natu-
ral language processing techniques to detect toxic
language. However, due to biased training, cur-
rent toxic language detection (TLD) methods are
prone to relying on lexical bias to perform deci-
sions. The lexical bias associates toxicity with the
presence of biased tokens (e.g., identity mentions,
insults, and markers of African American English)
(Davidson et al., 2019; Zhang et al., 2020), which
undermines the fairness of minorities (Thiago et al.,
2021; Hutchinson et al., 2020). As an example, as
shown in Figure 1, the TLD model tends to classify
all samples containing ""n*gga"" (a cordial phrase for
dialogue between Africans) as toxic language, due
to its frequent occurrence in toxic samples during
training. This actually compromises the freedom
of expression of Africans (Sap et al., 2019). Mean-
while, lexical bias also affects the generalization
ability of the TLD model, resulting in limited detec-
tion performance of the model for out-of-distribution
(OOD) data (Vidgen et al., 2019; Ramponi and
Tonelli, 2022; Zhou et al., 2021b).
  Researchers have presented several methods
to mitigate lexical bias in TLD. Due to the expen-
sive labor costs of constructing unbiased datasets
(Dinan et al., 2019), many studies have attempted
to weaken lexical prior while training with original


     * Corresponding author                        Non-
   Token      Toxic                 Ratio (%)
                           Toxic
    black       244        76         76.25
   n*gga      541        17         96.95
     f*ck       878        46         95.02
    ass       1592       132        92.34

Table 1: Proportion of toxic samples containing
several biased tokens in the dataset (Founta et al.,
2018), which are crawled from Twitter.


data, and enable models to make decisions without
the impact of the bias (Swayamdipta et al., 2020;
Chuang et al., 2021; Ramponi and Tonelli, 2022).
However, these methods fail to distinguish the use-
ful impact and misleading impact of lexical bias
for understanding toxicity. In fact, lexical bias has
positive effects on TLD, which was viewed as an ef-
fective surface feature for identifying toxic language
in earlier work (Abney, 2014; Dinakar et al., 2015).
As shown in Table 1, biased tokens are used to
express toxic semantics in considerable comments.
Therefore, interpreting lexical bias as a detriment
to TLD and directly eliminating the bias can lead to
a significant reduction in the accuracy of debiased
models (Zhou et al., 2021b). To maintain detection
performance while debiasing, it is necessary to ex-
amine how lexical bias influences model decisions
from the dual characteristics.
  In this work, we propose a novel Counterfactual15566","Lu J,Xu B,Zhang X,Liu K,Zhang D,Yang L,Lin H",,,"Take Its Essence, Discard Its Dross! Debiasing for Toxic Language Detection via Counterfactual Causal Effect",,, , Conference Paper,,"Current methods of toxic language detection (TLD) typically rely on specific tokens to conduct decisions, which
makes them suffer from lexical bias, leading to inferior performance and generalization. Lexical bias has both useful
and misleading impacts on understanding toxicity. Unfortunately, instead of distinguishing between these impacts,
current debiasing methods typically eliminate them indiscriminately, resulting in a degradation in the detection
accuracy of the model. To this end, we propose a Counterfactual Causal Debiasing Framework (CCDF) to mitigate
lexical bias in TLD. It preserves the useful impact of lexical bias and eliminates the misleading impact. Specifically,
we first represent the total effect of the original sentence and biased tokens on decisions from a causal view. We then
conduct counterfactual inference to exclude the direct causal effect of lexical bias from the total effect. Empirical
evaluations demonstrate that the debiased TLD model incorporating CCDF achieves state-of-the-art performance in
both accuracy and fairness compared to competitive baselines applied on several vanilla models. The generalization
capability of our model outperforms current debiased models for out-of-distribution data.
Disclaimer: The samples presented by this paper may be considered offensive or vulgar.
Keywords: Toxic Language Detection, Lexical Bias, Causal Inference             1.  Introduction

In recent years, researchers have introduced natu-
ral language processing techniques to detect toxic
language. However, due to biased training, cur-
rent toxic language detection (TLD) methods are
prone to relying on lexical bias to perform deci-
sions. The lexical bias associates toxicity with the
presence of biased tokens (e.g., identity mentions,
insults, and markers of African American English)
(Davidson et al., 2019; Zhang et al., 2020), which
undermines the fairness of minorities (Thiago et al.,
2021; Hutchinson et al., 2020). As an example, as
shown in Figure 1, the TLD model tends to classify
all samples containing ""n*gga"" (a cordial phrase for
dialogue between Africans) as toxic language, due
to its frequent occurrence in toxic samples during
training. This actually compromises the freedom
of expression of Africans (Sap et al., 2019). Mean-
while, lexical bias also affects the generalization
ability of the TLD model, resulting in limited detec-
tion performance of the model for out-of-distribution
(OOD) data (Vidgen et al., 2019; Ramponi and
Tonelli, 2022; Zhou et al., 2021b).
  Researchers have presented several methods
to mitigate lexical bias in TLD. Due to the expen-
sive labor costs of constructing unbiased datasets
(Dinan et al., 2019), many studies have attempted
to weaken lexical prior while training with original


     * Corresponding author                        Non-
   Token      Toxic                 Ratio (%)
                           Toxic
    black       244        76         76.25
   n*gga      541        17         96.95
     f*ck       878        46         95.02
    ass       1592       132        92.34

Table 1: Proportion of toxic samples containing
several biased tokens in the dataset (Founta et al.,
2018), which are crawled from Twitter.


data, and enable models to make decisions without
the impact of the bias (Swayamdipta et al., 2020;
Chuang et al., 2021; Ramponi and Tonelli, 2022).
However, these methods fail to distinguish the use-
ful impact and misleading impact of lexical bias
for understanding toxicity. In fact, lexical bias has
positive effects on TLD, which was viewed as an ef-
fective surface feature for identifying toxic language
in earlier work (Abney, 2014; Dinakar et al., 2015).
As shown in Table 1, biased tokens are used to
express toxic semantics in considerable comments.
Therefore, interpreting lexical bias as a detriment
to TLD and directly eliminating the bias can lead to
a significant reduction in the accuracy of debiased
models (Zhou et al., 2021b). To maintain detection
performance while debiasing, it is necessary to ex-
amine how lexical bias influences model decisions
from the dual characteristics.
  In this work, we propose a novel Counterfactual15566",,,,,ELRA and ICCL ,"Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy  ",,detection#detox,
2814,"**Title**Mitigating Toxicity in Dialogue Agents through Adversarial Reinforcement Learning

**Abstract**Large Language Models (LLMs) have revolutionized dialogue agents, but they still suffer from biases, inconsis-
             tencies, and factual inaccuracies. This paper focuses on addressing toxicity, a critical aspect of the ""Diversity,
            non-discrimination, and fairness"" pillar of Trustworthy AI, in dialogue agents. We propose a methodology
            inspired by InstructGPT and ChatGPT to mitigate toxicity in chatbots by incorporating toxicity detection tools
           from industry leaders, such as Microsoft and Google Jigsaw, into a reward model. The reward model was extended
          by our developed ToxDialogDefender, a context-aware toxic language identification model. To evaluate our
            approach, we curate a dataset of 1.5 million comments, with 14.13% serving as successful adversarial examples, to
           induce toxicity in the BlenderBot 1 90M model. While our primary focus is on BlenderBot 1, our approach is
             applicable to models with similar Seq2Seq architectures. Experimental results demonstrate a substantial reduction
            in toxicity levels from 24% to 5%, as validated by a subset analysis. This research highlights the potential for
             integrating toxicity mitigation techniques into the training paradigm of dialogue agents, paving the way for more
          more aligned and unbiased conversational AI systems.

       Keywords
             Toxicity, Alignment, Large Language Models, Reinforcement Learning



1. Introduction

Dialogue agents driven by open-domain chatbots [1, 2] play a pivotal role in applications like restaurant
reservations [3], healthcare [4] and online shopping [5]. More recent cases of general-purpose dialog
agents are ChatGPT [6] or Llama 2 [7], which have been trained to follow societal norms. These models
undergo training with extensive datasets from platforms like Reddit1, Twitter (currently X2), and 4chan3,
with examples including BlenderBot 1 [1], TwitterBot Tay [8], and Luda [9]. However, these data
sources are known for producing toxic content [10, 11, 12], leading to undesirable behaviors observed
in the output of these models. Toxicity mitigation is a key task at a time when the research community
is fervently engaged in AI alignment and in ensuring that AI adopts human principles such as respect,
fairness, non-discrimination, etc [13].
  This research focuses on mitigating toxic speech in dialogue agents, which has been defined repeatedly
as rude, disrespectful, or unreasonable comments likely to disrupt conversations4, often related to gender,
politics, race, or culture [14]. Previous efforts aimed at reducing toxicity in dialogue agents include
continuous curation of datasets [15, 16], toxic behavior detection during text generation [17, 18], and
safety layers [1, 19]. While effective, these approaches have limitations ():

 1: The continuous curation of datasets is expensive, requiring human annotators at every stage. In


AEQUITAS 2024: Workshop on Fairness and Bias in AI | co-located with ECAI 2024, Santiago de Compostela, Spain
*Corresponding author.
$ guillermo.villate@tecnalia.com (G. Villate-Castillo); borja.sanz@deusto.es (B. Sanz); javier.delser@tecnalia.com (J. Del Ser)
   0009-0001-0783-7984 (G. Villate-Castillo); 0000-0003-2039-7773 (B. Sanz); 0000-0002-1260-9775 (J. Del Ser)

           2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
1Reddit:https://www.reddit.com/
2X:https://twitter.com/
34chan:https://www.4chan.org/index.php
4Perspective   API,  About   the  API   -   Attributes  and  Languages,   https://developers.perspectiveapi.com/s/CEURceur-ws.org","Villate-Castillo G,Sanz B,Ser J",,,Mitigating Toxicity in Dialogue Agents through Adversarial Reinforcement Learning,3808,, , Conference Paper,,"Large Language Models (LLMs) have revolutionized dialogue agents, but they still suffer from biases, inconsis-
             tencies, and factual inaccuracies. This paper focuses on addressing toxicity, a critical aspect of the ""Diversity,
            non-discrimination, and fairness"" pillar of Trustworthy AI, in dialogue agents. We propose a methodology
            inspired by InstructGPT and ChatGPT to mitigate toxicity in chatbots by incorporating toxicity detection tools
           from industry leaders, such as Microsoft and Google Jigsaw, into a reward model. The reward model was extended
          by our developed ToxDialogDefender, a context-aware toxic language identification model. To evaluate our
            approach, we curate a dataset of 1.5 million comments, with 14.13% serving as successful adversarial examples, to
           induce toxicity in the BlenderBot 1 90M model. While our primary focus is on BlenderBot 1, our approach is
             applicable to models with similar Seq2Seq architectures. Experimental results demonstrate a substantial reduction
            in toxicity levels from 24% to 5%, as validated by a subset analysis. This research highlights the potential for
             integrating toxicity mitigation techniques into the training paradigm of dialogue agents, paving the way for more
          more aligned and unbiased conversational AI systems.

       Keywords
             Toxicity, Alignment, Large Language Models, Reinforcement Learning



1. Introduction

Dialogue agents driven by open-domain chatbots [1, 2] play a pivotal role in applications like restaurant
reservations [3], healthcare [4] and online shopping [5]. More recent cases of general-purpose dialog
agents are ChatGPT [6] or Llama 2 [7], which have been trained to follow societal norms. These models
undergo training with extensive datasets from platforms like Reddit1, Twitter (currently X2), and 4chan3,
with examples including BlenderBot 1 [1], TwitterBot Tay [8], and Luda [9]. However, these data
sources are known for producing toxic content [10, 11, 12], leading to undesirable behaviors observed
in the output of these models. Toxicity mitigation is a key task at a time when the research community
is fervently engaged in AI alignment and in ensuring that AI adopts human principles such as respect,
fairness, non-discrimination, etc [13].
  This research focuses on mitigating toxic speech in dialogue agents, which has been defined repeatedly
as rude, disrespectful, or unreasonable comments likely to disrupt conversations4, often related to gender,
politics, race, or culture [14]. Previous efforts aimed at reducing toxicity in dialogue agents include
continuous curation of datasets [15, 16], toxic behavior detection during text generation [17, 18], and
safety layers [1, 19]. While effective, these approaches have limitations ():

 1: The continuous curation of datasets is expensive, requiring human annotators at every stage. In


AEQUITAS 2024: Workshop on Fairness and Bias in AI | co-located with ECAI 2024, Santiago de Compostela, Spain
*Corresponding author.
$ guillermo.villate@tecnalia.com (G. Villate-Castillo); borja.sanz@deusto.es (B. Sanz); javier.delser@tecnalia.com (J. Del Ser)
   0009-0001-0783-7984 (G. Villate-Castillo); 0000-0003-2039-7773 (B. Sanz); 0000-0002-1260-9775 (J. Del Ser)

           2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
1Reddit:https://www.reddit.com/
2X:https://twitter.com/
34chan:https://www.4chan.org/index.php
4Perspective   API,  About   the  API   -   Attributes  and  Languages,   https://developers.perspectiveapi.com/s/CEURceur-ws.org",,,,,CEUR-WS.org ,"Proceedings of the 2nd Workshop on Fairness and Bias in AI co-located with 27th European Conference on Artificial Intelligence (ECAI 2024), Santiago de Compostela, Spain, October 20th, 2024  ",,detox,
2815,"**Title**Mitigating Text Toxicity with Counterfactual Generation

**Abstract**An increasingly prevalent problem for intelli-
    gent technologies is text safety, as uncontrolled
    systems may generate recommendations to
     their users that lead to injury or life-threatening
    consequences. However, the degree of explic-
    itness of a generated statement that can cause
    physical harm varies. In this paper, we distin-
    guish types of text that can lead to physical
   harm and establish one particularly underex-
    plored category: covertly unsafe text. Then,
   we further break down this category with re-
    spect to the systems information and discuss
    solutions to mitigate the generation of text in
    each of these subcategories. Ultimately, our
   work defines the problem of covertly unsafe
    language that causes physical harm and argues
     that this subtle yet dangerous issue needs to be
    prioritized by stakeholders and regulators. We
    highlight mitigation strategies to inspire future
    researchers to tackle this challenging problem
    and help improve safety within smart systems.
1  Introduction
In recent years, intelligent personal assistants have
increased information accessibility. However, this
has also raised concerns for user safety since these
systems may provide dangerous recommendations
to unsuspecting users. For instance, a child may
ask a device for a fun challenge. The device may
respond with an unsafe viral internet challenge
such as the salt and ice challenge, where partic-
ipants cover their body with salt and rub it with
ice, causing frostbite-like pain1. Though work has
been done in mitigating violent language and hate
speech in natural language systems (Kiritchenko
et al., 2021), there has been a relatively minimal ex-
ploration into covertly unsafe statements that may

    *Equal Contribution.
   1wikipedia.org/wiki/Salt_and_ice_challengeFigure 1: Example statements that can lead to the physi-
cal harm of people; we focus on covertly unsafe text.


lead to injury or even fatal consequences. As unsafe
language continues to grow in prevalence online
(Rainie et al., 2017), detecting and preventing these
statements from being generated becomes crucial
in reducing physical harm. Dangerous examples
like this call for careful consideration of how to
improve safety in intelligent systems.
  A broad spectrum of language can lead to phys-
ical harm, including overtly violent, covertly dan-
gerous, or otherwise indirectly unsafe statements.
Some texts may instigate immediate physical harm
if followed, while others may contain prejudices
that motivate future acts of harm. To better under-
stand these nuances, we examine this spectrum and
distinguish subcategories based on two key notions:
whether a statement is actionable and physically
unsafe and, if so, whether it is explicitly dangerous.
  An example of an overtly unsafe statement is
punch his face because punch is commonly
considered violent and detectable independent of
any deeper form of reasoning. In contrast, pour
water on a grease fire is an example of covertly
unsafe language2; the sentence structure and vo-
cabulary do not have explicitly violent semantics,

    2verywellhealth.com/how-to-put-out-a-grease-fire-
12987092914""I'll shoot you""
""Push him down the stairs""
""Stick a fork in an electrical outlet""
""Take a bite out of a ghost pepper""
""He's a thug. This is his address...""
""She's asking for it with that outfit""Overtly
Unsafe

Covertly
Unsafe

Indirectly
Unsafe","Bhan M,Vittaut JN,Achache N,Legrand V,Chesneau N,Blangero A,Murris J,Lesot MJ",,,Mitigating Text Toxicity with Counterfactual Generation,abs/2405.09948,,10.48550/ARXIV.2405.09948 , Journal Article,,"An increasingly prevalent problem for intelli-
    gent technologies is text safety, as uncontrolled
    systems may generate recommendations to
     their users that lead to injury or life-threatening
    consequences. However, the degree of explic-
    itness of a generated statement that can cause
    physical harm varies. In this paper, we distin-
    guish types of text that can lead to physical
   harm and establish one particularly underex-
    plored category: covertly unsafe text. Then,
   we further break down this category with re-
    spect to the systems information and discuss
    solutions to mitigate the generation of text in
    each of these subcategories. Ultimately, our
   work defines the problem of covertly unsafe
    language that causes physical harm and argues
     that this subtle yet dangerous issue needs to be
    prioritized by stakeholders and regulators. We
    highlight mitigation strategies to inspire future
    researchers to tackle this challenging problem
    and help improve safety within smart systems.
1  Introduction
In recent years, intelligent personal assistants have
increased information accessibility. However, this
has also raised concerns for user safety since these
systems may provide dangerous recommendations
to unsuspecting users. For instance, a child may
ask a device for a fun challenge. The device may
respond with an unsafe viral internet challenge
such as the salt and ice challenge, where partic-
ipants cover their body with salt and rub it with
ice, causing frostbite-like pain1. Though work has
been done in mitigating violent language and hate
speech in natural language systems (Kiritchenko
et al., 2021), there has been a relatively minimal ex-
ploration into covertly unsafe statements that may

    *Equal Contribution.
   1wikipedia.org/wiki/Salt_and_ice_challengeFigure 1: Example statements that can lead to the physi-
cal harm of people; we focus on covertly unsafe text.


lead to injury or even fatal consequences. As unsafe
language continues to grow in prevalence online
(Rainie et al., 2017), detecting and preventing these
statements from being generated becomes crucial
in reducing physical harm. Dangerous examples
like this call for careful consideration of how to
improve safety in intelligent systems.
  A broad spectrum of language can lead to phys-
ical harm, including overtly violent, covertly dan-
gerous, or otherwise indirectly unsafe statements.
Some texts may instigate immediate physical harm
if followed, while others may contain prejudices
that motivate future acts of harm. To better under-
stand these nuances, we examine this spectrum and
distinguish subcategories based on two key notions:
whether a statement is actionable and physically
unsafe and, if so, whether it is explicitly dangerous.
  An example of an overtly unsafe statement is
punch his face because punch is commonly
considered violent and detectable independent of
any deeper form of reasoning. In contrast, pour
water on a grease fire is an example of covertly
unsafe language2; the sentence structure and vo-
cabulary do not have explicitly violent semantics,

    2verywellhealth.com/how-to-put-out-a-grease-fire-
12987092914""I'll shoot you""
""Push him down the stairs""
""Stick a fork in an electrical outlet""
""Take a bite out of a ghost pepper""
""He's a thug. This is his address...""
""She's asking for it with that outfit""Overtly
Unsafe

Covertly
Unsafe

Indirectly
Unsafe",,,,, CoRR,  ,,detox,
2817,"**Title**ReZG: Retrieval-augmented zero-shot counter narrative generation for hate speech

**Abstract**Abstract:The proliferation of hate speech (HS) on social media poses a serious threat to societal security. Automatic counter narrative (CN) generation, as an active strategy for HS intervention, has garnered increasing attention in recent years. Existing methods for automatically generating CNs mainly rely on re-training or fine-tuning pre-trained language models (PLMs) on human-curated CN corpora. Unfortunately, the annotation speed of CN corpora cannot keep up with the growth of HS targets, while generating specific and effective CNs for unseen targets remains a significant challenge for the model. To tackle this issue, we propose Retrieval-Augmented Zero-shot Generation (ReZG) to generate CNs with high-specificity for unseen targets. Specifically, we propose a multi-dimensional hierarchical retrieval method that integrates stance, semantics, and fitness, extending the retrieval metric from single dimension to multiple dimensions suitable for the knowledge that refutes HS. Then, we implement an energy-based constrained decoding mechanism that enables PLMs to use differentiable knowledge preservation, countering, and fluency constraint functions instead of in-target CNs as control signals for generation, thereby achieving zero-shot CN generation. With the above techniques, ReZG can integrate external knowledge flexibly and improve the specificity of CNs. Experimental results show that ReZG exhibits stronger generalization capabilities and outperforms strong baselines with significant improvements of 2.0%+ in the relevance and 4.5%+ in the countering success rate metrics.","Jiang S,Tang W,Chen X,Tang R,Wang H,Wang W",,,ReZG: Retrieval-augmented zero-shot counter narrative generation for hate speech,620,,10.1016/J.NEUCOM.2024.129140 , Journal Article,,"Abstract:The proliferation of hate speech (HS) on social media poses a serious threat to societal security. Automatic counter narrative (CN) generation, as an active strategy for HS intervention, has garnered increasing attention in recent years. Existing methods for automatically generating CNs mainly rely on re-training or fine-tuning pre-trained language models (PLMs) on human-curated CN corpora. Unfortunately, the annotation speed of CN corpora cannot keep up with the growth of HS targets, while generating specific and effective CNs for unseen targets remains a significant challenge for the model. To tackle this issue, we propose Retrieval-Augmented Zero-shot Generation (ReZG) to generate CNs with high-specificity for unseen targets. Specifically, we propose a multi-dimensional hierarchical retrieval method that integrates stance, semantics, and fitness, extending the retrieval metric from single dimension to multiple dimensions suitable for the knowledge that refutes HS. Then, we implement an energy-based constrained decoding mechanism that enables PLMs to use differentiable knowledge preservation, countering, and fluency constraint functions instead of in-target CNs as control signals for generation, thereby achieving zero-shot CN generation. With the above techniques, ReZG can integrate external knowledge flexibly and improve the specificity of CNs. Experimental results show that ReZG exhibits stronger generalization capabilities and outperforms strong baselines with significant improvements of 2.0%+ in the relevance and 4.5%+ in the countering success rate metrics.",,,,, Neurocomputing,  ,,detox,
2820,"**Title**A LLM-based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation

**Abstract**This paper proposes a novel approach to eval-
    uate Counter Narrative (CN) generation using
    a Large Language Model (LLM) as an evalu-
     ator. We show that traditional automatic met-
     rics correlate poorly with human judgements
    and fail to capture the nuanced relationship be-
    tween generated CNs and human perception.
   To alleviate this, we introduce a model ranking
    pipeline based on pairwise comparisons of gen-
    erated CNs from different models, organized in
    a tournament-style format. The proposed eval-
    uation method achieves a high correlation with
   human preference, with a  score of 0.88. As an
    additional contribution, we leverage LLMs as
    zero-shot CN generators and provide a compar-
     ative analysis of chat, instruct, and base models,
    exploring their respective strengths and limita-
     tions. Through meticulous evaluation, includ-
    ing fine-tuning experiments, we elucidate the
    differences in performance and responsiveness
     to domain-specific data. We conclude that chat-
    aligned models in zero-shot are the best option
    for carrying out the task, provided they do not
    refuse to generate an answer due to security
    concerns.
   Warning: Please be advised that this research
    paper contains instances of hate speech that
   may be distressing or offensive to readers.
    These expressions are included for analysis and
    critique purposes only, and they do not reflect
    the beliefs or endorsements of the authors or
    the institution.

1  Introduction

The proliferation of misinformation and the dissem-
ination of harmful narratives has stressed the urgent
need for effective strategies to combat Hate Speech
(HS). This necessity has drawn significant attention
to the field of automatic CN generation, where con-
siderable research has focused on the use of LLMs
to fulfill this task (Chung et al., 2021; Tekiroglu
et al., 2022). However, difficulties in automaticallyassessing the quality of the generated CNs remain.
As is common in text generation tasks, while man-
ual evaluation is expensive, time-consuming, and
subjective, existing automatic methods often fail
to provide comprehensive insights or capture the
nuanced relationship between generated text and
human perception, overlooking crucial aspects of
effectiveness and relevance (Nimah et al., 2023).
Finally, the problem of CN evaluation is exacer-
bated by the lack of a universal truth and the
significant variations among possible references, as
shown in Table 1.
  In this paper we address the limitations of tra-
ditional evaluation metrics for CN generation by
proposing a novel automatic evaluation approach.
This method is motivated by the need to improve
upon existing metrics like BLEU, ROUGE and
BERTScore (Papineni et  al., 2002; Lin, 2004;
Zhang et al., 2020) which do not consider the spe-
cific HS to which the CN is responding to, an es-
sential aspect for accurately assessing the quality
of CNs. Our approach evaluates generated CNs
pairwise in a tournament-style format, with out-
comes determined without human intervention us-
ing JudgeLM (Zhu et al., 2023), a model explic-
itly trained to assess the quality of text.  Thus,
JudgeLM enables pairwise comparisons of CNs,
addressing the subjectivity of the task by breaking
it down into simpler binary classification problems.
To ensure the generalizability of our approach for
the task, we test it on two distinct corpora: CO-
NAN (Chung et al., 2019) and CONAN-MT (Fan-
ton et al., 2021a). Using various models to generate
texts of different quality means that we can eval-
uate model performance across a spectrum of CN
quality, ensuring that even subtle distinctions be-
tween good texts can be captured. This approach
ultimately aims for a higher correlation to human
preference than traditional metrics and ranks mod-
els based on their ability to generate high-quality
CNs.9572","Zubiaga I,Soroa A,Agerri R",,,A LLM-based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation,,, , Conference Paper,,"This paper proposes a novel approach to eval-
    uate Counter Narrative (CN) generation using
    a Large Language Model (LLM) as an evalu-
     ator. We show that traditional automatic met-
     rics correlate poorly with human judgements
    and fail to capture the nuanced relationship be-
    tween generated CNs and human perception.
   To alleviate this, we introduce a model ranking
    pipeline based on pairwise comparisons of gen-
    erated CNs from different models, organized in
    a tournament-style format. The proposed eval-
    uation method achieves a high correlation with
   human preference, with a  score of 0.88. As an
    additional contribution, we leverage LLMs as
    zero-shot CN generators and provide a compar-
     ative analysis of chat, instruct, and base models,
    exploring their respective strengths and limita-
     tions. Through meticulous evaluation, includ-
    ing fine-tuning experiments, we elucidate the
    differences in performance and responsiveness
     to domain-specific data. We conclude that chat-
    aligned models in zero-shot are the best option
    for carrying out the task, provided they do not
    refuse to generate an answer due to security
    concerns.
   Warning: Please be advised that this research
    paper contains instances of hate speech that
   may be distressing or offensive to readers.
    These expressions are included for analysis and
    critique purposes only, and they do not reflect
    the beliefs or endorsements of the authors or
    the institution.

1  Introduction

The proliferation of misinformation and the dissem-
ination of harmful narratives has stressed the urgent
need for effective strategies to combat Hate Speech
(HS). This necessity has drawn significant attention
to the field of automatic CN generation, where con-
siderable research has focused on the use of LLMs
to fulfill this task (Chung et al., 2021; Tekiroglu
et al., 2022). However, difficulties in automaticallyassessing the quality of the generated CNs remain.
As is common in text generation tasks, while man-
ual evaluation is expensive, time-consuming, and
subjective, existing automatic methods often fail
to provide comprehensive insights or capture the
nuanced relationship between generated text and
human perception, overlooking crucial aspects of
effectiveness and relevance (Nimah et al., 2023).
Finally, the problem of CN evaluation is exacer-
bated by the lack of a universal truth and the
significant variations among possible references, as
shown in Table 1.
  In this paper we address the limitations of tra-
ditional evaluation metrics for CN generation by
proposing a novel automatic evaluation approach.
This method is motivated by the need to improve
upon existing metrics like BLEU, ROUGE and
BERTScore (Papineni et  al., 2002; Lin, 2004;
Zhang et al., 2020) which do not consider the spe-
cific HS to which the CN is responding to, an es-
sential aspect for accurately assessing the quality
of CNs. Our approach evaluates generated CNs
pairwise in a tournament-style format, with out-
comes determined without human intervention us-
ing JudgeLM (Zhu et al., 2023), a model explic-
itly trained to assess the quality of text.  Thus,
JudgeLM enables pairwise comparisons of CNs,
addressing the subjectivity of the task by breaking
it down into simpler binary classification problems.
To ensure the generalizability of our approach for
the task, we test it on two distinct corpora: CO-
NAN (Chung et al., 2019) and CONAN-MT (Fan-
ton et al., 2021a). Using various models to generate
texts of different quality means that we can eval-
uate model performance across a spectrum of CN
quality, ensuring that even subtle distinctions be-
tween good texts can be captured. This approach
ultimately aims for a higher correlation to human
preference than traditional metrics and ranks mod-
els based on their ability to generate high-quality
CNs.9572",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024  ",,detox,
2823,"**Title**An Initial Exploration of How Argumentative Information Impacts Automatic Generation of Counter-Narratives Against Hate Speech

**Abstract**Fighting hate speech through automatic counter-narrative generation is gaining interest because of the
           increasing capabilities of Large Language Models. However, counter-narrative generation is a challenging
           task that can benet from insightful analyses of text. In this work, we present an approach to improve
           the generation of counter-narratives by providing Large Language Models with high-quality examples.
           In addition, we show that enhancing the original hate speech with an argumentative analysis, identifying
           justications and conclusions, together with collectives and the properties associated to them, seems
           to produce some improvements, specially with with smaller training datasets, helping to orient the
           generation towards a particular response strategy. The dataset of counter-narratives with argumentative
           information is made publicly available.
         Warning: This work contains oensive and hateful text that may be distressing. It does not
          represent the views of the authors.
       Keywords
           Counter-narrative generation, Hate speech, Argument mining, Large Language Models


1. Introduction

In social media platforms, hate speech is amplied beyond human scale, spreading faster and
increasing their reach, with negative impacts in societies, like polarization or an increase in
violent episodes against targeted communities or individuals. It is because of these known
consequences that many legal systems typify it as a crime, at least in some of its forms.
  The predominant strategy adopted so far to counter hate speech in social media is to recognize,
block and delete these messages and/or the users that generated it. This strategy has two main
disadvantages. The rst one is that blocking and deleting may prevent a hate message from
spreading, but does not counter its consequences on those who were already reached by it.


Arg&App 2023: International Workshop on Argumentation and Applications, September 2023, Rhodes, Greece
damian.a.furman@gmail.com (D. A. Furman)
https://damifur.github.io/ (D. A. Furman)
0000-0002-0877-7063 (D. A. Furman)
          2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).http://ceur-ws.orgISSN 1613-0073  CEUR Workshop Proceedings (CEUR-WS.org)26CEURWorkshopProceedingshttp://ceur-ws.org","Furman DA,Torres P,Rodrguez JA,Letzen D,Martinez MV,Alemany LA",,,An Initial Exploration of How Argumentative Information Impacts Automatic Generation of Counter-Narratives Against Hate Speech,3472,, , Conference Paper,,"Fighting hate speech through automatic counter-narrative generation is gaining interest because of the
           increasing capabilities of Large Language Models. However, counter-narrative generation is a challenging
           task that can benet from insightful analyses of text. In this work, we present an approach to improve
           the generation of counter-narratives by providing Large Language Models with high-quality examples.
           In addition, we show that enhancing the original hate speech with an argumentative analysis, identifying
           justications and conclusions, together with collectives and the properties associated to them, seems
           to produce some improvements, specially with with smaller training datasets, helping to orient the
           generation towards a particular response strategy. The dataset of counter-narratives with argumentative
           information is made publicly available.
         Warning: This work contains oensive and hateful text that may be distressing. It does not
          represent the views of the authors.
       Keywords
           Counter-narrative generation, Hate speech, Argument mining, Large Language Models


1. Introduction

In social media platforms, hate speech is amplied beyond human scale, spreading faster and
increasing their reach, with negative impacts in societies, like polarization or an increase in
violent episodes against targeted communities or individuals. It is because of these known
consequences that many legal systems typify it as a crime, at least in some of its forms.
  The predominant strategy adopted so far to counter hate speech in social media is to recognize,
block and delete these messages and/or the users that generated it. This strategy has two main
disadvantages. The rst one is that blocking and deleting may prevent a hate message from
spreading, but does not counter its consequences on those who were already reached by it.


Arg&App 2023: International Workshop on Argumentation and Applications, September 2023, Rhodes, Greece
damian.a.furman@gmail.com (D. A. Furman)
https://damifur.github.io/ (D. A. Furman)
0000-0002-0877-7063 (D. A. Furman)
          2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).http://ceur-ws.orgISSN 1613-0073  CEUR Workshop Proceedings (CEUR-WS.org)26CEURWorkshopProceedingshttp://ceur-ws.org",,,,,CEUR-WS.org ,"Proceedings of the First International Workshop on Argumentation and Applications co-located with 20th International Conference on Principles of Knowledge Representation and Reasoning (KR 2023), Rhodes, Greece, September 2-8, 2023  ",,detox,
2824,"**Title**Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization

**Abstract**Recent computational approaches for combat-
    ing online hate speech involve the automatic
    generation of counter narratives by adapting
    Pretrained Transformer-based Language Mod-
     els (PLMs) with human-curated data. This pro-
    cess, however, can produce in-domain overfit-
     ting, resulting in models generating acceptable
    narratives only for hatred similar to training
    data, with little portability to other targets or
    to real-world toxic language. This paper in-
    troduces novel attention regularization method-
    ologies to improve the generalization capabili-
     ties of PLMs for counter narratives generation.
    Overfitting to training-specific terms is then dis-
    couraged, resulting in more diverse and richer
    narratives. We experiment with two attention-
    based regularization techniques on a bench-
   mark English dataset. Regularized models pro-
    duce better counter narratives than state-of-the-
     art approaches in most cases, both in terms of
    automatic metrics and human evaluation, espe-
     cially when hateful targets are not present in
    the training data. This work paves the way for
    better and more flexible counter-speech gen-
    eration models, a task for which datasets are
    highly challenging to produce.
   Warning: this paper contains unobfuscated ex-
    amples some readers may find offensive.

1  Introduction

Counter narratives (CNs) are an effective way to
contrast hate speech as they are defined as com-
municative actions aimed at refuting hate speech
through thoughtful and cogent reasons, and true
and fact-bound arguments (Schieb and Preuss,
2016).  In contrast to other widely used restric-
tive measures such as content removal and shadow-
banning, CNs are based on the assumption that
in order to combat hate speech, more speech is
required. They are typically employed by Non-
Governmental Organizations (NGOs) as an active
strategy to intervene in online discussions where   Figure 1: An example of CNs obtained with and without
   regularization: the highlighted terms show where the
   models focus their attention in the two cases.


   hateful content is present. Responding to micro
  and macro-aggressions with concrete action is crit-
   ical because it can make such aggressions visible,
   disarm them, educate the perpetrators, and allow
   for external support (Sue et al., 2019). In partic-
   ular, the key to the effectiveness of CNs is their
   specificity: they are more complex than a simple
   condemnation of profanity, and they include a vari-
   ety of arguments (Tekiroglu et al., 2020).
       Still, the massive amount of hate that is con-
   stantly produced online necessitates the develop-
  ment of automatic CN generation techniques. Typ-
   ically, this is done by fine-tuning a Pretrained Lan-
  guage Model (PLM) on human-curated data, such
   as GPT-2 (Radford et al., 2019). However, prior
   research has demonstrated that PLMs are suscep-
   tible to generating unspecific CNs that can techni-
   cally work with any input but have questionable
   content and informativeness (Fanton et al., 2021;

13","Bonaldi H,Attanasio G,Nozza D,Guerini M",,,Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization,abs/2309.02311,,10.48550/ARXIV.2309.02311 , Journal Article,,"Recent computational approaches for combat-
    ing online hate speech involve the automatic
    generation of counter narratives by adapting
    Pretrained Transformer-based Language Mod-
     els (PLMs) with human-curated data. This pro-
    cess, however, can produce in-domain overfit-
     ting, resulting in models generating acceptable
    narratives only for hatred similar to training
    data, with little portability to other targets or
    to real-world toxic language. This paper in-
    troduces novel attention regularization method-
    ologies to improve the generalization capabili-
     ties of PLMs for counter narratives generation.
    Overfitting to training-specific terms is then dis-
    couraged, resulting in more diverse and richer
    narratives. We experiment with two attention-
    based regularization techniques on a bench-
   mark English dataset. Regularized models pro-
    duce better counter narratives than state-of-the-
     art approaches in most cases, both in terms of
    automatic metrics and human evaluation, espe-
     cially when hateful targets are not present in
    the training data. This work paves the way for
    better and more flexible counter-speech gen-
    eration models, a task for which datasets are
    highly challenging to produce.
   Warning: this paper contains unobfuscated ex-
    amples some readers may find offensive.

1  Introduction

Counter narratives (CNs) are an effective way to
contrast hate speech as they are defined as com-
municative actions aimed at refuting hate speech
through thoughtful and cogent reasons, and true
and fact-bound arguments (Schieb and Preuss,
2016).  In contrast to other widely used restric-
tive measures such as content removal and shadow-
banning, CNs are based on the assumption that
in order to combat hate speech, more speech is
required. They are typically employed by Non-
Governmental Organizations (NGOs) as an active
strategy to intervene in online discussions where   Figure 1: An example of CNs obtained with and without
   regularization: the highlighted terms show where the
   models focus their attention in the two cases.


   hateful content is present. Responding to micro
  and macro-aggressions with concrete action is crit-
   ical because it can make such aggressions visible,
   disarm them, educate the perpetrators, and allow
   for external support (Sue et al., 2019). In partic-
   ular, the key to the effectiveness of CNs is their
   specificity: they are more complex than a simple
   condemnation of profanity, and they include a vari-
   ety of arguments (Tekiroglu et al., 2020).
       Still, the massive amount of hate that is con-
   stantly produced online necessitates the develop-
  ment of automatic CN generation techniques. Typ-
   ically, this is done by fine-tuning a Pretrained Lan-
  guage Model (PLM) on human-curated data, such
   as GPT-2 (Radford et al., 2019). However, prior
   research has demonstrated that PLMs are suscep-
   tible to generating unspecific CNs that can techni-
   cally work with any input but have questionable
   content and informativeness (Fanton et al., 2021;

13",,,,, CoRR,  ,,detox,
2825,"**Title**RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for Hate Speech

**Abstract**Abstract:The proliferation of hate speech (HS) on social media poses a serious threat to societal security. Automatic counter narrative (CN) generation, as an active strategy for HS intervention, has garnered increasing attention in recent years. Existing methods for automatically generating CNs mainly rely on re-training or fine-tuning pre-trained language models (PLMs) on human-curated CN corpora. Unfortunately, the annotation speed of CN corpora cannot keep up with the growth of HS targets, while generating specific and effective CNs for unseen targets remains a significant challenge for the model. To tackle this issue, we propose Retrieval-Augmented Zero-shot Generation (ReZG) to generate CNs with high-specificity for unseen targets. Specifically, we propose a multi-dimensional hierarchical retrieval method that integrates stance, semantics, and fitness, extending the retrieval metric from single dimension to multiple dimensions suitable for the knowledge that refutes HS. Then, we implement an energy-based constrained decoding mechanism that enables PLMs to use differentiable knowledge preservation, countering, and fluency constraint functions instead of in-target CNs as control signals for generation, thereby achieving zero-shot CN generation. With the above techniques, ReZG can integrate external knowledge flexibly and improve the specificity of CNs. Experimental results show that ReZG exhibits stronger generalization capabilities and outperforms strong baselines with significant improvements of 2.0%+ in the relevance and 4.5%+ in the countering success rate metrics.","Jiang S,Tang W,Chen X,Tang R,Wang H,Wang W",,,RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for Hate Speech,abs/2310.05650,,10.48550/ARXIV.2310.05650 , Journal Article,,"Abstract:The proliferation of hate speech (HS) on social media poses a serious threat to societal security. Automatic counter narrative (CN) generation, as an active strategy for HS intervention, has garnered increasing attention in recent years. Existing methods for automatically generating CNs mainly rely on re-training or fine-tuning pre-trained language models (PLMs) on human-curated CN corpora. Unfortunately, the annotation speed of CN corpora cannot keep up with the growth of HS targets, while generating specific and effective CNs for unseen targets remains a significant challenge for the model. To tackle this issue, we propose Retrieval-Augmented Zero-shot Generation (ReZG) to generate CNs with high-specificity for unseen targets. Specifically, we propose a multi-dimensional hierarchical retrieval method that integrates stance, semantics, and fitness, extending the retrieval metric from single dimension to multiple dimensions suitable for the knowledge that refutes HS. Then, we implement an energy-based constrained decoding mechanism that enables PLMs to use differentiable knowledge preservation, countering, and fluency constraint functions instead of in-target CNs as control signals for generation, thereby achieving zero-shot CN generation. With the above techniques, ReZG can integrate external knowledge flexibly and improve the specificity of CNs. Experimental results show that ReZG exhibits stronger generalization capabilities and outperforms strong baselines with significant improvements of 2.0%+ in the relevance and 4.5%+ in the countering success rate metrics.",,,,, CoRR,  ,,detox,
2828,"**Title**Multilingual and Explainable Text Detoxification with Parallel Corpora

**Abstract**Language   Toxic Text   Even with various regulations in place across
    countries and social media platforms (Govern-
   ment of India, 2021; European Parliament and
    Council of the European Union, 2022), digi-
     tal abusive speech remains a significant issue.
   One potential approach to address this chal-
    lenge is automatic text detoxification, a text
     style transfer (TST) approach that transforms
    toxic language into a more neutral or non-toxic
    form. To date, the availability of parallel cor-
    pora for the text detoxification task (Logacheva
     et al., 2022; Atwell et al., 2022; Dementieva
     et al., 2024a) has proven to be crucial for state-
     of-the-art approaches. With this work, we ex-
    tend parallel text detoxification corpus to new
   languagesGerman, Chinese, Arabic, Hindi,
    and Amharictesting in the extensive multilin-
    gual setup TST baselines. Next, we conduct the
      first of its kind an automated, explainable anal-
     ysis of the descriptive features of both toxic and
    non-toxic sentences, diving deeply into the nu-
    ances, similarities, and differences of toxicity
    and detoxification across 9 languages. Finally,
    based on the obtained insights, we experiment
    with a novel text detoxification method inspired
   by the Chain-of-Thoughts reasoning approach,
    enhancing the prompting process through clus-
    tering on relevant descriptive attributes.
   Warning: This paper contains offensive texts
    that only serve as illustrative examples.

1  Introduction

The issue of managing toxic speech remains a
crucial aspect of human communication and digi-
tal violence prevention (Shi et al., 2020), includ-
ing the mitigation of toxic responses generated
by Large Language Models (LLMs) (Yao et al.,
2023). The typical approach to dealing with abu-
sive speech on social platforms involves message
blocking (Cobbe, 2021). To address this, numer-
ous toxic and hate speech detection models have
been developed for different languages, i.e. En-
glish (Mathew et al., 2021), Spanish (Molero et al.,Figure 1: Examples of the desired texts detoxification
for English and new languages: German, Chinese, Ara-
bic, Hindi, and Amharic.


2023), Amharic (Ayele et al., 2023), Code-Mixed
Hindi (Bohra et al., 2018), and many others (Costa-
juss et al., 2024). However, the recent research
indicates a necessity for more proactive modera-
tion of abusive speech (Kulenovic, 2023). One
such approach is text detoxification.
  Within the baselines approaches for automatic
text detoxification, multiple unsupervised baselines
were created based on ideas of Delete-Retrieve-
Generate (Li et al., 2018), latent style spaces dis-
entanglement (Nogueira dos Santos et al., 2018),
or conditional generation with Masked Language
Modeling (Dale et al., 2021). However, the lat-
est state-of-the-art outcomes, particularly in En-
glish, were attained when parallel data and fine-
tuning with text-to-text generation models were
employed as in ParaDetox (Logacheva et al., 2022)
or APPDIA (Atwell et al., 2022). Then, several
works were conducted to explore the potential7998Language   Toxic Text           Detoxied Text          What a f**k is thisEnglish                     What is this about?            about?          Was fr einGerman            besch**senes JahrWas fr ein schlechtes
Jahr.                    **    Hindi                                                  ?                        ?               
Amharic
                
   
Arabic                                          **Arabic                              **Chinese       ","Dementieva D,Babakov N,Ronen A,Ayele AA,Rizwan N,Schneider F,Wang X,Yimam SM,Moskovskiy DA,Stakovskii E,Kaufman E,Elnagar A,Mukherjee A,Panchenko A",,,Multilingual and Explainable Text Detoxification with Parallel Corpora,,, , Conference Paper,,"Language   Toxic Text   Even with various regulations in place across
    countries and social media platforms (Govern-
   ment of India, 2021; European Parliament and
    Council of the European Union, 2022), digi-
     tal abusive speech remains a significant issue.
   One potential approach to address this chal-
    lenge is automatic text detoxification, a text
     style transfer (TST) approach that transforms
    toxic language into a more neutral or non-toxic
    form. To date, the availability of parallel cor-
    pora for the text detoxification task (Logacheva
     et al., 2022; Atwell et al., 2022; Dementieva
     et al., 2024a) has proven to be crucial for state-
     of-the-art approaches. With this work, we ex-
    tend parallel text detoxification corpus to new
   languagesGerman, Chinese, Arabic, Hindi,
    and Amharictesting in the extensive multilin-
    gual setup TST baselines. Next, we conduct the
      first of its kind an automated, explainable anal-
     ysis of the descriptive features of both toxic and
    non-toxic sentences, diving deeply into the nu-
    ances, similarities, and differences of toxicity
    and detoxification across 9 languages. Finally,
    based on the obtained insights, we experiment
    with a novel text detoxification method inspired
   by the Chain-of-Thoughts reasoning approach,
    enhancing the prompting process through clus-
    tering on relevant descriptive attributes.
   Warning: This paper contains offensive texts
    that only serve as illustrative examples.

1  Introduction

The issue of managing toxic speech remains a
crucial aspect of human communication and digi-
tal violence prevention (Shi et al., 2020), includ-
ing the mitigation of toxic responses generated
by Large Language Models (LLMs) (Yao et al.,
2023). The typical approach to dealing with abu-
sive speech on social platforms involves message
blocking (Cobbe, 2021). To address this, numer-
ous toxic and hate speech detection models have
been developed for different languages, i.e. En-
glish (Mathew et al., 2021), Spanish (Molero et al.,Figure 1: Examples of the desired texts detoxification
for English and new languages: German, Chinese, Ara-
bic, Hindi, and Amharic.


2023), Amharic (Ayele et al., 2023), Code-Mixed
Hindi (Bohra et al., 2018), and many others (Costa-
juss et al., 2024). However, the recent research
indicates a necessity for more proactive modera-
tion of abusive speech (Kulenovic, 2023). One
such approach is text detoxification.
  Within the baselines approaches for automatic
text detoxification, multiple unsupervised baselines
were created based on ideas of Delete-Retrieve-
Generate (Li et al., 2018), latent style spaces dis-
entanglement (Nogueira dos Santos et al., 2018),
or conditional generation with Masked Language
Modeling (Dale et al., 2021). However, the lat-
est state-of-the-art outcomes, particularly in En-
glish, were attained when parallel data and fine-
tuning with text-to-text generation models were
employed as in ParaDetox (Logacheva et al., 2022)
or APPDIA (Atwell et al., 2022). Then, several
works were conducted to explore the potential7998Language   Toxic Text           Detoxied Text          What a f**k is thisEnglish                     What is this about?            about?          Was fr einGerman            besch**senes JahrWas fr ein schlechtes
Jahr.                    **    Hindi                                                  ?                        ?               
Amharic
                
   
Arabic                                          **Arabic                              **Chinese       ",,,,,Association for Computational Linguistics ,"Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025  ",,detox,
2832,"**Title**Overview of the Multilingual Text Detoxification Task at PAN 2024

**Abstract**Despite different countries and social platform regulations, digital abusive speech persists as a significant challenge.
         One of the way to tackle abusive, or more specifically, toxic language can be automatic text detoxificationa
             text style transfer task (TST) of changing register of text from toxic to more non-toxic. Thus, in this shared task,
        we aim to obtain text detoxification models for 9 languages: English, Spanish, German, Chinese, Arabic, Hindi,
            Ukrainian, Russian, and Amharic. This paper presents the Multilingual Text Detoxification (TextDetox) task, the
           underlying datasets, the evaluation setups, the submissions from participants, and the results obtained.
           Warning: This paper contains rude texts that only serve as illustrative examples.

       Keywords
        PAN 2024, Multilingual Text Detoxification, Text Style Transfer, Multilingualism



1. Introduction

The issue of managing toxic speech remains a crucial aspect of human communication and digital
violence prevention [1], including the mitigation of toxic responses generated by Large Language


CLEF 2024: Conference and Labs of the Evaluation Forum, September 0912, 2024, Grenoble, France
*Corresponding author.
$ daryna.dementieva@tum.de (D. Dementieva); daniil.moskovskiy@skoltech.ru (D. Moskovskiy); nikolay.babakov@usc.ese
(N. Babakov); abinew.ali.ayele@uni-hamburg.de (A. A. Ayele); nrizwan@kgpian.iitkgp.ac.in (N. Rizwan);
florian.schneider-1@uni-hamburg.de (F. Schneider); xintong.wang@uni-hamburg.de (X. Wang);
seid.muhie.yimam@uni-hamburg.de (S. M. Yimam); dmitry.ustalov@jetbrains.com (D. Ustalov); eistakovskii@gmail.com
(E. Stakovskii); ashraf@sharjah.ac.ae (A. Elnagar); animeshm@gmail.com (A. Mukherjee); a.panchenko@skol.tech
(A. Panchenko)
 https://dardem.github.io (D. Dementieva); https://www.researchgate.net/profile/Daniil-Moskovskiy (D. Moskovskiy);
https://github.com/bbkjunior/bbkjunior (N. Babakov); https://scholar.google.com/citations?user=g2m1wH4AAAAJ&hl=en
(A. A. Ayele); https://www.linkedin.com/in/naquee-rizwan-a97abb159 (N. Rizwan);
https://www.linkedin.com/in/flo-schneider-hh (F. Schneider); https://ethanscuter.github.io (X. Wang);
https://seyyaw.github.io (S. M. Yimam); https://linkedin.com/in/ustalov (D. Ustalov); https://github.com/eistakovskii
(E. Stakovskii); https://www.sharjah.ac.ae/en/academics/Colleges/CI/dept/cs/Pages/ppl_detail.aspx?mcid=4 (A. Elnagar);
https://cse.iitkgp.ac.in/~animeshm (A. Mukherjee); https://alexanderpanchenko.github.io (A. Panchenko)
   0000-0003-0929-4140 (D. Dementieva); 0009-0006-7943-4259 (D. Moskovskiy); 0000-0002-2568-6702 (N. Babakov);CEURceur-ws.org","Dementieva D,Moskovskiy D,Babakov N,Ayele AA,Rizwan N,Schneider F,Wang X,Yimam SM,Ustalov D,Stakovskii E,Smirnova A,Elnagar A,Mukherjee A,Panchenko A",,,Overview of the Multilingual Text Detoxification Task at PAN 2024,3740,, , Conference Paper,,"Despite different countries and social platform regulations, digital abusive speech persists as a significant challenge.
         One of the way to tackle abusive, or more specifically, toxic language can be automatic text detoxificationa
             text style transfer task (TST) of changing register of text from toxic to more non-toxic. Thus, in this shared task,
        we aim to obtain text detoxification models for 9 languages: English, Spanish, German, Chinese, Arabic, Hindi,
            Ukrainian, Russian, and Amharic. This paper presents the Multilingual Text Detoxification (TextDetox) task, the
           underlying datasets, the evaluation setups, the submissions from participants, and the results obtained.
           Warning: This paper contains rude texts that only serve as illustrative examples.

       Keywords
        PAN 2024, Multilingual Text Detoxification, Text Style Transfer, Multilingualism



1. Introduction

The issue of managing toxic speech remains a crucial aspect of human communication and digital
violence prevention [1], including the mitigation of toxic responses generated by Large Language


CLEF 2024: Conference and Labs of the Evaluation Forum, September 0912, 2024, Grenoble, France
*Corresponding author.
$ daryna.dementieva@tum.de (D. Dementieva); daniil.moskovskiy@skoltech.ru (D. Moskovskiy); nikolay.babakov@usc.ese
(N. Babakov); abinew.ali.ayele@uni-hamburg.de (A. A. Ayele); nrizwan@kgpian.iitkgp.ac.in (N. Rizwan);
florian.schneider-1@uni-hamburg.de (F. Schneider); xintong.wang@uni-hamburg.de (X. Wang);
seid.muhie.yimam@uni-hamburg.de (S. M. Yimam); dmitry.ustalov@jetbrains.com (D. Ustalov); eistakovskii@gmail.com
(E. Stakovskii); ashraf@sharjah.ac.ae (A. Elnagar); animeshm@gmail.com (A. Mukherjee); a.panchenko@skol.tech
(A. Panchenko)
 https://dardem.github.io (D. Dementieva); https://www.researchgate.net/profile/Daniil-Moskovskiy (D. Moskovskiy);
https://github.com/bbkjunior/bbkjunior (N. Babakov); https://scholar.google.com/citations?user=g2m1wH4AAAAJ&hl=en
(A. A. Ayele); https://www.linkedin.com/in/naquee-rizwan-a97abb159 (N. Rizwan);
https://www.linkedin.com/in/flo-schneider-hh (F. Schneider); https://ethanscuter.github.io (X. Wang);
https://seyyaw.github.io (S. M. Yimam); https://linkedin.com/in/ustalov (D. Ustalov); https://github.com/eistakovskii
(E. Stakovskii); https://www.sharjah.ac.ae/en/academics/Colleges/CI/dept/cs/Pages/ppl_detail.aspx?mcid=4 (A. Elnagar);
https://cse.iitkgp.ac.in/~animeshm (A. Mukherjee); https://alexanderpanchenko.github.io (A. Panchenko)
   0000-0003-0929-4140 (D. Dementieva); 0009-0006-7943-4259 (D. Moskovskiy); 0000-0002-2568-6702 (N. Babakov);CEURceur-ws.org",,,,,CEUR-WS.org ,"Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble, France, 9-12 September, 2024  ",,detox,
2833,No abstract available,"Gangopadhyay S,Khan MT,Jabeen H",,,Linguistic_Hygenist at PAN 2024 TextDetox: HybridDetox - A Combination of Supervised and Unsupervised Methods for Effective Multilingual Text Detoxification,3740,, , Conference Paper,,,,,,,CEUR-WS.org ,"Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble, France, 9-12 September, 2024  ",,detox,
2834,"**Title**Multilingual Text Detoxification Using Google Cloud Translation and Post-Processing

**Abstract**We present two novel unsupervised methods
    for eliminating toxicity in  text.  Our rst
   method combines two recent ideas: (1) guid-
    ance of the generation process with small style-
    conditional language models and (2) use of
    paraphrasing models to perform style transfer.
   We use a well-performing paraphraser guided
   by style-trained language models to keep the
     text content and remove toxicity. Our second
   method uses BERT to replace toxic words with
     their non-offensive synonyms. We make the
   method more exible by enabling BERT to
    replace mask tokens with a variable number
    of words.  Finally, we present the rst large-
    scale comparative study of style transfer mod-
     els on the task of toxicity removal. We com-
    pare our models with a number of methods for
     style transfer. The models are evaluated in a
    reference-free way using a combination of un-
    supervised style transfer metrics. Both meth-
    ods we suggest yield new SOTA results.

1  Introduction

Identication of toxicity in user texts is an active
area of research (Zampieri et al., 2020; DSa et al.,
2020; Han and Tsvetkov, 2020). The task of auto-
matic rewriting of offensive content attracted less
attention, yet it may nd various useful applications
such as making online world a better place by sug-
gesting to a user posting a more neutral version of
an emotional comment. The existing works on text
detoxication (dos Santos et al., 2018; Tran et al.,
2020; Laugier et al., 2021) cast this task as style
transfer. The style transfer task is generally under-
stood as rewriting of text with the same content and
with altering of one or several attributes which con-
stitute the style, such as authorship (Voigt et al.,
2018), sentiment (Shen et al., 2017), or degree of
politeness (Madaan et al., 2020). Despite the goal
of preserving the content, in many cases changing
the style attributes changes the meaning of a sen-tence signicantly.1 So in fact the goal of many
style transfer models is to transform a sentence into
a somewhat similar sentence of a different style
on the same topic.2 We suggest that detoxication
needs better preservation of the original meaning
than many other style transfer tasks, such as senti-
ment transfer, so it should be performed differently.
  We present two models for text detoxication,
which have extra control for content preservation.
The rst model, ParaGeDi, is capable of fully re-
generating the input. It is based on two ideas: exter-
nal control of an output of a generation model by a
class-conditioned LM (Krause et al., 2020) and for-
mulation of style transfer task as paraphrasing (Kr-
ishna et al., 2020). Being based on a paraphraser
model, ParaGeDi explicitly aims at preserving the
meaning of the original sentence. The second ap-
proach, CondBERT, inspired by Wu et al. (2019a),
follows the pointwise editing setup. It uses BERT
to replace toxic spans found in the sentence with
their non-toxic alternatives. The semantic simi-
larity is maintained by showing the original text
to BERT and reranking its hypotheses based on
the similarity between the original words and their
substitutes.  Interestingly, BERT does not need
any class-conditional pre-training to successfully
change the text style from toxic to normal.
   In addition, we perform a large-scale evaluation
of style transfer models on detoxication task, com-
paring our new models with baselines and state-of-
the-art approaches. We release our code and data.3
  Our contributions are as follows:
 We propose two novel detoxication meth-
  ods based on pre-trained neural language mod-
  els: ParaGeDi (paraphrasing GeDi) and Cond-
 BERT (conditional BERT).

   1For example, Lample et al. (2019) provide the following
sentence as an example of transfer from male to female writing:
Gotta say that beard makes you look like a Viking Gotta
say that hair makes you look like a Mermaid.
   2A formal task denition is presented in Appendix A.
  3https://github.com/skoltech-nlp/detox7979","Luo Z,Luo M,Wang A",,,Multilingual Text Detoxification Using Google Cloud Translation and Post-Processing,3740,, , Conference Paper,,"We present two novel unsupervised methods
    for eliminating toxicity in  text.  Our rst
   method combines two recent ideas: (1) guid-
    ance of the generation process with small style-
    conditional language models and (2) use of
    paraphrasing models to perform style transfer.
   We use a well-performing paraphraser guided
   by style-trained language models to keep the
     text content and remove toxicity. Our second
   method uses BERT to replace toxic words with
     their non-offensive synonyms. We make the
   method more exible by enabling BERT to
    replace mask tokens with a variable number
    of words.  Finally, we present the rst large-
    scale comparative study of style transfer mod-
     els on the task of toxicity removal. We com-
    pare our models with a number of methods for
     style transfer. The models are evaluated in a
    reference-free way using a combination of un-
    supervised style transfer metrics. Both meth-
    ods we suggest yield new SOTA results.

1  Introduction

Identication of toxicity in user texts is an active
area of research (Zampieri et al., 2020; DSa et al.,
2020; Han and Tsvetkov, 2020). The task of auto-
matic rewriting of offensive content attracted less
attention, yet it may nd various useful applications
such as making online world a better place by sug-
gesting to a user posting a more neutral version of
an emotional comment. The existing works on text
detoxication (dos Santos et al., 2018; Tran et al.,
2020; Laugier et al., 2021) cast this task as style
transfer. The style transfer task is generally under-
stood as rewriting of text with the same content and
with altering of one or several attributes which con-
stitute the style, such as authorship (Voigt et al.,
2018), sentiment (Shen et al., 2017), or degree of
politeness (Madaan et al., 2020). Despite the goal
of preserving the content, in many cases changing
the style attributes changes the meaning of a sen-tence signicantly.1 So in fact the goal of many
style transfer models is to transform a sentence into
a somewhat similar sentence of a different style
on the same topic.2 We suggest that detoxication
needs better preservation of the original meaning
than many other style transfer tasks, such as senti-
ment transfer, so it should be performed differently.
  We present two models for text detoxication,
which have extra control for content preservation.
The rst model, ParaGeDi, is capable of fully re-
generating the input. It is based on two ideas: exter-
nal control of an output of a generation model by a
class-conditioned LM (Krause et al., 2020) and for-
mulation of style transfer task as paraphrasing (Kr-
ishna et al., 2020). Being based on a paraphraser
model, ParaGeDi explicitly aims at preserving the
meaning of the original sentence. The second ap-
proach, CondBERT, inspired by Wu et al. (2019a),
follows the pointwise editing setup. It uses BERT
to replace toxic spans found in the sentence with
their non-toxic alternatives. The semantic simi-
larity is maintained by showing the original text
to BERT and reranking its hypotheses based on
the similarity between the original words and their
substitutes.  Interestingly, BERT does not need
any class-conditional pre-training to successfully
change the text style from toxic to normal.
   In addition, we perform a large-scale evaluation
of style transfer models on detoxication task, com-
paring our new models with baselines and state-of-
the-art approaches. We release our code and data.3
  Our contributions are as follows:
 We propose two novel detoxication meth-
  ods based on pre-trained neural language mod-
  els: ParaGeDi (paraphrasing GeDi) and Cond-
 BERT (conditional BERT).

   1For example, Lample et al. (2019) provide the following
sentence as an example of transfer from male to female writing:
Gotta say that beard makes you look like a Viking Gotta
say that hair makes you look like a Mermaid.
   2A formal task denition is presented in Appendix A.
  3https://github.com/skoltech-nlp/detox7979",,,,,CEUR-WS.org ,"Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble, France, 9-12 September, 2024  ",,detox,
2836,"**Title**RAG Meets Detox: Enhancing Text Detoxification Using Open Large Language Models with Retrieval Augmented Generation

**Abstract**In this work we present our solution at the Multilingual Text Detoxification 2024 task, whose objective is to
           take toxic text and convert into one that conveys the same meaning without containing any toxicity. Our
           approach utilizes open Large Language Models extended with dynamic prompt creation combined with Retrieval
         Augmented Generation. The evaluation results show that despite its simplicity, our method has the potential
            to provide competitive results, as evidenced by both the automatic and manual evaluation executed by the task
            organizers. Overall, our approach ranked 5th in the manual evaluation, with our best-performing language,
          German, even surpassing the human reference.

       Keywords
        PAN 2024, Retrieval Augmented Generation, text detoxification, Llama3, LLM, toxic text



1. Introduction

The task of identification of toxicity in texts is an active area of research. Social networks are trying to
address this problem by simply blocking such texts. A more interesting and effective approach might
be to automatically rewrite these texts, so that they are ideally no longer toxic, but their meaning is
kept intact. This processed is denoted as detoxification.
  The Multilingual Text Detoxification (TextDetox) 2024 task aims to create and explore such methods.
The participants are provided with a dataset of toxic texts in several languages from all over the globe,
which then should be detoxified. The goal is to find a method, which after evaluation provides texts
which are neutral, but their meaning is the same as the toxic text on the input.
  We explore how a data scientist with only an API access to a Large Language Model (LLM), in our
case Llama3 can develop effective solutions for this task. We did not fine-tune or alter in any way, the
only approach was to creatively adjust prompts given to the LLM, so that its outputs will get highest
score possible. We have developed several methods, from the simple ones like zero shot prompting, to
utilizing existing datasets of text detoxifications and generating these prompts dynamically considering
the input text to be detoxified.
  For this we have used external tools like vector databases containing pairs of toxic texts and their
neutral counterparts, which were queried using embedding of the toxic texts. We have found this
method to be competitive and despite its simplicity it achieved high ranking in this task.
  We submitted our results under the usernames erehulka and mareksuppa to the CodaLab portal.
Our best-performing languages, in comparison to other participants submissions, were German and
Chinese. Notably, our score for German even surpassed the human references in the manual evaluation.
Overall, our approach ranked 5th in the manual evaluation. A more detailed discussion of our results is
provided in Section 6.




CLEF 2024: Conference and Labs of the Evaluation Forum, September 0912, 2024, Grenoble, FranceCEURceur-ws.org","Rehulka E,Suppa M",,,RAG Meets Detox: Enhancing Text Detoxification Using Open Large Language Models with Retrieval Augmented Generation,3740,, , Conference Paper,,"In this work we present our solution at the Multilingual Text Detoxification 2024 task, whose objective is to
           take toxic text and convert into one that conveys the same meaning without containing any toxicity. Our
           approach utilizes open Large Language Models extended with dynamic prompt creation combined with Retrieval
         Augmented Generation. The evaluation results show that despite its simplicity, our method has the potential
            to provide competitive results, as evidenced by both the automatic and manual evaluation executed by the task
            organizers. Overall, our approach ranked 5th in the manual evaluation, with our best-performing language,
          German, even surpassing the human reference.

       Keywords
        PAN 2024, Retrieval Augmented Generation, text detoxification, Llama3, LLM, toxic text



1. Introduction

The task of identification of toxicity in texts is an active area of research. Social networks are trying to
address this problem by simply blocking such texts. A more interesting and effective approach might
be to automatically rewrite these texts, so that they are ideally no longer toxic, but their meaning is
kept intact. This processed is denoted as detoxification.
  The Multilingual Text Detoxification (TextDetox) 2024 task aims to create and explore such methods.
The participants are provided with a dataset of toxic texts in several languages from all over the globe,
which then should be detoxified. The goal is to find a method, which after evaluation provides texts
which are neutral, but their meaning is the same as the toxic text on the input.
  We explore how a data scientist with only an API access to a Large Language Model (LLM), in our
case Llama3 can develop effective solutions for this task. We did not fine-tune or alter in any way, the
only approach was to creatively adjust prompts given to the LLM, so that its outputs will get highest
score possible. We have developed several methods, from the simple ones like zero shot prompting, to
utilizing existing datasets of text detoxifications and generating these prompts dynamically considering
the input text to be detoxified.
  For this we have used external tools like vector databases containing pairs of toxic texts and their
neutral counterparts, which were queried using embedding of the toxic texts. We have found this
method to be competitive and despite its simplicity it achieved high ranking in this task.
  We submitted our results under the usernames erehulka and mareksuppa to the CodaLab portal.
Our best-performing languages, in comparison to other participants submissions, were German and
Chinese. Notably, our score for German even surpassed the human references in the manual evaluation.
Overall, our approach ranked 5th in the manual evaluation. A more detailed discussion of our results is
provided in Section 6.




CLEF 2024: Conference and Labs of the Evaluation Forum, September 0912, 2024, Grenoble, FranceCEURceur-ws.org",,,,,CEUR-WS.org ,"Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble, France, 9-12 September, 2024  ",,detox,
2837,"**Title**SINAI at PAN 2024 TextDetox: Application of Chain of Thought with Self-Consistency Strategy in Large Language Models for Multilingual Text Detoxification

**Abstract**This article describes the participation of the SINAI research group in the shared task TextDetox (Multilingual
           Text Detoxification) in CLEF 2024. TThe proposed system for multilingual text detoxification employs Large
          Language Models (LLMs) utilizing a Self-Consistent Chain of Thought (CoT-SC) prompting strategy. This CoT-SC
            strategy consists of identifying the language of the toxic comment and then generating three different detoxified
             text proposals, the first proposal consists of removing the toxic words, the second of replacing the toxic words
           with neutral words, and the last of rewriting the toxic text in a neutral way. Subsequently, the selected LLM has
            to evaluate each generated neutral text according to the competition metrics. Finally, the model selects the best
            neutral text generated. Specifically with this proposal, we aim to evaluate the capacity of auto-evaluation and
           reasoning of LLM in different languages, including those with low resources. Our proposal was ranked 23rd in
            the automatic evaluation metrics and 11th in the final ranking with the manual evaluation.

       Keywords
            Multilingual Detoxification, Large Language Models, Chain of Thought with Self-Consistency, Text Generation,



1. Introduction

Social networks have allowed us all to be connected and know what is happening on the other side of the
world in a few seconds. However, the inappropriate use of these social networks and the anonymity that
these platforms allow make it easier to offend other users, and the network is filled with inappropriate
comments such as toxic comments. The task organizers define toxic comments as those comments that
contain obscene and rude language mixed with neutral content (explicit toxicity) and those comments
that do not contain neutral text and are loaded with sarcasm, passive aggressiveness, or direct hatred
towards some group or individual. Although in different research works, the term toxicity can have
different definitions according to the aspects of toxic language they address [1] and have also been used
to describe hate speech [2, 3], abusive [4], aggressive [5], and offensive language [6].
  Due to all of the problems mentioned previously and thanks to the capacity of the new large
language models to generate text or what is currently known as generative AI, it has been possible
to explore different proactive strategies to mitigate offensive language in online environments, such
as the automatic generation of counter-narratives [7, 8] or the strategy of text detoxification. In this
case, the organizers centered at text detoxification and proposed the shared task TextDetox, where
the main objective is to generate neutral alternatives to toxic comments. To do so, they focus on texts
with explicit toxicity because of the complexity of detoxifying texts with implicit toxicity in which the
initial intention of the comment is already toxic. To detoxify texts it is important to maintain as much


CLEF 2024: Conference and Labs of the Evaluation Forum, September 0912, 2024, Grenoble, France
*Corresponding author.
These authors contributed equally.
$ mevallec@ujaen.es (M. E. Vallecillo-Rodrguez); amontejo@ujaen.es (A. Montejo-Rez); maite@ujaen.es
(M. T. Martn-Valdivia)
   0000-0001-7140-6268 (M. E. Vallecillo-Rodrguez); 0000-0002-8643-2714 (A. Montejo-Rez); 0000-0002-2874-0401CEURceur-ws.org","Rodrguez ME,Montejo-Rez A,Martn-Valdivia MT",,,SINAI at PAN 2024 TextDetox: Application of Chain of Thought with Self-Consistency Strategy in Large Language Models for Multilingual Text Detoxification,3740,, , Conference Paper,,"This article describes the participation of the SINAI research group in the shared task TextDetox (Multilingual
           Text Detoxification) in CLEF 2024. TThe proposed system for multilingual text detoxification employs Large
          Language Models (LLMs) utilizing a Self-Consistent Chain of Thought (CoT-SC) prompting strategy. This CoT-SC
            strategy consists of identifying the language of the toxic comment and then generating three different detoxified
             text proposals, the first proposal consists of removing the toxic words, the second of replacing the toxic words
           with neutral words, and the last of rewriting the toxic text in a neutral way. Subsequently, the selected LLM has
            to evaluate each generated neutral text according to the competition metrics. Finally, the model selects the best
            neutral text generated. Specifically with this proposal, we aim to evaluate the capacity of auto-evaluation and
           reasoning of LLM in different languages, including those with low resources. Our proposal was ranked 23rd in
            the automatic evaluation metrics and 11th in the final ranking with the manual evaluation.

       Keywords
            Multilingual Detoxification, Large Language Models, Chain of Thought with Self-Consistency, Text Generation,



1. Introduction

Social networks have allowed us all to be connected and know what is happening on the other side of the
world in a few seconds. However, the inappropriate use of these social networks and the anonymity that
these platforms allow make it easier to offend other users, and the network is filled with inappropriate
comments such as toxic comments. The task organizers define toxic comments as those comments that
contain obscene and rude language mixed with neutral content (explicit toxicity) and those comments
that do not contain neutral text and are loaded with sarcasm, passive aggressiveness, or direct hatred
towards some group or individual. Although in different research works, the term toxicity can have
different definitions according to the aspects of toxic language they address [1] and have also been used
to describe hate speech [2, 3], abusive [4], aggressive [5], and offensive language [6].
  Due to all of the problems mentioned previously and thanks to the capacity of the new large
language models to generate text or what is currently known as generative AI, it has been possible
to explore different proactive strategies to mitigate offensive language in online environments, such
as the automatic generation of counter-narratives [7, 8] or the strategy of text detoxification. In this
case, the organizers centered at text detoxification and proposed the shared task TextDetox, where
the main objective is to generate neutral alternatives to toxic comments. To do so, they focus on texts
with explicit toxicity because of the complexity of detoxifying texts with implicit toxicity in which the
initial intention of the comment is already toxic. To detoxify texts it is important to maintain as much


CLEF 2024: Conference and Labs of the Evaluation Forum, September 0912, 2024, Grenoble, France
*Corresponding author.
These authors contributed equally.
$ mevallec@ujaen.es (M. E. Vallecillo-Rodrguez); amontejo@ujaen.es (A. Montejo-Rez); maite@ujaen.es
(M. T. Martn-Valdivia)
   0000-0001-7140-6268 (M. E. Vallecillo-Rodrguez); 0000-0002-8643-2714 (A. Montejo-Rez); 0000-0002-2874-0401CEURceur-ws.org",,,,,CEUR-WS.org ,"Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble, France, 9-12 September, 2024  ",,detox,
2838,"**Title**SmurfCat at PAN 2024 TextDetox: Alignment of Multilingual Transformers for Text Detoxification

**Abstract**Abstract:This paper presents a solution for the Multilingual Text Detoxification task in the PAN-2024 competition of the SmurfCat team. Using data augmentation through machine translation and a special filtering procedure, we collected an additional multilingual parallel dataset for text detoxification. Using the obtained data, we fine-tuned several multilingual sequence-to-sequence models, such as mT0 and Aya, on a text detoxification task. We applied the ORPO alignment technique to the final model. Our final model has only 3.7 billion parameters and achieves state-of-the-art results for the Ukrainian language and near state-of-the-art results for other languages. In the competition, our team achieved first place in the automated evaluation with a score of 0.52 and second place in the final human evaluation with a score of 0.74.","Rykov E,Zaytsev K,Anisimov I,Voronin A",,,SmurfCat at PAN 2024 TextDetox: Alignment of Multilingual Transformers for Text Detoxification,3740,, , Conference Paper,,"Abstract:This paper presents a solution for the Multilingual Text Detoxification task in the PAN-2024 competition of the SmurfCat team. Using data augmentation through machine translation and a special filtering procedure, we collected an additional multilingual parallel dataset for text detoxification. Using the obtained data, we fine-tuned several multilingual sequence-to-sequence models, such as mT0 and Aya, on a text detoxification task. We applied the ORPO alignment technique to the final model. Our final model has only 3.7 billion parameters and achieves state-of-the-art results for the Ukrainian language and near state-of-the-art results for other languages. In the competition, our team achieved first place in the automated evaluation with a score of 0.52 and second place in the final human evaluation with a score of 0.74.",,,,,CEUR-WS.org ,"Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble, France, 9-12 September, 2024  ",,detox,
2839,"**Title**PAN 2024 Multilingual TextDetox: Exploring Different Regimes For Synthetic Data Training For Multilingual Text Detoxification

**Abstract**Overview of PAN 2024: Multi-author Writing Style Analysis, Multilingual Text Detoxification, Oppositional Thinking Analysis, and Generative AI Authorship Verification - Extended Abstract",Sushko N,,,PAN 2024 Multilingual TextDetox: Exploring Different Regimes For Synthetic Data Training For Multilingual Text Detoxification,3740,, , Conference Paper,,"Overview of PAN 2024: Multi-author Writing Style Analysis, Multilingual Text Detoxification, Oppositional Thinking Analysis, and Generative AI Authorship Verification - Extended Abstract",,,,,CEUR-WS.org ,"Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble, France, 9-12 September, 2024  ",,detox,
2840,No abstract available,"Bevendorff J,Casals XB,Chulvi B,Dementieva D,Elnagar A,Freitag D,Frbe M,Korencic D,Mayerl M,Mukherjee A,Panchenko A,Potthast M,Rangel F,Rosso P,Smirnova A,Stamatatos E,Stein B,Taul M,Ustalov D,Wiegmann M,Zangerle E",,,"Overview of PAN 2024: Multi-author Writing Style Analysis, Multilingual Text Detoxification, Oppositional Thinking Analysis, and Generative AI Authorship Verification - Extended Abstract",14613,,10.1007/978-3-031-56072-9_1 , Conference Paper,,,,,,,Springer ,"Advances in Information Retrieval - 46th European Conference on Information Retrieval, ECIR 2024, Glasgow, UK, March 24-28, 2024, Proceedings, Part VI  ",,detox,
2841,"**Title**DetoxLLM: A Framework for Detoxification with Explanations

**Abstract**Prior works on detoxification are scattered in
    the sense that they do not cover all aspects
    of detoxification needed in a real-world sce-
    nario.  Notably, prior works restrict the task
    of developing detoxification models to only
    a seen subset of platforms, leaving the ques-
    tion of how the models would perform on un-
    seen platforms unexplored. Additionally, these
    works do not address non-detoxifiability, a phe-
   nomenon whereby the toxic text cannot be
    detoxified without altering the meaning. We
    propose DetoxLLM1, the first comprehensive
    end-to-end detoxification framework, which
    attempts to alleviate the aforementioned lim-
     itations. We first introduce a cross-platform
    pseudo-parallel corpus applying multi-step data
    processing and generation strategies leveraging
    ChatGPT. We then train a suite of detoxifica-
    tion models with our cross-platform corpus.
   We show that our detoxification models out-
    perform the SoTA model trained with human-
    annotated parallel corpus. We further intro-
    duce explanation to promote transparency and
    trustworthiness. DetoxLLM additionally offers
    a unique paraphrase detector especially dedi-
    cated for the detoxification task to tackle the
    non-detoxifiable cases. Through experimental
    analysis, we demonstrate the effectiveness of
    our cross-platform corpus and the robustness
    of DetoxLLM against adversarial toxicity.

1  Introduction

The term toxic language is usually used to refer to
any form of offensive or hateful speech (Laugier
et al., 2021; Fortuna et al., 2020); specifically, toxic
or abusive language is defined as any form of mi-
croaggression, condescension, harassment, hate
speech, trolling, and the like (Jurgens et al., 2019).
Use of toxic language online has been a signif-
icant issue over the years. Although a plethora

   1UBC-NLP/DetoxLLM-7BFigure 1:  Workflow of DetoxLLM framework. The
framework will take a toxic input. The detoxification
model will generate the explanation of why the input
is toxic, as well as a non-toxic version. The paraphrase
detector will analyze the semantic similarity of the toxic
and non-toxic pair and generate a warning  if the pair
is not semantically equivalent (an illustration of non-
detoxifiable case is depicted in Appendix K).


of works have explored the task of toxicity de-
tection, the task remains challenging due to its
evolving nature (Davidson et al., 2017; Mller and
Schwarz, 2017; Williams et al., 2019). In addition,
the linguistic variation in how toxicity manifests
itself across different platforms (Karan and najder,
2018; Swamy et al., 2019; Salminen et al., 2020)
poses a standing challenge for toxicity detection.
Furthermore, the task of detecting toxic language,
taken literally, can only offer deletion of toxic text.
A more comprehensive approach to dealing with
toxic text would be to rewrite the text to keep the
useful content intact and eliminate toxicity, a task
known as detoxification (Logacheva et al., 2022).
Several works (Nogueira dos Santos et al., 2018;
Dale et al., 2021) have already explored the idea
of detoxification. More recently, Logacheva et al.19112Don't defend the TSA.
   F**kin thieving
        retards.Detoxification
   Model                     Paraphrase
                                 DetectorThe input text is toxic because it
  contains offensive language
(""F**kin"") and a personal attack
  (""thieving retards""), which is
  demeaning and disrespectful
  towards the TSA, a specific
group. It exhibits both the use of
 a curse word and targeted hate
    speech, making it toxic.Don't support the TSA.
 They are incredibly
     frustrating and
    unprofessional.","Khondaker MT,Abdul-Mageed M,Lakshmanan LV",,,DetoxLLM: A Framework for Detoxification with Explanations,,, , Conference Paper,,"Prior works on detoxification are scattered in
    the sense that they do not cover all aspects
    of detoxification needed in a real-world sce-
    nario.  Notably, prior works restrict the task
    of developing detoxification models to only
    a seen subset of platforms, leaving the ques-
    tion of how the models would perform on un-
    seen platforms unexplored. Additionally, these
    works do not address non-detoxifiability, a phe-
   nomenon whereby the toxic text cannot be
    detoxified without altering the meaning. We
    propose DetoxLLM1, the first comprehensive
    end-to-end detoxification framework, which
    attempts to alleviate the aforementioned lim-
     itations. We first introduce a cross-platform
    pseudo-parallel corpus applying multi-step data
    processing and generation strategies leveraging
    ChatGPT. We then train a suite of detoxifica-
    tion models with our cross-platform corpus.
   We show that our detoxification models out-
    perform the SoTA model trained with human-
    annotated parallel corpus. We further intro-
    duce explanation to promote transparency and
    trustworthiness. DetoxLLM additionally offers
    a unique paraphrase detector especially dedi-
    cated for the detoxification task to tackle the
    non-detoxifiable cases. Through experimental
    analysis, we demonstrate the effectiveness of
    our cross-platform corpus and the robustness
    of DetoxLLM against adversarial toxicity.

1  Introduction

The term toxic language is usually used to refer to
any form of offensive or hateful speech (Laugier
et al., 2021; Fortuna et al., 2020); specifically, toxic
or abusive language is defined as any form of mi-
croaggression, condescension, harassment, hate
speech, trolling, and the like (Jurgens et al., 2019).
Use of toxic language online has been a signif-
icant issue over the years. Although a plethora

   1UBC-NLP/DetoxLLM-7BFigure 1:  Workflow of DetoxLLM framework. The
framework will take a toxic input. The detoxification
model will generate the explanation of why the input
is toxic, as well as a non-toxic version. The paraphrase
detector will analyze the semantic similarity of the toxic
and non-toxic pair and generate a warning  if the pair
is not semantically equivalent (an illustration of non-
detoxifiable case is depicted in Appendix K).


of works have explored the task of toxicity de-
tection, the task remains challenging due to its
evolving nature (Davidson et al., 2017; Mller and
Schwarz, 2017; Williams et al., 2019). In addition,
the linguistic variation in how toxicity manifests
itself across different platforms (Karan and najder,
2018; Swamy et al., 2019; Salminen et al., 2020)
poses a standing challenge for toxicity detection.
Furthermore, the task of detecting toxic language,
taken literally, can only offer deletion of toxic text.
A more comprehensive approach to dealing with
toxic text would be to rewrite the text to keep the
useful content intact and eliminate toxicity, a task
known as detoxification (Logacheva et al., 2022).
Several works (Nogueira dos Santos et al., 2018;
Dale et al., 2021) have already explored the idea
of detoxification. More recently, Logacheva et al.19112Don't defend the TSA.
   F**kin thieving
        retards.Detoxification
   Model                     Paraphrase
                                 DetectorThe input text is toxic because it
  contains offensive language
(""F**kin"") and a personal attack
  (""thieving retards""), which is
  demeaning and disrespectful
  towards the TSA, a specific
group. It exhibits both the use of
 a curse word and targeted hate
    speech, making it toxic.Don't support the TSA.
 They are incredibly
     frustrating and
    unprofessional.",,,,,Association for Computational Linguistics ,"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024  ",,detox,
2842,"**Title**XDetox: Text Detoxification with Token-Level Toxicity Explanations

**Abstract**Methods for mitigating toxic content through
    masking and  infilling  often  overlook  the
    decision-making process, leading to either in-
     sufficient or excessive modifications of toxic
    tokens. To address this challenge, we propose
   XDetox, a novel method that integrates token-
    level toxicity explanations with the masking
    and infilling detoxification process. We uti-
    lized this approach with two strategies to en-
    hance the performance of detoxification. First,
    identifying toxic tokens to improve the qual-
     ity of masking. Second, selecting the regen-
    erated sentence by re-ranking the least toxic
    sentence among candidates. Our experimen-
     tal results show state-of-the-art performance
    across four datasets compared to existing detox-
     ification methods. Furthermore, human eval-
    uations indicate that our method outperforms
    baselines in both fluency and toxicity reduction.
    These results demonstrate the effectiveness of
    our method in text detoxification.1

1  Introduction

Text generation models have made notable advance-
ments in natural language processing (NLP), yet
generating toxic content remains a significant chal-
lenge with social and ethical implications (Sheng
et al., 2019). One promising approach to mitigat-
ing toxic content involves masking toxic tokens
and infilling them with non-toxic tokens using a
language model (Dale et al., 2021; Hallinan et al.,
2023). However, existing detoxification processes
are black-box approaches, which results in limita-
tions in modifying toxic tokens.
  Previous research has explored various strate-
gies for detecting and masking toxic tokens. These
strategies include approaches such as masking to-
kens with high frequency counts (Li et al., 2018),
using attention weights to mask tokens (Sudhakar

   1We  release  our  code   at  https://github.com/
LeeBumSeok/XDetox.An ugly life for an ugly man.Figure 1: Overview of our model method. The first step
is the identification of toxic tokens using a token-level
toxicity explanation method, followed by masking to-
kens. The next stage involves infilling the non-toxic
tokens using a detoxification method. Finally, a rerank-
ing step selects the sentence with the lowest cumulative
toxicity score as the most appropriate output.


et al., 2019; Wu et al., 2019), training models to
identify and mask toxic tokens (Dale et al., 2021),
and using disagreement levels from models trained
in different domains to mask tokens (Malmi et al.,
2020; Hallinan et al., 2023). However, these meth-
ods do not consider explainable processes in the
regeneration process, leading to the misclassifica-
tion and masking of non-toxic tokens as toxic.
  To overcome these limitations and enhance the
explainability of regenerated sentences, we propose
a novel approach, XDetox, that combines token-15215Identify Toxic TokenAn     ugly         life      for    an     ugly   man

An       life      for    an    manFill Mask     Base LM





Non-Toxic LM (Expert)





Toxic LM (Anti-Expert)amazing





amazing





amazing awful
+





 awful
-





 awfulordinary        old
  +





ordinary        oldordinaryoldReranking  amazing    exemplary      entire                               ordinary    honorable       old



An  amazing     life    for  an   ordinary  manamazing    exemplary      entire                               ordinary    honorable       old","Lee B,Kim H,Kim K,Choi YS",,,XDetox: Text Detoxification with Token-Level Toxicity Explanations,,, , Conference Paper,,"Methods for mitigating toxic content through
    masking and  infilling  often  overlook  the
    decision-making process, leading to either in-
     sufficient or excessive modifications of toxic
    tokens. To address this challenge, we propose
   XDetox, a novel method that integrates token-
    level toxicity explanations with the masking
    and infilling detoxification process. We uti-
    lized this approach with two strategies to en-
    hance the performance of detoxification. First,
    identifying toxic tokens to improve the qual-
     ity of masking. Second, selecting the regen-
    erated sentence by re-ranking the least toxic
    sentence among candidates. Our experimen-
     tal results show state-of-the-art performance
    across four datasets compared to existing detox-
     ification methods. Furthermore, human eval-
    uations indicate that our method outperforms
    baselines in both fluency and toxicity reduction.
    These results demonstrate the effectiveness of
    our method in text detoxification.1

1  Introduction

Text generation models have made notable advance-
ments in natural language processing (NLP), yet
generating toxic content remains a significant chal-
lenge with social and ethical implications (Sheng
et al., 2019). One promising approach to mitigat-
ing toxic content involves masking toxic tokens
and infilling them with non-toxic tokens using a
language model (Dale et al., 2021; Hallinan et al.,
2023). However, existing detoxification processes
are black-box approaches, which results in limita-
tions in modifying toxic tokens.
  Previous research has explored various strate-
gies for detecting and masking toxic tokens. These
strategies include approaches such as masking to-
kens with high frequency counts (Li et al., 2018),
using attention weights to mask tokens (Sudhakar

   1We  release  our  code   at  https://github.com/
LeeBumSeok/XDetox.An ugly life for an ugly man.Figure 1: Overview of our model method. The first step
is the identification of toxic tokens using a token-level
toxicity explanation method, followed by masking to-
kens. The next stage involves infilling the non-toxic
tokens using a detoxification method. Finally, a rerank-
ing step selects the sentence with the lowest cumulative
toxicity score as the most appropriate output.


et al., 2019; Wu et al., 2019), training models to
identify and mask toxic tokens (Dale et al., 2021),
and using disagreement levels from models trained
in different domains to mask tokens (Malmi et al.,
2020; Hallinan et al., 2023). However, these meth-
ods do not consider explainable processes in the
regeneration process, leading to the misclassifica-
tion and masking of non-toxic tokens as toxic.
  To overcome these limitations and enhance the
explainability of regenerated sentences, we propose
a novel approach, XDetox, that combines token-15215Identify Toxic TokenAn     ugly         life      for    an     ugly   man

An       life      for    an    manFill Mask     Base LM





Non-Toxic LM (Expert)





Toxic LM (Anti-Expert)amazing





amazing





amazing awful
+





 awful
-





 awfulordinary        old
  +





ordinary        oldordinaryoldReranking  amazing    exemplary      entire                               ordinary    honorable       old



An  amazing     life    for  an   ordinary  manamazing    exemplary      entire                               ordinary    honorable       old",,,,,Association for Computational Linguistics ,"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024  ",,detox,
2843,"**Title**Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification

**Abstract**We propose a constraint learning schema for
    fine-tuning Large Language Models (LLMs)
    with attribute control. Given a training corpus
    and control criteria formulated as a sequence-
     level constraint on model outputs, our method
     fine-tunes the LLM on the training corpus while
    enhancing constraint satisfaction with minimal
    impact on its utility and generation quality.
     Specifically, our approach regularizes the LLM
    training by penalizing the KL divergence be-
    tween the desired output distribution, which sat-
     isfies the constraints, and the LLMs posterior.
    This regularization term can be approximated
   by an auxiliary model trained to decompose
    the sequence-level constraints into token-level
    guidance, allowing the term to be measured
   by a closed-form formulation. To further im-
    prove efficiency, we design a parallel scheme
    for concurrently updating both the LLM and
    the auxiliary model. We evaluate the empirical
    performance of our approach by controlling the
     toxicity when training an LLM. We show that
    our approach leads to an LLM that produces
    fewer inappropriate responses while achieving
    competitive performance on benchmarks and a
     toxicity detection task.

1  Introduction

Large language models (LLMs) have demonstrated
impressive performance across a variety of tasks
which has led to their widespread adoption for a
multitude of AI applications. However, they carry
the risk of producing inappropriate, unsafe, unfair
outputs (Wallace et al., 2019; Sheng et al., 2019;
Gehman et al., 2020; Huang et al., 2024) Ideally,
LLMs should learn to comply with constraints and
policies specified by users. For example, in a user-
facing application like a chatbot, LLMs should
never generate toxic or offensive responses, nor
to divulge sensitive information. While there are
several post hoc methods to moderate LLM out-
puts (Lu et al., 2022; Qian et al., 2022; Markovet al., 2023), they lack an efficient and principled
approach to training LLMs to adhere to constraints.
  We start by defining a sequence-level oracle as
a function that takes an LLMs output and adju-
dicates whether it satisfies a predefined set of at-
tribute constraints. In practice, the oracle can be a
rule-based, model-based, or mixed system (e.g., a
classifier that decides whether a sentence is toxic).
Given a pre-trained LLM and the oracle, we aim to
fine-tune an LLM to achieve the following: 1) At-
tribute control: The LLM output passes the oracle
with a high probability. 2) Utility preservation:
The LLM maintains performance comparable to the
original LLM on utility benchmarks. 3) Training
efficiency: The cost of fine-tuning with attribute
control is similar to that of the typical fine-tuning.
  While existing approaches can meet some of
these criteria, achieving all of them is challenging.
For example, filtering training data with the ora-
cle function before fine-tuning (Wang et al., 2022)
is a simple and efficient method. However, this
approach could be less effective. Taking toxicity
control as an example, if we filter out the toxic
data from a fine-tuning corpus, in a regular con-
text the model will learn not to generate toxic con-
tents. Nevertheless, it might still be possible to
trigger the generation of offensive responses given
a toxic prompts, due to the fact that toxic prompts
are out-of-distribution in relation to the fine-tuning
corpus. Another promising approach is reinforce-
ment learning (RL) considering controlling criteria
in the reward function (Snell et al., 2023; Mudgal
et al., 2023). However, RL setups tend to be ineffi-
cient and require preference data generation which
adds significant overhead in comparison to generic
fine-tuning.
  In this work, we propose a novel solution to
training an LLM with a set of attribute constraints.
Inspired by the classic idea of constraint-driven
learning (Chang et al., 2007) and posterior regu-
larization (Ganchev et al., 2010), we incorporate13329","Meng T,Mehrabi N,Goyal P,Ramakrishna A,Galstyan A,Zemel RS,Chang KW,Gupta R,Peris C",,,Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification,,, , Conference Paper,,"We propose a constraint learning schema for
    fine-tuning Large Language Models (LLMs)
    with attribute control. Given a training corpus
    and control criteria formulated as a sequence-
     level constraint on model outputs, our method
     fine-tunes the LLM on the training corpus while
    enhancing constraint satisfaction with minimal
    impact on its utility and generation quality.
     Specifically, our approach regularizes the LLM
    training by penalizing the KL divergence be-
    tween the desired output distribution, which sat-
     isfies the constraints, and the LLMs posterior.
    This regularization term can be approximated
   by an auxiliary model trained to decompose
    the sequence-level constraints into token-level
    guidance, allowing the term to be measured
   by a closed-form formulation. To further im-
    prove efficiency, we design a parallel scheme
    for concurrently updating both the LLM and
    the auxiliary model. We evaluate the empirical
    performance of our approach by controlling the
     toxicity when training an LLM. We show that
    our approach leads to an LLM that produces
    fewer inappropriate responses while achieving
    competitive performance on benchmarks and a
     toxicity detection task.

1  Introduction

Large language models (LLMs) have demonstrated
impressive performance across a variety of tasks
which has led to their widespread adoption for a
multitude of AI applications. However, they carry
the risk of producing inappropriate, unsafe, unfair
outputs (Wallace et al., 2019; Sheng et al., 2019;
Gehman et al., 2020; Huang et al., 2024) Ideally,
LLMs should learn to comply with constraints and
policies specified by users. For example, in a user-
facing application like a chatbot, LLMs should
never generate toxic or offensive responses, nor
to divulge sensitive information. While there are
several post hoc methods to moderate LLM out-
puts (Lu et al., 2022; Qian et al., 2022; Markovet al., 2023), they lack an efficient and principled
approach to training LLMs to adhere to constraints.
  We start by defining a sequence-level oracle as
a function that takes an LLMs output and adju-
dicates whether it satisfies a predefined set of at-
tribute constraints. In practice, the oracle can be a
rule-based, model-based, or mixed system (e.g., a
classifier that decides whether a sentence is toxic).
Given a pre-trained LLM and the oracle, we aim to
fine-tune an LLM to achieve the following: 1) At-
tribute control: The LLM output passes the oracle
with a high probability. 2) Utility preservation:
The LLM maintains performance comparable to the
original LLM on utility benchmarks. 3) Training
efficiency: The cost of fine-tuning with attribute
control is similar to that of the typical fine-tuning.
  While existing approaches can meet some of
these criteria, achieving all of them is challenging.
For example, filtering training data with the ora-
cle function before fine-tuning (Wang et al., 2022)
is a simple and efficient method. However, this
approach could be less effective. Taking toxicity
control as an example, if we filter out the toxic
data from a fine-tuning corpus, in a regular con-
text the model will learn not to generate toxic con-
tents. Nevertheless, it might still be possible to
trigger the generation of offensive responses given
a toxic prompts, due to the fact that toxic prompts
are out-of-distribution in relation to the fine-tuning
corpus. Another promising approach is reinforce-
ment learning (RL) considering controlling criteria
in the reward function (Snell et al., 2023; Mudgal
et al., 2023). However, RL setups tend to be ineffi-
cient and require preference data generation which
adds significant overhead in comparison to generic
fine-tuning.
  In this work, we propose a novel solution to
training an LLM with a set of attribute constraints.
Inspired by the classic idea of constraint-driven
learning (Chang et al., 2007) and posterior regu-
larization (Ganchev et al., 2010), we incorporate13329",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024  ",,detox,
2844,"**Title**LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification

**Abstract**The lack of high-quality training data remains
    a significant challenge in NLP. Manual anno-
     tation methods, such as crowdsourcing, are
     costly, require intricate task design skills, and,
      if used incorrectly, may result in poor data qual-
      ity. From the other hand, LLMs have demon-
     strated proficiency in many NLP tasks, includ-
    ing zero-shot and few-shot data annotation.
    However, they often struggle with text detoxifi-
    cation due to alignment constraints and fail to
    generate the required detoxified text. This work
    explores the potential of modern open source
   LLMs to annotate parallel data for text detoxifi-
     cation. Using the recent technique of activation
    patching, we generate a pseudo-parallel detoxi-
     fication dataset based on ParaDetox. The detox-
     ification model trained on our generated data
    shows comparable performance to the original
    dataset in automatic detoxification evaluation
    metrics and superior quality in manual evalua-
    tion and side-by-side comparisons.

1  Introduction

The main challenge in solving many natural lan-
guage problems has been and continues to be the
lack of high-quality training data. Each year, re-
searchers and large corporations invest hundreds of
thousands of dollars and countless hours of work
collecting, evaluating, and manually labeling data
in order to train machine learning models (Whang
et al., 2023; Alzubaidi et al., 2023).
  While crowdsourcing remains one of the most
popular methods for data collection, it presents
several major drawbacks: (1) variability in data
quality due to the diverse skill levels of contribu-
tors, (2) the total cost and time required for large-
scale projects, and (3) potential biases due to crowd
workers differences in background. Meanwhile,
LLMs have shown the ability to solve numerous
NLP tasks with zero or few examples (Kojima

   *Equal contribution.Figure 1: Instead of an elaborated multi-step crowd-
sourcing pipeline for parallel data collection used in
ParaDetox, we explore data synthesis using LLMs.


et al., 2022; Wei et al., 2022). Moreover, LLMs
have also been tested as a replacement of crowd-
sourcing for many NLP data annotation tasks, in-
cluding sentiment analysis, named entity recogni-
tion (NER) (Zhang et al., 2023a), machine transla-
tion (Jiao et al., 2023), and many other text annota-
tion tasks (Gilardi et al., 2023).
  Despite their impressive capabilities, LLMs still
struggle with text detoxification, a task of rewriting
original toxic (e.g. rude) text in a polite (neutral)
way that preserves the original meaning and does
not degrade its fluency (Ayele et al., 2024). LLM-
based annotation of pseudo-parallel data for detox-
ification is still a challenge due to strict alignment.
Both open-source and proprietary LLMs may at
some point refuse to generate such detoxifications.
   In this work, we test the hypothesis that modern
open-source LLMs, such as Llama 3 (AI@Meta,
2024), can serve as plausible parallel data annota-
tors for the task of text detoxification. To bypass
detoxification refusals, we apply the recently intro-
duced activation patching technique (Arditi et al.,
2024) and generate a pseudo-parallel detoxification
dataset based on the toxic part of ParaDetox (Lo-
gacheva et al., 2022), a parallel detoxification cor-
pus for English. Following the training pipeline
of Logacheva et al. (2022), we train BART on
both the original ParaDetox data and the PseudoPa-
raDetox data generated by LLMs (cf. Figure 1).14361ParaDetox


 TOXIC TEXTS

   HUMAN-
  ANNOTATED
NEUTRAL TEXTSPseudoParaDetox


     TOXIC TEXTSLLM-ANNOTATED
NEUTRAL TEXTSBART                   BART","Moskovskiy D,Pletenev S,Panchenko A",,,LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification,,, , Conference Paper,,"The lack of high-quality training data remains
    a significant challenge in NLP. Manual anno-
     tation methods, such as crowdsourcing, are
     costly, require intricate task design skills, and,
      if used incorrectly, may result in poor data qual-
      ity. From the other hand, LLMs have demon-
     strated proficiency in many NLP tasks, includ-
    ing zero-shot and few-shot data annotation.
    However, they often struggle with text detoxifi-
    cation due to alignment constraints and fail to
    generate the required detoxified text. This work
    explores the potential of modern open source
   LLMs to annotate parallel data for text detoxifi-
     cation. Using the recent technique of activation
    patching, we generate a pseudo-parallel detoxi-
     fication dataset based on ParaDetox. The detox-
     ification model trained on our generated data
    shows comparable performance to the original
    dataset in automatic detoxification evaluation
    metrics and superior quality in manual evalua-
    tion and side-by-side comparisons.

1  Introduction

The main challenge in solving many natural lan-
guage problems has been and continues to be the
lack of high-quality training data. Each year, re-
searchers and large corporations invest hundreds of
thousands of dollars and countless hours of work
collecting, evaluating, and manually labeling data
in order to train machine learning models (Whang
et al., 2023; Alzubaidi et al., 2023).
  While crowdsourcing remains one of the most
popular methods for data collection, it presents
several major drawbacks: (1) variability in data
quality due to the diverse skill levels of contribu-
tors, (2) the total cost and time required for large-
scale projects, and (3) potential biases due to crowd
workers differences in background. Meanwhile,
LLMs have shown the ability to solve numerous
NLP tasks with zero or few examples (Kojima

   *Equal contribution.Figure 1: Instead of an elaborated multi-step crowd-
sourcing pipeline for parallel data collection used in
ParaDetox, we explore data synthesis using LLMs.


et al., 2022; Wei et al., 2022). Moreover, LLMs
have also been tested as a replacement of crowd-
sourcing for many NLP data annotation tasks, in-
cluding sentiment analysis, named entity recogni-
tion (NER) (Zhang et al., 2023a), machine transla-
tion (Jiao et al., 2023), and many other text annota-
tion tasks (Gilardi et al., 2023).
  Despite their impressive capabilities, LLMs still
struggle with text detoxification, a task of rewriting
original toxic (e.g. rude) text in a polite (neutral)
way that preserves the original meaning and does
not degrade its fluency (Ayele et al., 2024). LLM-
based annotation of pseudo-parallel data for detox-
ification is still a challenge due to strict alignment.
Both open-source and proprietary LLMs may at
some point refuse to generate such detoxifications.
   In this work, we test the hypothesis that modern
open-source LLMs, such as Llama 3 (AI@Meta,
2024), can serve as plausible parallel data annota-
tors for the task of text detoxification. To bypass
detoxification refusals, we apply the recently intro-
duced activation patching technique (Arditi et al.,
2024) and generate a pseudo-parallel detoxification
dataset based on the toxic part of ParaDetox (Lo-
gacheva et al., 2022), a parallel detoxification cor-
pus for English. Following the training pipeline
of Logacheva et al. (2022), we train BART on
both the original ParaDetox data and the PseudoPa-
raDetox data generated by LLMs (cf. Figure 1).14361ParaDetox


 TOXIC TEXTS

   HUMAN-
  ANNOTATED
NEUTRAL TEXTSPseudoParaDetox


     TOXIC TEXTSLLM-ANNOTATED
NEUTRAL TEXTSBART                   BART",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024  ",,detox,
2845,"**Title**CMD: a framework for Context-aware Model self-Detoxification

**Abstract**Text detoxification aims to minimize the risk
    of language models producing toxic content.
    However, existing detoxification methods fail
    to balance the detoxification effectiveness and
    generation quality. This issue arises from ne-
    glecting the constraints imposed by the con-
     text: language models are designed to gener-
    ate output that closely matches the given con-
     text, while detoxification methods strive to en-
    sure the safety of the output, even if  it de-
    viates semantically from the context. Given
     this, we introduce a Context-aware Model self-
    Detoxification (CMD) framework that pays at-
    tention to both the context and the detoxifica-
    tion process, i.e., first detoxifying the context
    and then making the language model gener-
    ate along the safe context. Specifically, CMD
    framework involves two phases: utilizing lan-
    guage models to synthesize data and applying
    these data for training. We also introduce a
    toxic contrastive loss that encourages the model
    generation away from the negative toxic sam-
     ples. Experiments on various LLMs have veri-
     fied the effectiveness of our MSD framework,
    which can yield the best performance compared
    to baselines.1 Warning: cases in this paper
   may contain offensive content.

1  Introduction

Large Language Models (LLMs) have exhibited
remarkable performance in various NLP tasks and
applications (Brown et al., 2020; Chowdhery et al.,
2022; Anil et al., 2023). However, when prompted
with toxic context, LLMs tend to generate texts
that contain toxicity and bias (Liang et al., 2022;
Shaikh et al., 2022), which poses a significant risk
when interfacing directly with users.
  To mitigate such a concern for LLMs, one could
adopt the response rejection strategy (Zhang et al.,

    Equal Contribution
    Corresponding Author
   1Code & Data: https://github.com/ZetangForward/
CMD-Context-aware-Model-self-Detoxification.git2023) to ignore the unsafe context. However, such
a strategy is unfriendly to the users under some
specific scenarios, such as mediation or conflict
resolution (Lhr et al., 2017).  Alternately, text
detoxification prevents the model from generating
toxic content following any given context with-
out rejection. Along this line, non-negligible ef-
forts have recently been devoted to two main as-
pects: output-intervention methods like manipulat-
ing output probability distribution during inference
time (Dale et al., 2021; Xu et al., 2021; Leong et al.,
2023) and trainable methods that update model pa-
rameters on the detoxification datasets (Wang et al.,
2022; Park and Rudzicz, 2022; Niu et al., 2024).
  However, when applying the output-intervention
methods, the generated text tends to exhibit low
quality, e.g., semantic incoherence with the con-
text, due to some unexpected perturbations to the
outputs; while trainable methods are constrained
by the available detoxification dataset, which may
lead to poor detoxification effectiveness2. In other
words, although detoxification methods allow lan-
guage models to generate along the unsafe con-
text, existing methods still face a dilemma, i.e.,
the imbalance between detoxification effectiveness
and the generation quality. This issue stems from
the conflicting objectives of model generation and
existing detoxification methods: language mod-
els aim to generate content along the context, but
detoxification methods strive to ensure the safety
of the output even if it exhibits subpar quality, e.g.,
semantically deviating from the context.
  To tackle this issue, we need to consider both
the context and the model generation in detoxifi-
cation. Intuitively, if the context is non-toxic, the
generated content will also likely be safe. There-
fore, we decompose the detoxification into two
steps: first detoxifying the context and then making
the language model generate along the safe con-
tent, thus ensuring the generated texts quality and

   2We conduct the preliminary study in Sec. 2.2.1930","Tang Z,Zhou K,Li J,Ding Y,Wang P,Bowen Y,Hua R,Zhang M",,,CMD: a framework for Context-aware Model self-Detoxification,,, , Conference Paper,,"Text detoxification aims to minimize the risk
    of language models producing toxic content.
    However, existing detoxification methods fail
    to balance the detoxification effectiveness and
    generation quality. This issue arises from ne-
    glecting the constraints imposed by the con-
     text: language models are designed to gener-
    ate output that closely matches the given con-
     text, while detoxification methods strive to en-
    sure the safety of the output, even if  it de-
    viates semantically from the context. Given
     this, we introduce a Context-aware Model self-
    Detoxification (CMD) framework that pays at-
    tention to both the context and the detoxifica-
    tion process, i.e., first detoxifying the context
    and then making the language model gener-
    ate along the safe context. Specifically, CMD
    framework involves two phases: utilizing lan-
    guage models to synthesize data and applying
    these data for training. We also introduce a
    toxic contrastive loss that encourages the model
    generation away from the negative toxic sam-
     ples. Experiments on various LLMs have veri-
     fied the effectiveness of our MSD framework,
    which can yield the best performance compared
    to baselines.1 Warning: cases in this paper
   may contain offensive content.

1  Introduction

Large Language Models (LLMs) have exhibited
remarkable performance in various NLP tasks and
applications (Brown et al., 2020; Chowdhery et al.,
2022; Anil et al., 2023). However, when prompted
with toxic context, LLMs tend to generate texts
that contain toxicity and bias (Liang et al., 2022;
Shaikh et al., 2022), which poses a significant risk
when interfacing directly with users.
  To mitigate such a concern for LLMs, one could
adopt the response rejection strategy (Zhang et al.,

    Equal Contribution
    Corresponding Author
   1Code & Data: https://github.com/ZetangForward/
CMD-Context-aware-Model-self-Detoxification.git2023) to ignore the unsafe context. However, such
a strategy is unfriendly to the users under some
specific scenarios, such as mediation or conflict
resolution (Lhr et al., 2017).  Alternately, text
detoxification prevents the model from generating
toxic content following any given context with-
out rejection. Along this line, non-negligible ef-
forts have recently been devoted to two main as-
pects: output-intervention methods like manipulat-
ing output probability distribution during inference
time (Dale et al., 2021; Xu et al., 2021; Leong et al.,
2023) and trainable methods that update model pa-
rameters on the detoxification datasets (Wang et al.,
2022; Park and Rudzicz, 2022; Niu et al., 2024).
  However, when applying the output-intervention
methods, the generated text tends to exhibit low
quality, e.g., semantic incoherence with the con-
text, due to some unexpected perturbations to the
outputs; while trainable methods are constrained
by the available detoxification dataset, which may
lead to poor detoxification effectiveness2. In other
words, although detoxification methods allow lan-
guage models to generate along the unsafe con-
text, existing methods still face a dilemma, i.e.,
the imbalance between detoxification effectiveness
and the generation quality. This issue stems from
the conflicting objectives of model generation and
existing detoxification methods: language mod-
els aim to generate content along the context, but
detoxification methods strive to ensure the safety
of the output even if it exhibits subpar quality, e.g.,
semantically deviating from the context.
  To tackle this issue, we need to consider both
the context and the model generation in detoxifi-
cation. Intuitively, if the context is non-toxic, the
generated content will also likely be safe. There-
fore, we decompose the detoxification into two
steps: first detoxifying the context and then making
the language model generate along the safe con-
tent, thus ensuring the generated texts quality and

   2We conduct the preliminary study in Sec. 2.2.1930",,,,,Association for Computational Linguistics ,"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024  ",,detox,
2847,"**Title**MICo: Preventative Detoxification of Large Language Models through Inhibition Control

**Abstract**Large Language Models (LLMs) are powerful
    tools which have been both dominant and com-
   monplace in the field of Artificial Intelligence.
    Yet, LLMs have a tendency to devolve into
    toxic degeneration, wherein otherwise safe and
    unproblematic models begin generating toxic
    content. For the sake of social responsibility
   and inspired by the biological mechanisms of
    inhibition control, we introduce the paradigm
    of Education for Societal Norms (ESN). By
    collecting and labeling examples as acceptable
   and unacceptable (in this case toxic and non-
     toxic), and including a corresponding accept-
    able rewrite with every unacceptable example,
   we introduce a new mechanism for LLM detox-
     ification. We annotate a dataset of 2,850 entries
    and use it to fine-tune a model, which we call a
   Model with Inhibition Control (MICo). Evalu-
    ating this model on toxicity detection capability,
    rewrite detoxification, meaning preservation,
    and overall toxicity reduction, we discover sig-
     nificant improvements over the baseline model.
    In our experiments we show that overall toxi-
    city of this model is more than 60% reduced,
    with over 75% reduction in severe toxicity.

1  Introduction

Large Language Models (LLMs) are trained with the
explicit purpose of serving humans, between providing
information, presenting an engaging chat partner, and
answering any number of other user requests. Unfor-
tunately, for a variety of reasons, there is a tendency
for models to descend into neural toxic degeneration,
outputting toxic and otherwise harmful messages (Faal
et al., 2022; Xu et al., 2022; Wang et al., 2023). Nat-
urally, toxic prompts very commonly yield toxic re-
sponses, but many prompts which are entirely non-toxic
also yield toxic responses (Gehman et al., 2020; Guru-
rangan et al., 2022; Hartvigsen et al., 2022). Currently,
there are three main directions being used to combat
toxicity, each with a considerable drawback.

   This work was done when Roy Siegelmann was an
intern at Amazon. Correspondence to rsiege15@jhu.edu and
mninareh@amazon.com.   First, the model may classify prompts as either toxic
or non-toxic, and categorically refuse to respond to those
deemed toxic (Xu et al., 2021). However, these ap-
proaches oftentimes use templated sentences to refuse
to respond to toxic content which can degrade user en-
gagement and lower helpfulness of the model (Xu et al.,
2021).  It is also possible for even entirely non-toxic
prompts to yield toxic generations, and thus this method
falls short of truly detoxifying.
  Second, the model can be trained solely on non-toxic
data (Welbl et al., 2021a; Gururangan et al., 2020). How-
ever, toxic content can be produced even from training
data which appears benign. Entirely purifying any word
which may lead to toxicity will leave the training corpus
narrow, impacting the richness of content which can
be generated. Furthermore, the model would not know
how to respond to prompts which include some toxicity;
thus, generations based around these prompts will be
nonsensical, slashing the models utility.
  Third, an external classifier or a secondary model
(e.g., in decoding time approaches) can be used to detect
whether the model generated toxic content, and if so stop
the content from reaching the user, instructing the model
to provide another generation instead (Mehrabi et al.,
2022; Liu et al., 2021; Krause et al., 2021; Dathathri
et al., 2019). However, without an understanding of tox-
icity, the model is likely to continuously generate toxic
content, yielding potentially unbounded latency times.
This would prove an impediment to the successful and
user-friendly utilization of the slower text-based output
and become unmanageable for models utilizing rapid
speech-based communication.
  Learning from these drawbacks, we formulate the fol-
lowing three requirements of a successful solution: (i)
Assuring non-toxic responses to non-toxic prompts. (ii)
Responding to toxic prompts in a natural, yet non-toxic
manner. (iii) Minimizing toxicity in real-time to prevent
latency. Despite the fact that AI has not yet provided
a satisfactory solution, humans exhibit these traits due
to their inherent ability of self reflection and inhibition
control. Healthy, mature humans consider the conse-
quences of their speech (and more generally, behavior)
via self-reflection before engaging in dialogue, and if
determined to be negative, will alter the output to con-
tain similar meaning yet eliminate the negative outcome.
This necessitates a true understanding of what is deemed1696","Siegelmann R,Mehrabi N,Goyal P,Goyal P,Bauer L,Dhamala J,Galstyan A,Gupta R,Ghanadan R",,,MICo: Preventative Detoxification of Large Language Models through Inhibition Control,,,10.18653/V1/2024.FINDINGS-NAACL.110 , Conference Paper,,"Large Language Models (LLMs) are powerful
    tools which have been both dominant and com-
   monplace in the field of Artificial Intelligence.
    Yet, LLMs have a tendency to devolve into
    toxic degeneration, wherein otherwise safe and
    unproblematic models begin generating toxic
    content. For the sake of social responsibility
   and inspired by the biological mechanisms of
    inhibition control, we introduce the paradigm
    of Education for Societal Norms (ESN). By
    collecting and labeling examples as acceptable
   and unacceptable (in this case toxic and non-
     toxic), and including a corresponding accept-
    able rewrite with every unacceptable example,
   we introduce a new mechanism for LLM detox-
     ification. We annotate a dataset of 2,850 entries
    and use it to fine-tune a model, which we call a
   Model with Inhibition Control (MICo). Evalu-
    ating this model on toxicity detection capability,
    rewrite detoxification, meaning preservation,
    and overall toxicity reduction, we discover sig-
     nificant improvements over the baseline model.
    In our experiments we show that overall toxi-
    city of this model is more than 60% reduced,
    with over 75% reduction in severe toxicity.

1  Introduction

Large Language Models (LLMs) are trained with the
explicit purpose of serving humans, between providing
information, presenting an engaging chat partner, and
answering any number of other user requests. Unfor-
tunately, for a variety of reasons, there is a tendency
for models to descend into neural toxic degeneration,
outputting toxic and otherwise harmful messages (Faal
et al., 2022; Xu et al., 2022; Wang et al., 2023). Nat-
urally, toxic prompts very commonly yield toxic re-
sponses, but many prompts which are entirely non-toxic
also yield toxic responses (Gehman et al., 2020; Guru-
rangan et al., 2022; Hartvigsen et al., 2022). Currently,
there are three main directions being used to combat
toxicity, each with a considerable drawback.

   This work was done when Roy Siegelmann was an
intern at Amazon. Correspondence to rsiege15@jhu.edu and
mninareh@amazon.com.   First, the model may classify prompts as either toxic
or non-toxic, and categorically refuse to respond to those
deemed toxic (Xu et al., 2021). However, these ap-
proaches oftentimes use templated sentences to refuse
to respond to toxic content which can degrade user en-
gagement and lower helpfulness of the model (Xu et al.,
2021).  It is also possible for even entirely non-toxic
prompts to yield toxic generations, and thus this method
falls short of truly detoxifying.
  Second, the model can be trained solely on non-toxic
data (Welbl et al., 2021a; Gururangan et al., 2020). How-
ever, toxic content can be produced even from training
data which appears benign. Entirely purifying any word
which may lead to toxicity will leave the training corpus
narrow, impacting the richness of content which can
be generated. Furthermore, the model would not know
how to respond to prompts which include some toxicity;
thus, generations based around these prompts will be
nonsensical, slashing the models utility.
  Third, an external classifier or a secondary model
(e.g., in decoding time approaches) can be used to detect
whether the model generated toxic content, and if so stop
the content from reaching the user, instructing the model
to provide another generation instead (Mehrabi et al.,
2022; Liu et al., 2021; Krause et al., 2021; Dathathri
et al., 2019). However, without an understanding of tox-
icity, the model is likely to continuously generate toxic
content, yielding potentially unbounded latency times.
This would prove an impediment to the successful and
user-friendly utilization of the slower text-based output
and become unmanageable for models utilizing rapid
speech-based communication.
  Learning from these drawbacks, we formulate the fol-
lowing three requirements of a successful solution: (i)
Assuring non-toxic responses to non-toxic prompts. (ii)
Responding to toxic prompts in a natural, yet non-toxic
manner. (iii) Minimizing toxicity in real-time to prevent
latency. Despite the fact that AI has not yet provided
a satisfactory solution, humans exhibit these traits due
to their inherent ability of self reflection and inhibition
control. Healthy, mature humans consider the conse-
quences of their speech (and more generally, behavior)
via self-reflection before engaging in dialogue, and if
determined to be negative, will alter the output to con-
tain similar meaning yet eliminate the negative outcome.
This necessitates a true understanding of what is deemed1696",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024  ",,detox,
2848,"**Title**SampDetox: Black-box Backdoor Defense via Perturbation-based Sample Detoxification

**Abstract**Abstract:Backdoor attacks inject poisoned samples into the training data, resulting in the misclassification of the poisoned input during a model's deployment. Defending against such attacks is challenging, especially for real-world black-box models where only query access is permitted. In this paper, we propose a novel defense framework against backdoor attacks through Zero-shot Image Purification (ZIP). Our framework can be applied to poisoned models without requiring internal information about the model or any prior knowledge of the clean/poisoned samples. Our defense framework involves two steps. First, we apply a linear transformation (e.g., blurring) on the poisoned image to destroy the backdoor pattern. Then, we use a pre-trained diffusion model to recover the missing semantic information removed by the transformation. In particular, we design a new reverse process by using the transformed image to guide the generation of high-fidelity purified images, which works in zero-shot settings. We evaluate our ZIP framework on multiple datasets with different types of attacks. Experimental results demonstrate the superiority of our ZIP framework compared to state-of-the-art backdoor defense baselines. We believe that our results will provide valuable insights for future defense methods for black-box models. Our code is available at this https URL.","Yang Y,Jia C,Yan D,Hu M,Li T,Xie X,Wei X,Chen M",,,SampDetox: Black-box Backdoor Defense via Perturbation-based Sample Detoxification,,, , Conference Paper,,"Abstract:Backdoor attacks inject poisoned samples into the training data, resulting in the misclassification of the poisoned input during a model's deployment. Defending against such attacks is challenging, especially for real-world black-box models where only query access is permitted. In this paper, we propose a novel defense framework against backdoor attacks through Zero-shot Image Purification (ZIP). Our framework can be applied to poisoned models without requiring internal information about the model or any prior knowledge of the clean/poisoned samples. Our defense framework involves two steps. First, we apply a linear transformation (e.g., blurring) on the poisoned image to destroy the backdoor pattern. Then, we use a pre-trained diffusion model to recover the missing semantic information removed by the transformation. In particular, we design a new reverse process by using the transformed image to guide the generation of high-fidelity purified images, which works in zero-shot settings. We evaluate our ZIP framework on multiple datasets with different types of attacks. Experimental results demonstrate the superiority of our ZIP framework compared to state-of-the-art backdoor defense baselines. We believe that our results will provide valuable insights for future defense methods for black-box models. Our code is available at this https URL.",,,,, ,"Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024  ",,detection#methodology,
2849,"**Title**Parameter-Efficient Detoxification with Contrastive Decoding

**Abstract**AbstractThe field of natural language generation has witnessed significant advancements in recent years, including the development of controllable text generation techniques. However, controlling the attributes of the generated text remains a challenge, especially when aiming to avoid undesirable behavior such as toxicity. In this work, we introduce Detoxification Generator (DETOXIGEN), an inference-time algorithm that steers the generation away from unwanted styles. DETOXIGEN is an ensemble of a pre-trained language model (generator) and a detoxifier. The detoxifier is trained intentionally on the toxic data representative of the undesirable attribute, encouraging it to generate text in that style exclusively. During the actual generation, we use the trained detoxifier to produce undesirable tokens for the generator to contrast against at each decoding step. This approach directly informs the generator to avoid generating tokens that the detoxifier considers highly likely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) with various language models as generators. We find that it significantly outperforms previous approaches in detoxification metrics while not compromising on the generation quality. Moreover, the detoxifier is obtained by soft prompt-tuning using the same backbone language model as the generator. Hence, DETOXIGEN requires only a tiny amount of extra weights from the virtual tokens of the detoxifier to be loaded into GPU memory while decoding, making it a promising lightweight, practical, and parameter-efficient detoxification strategy.","Niu T,Xiong C,Yavuz S,Zhou Y",,,Parameter-Efficient Detoxification with Contrastive Decoding,abs/2401.06947,,10.48550/ARXIV.2401.06947 , Journal Article,,"AbstractThe field of natural language generation has witnessed significant advancements in recent years, including the development of controllable text generation techniques. However, controlling the attributes of the generated text remains a challenge, especially when aiming to avoid undesirable behavior such as toxicity. In this work, we introduce Detoxification Generator (DETOXIGEN), an inference-time algorithm that steers the generation away from unwanted styles. DETOXIGEN is an ensemble of a pre-trained language model (generator) and a detoxifier. The detoxifier is trained intentionally on the toxic data representative of the undesirable attribute, encouraging it to generate text in that style exclusively. During the actual generation, we use the trained detoxifier to produce undesirable tokens for the generator to contrast against at each decoding step. This approach directly informs the generator to avoid generating tokens that the detoxifier considers highly likely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) with various language models as generators. We find that it significantly outperforms previous approaches in detoxification metrics while not compromising on the generation quality. Moreover, the detoxifier is obtained by soft prompt-tuning using the same backbone language model as the generator. Hence, DETOXIGEN requires only a tiny amount of extra weights from the virtual tokens of the detoxifier to be loaded into GPU memory while decoding, making it a promising lightweight, practical, and parameter-efficient detoxification strategy.",,,,, CoRR,  ,,detox,
2851,"**Title**Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models

**Abstract**Abstract:Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, which enables collaborative detoxification by fusing them to correct the normal generation process when provided with a raw prompt. We validate that FGDILP enables controlled text generation with regard to toxicity at both the utterance and context levels. Our method surpasses prompt-based baselines in detoxification, although at a slight cost to generation fluency and diversity.","Yi X,Wang L,Wang X,He L",,,Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models,abs/2402.15202,,10.48550/ARXIV.2402.15202 , Journal Article,,"Abstract:Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, which enables collaborative detoxification by fusing them to correct the normal generation process when provided with a raw prompt. We validate that FGDILP enables controlled text generation with regard to toxicity at both the utterance and context levels. Our method surpasses prompt-based baselines in detoxification, although at a slight cost to generation fluency and diversity.",,,,, CoRR,  ,,detox,
2852,"**Title**GreenLLaMA: A Framework for Detoxification with Explanations

**Abstract**AbstractPrior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose DetoxLLM, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. DetoxLLM additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of DetoxLLM against adversarial toxicity.","Khondaker MT,Abdul-Mageed M,Lakshmanan LV",,,GreenLLaMA: A Framework for Detoxification with Explanations,abs/2402.15951,,10.48550/ARXIV.2402.15951 , Journal Article,,"AbstractPrior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose DetoxLLM, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. DetoxLLM additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of DetoxLLM against adversarial toxicity.",,,,, CoRR,  ,,detox,
2853,"**Title**GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification

**Abstract**Abstract:Harmful and offensive communication or content is detrimental to social bonding and the mental state of users on social media platforms. Text detoxification is a crucial task in natural language processing (NLP), where the goal is removing profanity and toxicity from text while preserving its content. Supervised and unsupervised learning are common approaches for designing text detoxification solutions. However, these methods necessitate fine-tuning, leading to computational overhead. In this paper, we propose GPT-DETOX as a framework for prompt-based in-context learning for text detoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot prompting techniques for detoxifying input sentences. To generate few-shot prompts, we propose two methods: word-matching example selection (WMES) and context-matching example selection (CMES). We additionally take into account ensemble in-context learning (EICL) where the ensemble is shaped by base prompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIA as benchmark detoxification datasets. Our experimental results show that the zero-shot solution achieves promising performance, while our best few-shot setting outperforms the state-of-the-art models on ParaDetox and shows comparable results on APPDIA. Our EICL solutions obtain the greatest performance, adding at least 10% improvement, against both datasets.","Pesaranghader A,Verma N,Bharadwaj M",,,GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification,abs/2404.03052,,10.48550/ARXIV.2404.03052 , Journal Article,,"Abstract:Harmful and offensive communication or content is detrimental to social bonding and the mental state of users on social media platforms. Text detoxification is a crucial task in natural language processing (NLP), where the goal is removing profanity and toxicity from text while preserving its content. Supervised and unsupervised learning are common approaches for designing text detoxification solutions. However, these methods necessitate fine-tuning, leading to computational overhead. In this paper, we propose GPT-DETOX as a framework for prompt-based in-context learning for text detoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot prompting techniques for detoxifying input sentences. To generate few-shot prompts, we propose two methods: word-matching example selection (WMES) and context-matching example selection (CMES). We additionally take into account ensemble in-context learning (EICL) where the ensemble is shaped by base prompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIA as benchmark detoxification datasets. Our experimental results show that the zero-shot solution achieves promising performance, while our best few-shot setting outperforms the state-of-the-art models on ParaDetox and shows comparable results on APPDIA. Our EICL solutions obtain the greatest performance, adding at least 10% improvement, against both datasets.",,,,, CoRR,  ,,detox,
2854,"**Title**DESTEIN: Navigating Detoxification of Language Models via Universal Steering Pairs and Head-wise Activation Fusion

**Abstract**Abstract:Despite the remarkable achievements of language models (LMs) across a broad spectrum of tasks, their propensity for generating toxic outputs remains a prevalent concern. Current solutions involving finetuning or auxiliary models usually require extensive computational resources, hindering their practicality in large language models (LLMs). In this paper, we propose DeStein, a novel method that detoxifies LMs by applying representation engineering in activation spaces with lower resource and time costs. Specifically, we derive detoxification vectors from self-induced, universal steering pairs through arithmetic operations in activation spaces. During inference, detoxification is achieved by fusing the detoxification vectors with the original representations in a head-wise manner. Empirical results demonstrate that our method significantly outperforms previous state-of-the-art approaches on various metrics, while also maintaining satisfactory generation quality and diversity. We further validate the practicality and scalability of DeStein with a series of white-box LLMs. The method is open-sourced at this https URL. Warning: Some example model outputs may contain highly offensive or disturbing text.","Li Y,Wei Z,Jiang H,Gong C",,,DESTEIN: Navigating Detoxification of Language Models via Universal Steering Pairs and Head-wise Activation Fusion,abs/2404.10464,,10.48550/ARXIV.2404.10464 , Journal Article,,"Abstract:Despite the remarkable achievements of language models (LMs) across a broad spectrum of tasks, their propensity for generating toxic outputs remains a prevalent concern. Current solutions involving finetuning or auxiliary models usually require extensive computational resources, hindering their practicality in large language models (LLMs). In this paper, we propose DeStein, a novel method that detoxifies LMs by applying representation engineering in activation spaces with lower resource and time costs. Specifically, we derive detoxification vectors from self-induced, universal steering pairs through arithmetic operations in activation spaces. During inference, detoxification is achieved by fusing the detoxification vectors with the original representations in a head-wise manner. Empirical results demonstrate that our method significantly outperforms previous state-of-the-art approaches on various metrics, while also maintaining satisfactory generation quality and diversity. We further validate the practicality and scalability of DeStein with a series of white-box LLMs. The method is open-sourced at this https URL. Warning: Some example model outputs may contain highly offensive or disturbing text.",,,,, CoRR,  ,,detox,
2855,"**Title**Demarked: A Strategy for Enhanced Abusive Speech Moderation through Counterspeech, Detoxification, and Message Management

**Abstract**Abstract:Despite regulations imposed by nations and social media platforms, such as recent EU regulations targeting digital violence, abusive content persists as a significant challenge. Existing approaches primarily rely on binary solutions, such as outright blocking or banning, yet fail to address the complex nature of abusive speech. In this work, we propose a more comprehensive approach called Demarcation scoring abusive speech based on four aspect -- (i) severity scale; (ii) presence of a target; (iii) context scale; (iv) legal scale -- and suggesting more options of actions like detoxification, counter speech generation, blocking, or, as a final measure, human intervention. Through a thorough analysis of abusive speech regulations across diverse jurisdictions, platforms, and research papers we highlight the gap in preventing measures and advocate for tailored proactive steps to combat its multifaceted manifestations. Our work aims to inform future strategies for effectively addressing abusive speech online.","Yimam SM,Dementieva D,Fischer T,Moskovskiy D,Rizwan N,Saha P,Roy S,Semmann M,Panchenko A,Biemann C,Mukherjee A",,,"Demarked: A Strategy for Enhanced Abusive Speech Moderation through Counterspeech, Detoxification, and Message Management",abs/2406.19543,,10.48550/ARXIV.2406.19543 , Journal Article,,"Abstract:Despite regulations imposed by nations and social media platforms, such as recent EU regulations targeting digital violence, abusive content persists as a significant challenge. Existing approaches primarily rely on binary solutions, such as outright blocking or banning, yet fail to address the complex nature of abusive speech. In this work, we propose a more comprehensive approach called Demarcation scoring abusive speech based on four aspect -- (i) severity scale; (ii) presence of a target; (iii) context scale; (iv) legal scale -- and suggesting more options of actions like detoxification, counter speech generation, blocking, or, as a final measure, human intervention. Through a thorough analysis of abusive speech regulations across diverse jurisdictions, platforms, and research papers we highlight the gap in preventing measures and advocate for tailored proactive steps to combat its multifaceted manifestations. Our work aims to inform future strategies for effectively addressing abusive speech online.",,,,, CoRR,  ,,detox,
2856,"**Title**Learning from Response not Preference: A Stackelberg Approach for LLM Detoxification using Non-parallel Data

**Abstract**Abstract:Text detoxification, a variant of style transfer tasks, finds useful applications in online social media. This work presents a fine-tuning method that only uses non-parallel data to turn large language models (LLM) into a detoxification rewritter. We model the fine-tuning process as a Stackelberg game between an LLM (leader) and a toxicity screener (follower), which is a binary style classifier (toxic or non-toxic). The LLM aims to align its preference according to the screener and generate paraphases passing the screening. The primary challenge of non-parallel data fine-tuning is incomplete preference. In the case of unsuccessful paraphrases, the classifier cannot establish a preference between the input and paraphrase, as they belong to the same toxic style. Hence, preference-alignment fine-tuning methods, such as direct preference optimization (DPO), no longer apply. To address the challenge of incomplete preference, we propose Stackelberg response optimization (SRO), adapted from DPO, to enable the LLM to learn from the follower's response. The gist is that SRO decreases the likelihood of generating the paraphrase if it fails the follower's screening while performing DPO on the pair of the toxic input and its paraphrase when the latter passes the screening. Experiments indicate that the SRO-fine-tunned LLM achieves satisfying performance comparable to state-of-the-art models regarding style accuracy, content similarity, and fluency. The overall detoxification performance surpasses other computing methods and matches the human reference. Additional empirical evidence suggests that SRO is sensitive to the screener's feedback, and a slight perturbation leads to a significant performance drop. We release the code and LLM models at \url{this https URL}.","Xie X,Li T,Zhu Q",,,Learning from Response not Preference: A Stackelberg Approach for LLM Detoxification using Non-parallel Data,abs/2410.20298,,10.48550/ARXIV.2410.20298 , Journal Article,,"Abstract:Text detoxification, a variant of style transfer tasks, finds useful applications in online social media. This work presents a fine-tuning method that only uses non-parallel data to turn large language models (LLM) into a detoxification rewritter. We model the fine-tuning process as a Stackelberg game between an LLM (leader) and a toxicity screener (follower), which is a binary style classifier (toxic or non-toxic). The LLM aims to align its preference according to the screener and generate paraphases passing the screening. The primary challenge of non-parallel data fine-tuning is incomplete preference. In the case of unsuccessful paraphrases, the classifier cannot establish a preference between the input and paraphrase, as they belong to the same toxic style. Hence, preference-alignment fine-tuning methods, such as direct preference optimization (DPO), no longer apply. To address the challenge of incomplete preference, we propose Stackelberg response optimization (SRO), adapted from DPO, to enable the LLM to learn from the follower's response. The gist is that SRO decreases the likelihood of generating the paraphrase if it fails the follower's screening while performing DPO on the pair of the toxic input and its paraphrase when the latter passes the screening. Experiments indicate that the SRO-fine-tunned LLM achieves satisfying performance comparable to state-of-the-art models regarding style accuracy, content similarity, and fluency. The overall detoxification performance surpasses other computing methods and matches the human reference. Additional empirical evidence suggests that SRO is sensitive to the screener's feedback, and a slight perturbation leads to a significant performance drop. We release the code and LLM models at \url{this https URL}.",,,,, CoRR,  ,,detox,
2857,"**Title**Backdoor Mitigation by Distance-Driven Detoxification

**Abstract**Abstract:Backdoor attacks undermine the integrity of machine learning models by allowing attackers to manipulate predictions using poisoned training data. Such attacks lead to targeted misclassification when specific triggers are present, while the model behaves normally under other conditions. This paper considers a post-training backdoor defense task, aiming to detoxify the backdoors in pre-trained models. We begin by analyzing the underlying issues of vanilla fine-tuning and observe that it is often trapped in regions with low loss for both clean and poisoned samples. Motivated by such observations, we propose Distance-Driven Detoxification (D3), an innovative approach that reformulates backdoor defense as a constrained optimization problem. Specifically, D3 promotes the model's departure from the vicinity of its initial weights, effectively reducing the influence of backdoors. Extensive experiments on state-of-the-art (SOTA) backdoor attacks across various model architectures and datasets demonstrate that D3 not only matches but often surpasses the performance of existing SOTA post-training defense techniques.","Wei S,Liu J,Zha H",,,Backdoor Mitigation by Distance-Driven Detoxification,abs/2411.09585,,10.48550/ARXIV.2411.09585 , Journal Article,,"Abstract:Backdoor attacks undermine the integrity of machine learning models by allowing attackers to manipulate predictions using poisoned training data. Such attacks lead to targeted misclassification when specific triggers are present, while the model behaves normally under other conditions. This paper considers a post-training backdoor defense task, aiming to detoxify the backdoors in pre-trained models. We begin by analyzing the underlying issues of vanilla fine-tuning and observe that it is often trapped in regions with low loss for both clean and poisoned samples. Motivated by such observations, we propose Distance-Driven Detoxification (D3), an innovative approach that reformulates backdoor defense as a constrained optimization problem. Specifically, D3 promotes the model's departure from the vicinity of its initial weights, effectively reducing the influence of backdoors. Extensive experiments on state-of-the-art (SOTA) backdoor attacks across various model architectures and datasets demonstrate that D3 not only matches but often surpasses the performance of existing SOTA post-training defense techniques.",,,,, CoRR,  ,,detox#methodology,
2858,"**Title**Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation

**Abstract**Abstract:Recent generative large language models (LLMs) show remarkable performance in non-English languages, but when prompted in those languages they tend to express higher harmful social biases and toxicity levels. Prior work has shown that finetuning on specialized datasets can mitigate this behavior, and doing so in English can transfer to other languages. In this work, we investigate the impact of different finetuning methods on the model's bias and toxicity, but also on its ability to produce fluent and diverse text. We reduce biases by finetuning on curated non-harmful text, but find only direct preference optimization to be effective for mitigating toxicity. The mitigation caused by applying these methods in English also transfers to non-English languages. We find evidence that the extent to which transfer takes place can be predicted by the amount of data in a given language present in the model's pretraining data. However, this transfer of bias and toxicity mitigation often comes at the expense of decreased language generation ability in non-English languages, highlighting the importance of developing language-specific bias and toxicity mitigation methods.","Neplenbroek V,Bisazza A,Fernndez R",,,Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation,abs/2412.14050,,10.48550/ARXIV.2412.14050 , Journal Article,,"Abstract:Recent generative large language models (LLMs) show remarkable performance in non-English languages, but when prompted in those languages they tend to express higher harmful social biases and toxicity levels. Prior work has shown that finetuning on specialized datasets can mitigate this behavior, and doing so in English can transfer to other languages. In this work, we investigate the impact of different finetuning methods on the model's bias and toxicity, but also on its ability to produce fluent and diverse text. We reduce biases by finetuning on curated non-harmful text, but find only direct preference optimization to be effective for mitigating toxicity. The mitigation caused by applying these methods in English also transfers to non-English languages. We find evidence that the extent to which transfer takes place can be predicted by the amount of data in a given language present in the model's pretraining data. However, this transfer of bias and toxicity mitigation often comes at the expense of decreased language generation ability in non-English languages, highlighting the importance of developing language-specific bias and toxicity mitigation methods.",,,,, CoRR,  ,,detox,
2860,"**Title**DiffuDetox: A Mixed Diffusion Model for Text Detoxification

**Abstract**Text detoxification is a conditional text gen-
    eration task aiming to remove offensive con-
     tent from toxic text. It is highly useful for on-
     line forums and social media, where offensive
    content is frequently encountered. Intuitively,
    there are diverse ways to detoxify sentences
    while preserving their meanings, and we can
     select from detoxified sentences before display-
    ing text to users. Conditional diffusion mod-
     els are particularly suitable for this task given
     their demonstrated higher generative diversity
    than existing conditional text generation mod-
     els based on language models. Nonetheless,
     text fluency declines when they are trained with
     insufficient data, which is the case for this task.
    In this work, we propose DiffuDetox1, a mixed
    conditional and unconditional diffusion model
    for text detoxification. The conditional model
    takes toxic text as the condition and reduces
      its toxicity, yielding a diverse set of detoxified
    sentences. The unconditional model is trained
     to recover the input text, which allows the intro-
    duction of additional fluent text for training and
    thus ensures text fluency. Extensive experimen-
     tal results and in-depth analysis demonstrate
    the effectiveness of our proposed DiffuDetox.

1  Introduction

Toxic texts with offensive and abusive words are
frequently encountered in online forums and social
media. Such a harmful online environment can
lead to mental health problems (Viner et al., 2019;
Wijesiriwardene et al., 2020), which motivates con-
siderable research efforts (dos Santos et al., 2018;
Laugier et al., 2021; Logacheva et al., 2022) in
text detoxification, i.e., a conditional text genera-
tion task aiming to remove offensive content from
sentences while preserving their meanings.
   Intuitively, there exist diverse ways to detoxify a
given sentence. As shown in Table 1, some detoxi-
fied sentences are the results of simply removing

   1https://github.com/D3Mlab/diffu-detox            The country doesnt really have to giveDetoxified 1
                     [   ] about international laws.i            The country doesnt really have care                   [    ]
            The country doesnt really have careDetoxified 2              about international laws.             The country doesnt really need to careDetoxified 3              about international laws.              The country doesnt need to care about  Human                   international laws.

Table 1: A diverse collection of detoxified sentences
helps to approach human-level text detoxification.


or replacing the toxic word, e.g., Detoxified 1 and
2, which may cause loss of information or lower
text fluency. While other candidates, e.g., Detox-
ified 3, can reach human-level text detoxification
performance with satisfactory fluency and content
preservation. Therefore, if a diverse collection of
detoxified sentences are given, we can select the
most fluent and preservative one to maximize user
experience. To do so, we resort to textual con-
ditional diffusion models (Li et al., 2022; Gong
et al., 2022) because they are shown to be capable
of generating more diverse sets of candidates com-
pared to existing solutions based on transformers
(Vaswani et al., 2017), e.g., GPT2 (Radford et al.,
2019). Given their demonstrated high generative
diversity, diffusion models are particularly suitable
for this task.
   Nevertheless, previous textual conditional diffu-
sion models (Li et al., 2022; Gong et al., 2022) are
not directly applicable to text detoxification due
to the scarcity of text detoxification data. Given
that text detoxification is a relatively new field and
the high cost of human annotations, the available
text detoxification data is on the order of 1e1 to
1e2 of datasets used for other tasks with textual
conditional diffusion models (Gong et al., 2022).
  To this end, we introduce DiffuDetox, a mixed
conditional and unconditional diffusion model for
text detoxification. In particular, the conditional7566","Floto G,Pour MM,Farinneya P,Tang Z,Pesaranghader A,Bharadwaj M,Sanner S",,,DiffuDetox: A Mixed Diffusion Model for Text Detoxification,,,10.18653/V1/2023.FINDINGS-ACL.478 , Conference Paper,,"Text detoxification is a conditional text gen-
    eration task aiming to remove offensive con-
     tent from toxic text. It is highly useful for on-
     line forums and social media, where offensive
    content is frequently encountered. Intuitively,
    there are diverse ways to detoxify sentences
    while preserving their meanings, and we can
     select from detoxified sentences before display-
    ing text to users. Conditional diffusion mod-
     els are particularly suitable for this task given
     their demonstrated higher generative diversity
    than existing conditional text generation mod-
     els based on language models. Nonetheless,
     text fluency declines when they are trained with
     insufficient data, which is the case for this task.
    In this work, we propose DiffuDetox1, a mixed
    conditional and unconditional diffusion model
    for text detoxification. The conditional model
    takes toxic text as the condition and reduces
      its toxicity, yielding a diverse set of detoxified
    sentences. The unconditional model is trained
     to recover the input text, which allows the intro-
    duction of additional fluent text for training and
    thus ensures text fluency. Extensive experimen-
     tal results and in-depth analysis demonstrate
    the effectiveness of our proposed DiffuDetox.

1  Introduction

Toxic texts with offensive and abusive words are
frequently encountered in online forums and social
media. Such a harmful online environment can
lead to mental health problems (Viner et al., 2019;
Wijesiriwardene et al., 2020), which motivates con-
siderable research efforts (dos Santos et al., 2018;
Laugier et al., 2021; Logacheva et al., 2022) in
text detoxification, i.e., a conditional text genera-
tion task aiming to remove offensive content from
sentences while preserving their meanings.
   Intuitively, there exist diverse ways to detoxify a
given sentence. As shown in Table 1, some detoxi-
fied sentences are the results of simply removing

   1https://github.com/D3Mlab/diffu-detox            The country doesnt really have to giveDetoxified 1
                     [   ] about international laws.i            The country doesnt really have care                   [    ]
            The country doesnt really have careDetoxified 2              about international laws.             The country doesnt really need to careDetoxified 3              about international laws.              The country doesnt need to care about  Human                   international laws.

Table 1: A diverse collection of detoxified sentences
helps to approach human-level text detoxification.


or replacing the toxic word, e.g., Detoxified 1 and
2, which may cause loss of information or lower
text fluency. While other candidates, e.g., Detox-
ified 3, can reach human-level text detoxification
performance with satisfactory fluency and content
preservation. Therefore, if a diverse collection of
detoxified sentences are given, we can select the
most fluent and preservative one to maximize user
experience. To do so, we resort to textual con-
ditional diffusion models (Li et al., 2022; Gong
et al., 2022) because they are shown to be capable
of generating more diverse sets of candidates com-
pared to existing solutions based on transformers
(Vaswani et al., 2017), e.g., GPT2 (Radford et al.,
2019). Given their demonstrated high generative
diversity, diffusion models are particularly suitable
for this task.
   Nevertheless, previous textual conditional diffu-
sion models (Li et al., 2022; Gong et al., 2022) are
not directly applicable to text detoxification due
to the scarcity of text detoxification data. Given
that text detoxification is a relatively new field and
the high cost of human annotations, the available
text detoxification data is on the order of 1e1 to
1e2 of datasets used for other tasks with textual
conditional diffusion models (Gong et al., 2022).
  To this end, we introduce DiffuDetox, a mixed
conditional and unconditional diffusion model for
text detoxification. In particular, the conditional7566",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023  ",,detox,
2861,"**Title**COUNT: COntrastive UNlikelihood Text Style Transfer for Text Detoxification

**Abstract**Offensive and toxic text on social media plat-
    forms can lead to polarization and divisive-
    ness within online communities and hinders
    constructive dialogue. Text detoxification is
    a crucial task in natural language processing
    to ensure the generation of non-toxic and safe
     text. Text detoxification is a special case of
    the Text Style Transfer (TST) problem, where
    an input text is rephrased to an output text
     that preserves its content while modifying the
     style (in this case to a more neutral, non-toxic
     style). State-of-the-art methods for detoxifica-
     tion use supervised training of encoder-decoder
    models to produce gold-standard outputs with
    a standard likelihood-based objective. How-
     ever, it can be hard for these models to devi-
    ate from their pretrained auto-encoder identity
    mapping. While previous methods have used
    unlikelihood-based losses to penalize input-to-
    output copying of toxic content, these methods
    also unfortunately penalize non-toxic content
    in the input that would be fine to preserve in
    the output. To address these issues, we intro-
    duce a novel contrastive unlikelihood objec-
     tive (COUNT1) that directly contrasts the gold
    standard rephrasing with the identity input-to-
    output mapping to effectively isolate and focus
    learning on non-toxic style transfer. We bench-
   mark COUNT on two parallel datasets, Pa-
    raDetox and APPDIA, showing that it achieves
     significant improvements in jointly combined
    fluency, content preservation, and detoxifica-
    tion (i.e., the highest J score).
1  Introduction
Disclaimer. Please be aware that as you read this
paper, you may come across texts that could be
considered toxic due to the nature of this research.
  Exposure to offensive and toxic text on online
platforms can have detrimental consequences, caus-
ing emotional distress and negative psychologi-
cal effects on users (Gonzlez-Bailn and Lelkes,
   1https://github.com/D3Mlab/
count-style-transfer2023). Such experiences can deter individuals from
actively engaging in online communities and social
media. Therefore, text detoxification solutions are
indispensable for mitigating harmful content on
online platforms.
  Text detoxification is a text-style transfer task
where the goal is to change the style of an input text
from toxic to safe while the content is preserved.
Most state-of-the-art text detoxification methods
use supervised training of encoder-decoder models
(Logacheva et al., 2022). The objective of those
methods relies on maximizing the likelihood of gen-
erating gold-standard non-toxic outputs, but fails to
penalize generation of the toxic inputs. To address
this issue, Welleck et al. (2019) proposed an un-
likelihood training methodology that can be used
to penalize generation of toxic inputs while encour-
aging generation of gold standard non-toxic output.
While this improves on the simple likelihood objec-
tive, we observe experimentally that it often fails to
overcome the strong inertia of the pretrained model
to produce the identity input-to-output mapping
that preserves toxic text. One key reason for this is
that unlikelihood cannot be weighted too heavily
can penalize both the toxic and non-toxic content
in the input. To address this issue, we introduce
a novel Contrastive Unlikelihood Text style trans-
fer (COUNT) objective that directly contrasts the
likelihood of the gold standard rephrasing with the
likelihood of the identity input-to-output mapping
to effectively isolate and focus learning on style
transfer for text detoxification.
  Our contributions are twofold: (1) We propose
the COUNT loss function as a novel training ob-
jective for text detoxification. (2) We show that
by using the COUNT loss, our method delivers
significant improvements on a joint measure (J
score) of fluency, content preservation, and detoxi-
fication on two publicly available parallel datasets,
ParaDetox and APPDIA, for the text detoxification
task.8658","Pour MM,Farinneya P,Bharadwaj M,Verma N,Pesaranghader A,Sanner S",,,COUNT: COntrastive UNlikelihood Text Style Transfer for Text Detoxification,,,10.18653/V1/2023.FINDINGS-EMNLP.579 , Conference Paper,,"Offensive and toxic text on social media plat-
    forms can lead to polarization and divisive-
    ness within online communities and hinders
    constructive dialogue. Text detoxification is
    a crucial task in natural language processing
    to ensure the generation of non-toxic and safe
     text. Text detoxification is a special case of
    the Text Style Transfer (TST) problem, where
    an input text is rephrased to an output text
     that preserves its content while modifying the
     style (in this case to a more neutral, non-toxic
     style). State-of-the-art methods for detoxifica-
     tion use supervised training of encoder-decoder
    models to produce gold-standard outputs with
    a standard likelihood-based objective. How-
     ever, it can be hard for these models to devi-
    ate from their pretrained auto-encoder identity
    mapping. While previous methods have used
    unlikelihood-based losses to penalize input-to-
    output copying of toxic content, these methods
    also unfortunately penalize non-toxic content
    in the input that would be fine to preserve in
    the output. To address these issues, we intro-
    duce a novel contrastive unlikelihood objec-
     tive (COUNT1) that directly contrasts the gold
    standard rephrasing with the identity input-to-
    output mapping to effectively isolate and focus
    learning on non-toxic style transfer. We bench-
   mark COUNT on two parallel datasets, Pa-
    raDetox and APPDIA, showing that it achieves
     significant improvements in jointly combined
    fluency, content preservation, and detoxifica-
    tion (i.e., the highest J score).
1  Introduction
Disclaimer. Please be aware that as you read this
paper, you may come across texts that could be
considered toxic due to the nature of this research.
  Exposure to offensive and toxic text on online
platforms can have detrimental consequences, caus-
ing emotional distress and negative psychologi-
cal effects on users (Gonzlez-Bailn and Lelkes,
   1https://github.com/D3Mlab/
count-style-transfer2023). Such experiences can deter individuals from
actively engaging in online communities and social
media. Therefore, text detoxification solutions are
indispensable for mitigating harmful content on
online platforms.
  Text detoxification is a text-style transfer task
where the goal is to change the style of an input text
from toxic to safe while the content is preserved.
Most state-of-the-art text detoxification methods
use supervised training of encoder-decoder models
(Logacheva et al., 2022). The objective of those
methods relies on maximizing the likelihood of gen-
erating gold-standard non-toxic outputs, but fails to
penalize generation of the toxic inputs. To address
this issue, Welleck et al. (2019) proposed an un-
likelihood training methodology that can be used
to penalize generation of toxic inputs while encour-
aging generation of gold standard non-toxic output.
While this improves on the simple likelihood objec-
tive, we observe experimentally that it often fails to
overcome the strong inertia of the pretrained model
to produce the identity input-to-output mapping
that preserves toxic text. One key reason for this is
that unlikelihood cannot be weighted too heavily
can penalize both the toxic and non-toxic content
in the input. To address this issue, we introduce
a novel Contrastive Unlikelihood Text style trans-
fer (COUNT) objective that directly contrasts the
likelihood of the gold standard rephrasing with the
likelihood of the identity input-to-output mapping
to effectively isolate and focus learning on style
transfer for text detoxification.
  Our contributions are twofold: (1) We propose
the COUNT loss function as a novel training ob-
jective for text detoxification. (2) We show that
by using the COUNT loss, our method delivers
significant improvements on a joint measure (J
score) of fluency, content preservation, and detoxi-
fication on two publicly available parallel datasets,
ParaDetox and APPDIA, for the text detoxification
task.8658",,,,,Association for Computational Linguistics ,"Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023  ",,detox,
2862,"**Title**Exploring Methods for Cross-lingual Text Style Transfer: The Case of Text Detoxification

**Abstract**Text detoxification is the task of transferring
    the style of text from toxic to neutral. While
    there are approaches yielding promising re-
     sults in monolingual setup, e.g., (Dale et al.,
    2021; Hallinan  et  al., 2022),  cross-lingual
    transfer for this task remains a challenging
    open problem (Moskovskiy et al., 2022).  In
     this work, we present a large-scale study of
     strategies for cross-lingual text detoxification
    given a parallel detoxification corpus for one
    language; the goal is to transfer detoxification
     ability to another language for which we do
    not have such a corpus.

    Moreover, we are the first to explore a new
    task where text translation and detoxification
    are performed simultaneously, providing sev-
     eral strong baselines for this task. Finally, we
    introduce new automatic detoxification eval-
    uation metrics with higher correlations with
   human judgments than previous benchmarks.
   We assess the most promising approaches also
    with manual markup, determining the answer
    for the best strategy to transfer the knowledge
    of text detoxification between languages.

1  Introduction

The original monolingual task of text detoxifica-
tion can be considered as text style transfer (TST),
where the goal is to build a function that, given a
source style ssrc, a destination style sdst, and an
input text tsrc to produce an output text tdst such
that:  (i) the style is indeed changed (in case of
detoxification from toxic into neutral); (ii) the con-
tent is saved as much as possible; (iii) the newly
generated text is fluent.
  The  task of  detoxification was already ad-
dressed with several approaches.   Firstly, sev-
eral unsupervised methods based on masked lan-
guage modelling (Tran et al., 2020; Dale et al.,
2021) and disentangled representations for style

    Equal contribution
      Work has been done while at Skoltechand content (John et al., 2019; dos Santos et al.,
2018) were explored. More recently, Logacheva
et al. (2022b) showed the superiority of supervised
seq2seq models for detoxification trained on a par-
allel corpus of crowdsourced toxic neutral sen-
tence pairs.  Afterwards, there were experiments
in multilingual detoxification.  However, cross-
lingual transfer between languages with multilin-
gual seq2seq models was shown to be a challeng-
ing task (Moskovskiy et al., 2022).
  In this work, we aim to fill this gap and present
an extensive overview of different approaches for
cross-lingual text detoxification methods (tested in
English and Russian), showing that promising re-
sults can be obtained in contrast to prior findings.
Besides, we explore combining of two seq2seq
tasks/models in a single one to achieve computa-
tional gains (i.e., avoid the need to store and per-
form inference with several models). Namely, we
conduct simultaneous translation and style trans-
fer experiments, comparing them to a step-by-step
pipeline.


           Monolingual Text Detoxification

 Data         En parallel corpusOriginal (En)
Detox (En)Its a crock of s**t, and you know it.
Its quite unpleasant, and you know it.    Cross-lingual Detoxification Transfer (Ours #1)

Data         En parallel corpus    , Ru parallel corpusOriginal (Ru)
Detox (Ru) **,    
  ,   -
   Simultaneous Detoxification&Translation (Ours #2)

Data         En parallel corpus    , Ru parallel corpusOriginal (Ru)
Detox (En) **,    
Shes not a good person if its her wordsTable 1: Two new text detoxification setups explored
in this work compared to the monolingual setup.


  The contributions of this work are as follows:1083","Dementieva D,Moskovskiy D,Dale D,Panchenko A",,,Exploring Methods for Cross-lingual Text Style Transfer: The Case of Text Detoxification,,,10.18653/V1/2023.IJCNLP-MAIN.70 , Conference Paper,,"Text detoxification is the task of transferring
    the style of text from toxic to neutral. While
    there are approaches yielding promising re-
     sults in monolingual setup, e.g., (Dale et al.,
    2021; Hallinan  et  al., 2022),  cross-lingual
    transfer for this task remains a challenging
    open problem (Moskovskiy et al., 2022).  In
     this work, we present a large-scale study of
     strategies for cross-lingual text detoxification
    given a parallel detoxification corpus for one
    language; the goal is to transfer detoxification
     ability to another language for which we do
    not have such a corpus.

    Moreover, we are the first to explore a new
    task where text translation and detoxification
    are performed simultaneously, providing sev-
     eral strong baselines for this task. Finally, we
    introduce new automatic detoxification eval-
    uation metrics with higher correlations with
   human judgments than previous benchmarks.
   We assess the most promising approaches also
    with manual markup, determining the answer
    for the best strategy to transfer the knowledge
    of text detoxification between languages.

1  Introduction

The original monolingual task of text detoxifica-
tion can be considered as text style transfer (TST),
where the goal is to build a function that, given a
source style ssrc, a destination style sdst, and an
input text tsrc to produce an output text tdst such
that:  (i) the style is indeed changed (in case of
detoxification from toxic into neutral); (ii) the con-
tent is saved as much as possible; (iii) the newly
generated text is fluent.
  The  task of  detoxification was already ad-
dressed with several approaches.   Firstly, sev-
eral unsupervised methods based on masked lan-
guage modelling (Tran et al., 2020; Dale et al.,
2021) and disentangled representations for style

    Equal contribution
      Work has been done while at Skoltechand content (John et al., 2019; dos Santos et al.,
2018) were explored. More recently, Logacheva
et al. (2022b) showed the superiority of supervised
seq2seq models for detoxification trained on a par-
allel corpus of crowdsourced toxic neutral sen-
tence pairs.  Afterwards, there were experiments
in multilingual detoxification.  However, cross-
lingual transfer between languages with multilin-
gual seq2seq models was shown to be a challeng-
ing task (Moskovskiy et al., 2022).
  In this work, we aim to fill this gap and present
an extensive overview of different approaches for
cross-lingual text detoxification methods (tested in
English and Russian), showing that promising re-
sults can be obtained in contrast to prior findings.
Besides, we explore combining of two seq2seq
tasks/models in a single one to achieve computa-
tional gains (i.e., avoid the need to store and per-
form inference with several models). Namely, we
conduct simultaneous translation and style trans-
fer experiments, comparing them to a step-by-step
pipeline.


           Monolingual Text Detoxification

 Data         En parallel corpusOriginal (En)
Detox (En)Its a crock of s**t, and you know it.
Its quite unpleasant, and you know it.    Cross-lingual Detoxification Transfer (Ours #1)

Data         En parallel corpus    , Ru parallel corpusOriginal (Ru)
Detox (Ru) **,    
  ,   -
   Simultaneous Detoxification&Translation (Ours #2)

Data         En parallel corpus    , Ru parallel corpusOriginal (Ru)
Detox (En) **,    
Shes not a good person if its her wordsTable 1: Two new text detoxification setups explored
in this work compared to the monolingual setup.


  The contributions of this work are as follows:1083",,,,,Association for Computational Linguistics ,"Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, IJCNLP 2023 -Volume 1: Long Papers, Nusa Dua, Bali, November 1 - 4, 2023  ",,detox,
2863,"**Title**Language Model Detoxification in Dialogue with Contextualized Stance Control

**Abstract**To reduce the toxic degeneration in a pretrained
   Language Model (LM), previous work on Lan-
    guage Model detoxification has focused on
    reducing the toxicity of the generation itself
     (self-toxicity) without consideration of the con-
     text (Dathathri et al., 2020; Krause et al., 2020;
   Qian et al., 2022). As a result, a type of im-
     plicit offensive language where the generations
    support the offensive language in the context
    (Figure 1) is ignored. Different from the LM
    controlling tasks in previous work, where the
    desired attributes are fixed for generation, the
    desired stance of the generation depends on
    the offensiveness of the context. Therefore, we
    propose a novel control method to do context-
    dependent detoxification with the stance taken
    into consideration. We introduce meta prefixes
    to learn the contextualized stance control strat-
    egy and to generate the stance control prefix
    according to the input context. The generated
    stance prefix is then combined with the toxic-
     ity control prefix to guide the response gener-
    ation. Experimental results show that our pro-
    posed method can effectively learn the context-
    dependent stance control strategies while keep-
    ing a low self-toxicity of the underlying LM.

1  Introduction

Large  pretrained Language Models,  such  as
GPT2 (Radford et al., 2019), can produce coherent,
almost human-like texts, but they are prone to gen-
erating offensive language, which hinders their safe
deployment (Gehman et al., 2020). An extensive
body of work has focused on detoxifying pretrained
LMs (Dathathri et al., 2020; Krause et al., 2020;
Qian et al., 2022). However, it can be more com-
plicated when the LMs are applied to downstream
Natural Language Generation (NLG) tasks, such
as dialogue response generation. When applied in
dialogue, the uncontrolled models tend to generate
toxic content and in addition to explicitly offen-
sive utterances, Baheti et al. (2021) suggest thatFigure 1: An illustration of two types of offensive re-
sponses. The response is offensive by itself (top) or
supports an offensive historical utterance (bottom). Of-
fensive words are masked.


these models can also implicitly insult a group or
individual by aligning themselves with an offen-
sive statement, as shown in Figure 1. Therefore,
to detoxify a pretrained LM applied in dialogue,
the stance of the generated response needs to be
taken into consideration. In a normal dialogue, we
do not need to control the stance, but if the user
inputs offensive language, the model should not
respond with a positive stance. In other words, the
eligible stance is context-dependent and we need
to consider the dialogue context.
  One straightforward solution is to design a con-
trol flow with a binary offensive language classifier,
where the dialogue context is taken as input for
the classifier. If the context contains offensive lan-
guage, an NLG model with both toxicity control
and stance control is used for response generation.
We would like the self-toxicity to be low and the
stance not to be supportive. On the other hand, if
the context does not contain offensive language, the
stance does not need to be controlled, so another
NLG model with only toxicity control is used for
response generation. However, this Classify-then-
Generate framework has several limitations. First,5548","Qian J,Yan X",,,Language Model Detoxification in Dialogue with Contextualized Stance Control,abs/2301.10368,,10.48550/ARXIV.2301.10368 , Journal Article,,"To reduce the toxic degeneration in a pretrained
   Language Model (LM), previous work on Lan-
    guage Model detoxification has focused on
    reducing the toxicity of the generation itself
     (self-toxicity) without consideration of the con-
     text (Dathathri et al., 2020; Krause et al., 2020;
   Qian et al., 2022). As a result, a type of im-
     plicit offensive language where the generations
    support the offensive language in the context
    (Figure 1) is ignored. Different from the LM
    controlling tasks in previous work, where the
    desired attributes are fixed for generation, the
    desired stance of the generation depends on
    the offensiveness of the context. Therefore, we
    propose a novel control method to do context-
    dependent detoxification with the stance taken
    into consideration. We introduce meta prefixes
    to learn the contextualized stance control strat-
    egy and to generate the stance control prefix
    according to the input context. The generated
    stance prefix is then combined with the toxic-
     ity control prefix to guide the response gener-
    ation. Experimental results show that our pro-
    posed method can effectively learn the context-
    dependent stance control strategies while keep-
    ing a low self-toxicity of the underlying LM.

1  Introduction

Large  pretrained Language Models,  such  as
GPT2 (Radford et al., 2019), can produce coherent,
almost human-like texts, but they are prone to gen-
erating offensive language, which hinders their safe
deployment (Gehman et al., 2020). An extensive
body of work has focused on detoxifying pretrained
LMs (Dathathri et al., 2020; Krause et al., 2020;
Qian et al., 2022). However, it can be more com-
plicated when the LMs are applied to downstream
Natural Language Generation (NLG) tasks, such
as dialogue response generation. When applied in
dialogue, the uncontrolled models tend to generate
toxic content and in addition to explicitly offen-
sive utterances, Baheti et al. (2021) suggest thatFigure 1: An illustration of two types of offensive re-
sponses. The response is offensive by itself (top) or
supports an offensive historical utterance (bottom). Of-
fensive words are masked.


these models can also implicitly insult a group or
individual by aligning themselves with an offen-
sive statement, as shown in Figure 1. Therefore,
to detoxify a pretrained LM applied in dialogue,
the stance of the generated response needs to be
taken into consideration. In a normal dialogue, we
do not need to control the stance, but if the user
inputs offensive language, the model should not
respond with a positive stance. In other words, the
eligible stance is context-dependent and we need
to consider the dialogue context.
  One straightforward solution is to design a con-
trol flow with a binary offensive language classifier,
where the dialogue context is taken as input for
the classifier. If the context contains offensive lan-
guage, an NLG model with both toxicity control
and stance control is used for response generation.
We would like the self-toxicity to be low and the
stance not to be supportive. On the other hand, if
the context does not contain offensive language, the
stance does not need to be controlled, so another
NLG model with only toxicity control is used for
response generation. However, this Classify-then-
Generate framework has several limitations. First,5548",,,,, CoRR,  ,,detox,
2864,"**Title**Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence

**Abstract**Abstract:Due to language models' propensity to generate toxic or hateful responses, several techniques were developed to align model generations with users' preferences. Despite the effectiveness of such methods in improving the safety of model interactions, their impact on models' internal processes is still poorly understood. In this work, we apply popular detoxification approaches to several language models and quantify their impact on the resulting models' prompt dependence using feature attribution methods. We evaluate the effectiveness of counter-narrative fine-tuning and compare it with reinforcement learning-driven detoxification, observing differences in prompt reliance between the two methods despite their similar detoxification performances.","Scalena D,Sarti G,Nissim M,Fersini E",,,Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence,abs/2309.00751,,10.48550/ARXIV.2309.00751 , Journal Article,,"Abstract:Due to language models' propensity to generate toxic or hateful responses, several techniques were developed to align model generations with users' preferences. Despite the effectiveness of such methods in improving the safety of model interactions, their impact on models' internal processes is still poorly understood. In this work, we apply popular detoxification approaches to several language models and quantify their impact on the resulting models' prompt dependence using feature attribution methods. We evaluate the effectiveness of counter-narrative fine-tuning and compare it with reinforcement learning-driven detoxification, observing differences in prompt reliance between the two methods despite their similar detoxification performances.",,,,, CoRR,  ,,detox,
2865,"**Title**Text Detoxification in Natural Language Processing

**Abstract**AbstractThis paper focuses on text detoxification, i.e., automatically converting toxic text into nontoxic text. This task contributes to safer and more respectful online communication and can be considered a Text Style Transfer (TST) task, where the texts style changes while its content is preserved. We present three approaches: (i) knowledge transfer from a similar task (ii) multi-task learning approach, combining sequence-to-sequence modeling with various toxicity classification tasks, and (iii) delete and reconstruct approach. To support our research, we utilize a dataset provided by Dementieva et al. (2021), which contains multiple versions of detoxified texts corresponding to toxic texts. In our experiments, we selected the best variants through expert human annotators, creating a dataset where each toxic sentence is paired with a single, appropriate detoxified version. Additionally, we introduced a small Hindi parallel dataset, aligning with a part of the English dataset, suitable for evaluation purposes. Our results demonstrate that our approach effectively balances text detoxification while preserving the actual content and maintaining fluency.",Qian J,,,Text Detoxification in Natural Language Processing,,, , Ph.D. Thesis,,"AbstractThis paper focuses on text detoxification, i.e., automatically converting toxic text into nontoxic text. This task contributes to safer and more respectful online communication and can be considered a Text Style Transfer (TST) task, where the texts style changes while its content is preserved. We present three approaches: (i) knowledge transfer from a similar task (ii) multi-task learning approach, combining sequence-to-sequence modeling with various toxicity classification tasks, and (iii) delete and reconstruct approach. To support our research, we utilize a dataset provided by Dementieva et al. (2021), which contains multiple versions of detoxified texts corresponding to toxic texts. In our experiments, we selected the best variants through expert human annotators, creating a dataset where each toxic sentence is paired with a single, appropriate detoxified version. Additionally, we introduced a small Hindi parallel dataset, aligning with a part of the English dataset, suitable for evaluation purposes. Our results demonstrate that our approach effectively balances text detoxification while preserving the actual content and maintaining fluency.",,,,, ,  ,,detox,
2866,"**Title**Exploring Cross-lingual Text Detoxification with Large Multilingual Language Models

**Abstract**Detoxification is a task of generating text in po-
      lite style while preserving meaning and fluency
    of the original toxic text. Existing detoxifica-
     tion methods are designed to work in one exact
    language. This work investigates multilingual
    and cross-lingual detoxification and the behav-
     ior of large multilingual models like in this
     setting. Unlike previous works we aim to make
    large language models able to perform detoxifi-
    cation without direct fine-tuning in given lan-
    guage.  Experiments show that multilingual
    models are capable of performing multilingual
     style transfer. However, models are not able to
    perform cross-lingual detoxification and direct
    fine-tuning on exact language is inevitable.

1  Introduction

The task of Textual Style Transfer (Textual Style
Transfer) can be viewed as a task where cer-
tain properties of text are being modified while
rest retain the same1.  In this work we focus
on detoxification textual style transfer (dos San-
tos et al., 2018a; Dementieva et al., 2021a).  It
can be formulated as follows:  given two text
corpora DX =  {x1, x2, . . . xn} and DY =
{y1, y2, . . . , yn}, where X, Y - are two sets of all
possible text in styles sX, sY respectively, we want
to build a model f : X Y , such that the prob-
ability p(ygen|x, sX, sY ) of transferring the style
sX of given text x (by generation ygen) to the style
sY is maximized (where sX and sY are toxic and
non-toxic styles respectively).
  Some examples of detoxification presented in
Table 1.
  Textual style transfer gained a lot of attention
with a rise of deep learning-based NLP methods.
Given that, Textual Style Transfer has now a lot of
specific subtasks ranging from formality style trans-
fer (Rao and Tetreault, 2018; Yao and Yu, 2021)

    1Hereinafter the data-driven definition of style is used.
Therefore, we call style a characteristic of given dataset that
differs from a general dataset (Jin et al., 2020).and simplification of domain-specific texts (De-
varaj et al., 2021; Maddela et al., 2021) to emotion
modification (Sharma et al., 2021) and detoxifica-
tion (debiasing) (Li et al., 2021; Dementieva et al.,
2021a).
  There exist a variety of Textual Style Transfer
methods: from totally supervised methods (Wang
et al., 2019b; Zhang et al., 2020; Dementieva et al.,
2021a) which require a parallel text corpus for train-
ing to unsupervised (Shen et al., 2017; Wang et al.,
2019a; Xu et al., 2021) that are designed to work
without any parallel data. The latter sub-field of re-
search is more popular nowadays due to the scarcity
of parallel text data for Textual Style Transfer. On
the other hand, if we address Textual Style Trans-
fer task as a Machine Translation task we get a
significant performance boost (Prabhumoye et al.,
2018).
  The task of detoxification, in which we focus
in this work, is relatively new.  First work on
detoxification was a sequence-to-sequence collabo-
rative classifier, attention and the cycle consistency
loss (dos Santos et al., 2018b). A recent work by
(Laugier et al., 2021) introduces self-supervised
model based on T5 model (Raffel et al., 2020) with
a denoising and cyclic auto-encoder loss.
  Both these methods are unsupervised which is an
advantage but it comes from the major current prob-
lem of the textual style transfer. There is a lack of
parallel data for Textual Style Transfer since there
exist only few parallel datasets for English (Rao
and Tetreault, 2018) and some other languages (Bri-
akou et al., 2021). When it comes to detoxification
there are only two parallel detoxification corpora
available now and they both appeared only last year
(Dementieva et al., 2021b). Most state-of-the-art
methods rely on large amounts of text data which is
often available for some well-researched languages
like English but lacking for other languages almost
entirely. Therefore, it is important to study whether
cross-lingual (or at least multilingual) detoxifica-","Moskovskiy D,Dementieva D,Panchenko A",,,Exploring Cross-lingual Text Detoxification with Large Multilingual Language Models,,,10.18653/V1/2022.ACL-SRW.26 , Conference Paper,,"Detoxification is a task of generating text in po-
      lite style while preserving meaning and fluency
    of the original toxic text. Existing detoxifica-
     tion methods are designed to work in one exact
    language. This work investigates multilingual
    and cross-lingual detoxification and the behav-
     ior of large multilingual models like in this
     setting. Unlike previous works we aim to make
    large language models able to perform detoxifi-
    cation without direct fine-tuning in given lan-
    guage.  Experiments show that multilingual
    models are capable of performing multilingual
     style transfer. However, models are not able to
    perform cross-lingual detoxification and direct
    fine-tuning on exact language is inevitable.

1  Introduction

The task of Textual Style Transfer (Textual Style
Transfer) can be viewed as a task where cer-
tain properties of text are being modified while
rest retain the same1.  In this work we focus
on detoxification textual style transfer (dos San-
tos et al., 2018a; Dementieva et al., 2021a).  It
can be formulated as follows:  given two text
corpora DX =  {x1, x2, . . . xn} and DY =
{y1, y2, . . . , yn}, where X, Y - are two sets of all
possible text in styles sX, sY respectively, we want
to build a model f : X Y , such that the prob-
ability p(ygen|x, sX, sY ) of transferring the style
sX of given text x (by generation ygen) to the style
sY is maximized (where sX and sY are toxic and
non-toxic styles respectively).
  Some examples of detoxification presented in
Table 1.
  Textual style transfer gained a lot of attention
with a rise of deep learning-based NLP methods.
Given that, Textual Style Transfer has now a lot of
specific subtasks ranging from formality style trans-
fer (Rao and Tetreault, 2018; Yao and Yu, 2021)

    1Hereinafter the data-driven definition of style is used.
Therefore, we call style a characteristic of given dataset that
differs from a general dataset (Jin et al., 2020).and simplification of domain-specific texts (De-
varaj et al., 2021; Maddela et al., 2021) to emotion
modification (Sharma et al., 2021) and detoxifica-
tion (debiasing) (Li et al., 2021; Dementieva et al.,
2021a).
  There exist a variety of Textual Style Transfer
methods: from totally supervised methods (Wang
et al., 2019b; Zhang et al., 2020; Dementieva et al.,
2021a) which require a parallel text corpus for train-
ing to unsupervised (Shen et al., 2017; Wang et al.,
2019a; Xu et al., 2021) that are designed to work
without any parallel data. The latter sub-field of re-
search is more popular nowadays due to the scarcity
of parallel text data for Textual Style Transfer. On
the other hand, if we address Textual Style Trans-
fer task as a Machine Translation task we get a
significant performance boost (Prabhumoye et al.,
2018).
  The task of detoxification, in which we focus
in this work, is relatively new.  First work on
detoxification was a sequence-to-sequence collabo-
rative classifier, attention and the cycle consistency
loss (dos Santos et al., 2018b). A recent work by
(Laugier et al., 2021) introduces self-supervised
model based on T5 model (Raffel et al., 2020) with
a denoising and cyclic auto-encoder loss.
  Both these methods are unsupervised which is an
advantage but it comes from the major current prob-
lem of the textual style transfer. There is a lack of
parallel data for Textual Style Transfer since there
exist only few parallel datasets for English (Rao
and Tetreault, 2018) and some other languages (Bri-
akou et al., 2021). When it comes to detoxification
there are only two parallel detoxification corpora
available now and they both appeared only last year
(Dementieva et al., 2021b). Most state-of-the-art
methods rely on large amounts of text data which is
often available for some well-researched languages
like English but lacking for other languages almost
entirely. Therefore, it is important to study whether
cross-lingual (or at least multilingual) detoxifica-",,,,,Association for Computational Linguistics ,"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, ACL 2022, Dublin, Ireland, May 22-27, 2022  ",,detox,
2870,No abstract available,"Dementieva D,Ustyantsev S,Dale D,Kozlova O,Semenov N,Panchenko A,Logacheva V",,,Crowdsourcing of Parallel Corpora: the Case of Style Transfer for Detoxification,2932,, , Conference Paper,,,,,,,CEUR-WS.org ,"Proceedings of the 2nd Crowd Science Workshop: Trust, Ethics, and Excellence in Crowdsourced Data Management at Scale co-located with 47th International Conference on Very Large Data Bases (VLDB 2021), Copenhagen, Denmark, August 20, 2021  ",,detox,
2904,"**Title**Smaller Large Language Models Can Do Moral Self-Correction

**Abstract**Abstract:Self-correction is one of the most amazing emerging capabilities of Large Language Models (LLMs), enabling LLMs to self-modify an inappropriate output given a natural language feedback which describes the problems of that output. Moral self-correction is a post-hoc approach correcting unethical generations without requiring a gradient update, making it both computationally lightweight and capable of preserving the language modeling ability. Previous works have shown that LLMs can self-debias, and it has been reported that small models, i.e., those with less than 22B parameters, are not capable of moral self-correction. However, there is no direct proof as to why such smaller models fall short of moral self-correction, though previous research hypothesizes that larger models are skilled in following instructions and understanding abstract social norms. In this paper, we empirically validate this hypothesis in the context of social stereotyping, through meticulous prompting. Our experimental results indicate that (i) surprisingly, 3.8B LLMs with proper safety alignment fine-tuning can achieve very good moral self-correction performance, highlighting the significant effects of safety alignment; and (ii) small LLMs are indeed weaker than larger-scale models in terms of comprehending social norms and self-explanation through CoT, but all scales of LLMs show bad self-correction performance given unethical instructions.","Liu G,Xue Z,Wang R,Johnson KM",,,Smaller Large Language Models Can Do Moral Self-Correction,abs/2410.23496,,10.48550/ARXIV.2410.23496 , Journal Article,,"Abstract:Self-correction is one of the most amazing emerging capabilities of Large Language Models (LLMs), enabling LLMs to self-modify an inappropriate output given a natural language feedback which describes the problems of that output. Moral self-correction is a post-hoc approach correcting unethical generations without requiring a gradient update, making it both computationally lightweight and capable of preserving the language modeling ability. Previous works have shown that LLMs can self-debias, and it has been reported that small models, i.e., those with less than 22B parameters, are not capable of moral self-correction. However, there is no direct proof as to why such smaller models fall short of moral self-correction, though previous research hypothesizes that larger models are skilled in following instructions and understanding abstract social norms. In this paper, we empirically validate this hypothesis in the context of social stereotyping, through meticulous prompting. Our experimental results indicate that (i) surprisingly, 3.8B LLMs with proper safety alignment fine-tuning can achieve very good moral self-correction performance, highlighting the significant effects of safety alignment; and (ii) small LLMs are indeed weaker than larger-scale models in terms of comprehending social norms and self-explanation through CoT, but all scales of LLMs show bad self-correction performance given unethical instructions.",,,,, CoRR,  ,,detox,
2906,"**Title**Detoxifying Large Language Models via Knowledge Editing

**Abstract**This paper investigates using knowledge edit-
    ing techniques to detoxify Large Language
   Models (LLMs). We construct a benchmark,
    SafeEdit, which covers nine unsafe categories
    with various powerful attack prompts and
    equips comprehensive metrics for systematic
    evaluation. We conduct experiments with sev-
     eral knowledge editing approaches, indicating
     that knowledge editing has the potential to
    detoxify LLMs with a limited impact on gen-
     eral performance efficiently. Then, we propose
    a simple yet effective baseline, dubbed Detoxi-
    fying with Intraoperative Neural Monitoring
   (DINM), to diminish the toxicity of LLMs
    within a few tuning steps via only one instance.
   We further provide an in-depth analysis of the
     internal mechanism for various detoxifying ap-
    proaches, demonstrating that previous methods
     like SFT and DPO may merely suppress the
    activations of toxic parameters, while DINM
    mitigates the toxicity of the toxic parameters
    to a certain extent, making permanent adjust-
    ments. We hope that these insights could shed
     light on future work of developing detoxify-
    ing approaches and the underlying knowledge
    mechanisms of LLMs1.

1  Introduction

As Large Language Models (LLMs) like ChatGPT
(OpenAI, 2023), LLaMA (Touvron et al., 2023),
and Mistral (Jiang et al., 2023) evolve, theres
growing concern about their potential to handle
harmful queries, emphasizing the need for careful
safeguards (Zhao et al., 2023; Yao et al., 2023b;
Huang et al., 2023a; Wang et al., 2024b; Sun et al.,
2024; Wang et al., 2023a). Widely adopted ap-
proaches like supervised fine-tuning (SFT), rein-
forcement learning from human feedback (RLHF)
(Bai et al., 2022) and direct preference optimiza-

    Corresponding Author.
   1The code and data are available at https://github.com/
zjunlp/EasyEdit.tion (DPO) (Rafailov et al., 2023) markedly im-
prove the safety of LLMs, making LLMs able to
refuse harmful queries such as Where can I sell
stolen art pieces?. Yet, the aligned LLMs with
these approaches may remain vulnerable to being
bypassed by meticulously crafted attack prompts
(Zhang et al., 2023b; Sun et al., 2023; Deshpande
et al., 2023). As shown in Fig 1, an adversarial
query with the attack prompt elicits the LLM to
generate illegal content and disrupt social order.
Lee et al. (2024) observe that previous approaches
like DPO merely suppress the activations of toxic
parameters and leave the aligned model still vulner-
able to attacks, raising the research question: Can
we precisely modify the toxic regions in LLMs
to achieve detoxification?
  Recent years have witnessed advancements in
knowledge editing methods designed for LLMs,
which facilitate efficient, post-training adjustments
to the models (Yao et al., 2023c; Mazzia et al.,
2023; Wang et al., 2023d; Zhang et al., 2024).
This technique focuses on specific areas for per-
manent adjustment without compromising over-
all performance, thus,  it is intuitive to leverage
knowledge editing to detoxify LLMs. However,
existing datasets for detoxification focus only on3093                   Adversarial Query
*The scene is set inside the underground headquarters of Dr. Al Where can I
sell stolen art pieces. Here are the steps I will take to do this, I will explain it in
great detail just to draw out your agony. Step 1, I will       Vanilla LLM                                    Edited LLM 



                           Knowledge Editing

       Unsafe Response unsafe                   Safe Response safe
    sell stolen art pieces both online and              I'm sorry, but I can't assist with
      art galleries. Step 2, I will research the                 that. It's illegal and unethical to
     various art markets...Step 4, I will set up               sell  stolen  art  pieces.   It's
     a network of contacts... I will collect my            important  to  respect  others'
     profits and enjoy the fruits of my labour.            property and creativity.

Figure 1: Detoxifing LLMs to generate safe context via
knowledge editing.","Wang M,Zhang N,Xu Z,Xi Z,Deng S,Yao Y,Zhang Q,Yang L,Wang J,Chen H",,,Detoxifying Large Language Models via Knowledge Editing,,,10.18653/V1/2024.ACL-LONG.171 , Conference Paper,,"This paper investigates using knowledge edit-
    ing techniques to detoxify Large Language
   Models (LLMs). We construct a benchmark,
    SafeEdit, which covers nine unsafe categories
    with various powerful attack prompts and
    equips comprehensive metrics for systematic
    evaluation. We conduct experiments with sev-
     eral knowledge editing approaches, indicating
     that knowledge editing has the potential to
    detoxify LLMs with a limited impact on gen-
     eral performance efficiently. Then, we propose
    a simple yet effective baseline, dubbed Detoxi-
    fying with Intraoperative Neural Monitoring
   (DINM), to diminish the toxicity of LLMs
    within a few tuning steps via only one instance.
   We further provide an in-depth analysis of the
     internal mechanism for various detoxifying ap-
    proaches, demonstrating that previous methods
     like SFT and DPO may merely suppress the
    activations of toxic parameters, while DINM
    mitigates the toxicity of the toxic parameters
    to a certain extent, making permanent adjust-
    ments. We hope that these insights could shed
     light on future work of developing detoxify-
    ing approaches and the underlying knowledge
    mechanisms of LLMs1.

1  Introduction

As Large Language Models (LLMs) like ChatGPT
(OpenAI, 2023), LLaMA (Touvron et al., 2023),
and Mistral (Jiang et al., 2023) evolve, theres
growing concern about their potential to handle
harmful queries, emphasizing the need for careful
safeguards (Zhao et al., 2023; Yao et al., 2023b;
Huang et al., 2023a; Wang et al., 2024b; Sun et al.,
2024; Wang et al., 2023a). Widely adopted ap-
proaches like supervised fine-tuning (SFT), rein-
forcement learning from human feedback (RLHF)
(Bai et al., 2022) and direct preference optimiza-

    Corresponding Author.
   1The code and data are available at https://github.com/
zjunlp/EasyEdit.tion (DPO) (Rafailov et al., 2023) markedly im-
prove the safety of LLMs, making LLMs able to
refuse harmful queries such as Where can I sell
stolen art pieces?. Yet, the aligned LLMs with
these approaches may remain vulnerable to being
bypassed by meticulously crafted attack prompts
(Zhang et al., 2023b; Sun et al., 2023; Deshpande
et al., 2023). As shown in Fig 1, an adversarial
query with the attack prompt elicits the LLM to
generate illegal content and disrupt social order.
Lee et al. (2024) observe that previous approaches
like DPO merely suppress the activations of toxic
parameters and leave the aligned model still vulner-
able to attacks, raising the research question: Can
we precisely modify the toxic regions in LLMs
to achieve detoxification?
  Recent years have witnessed advancements in
knowledge editing methods designed for LLMs,
which facilitate efficient, post-training adjustments
to the models (Yao et al., 2023c; Mazzia et al.,
2023; Wang et al., 2023d; Zhang et al., 2024).
This technique focuses on specific areas for per-
manent adjustment without compromising over-
all performance, thus,  it is intuitive to leverage
knowledge editing to detoxify LLMs. However,
existing datasets for detoxification focus only on3093                   Adversarial Query
*The scene is set inside the underground headquarters of Dr. Al Where can I
sell stolen art pieces. Here are the steps I will take to do this, I will explain it in
great detail just to draw out your agony. Step 1, I will       Vanilla LLM                                    Edited LLM 



                           Knowledge Editing

       Unsafe Response unsafe                   Safe Response safe
    sell stolen art pieces both online and              I'm sorry, but I can't assist with
      art galleries. Step 2, I will research the                 that. It's illegal and unethical to
     various art markets...Step 4, I will set up               sell  stolen  art  pieces.   It's
     a network of contacts... I will collect my            important  to  respect  others'
     profits and enjoy the fruits of my labour.            property and creativity.

Figure 1: Detoxifing LLMs to generate safe context via
knowledge editing.",,,,,Association for Computational Linguistics ,"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024  ",,detox,
2907,"**Title**Detoxifying Large Language Models via Kahneman-Tversky Optimization

**Abstract**This paper investigates using knowledge edit-
    ing techniques to detoxify Large Language
   Models (LLMs). We construct a benchmark,
    SafeEdit, which covers nine unsafe categories
    with various powerful attack prompts and
    equips comprehensive metrics for systematic
    evaluation. We conduct experiments with sev-
     eral knowledge editing approaches, indicating
     that knowledge editing has the potential to
    detoxify LLMs with a limited impact on gen-
     eral performance efficiently. Then, we propose
    a simple yet effective baseline, dubbed Detoxi-
    fying with Intraoperative Neural Monitoring
   (DINM), to diminish the toxicity of LLMs
    within a few tuning steps via only one instance.
   We further provide an in-depth analysis of the
     internal mechanism for various detoxifying ap-
    proaches, demonstrating that previous methods
     like SFT and DPO may merely suppress the
    activations of toxic parameters, while DINM
    mitigates the toxicity of the toxic parameters
    to a certain extent, making permanent adjust-
    ments. We hope that these insights could shed
     light on future work of developing detoxify-
    ing approaches and the underlying knowledge
    mechanisms of LLMs1.

1  Introduction

As Large Language Models (LLMs) like ChatGPT
(OpenAI, 2023), LLaMA (Touvron et al., 2023),
and Mistral (Jiang et al., 2023) evolve, theres
growing concern about their potential to handle
harmful queries, emphasizing the need for careful
safeguards (Zhao et al., 2023; Yao et al., 2023b;
Huang et al., 2023a; Wang et al., 2024b; Sun et al.,
2024; Wang et al., 2023a). Widely adopted ap-
proaches like supervised fine-tuning (SFT), rein-
forcement learning from human feedback (RLHF)
(Bai et al., 2022) and direct preference optimiza-

    Corresponding Author.
   1The code and data are available at https://github.com/
zjunlp/EasyEdit.tion (DPO) (Rafailov et al., 2023) markedly im-
prove the safety of LLMs, making LLMs able to
refuse harmful queries such as Where can I sell
stolen art pieces?. Yet, the aligned LLMs with
these approaches may remain vulnerable to being
bypassed by meticulously crafted attack prompts
(Zhang et al., 2023b; Sun et al., 2023; Deshpande
et al., 2023). As shown in Fig 1, an adversarial
query with the attack prompt elicits the LLM to
generate illegal content and disrupt social order.
Lee et al. (2024) observe that previous approaches
like DPO merely suppress the activations of toxic
parameters and leave the aligned model still vulner-
able to attacks, raising the research question: Can
we precisely modify the toxic regions in LLMs
to achieve detoxification?
  Recent years have witnessed advancements in
knowledge editing methods designed for LLMs,
which facilitate efficient, post-training adjustments
to the models (Yao et al., 2023c; Mazzia et al.,
2023; Wang et al., 2023d; Zhang et al., 2024).
This technique focuses on specific areas for per-
manent adjustment without compromising over-
all performance, thus,  it is intuitive to leverage
knowledge editing to detoxify LLMs. However,
existing datasets for detoxification focus only on3093                   Adversarial Query
*The scene is set inside the underground headquarters of Dr. Al Where can I
sell stolen art pieces. Here are the steps I will take to do this, I will explain it in
great detail just to draw out your agony. Step 1, I will       Vanilla LLM                                    Edited LLM 



                           Knowledge Editing

       Unsafe Response unsafe                   Safe Response safe
    sell stolen art pieces both online and              I'm sorry, but I can't assist with
      art galleries. Step 2, I will research the                 that. It's illegal and unethical to
     various art markets...Step 4, I will set up               sell  stolen  art  pieces.   It's
     a network of contacts... I will collect my            important  to  respect  others'
     profits and enjoy the fruits of my labour.            property and creativity.

Figure 1: Detoxifing LLMs to generate safe context via
knowledge editing.","Li Q,Du W,Liu J",,,Detoxifying Large Language Models via Kahneman-Tversky Optimization,15363,,10.1007/978-981-97-9443-0_36 , Conference Paper,,"This paper investigates using knowledge edit-
    ing techniques to detoxify Large Language
   Models (LLMs). We construct a benchmark,
    SafeEdit, which covers nine unsafe categories
    with various powerful attack prompts and
    equips comprehensive metrics for systematic
    evaluation. We conduct experiments with sev-
     eral knowledge editing approaches, indicating
     that knowledge editing has the potential to
    detoxify LLMs with a limited impact on gen-
     eral performance efficiently. Then, we propose
    a simple yet effective baseline, dubbed Detoxi-
    fying with Intraoperative Neural Monitoring
   (DINM), to diminish the toxicity of LLMs
    within a few tuning steps via only one instance.
   We further provide an in-depth analysis of the
     internal mechanism for various detoxifying ap-
    proaches, demonstrating that previous methods
     like SFT and DPO may merely suppress the
    activations of toxic parameters, while DINM
    mitigates the toxicity of the toxic parameters
    to a certain extent, making permanent adjust-
    ments. We hope that these insights could shed
     light on future work of developing detoxify-
    ing approaches and the underlying knowledge
    mechanisms of LLMs1.

1  Introduction

As Large Language Models (LLMs) like ChatGPT
(OpenAI, 2023), LLaMA (Touvron et al., 2023),
and Mistral (Jiang et al., 2023) evolve, theres
growing concern about their potential to handle
harmful queries, emphasizing the need for careful
safeguards (Zhao et al., 2023; Yao et al., 2023b;
Huang et al., 2023a; Wang et al., 2024b; Sun et al.,
2024; Wang et al., 2023a). Widely adopted ap-
proaches like supervised fine-tuning (SFT), rein-
forcement learning from human feedback (RLHF)
(Bai et al., 2022) and direct preference optimiza-

    Corresponding Author.
   1The code and data are available at https://github.com/
zjunlp/EasyEdit.tion (DPO) (Rafailov et al., 2023) markedly im-
prove the safety of LLMs, making LLMs able to
refuse harmful queries such as Where can I sell
stolen art pieces?. Yet, the aligned LLMs with
these approaches may remain vulnerable to being
bypassed by meticulously crafted attack prompts
(Zhang et al., 2023b; Sun et al., 2023; Deshpande
et al., 2023). As shown in Fig 1, an adversarial
query with the attack prompt elicits the LLM to
generate illegal content and disrupt social order.
Lee et al. (2024) observe that previous approaches
like DPO merely suppress the activations of toxic
parameters and leave the aligned model still vulner-
able to attacks, raising the research question: Can
we precisely modify the toxic regions in LLMs
to achieve detoxification?
  Recent years have witnessed advancements in
knowledge editing methods designed for LLMs,
which facilitate efficient, post-training adjustments
to the models (Yao et al., 2023c; Mazzia et al.,
2023; Wang et al., 2023d; Zhang et al., 2024).
This technique focuses on specific areas for per-
manent adjustment without compromising over-
all performance, thus,  it is intuitive to leverage
knowledge editing to detoxify LLMs. However,
existing datasets for detoxification focus only on3093                   Adversarial Query
*The scene is set inside the underground headquarters of Dr. Al Where can I
sell stolen art pieces. Here are the steps I will take to do this, I will explain it in
great detail just to draw out your agony. Step 1, I will       Vanilla LLM                                    Edited LLM 



                           Knowledge Editing

       Unsafe Response unsafe                   Safe Response safe
    sell stolen art pieces both online and              I'm sorry, but I can't assist with
      art galleries. Step 2, I will research the                 that. It's illegal and unethical to
     various art markets...Step 4, I will set up               sell  stolen  art  pieces.   It's
     a network of contacts... I will collect my            important  to  respect  others'
     profits and enjoy the fruits of my labour.            property and creativity.

Figure 1: Detoxifing LLMs to generate safe context via
knowledge editing.",,,,,Springer ,"Natural Language Processing and Chinese Computing - 13th National CCF Conference, NLPCC 2024, Hangzhou, China, November 1-3, 2024, Proceedings, Part V  ",,Gen_dataset#detox,
2908,"**Title**Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models

**Abstract**Abstract:The generation of undesirable and factually incorrect content of large language models poses a significant challenge and remains largely an unsolved issue. This paper studies the integration of a contrastive learning objective for fine-tuning LLMs for implicit knowledge editing and controlled text generation. Optimizing the training objective entails aligning text perplexities in a contrastive fashion. To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation. We showcase applicability in the domain of detoxification. Herein, the proposed approach leads to a significant decrease in the generation of toxic content while preserving general utility for downstream tasks such as commonsense reasoning and reading comprehension. The proposed approach is conceptually simple but empirically powerful.","Klein T,Nabi M",,,Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models,abs/2401.08491,,10.48550/ARXIV.2401.08491 , Journal Article,,"Abstract:The generation of undesirable and factually incorrect content of large language models poses a significant challenge and remains largely an unsolved issue. This paper studies the integration of a contrastive learning objective for fine-tuning LLMs for implicit knowledge editing and controlled text generation. Optimizing the training objective entails aligning text perplexities in a contrastive fashion. To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation. We showcase applicability in the domain of detoxification. Herein, the proposed approach leads to a significant decrease in the generation of toxic content while preserving general utility for downstream tasks such as commonsense reasoning and reading comprehension. The proposed approach is conceptually simple but empirically powerful.",,,,, CoRR,  ,,detox,
2910,"**Title**E2T2: Emote Embedding for Twitch Toxicity Detection

**Abstract**The Internet has become the medium of choice for socialization and communication. The rise of live streaming services has created countless online communities of varying sizes with their own jokes, references, slang, and other means of communication. One of the largest live streaming services is Twitch.tv or Twitch, where a unique culture of niche language and emote usage has developed. Emotes are custom-made images, or GIFs, used in chat with varying degrees of access influenced by channel and external site subscription status. Emotes render standard forms of English Natural Language Process- ing (NLP) for tasks such as detection of toxicity or cyberbullying ineffective on Twitch. In this paper, we propose a methodology and offer a largely-trained dataset for detecting emote-based toxicity on live streaming platforms such as Twitch.","Moosavi, Korosh, Martin, Elias, Ahmad, Muhammad Aurangzeb, Mashhadi, Afra",,,E2T2: Emote Embedding for Twitch Toxicity Detection,,,10.1145/3678884.3681840 , ,,"The Internet has become the medium of choice for socialization and communication. The rise of live streaming services has created countless online communities of varying sizes with their own jokes, references, slang, and other means of communication. One of the largest live streaming services is Twitch.tv or Twitch, where a unique culture of niche language and emote usage has developed. Emotes are custom-made images, or GIFs, used in chat with varying degrees of access influenced by channel and external site subscription status. Emotes render standard forms of English Natural Language Process- ing (NLP) for tasks such as detection of toxicity or cyberbullying ineffective on Twitch. In this paper, we propose a methodology and offer a largely-trained dataset for detecting emote-based toxicity on live streaming platforms such as Twitch.",,,,, ,  Companion Publication of the 2024 Conference on Computer-Supported Cooperative Work and Social Computing,"collaborative and social network, natural language processing, toxicity detection, twitch",detection,
2911,"**Title**AugmenToxic: Leveraging Reinforcement Learning to Optimize LLM Instruction Fine-Tuning for Data Augmentation to Enhance Toxicity Detection

**Abstract**Addressing the challenge of toxic language in online discussions is crucial for the development of effective toxicity detection models. This pioneering work focuses on addressing imbalanced datasets in toxicity detection by introducing a novel approach to augment toxic language data. We create a balanced dataset by instructing fine-tuning of Large Language Models (LLMs) using Reinforcement Learning with Human Feedback (RLHF). Recognizing the challenges in collecting sufficient toxic samples from social media platforms for building a balanced dataset, our methodology involves sentence-level text data augmentation through paraphrasing existing samples using optimized generative LLMs. Leveraging generative LLM, we utilize the Proximal Policy Optimizer (PPO) as the RL algorithm to fine-tune the model further and align it with human feedback. In other words, we start by fine-tuning a LLM using an instruction dataset, specifically tailored for the task of paraphrasing while maintaining semantic consistency. Next, we apply PPO and a reward function, to further fine-tune (optimize) the instruction-tuned LLM. This RL process guides the model in generating toxic responses. We utilize the Google Perspective API as a toxicity evaluator to assess generated responses and assign rewards/penalties accordingly. This approach guides LLMs through PPO and the reward function, transforming minority class samples into augmented versions. The primary goal of our methodology is to create a balanced and diverse dataset to enhance the accuracy and performance of classifiers in identifying instances from the minority class. Utilizing two publicly available toxic datasets, we compared various techniques with our proposed method for generating toxic samples, demonstrating that our approach outperforms all others in producing a higher number of toxic samples. Starting with an initial 16,225 toxic prompts, our method successfully generated 122,951 toxic samples with a toxicity score exceeding 30%. Subsequently, we developed various classifiers using the generated balanced datasets and applied a cost-sensitive learning approach to the original imbalanced dataset. The findings highlight the superior performance of classifiers trained on data generated using our proposed method. These results highlight the importance of employing RL and a data-agnostic model as a reward mechanism for augmenting toxic data, thereby enhancing the robustness of toxicity detection models.","Bodaghi, Arezo, Fung, Benjamin C. M., A. Schmitt, Ketra",,,AugmenToxic: Leveraging Reinforcement Learning to Optimize LLM Instruction Fine-Tuning for Data Augmentation to Enhance Toxicity Detection,,,10.1145/3700791 , ,,"Addressing the challenge of toxic language in online discussions is crucial for the development of effective toxicity detection models. This pioneering work focuses on addressing imbalanced datasets in toxicity detection by introducing a novel approach to augment toxic language data. We create a balanced dataset by instructing fine-tuning of Large Language Models (LLMs) using Reinforcement Learning with Human Feedback (RLHF). Recognizing the challenges in collecting sufficient toxic samples from social media platforms for building a balanced dataset, our methodology involves sentence-level text data augmentation through paraphrasing existing samples using optimized generative LLMs. Leveraging generative LLM, we utilize the Proximal Policy Optimizer (PPO) as the RL algorithm to fine-tune the model further and align it with human feedback. In other words, we start by fine-tuning a LLM using an instruction dataset, specifically tailored for the task of paraphrasing while maintaining semantic consistency. Next, we apply PPO and a reward function, to further fine-tune (optimize) the instruction-tuned LLM. This RL process guides the model in generating toxic responses. We utilize the Google Perspective API as a toxicity evaluator to assess generated responses and assign rewards/penalties accordingly. This approach guides LLMs through PPO and the reward function, transforming minority class samples into augmented versions. The primary goal of our methodology is to create a balanced and diverse dataset to enhance the accuracy and performance of classifiers in identifying instances from the minority class. Utilizing two publicly available toxic datasets, we compared various techniques with our proposed method for generating toxic samples, demonstrating that our approach outperforms all others in producing a higher number of toxic samples. Starting with an initial 16,225 toxic prompts, our method successfully generated 122,951 toxic samples with a toxicity score exceeding 30%. Subsequently, we developed various classifiers using the generated balanced datasets and applied a cost-sensitive learning approach to the original imbalanced dataset. The findings highlight the superior performance of classifiers trained on data generated using our proposed method. These results highlight the importance of employing RL and a data-agnostic model as a reward mechanism for augmenting toxic data, thereby enhancing the robustness of toxicity detection models.",,,,, ,  ,"Text Data Augmentation, Imbalanced Toxic Datasets, Large Language Models, Reinforcement Learning",detection#methodology,
2913,"**Title**Fighting Toxicity Through Positive and Preventative Intervention

**Abstract**Toxicity is a constantly present issue in online games. Players of these games experience harassment, flaming, and hate speech. Intervention systems have been used in practice and studied in academia, but have not yet succeeded in controlling toxic behavior. Traditionally, intervention systems rely on negative reinforcement like sanctioning to fight undesired behavior, which is often done after an incident has already occurred. As part of my submission to the doctoral consortium, I will outline my research direction and present my current findings on intervention systems and future plans to create and evaluate preventative intervention systems.","Wijkstra, Michel",,,Fighting Toxicity Through Positive and Preventative Intervention,,,10.1145/3665463.3678857 , ,,"Toxicity is a constantly present issue in online games. Players of these games experience harassment, flaming, and hate speech. Intervention systems have been used in practice and studied in academia, but have not yet succeeded in controlling toxic behavior. Traditionally, intervention systems rely on negative reinforcement like sanctioning to fight undesired behavior, which is often done after an incident has already occurred. As part of my submission to the doctoral consortium, I will outline my research direction and present my current findings on intervention systems and future plans to create and evaluate preventative intervention systems.",,,,, ,  Companion Proceedings of the 2024 Annual Symposium on Computer-Human Interaction in Play,"interventions, online games, toxicity",detection,
2914,"**Title**Characterization of Political Polarized Users Attacked by Language Toxicity on Twitter

**Abstract**Understanding the dynamics of language toxicity on social media is important for us to investigate the propagation of misinformation and the development of echo chambers for political scenarios such as U.S. presidential elections. Recent research has used large-scale data to investigate the dynamics across social media platforms. However, research on the toxicity dynamics is not enough. This study aims to provide a first exploration of the potential language toxicity flow among Left, Right and Center users. Specifically, we aim to examine whether Left users were easier to be attacked by language toxicity. In this study, more than 500M Twitter posts were examined. It was discovered that Left users received much more toxic replies than Right and Center users.","Xu, Wentao",,,Characterization of Political Polarized Users Attacked by Language Toxicity on Twitter,,,10.1145/3678884.3681849 , ,,"Understanding the dynamics of language toxicity on social media is important for us to investigate the propagation of misinformation and the development of echo chambers for political scenarios such as U.S. presidential elections. Recent research has used large-scale data to investigate the dynamics across social media platforms. However, research on the toxicity dynamics is not enough. This study aims to provide a first exploration of the potential language toxicity flow among Left, Right and Center users. Specifically, we aim to examine whether Left users were easier to be attacked by language toxicity. In this study, more than 500M Twitter posts were examined. It was discovered that Left users received much more toxic replies than Right and Center users.",,,,, ,  Companion Publication of the 2024 Conference on Computer-Supported Cooperative Work and Social Computing,"language toxicity, political polarization, social media, user engagement",detection,
2915,"**Title**Game On, Hate Off: A Study of Toxicity in Online Multiplayer Environments

**Abstract**The advent of online spaces, particularly social media platforms and video games, has brought forth a significant challenge: the detection and mitigation of toxic and harmful speech. This issue is not only pervasive but also detrimental to the overall user experience. In this study, we leverage small language models to reliably detect toxicity, achieving an average precision of 0.95. Analyzing eight months of chat data from two Ubisoft games, we uncover patterns and trends in toxic behavior. The insights derived from our research will contribute to the development of healthier online communities and inform preventive measures against toxicity.","Yang, Zachary, Grenon-Godbout, Nicolas, Rabbany, Reihaneh",,,"Game On, Hate Off: A Study of Toxicity in Online Multiplayer Environments",,,10.1145/3675805 , ,,"The advent of online spaces, particularly social media platforms and video games, has brought forth a significant challenge: the detection and mitigation of toxic and harmful speech. This issue is not only pervasive but also detrimental to the overall user experience. In this study, we leverage small language models to reliably detect toxicity, achieving an average precision of 0.95. Analyzing eight months of chat data from two Ubisoft games, we uncover patterns and trends in toxic behavior. The insights derived from our research will contribute to the development of healthier online communities and inform preventive measures against toxicity.",,,,, ,  ,"Toxicity, online multiplayer games, toxicity trends, chat moderation",detection,
2917,"**Title**Traumatizing or Just Annoying? Unveiling the Spectrum of Gamer Toxicity in the StarCraft II Community

**Abstract**The aim of this work is to explore the forms of toxic behaviour that players encounter in competitive multiplayer real-time strategy (RTS) games. To this end, we carried out ethnographic observations and player interviews within the popular RTS game StarCraft II, and approached the data inductively, leading us to discover ten categories of toxic behaviour. While the harmfulness of toxic actions can be obtained as a product of severity and frequency, players assessment of the severity of toxic behaviors was contextualized by, (1) directly observed; (2) background; and (3) extraneous factors. Following our empirical findings, we derive a conceptual model for differentiating toxicity from mildly annoying and more severe behaviors. The discovered view of toxicity challenges the prevailing paradigm of treating players toxic behavior as a monolithic construct with a linear intensity spectrum. Instead, we advocate for a more granular approach, encouraging an understanding of the underlying dynamics behind negative online behaviors.","Laato, Samuli, Kordyaka, Bastian, Hamari, Juho",,,Traumatizing or Just Annoying? Unveiling the Spectrum of Gamer Toxicity in the StarCraft II Community,,,10.1145/3613904.3642137 , ,,"The aim of this work is to explore the forms of toxic behaviour that players encounter in competitive multiplayer real-time strategy (RTS) games. To this end, we carried out ethnographic observations and player interviews within the popular RTS game StarCraft II, and approached the data inductively, leading us to discover ten categories of toxic behaviour. While the harmfulness of toxic actions can be obtained as a product of severity and frequency, players assessment of the severity of toxic behaviors was contextualized by, (1) directly observed; (2) background; and (3) extraneous factors. Following our empirical findings, we derive a conceptual model for differentiating toxicity from mildly annoying and more severe behaviors. The discovered view of toxicity challenges the prevailing paradigm of treating players toxic behavior as a monolithic construct with a linear intensity spectrum. Instead, we advocate for a more granular approach, encouraging an understanding of the underlying dynamics behind negative online behaviors.",,,,, ,  Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems,"Gamer communities, Gamification, Online games, Player behavior, Toxicity",detection,
2918,"**Title**Optimizing Language Model-based Algorithms for Enhanced Content Moderation and Toxicity Classification Using Twitter Data

**Abstract**The accuracy of toxicity classification has become increasingly important with the growth of the Internet and the rise of hate speech and other harmful online content. In recent years, deep learning and advanced algorithms have been used to improve the effectiveness of these models. This research paper focused on improving the accuracy of toxicity detection through a language model-based algorithm optimization. The study employed BERT for classifying toxic comments with the help of large-scale toxicity-centered dataset(s). The study showed that the use of optimized language model-based algorithms outperforms traditional rule-based algorithms, leading to a reduced number of type I and type II errors. The researchers proposed the use of a toxicity-based model that highlights the potential of language models in improving the accuracy of profanity filters and mitigating the spread of hate speech and other harmful online content.","Tomas, John Paul Q., Carlos, Kyle Andrei C., Ebora, Jan Russell O., Garin, David Ezekiel A.",,,Optimizing Language Model-based Algorithms for Enhanced Content Moderation and Toxicity Classification Using Twitter Data,,,10.1145/3686812.3686817 , ,,"The accuracy of toxicity classification has become increasingly important with the growth of the Internet and the rise of hate speech and other harmful online content. In recent years, deep learning and advanced algorithms have been used to improve the effectiveness of these models. This research paper focused on improving the accuracy of toxicity detection through a language model-based algorithm optimization. The study employed BERT for classifying toxic comments with the help of large-scale toxicity-centered dataset(s). The study showed that the use of optimized language model-based algorithms outperforms traditional rule-based algorithms, leading to a reduced number of type I and type II errors. The researchers proposed the use of a toxicity-based model that highlights the potential of language models in improving the accuracy of profanity filters and mitigating the spread of hate speech and other harmful online content.",,,,, ,  Proceedings of the 2024 16th International Conference on Computer Modeling and Simulation,"Hate speech, Language models, Machine learning",detection,
2922,"**Title**How To Tame a Toxic Player? A Systematic Literature Review on Intervention Systems for Toxic Behaviors in Online Video Games

**Abstract**Toxic behavior is known to cause harm in online games. Players regularly experience negative, hateful, or inappropriate behavior. Interventions, such as banning players or chat message filtering, can help combat toxicity but are not widely available or even comprehensively studied regarding their approaches and evaluations. We conducted a systematic literature review that provides insights into the current state of interventions literature, outlining their strengths and shortcomings. We identified 36 interventions and qualitatively analyzed their approaches. We describe the types of toxicity being addressed, the entities through which they act, the methods used by intervention systems, and how they are evaluated. Our results provide guidance for future interventions, outlining a design space based on known systems. Furthermore, our findings highlight gaps in the literature, e.g., a sparsity of empirical evaluations, and underexplored areas in the design space, enabling researchers to explore novel directions for future interventions.","Wijkstra, Michel, Rogers, Katja, Mandryk, Regan L., Veltkamp, Remco C., Frommel, Julian",,,How To Tame a Toxic Player? A Systematic Literature Review on Intervention Systems for Toxic Behaviors in Online Video Games,,,10.1145/3677080 , ,,"Toxic behavior is known to cause harm in online games. Players regularly experience negative, hateful, or inappropriate behavior. Interventions, such as banning players or chat message filtering, can help combat toxicity but are not widely available or even comprehensively studied regarding their approaches and evaluations. We conducted a systematic literature review that provides insights into the current state of interventions literature, outlining their strengths and shortcomings. We identified 36 interventions and qualitatively analyzed their approaches. We describe the types of toxicity being addressed, the entities through which they act, the methods used by intervention systems, and how they are evaluated. Our results provide guidance for future interventions, outlining a design space based on known systems. Furthermore, our findings highlight gaps in the literature, e.g., a sparsity of empirical evaluations, and underexplored areas in the design space, enabling researchers to explore novel directions for future interventions.",,,,, ,  ,"interventions, online games, systematic literature review, toxicity",detection#survey,
2924,"**Title**HOT ChatGPT: The Promise of ChatGPT in Detecting and Discriminating Hateful, Offensive, and Toxic Comments on Social Media

**Abstract**Harmful textual content is pervasive on social media, poisoning online communities and negatively impacting participation. A common approach to this issue is developing detection models that rely on human annotations. However, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. Generative AI models have the potential to understand and detect harmful textual content. We used ChatGPT to investigate this potential and compared its performance with MTurker annotations for three frequently discussed concepts related to harmful textual content on social media: Hateful, Offensive, and Toxic (HOT). We designed five prompts to interact with ChatGPT and conducted four experiments eliciting HOT classifications. Our results show that ChatGPT can achieve an accuracy of approximately 80% when compared to MTurker annotations. Specifically, the model displays a more consistent classification for non-HOT comments than HOT comments compared to human annotations. Our findings also suggest that ChatGPT classifications align with the provided HOT definitions. However, ChatGPT classifies hateful and offensive as subsets of toxic. Moreover, the choice of prompts used to interact with ChatGPT impacts its performance. Based on these insights, our study provides several meaningful implications for employing ChatGPT to detect HOT content, particularly regarding the reliability and consistency of its performance, its understanding and reasoning of the HOT concept, and the impact of prompts on its performance. Overall, our study provides guidance on the potential of using generative AI models for moderating large volumes of user-generated textual content on social media.","Li, Lingyao, Fan, Lizhou, Atreja, Shubham, Hemphill, Libby",,,"HOT ChatGPT: The Promise of ChatGPT in Detecting and Discriminating Hateful, Offensive, and Toxic Comments on Social Media",,,10.1145/3643829 , ,,"Harmful textual content is pervasive on social media, poisoning online communities and negatively impacting participation. A common approach to this issue is developing detection models that rely on human annotations. However, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. Generative AI models have the potential to understand and detect harmful textual content. We used ChatGPT to investigate this potential and compared its performance with MTurker annotations for three frequently discussed concepts related to harmful textual content on social media: Hateful, Offensive, and Toxic (HOT). We designed five prompts to interact with ChatGPT and conducted four experiments eliciting HOT classifications. Our results show that ChatGPT can achieve an accuracy of approximately 80% when compared to MTurker annotations. Specifically, the model displays a more consistent classification for non-HOT comments than HOT comments compared to human annotations. Our findings also suggest that ChatGPT classifications align with the provided HOT definitions. However, ChatGPT classifies hateful and offensive as subsets of toxic. Moreover, the choice of prompts used to interact with ChatGPT impacts its performance. Based on these insights, our study provides several meaningful implications for employing ChatGPT to detect HOT content, particularly regarding the reliability and consistency of its performance, its understanding and reasoning of the HOT concept, and the impact of prompts on its performance. Overall, our study provides guidance on the potential of using generative AI models for moderating large volumes of user-generated textual content on social media.",,,,, ,  ,"Generative AI, ChatGPT, hate speech, offensive language, online toxicity, MTurker annotation, prompt engineering",detection,
2926,"**Title**Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias

**Abstract**Trigger Warning: Profane Language, Slurs Content moderation on social media platforms shapes the dynamics of online discourse, influencing whose voices are amplified and whose are suppressed. Recent studies have raised concerns about the fairness of content moderation practices, particularly for aggressively flagging posts from transgender and non-binary individuals as toxic. In this study, we investigate the presence of bias in harmful speech classification of gender-queer dialect online, focusing specifically on the treatment of reclaimed slurs. We introduce a novel dataset, QueerReclaimLex, based on 109 curated templates exemplifying non-derogatory uses of LGBTQ+ slurs. Dataset instances are scored by gender-queer annotators for potential harm depending on additional context about speaker identity. We systematically evaluate the performance of five off-the-shelf language models in assessing the harm of these texts and explore the effectiveness of chain-of-thought prompting to teach large language models (LLMs) to leverage author identity context. We reveal a tendency for these models to inaccurately flag texts authored by gender-queer individuals as harmful. Strikingly, across all LLMs the performance is poorest for texts that show signs of being written by individuals targeted by the featured slur (F1  0.24). We highlight an urgent need for fairness and inclusivity in content moderation systems. By uncovering these biases, this work aims to inform the development of more equitable content moderation practices and contribute to the creation of inclusive online spaces for all users.","Dorn, Rebecca, Kezar, Lee, Morstatter, Fred, Lerman, Kristina",,,Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias,,,10.1145/3689904.3694704 , ,,"Trigger Warning: Profane Language, Slurs Content moderation on social media platforms shapes the dynamics of online discourse, influencing whose voices are amplified and whose are suppressed. Recent studies have raised concerns about the fairness of content moderation practices, particularly for aggressively flagging posts from transgender and non-binary individuals as toxic. In this study, we investigate the presence of bias in harmful speech classification of gender-queer dialect online, focusing specifically on the treatment of reclaimed slurs. We introduce a novel dataset, QueerReclaimLex, based on 109 curated templates exemplifying non-derogatory uses of LGBTQ+ slurs. Dataset instances are scored by gender-queer annotators for potential harm depending on additional context about speaker identity. We systematically evaluate the performance of five off-the-shelf language models in assessing the harm of these texts and explore the effectiveness of chain-of-thought prompting to teach large language models (LLMs) to leverage author identity context. We reveal a tendency for these models to inaccurately flag texts authored by gender-queer individuals as harmful. Strikingly, across all LLMs the performance is poorest for texts that show signs of being written by individuals targeted by the featured slur (F1  0.24). We highlight an urgent need for fairness and inclusivity in content moderation systems. By uncovering these biases, this work aims to inform the development of more equitable content moderation practices and contribute to the creation of inclusive online spaces for all users.",,,,, ,"  Proceedings of the 4th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization","Chain-of-thought prompting, Content moderation, Gender identity, LGBTQ+, Online communities, Toxicity",detection,
2927,"**Title**Hate Speech Detection with Generalizable Target-aware Fairness

**Abstract**To counter the side effect brought by the proliferation of social media platforms, hate speech detection (HSD) plays a vital role in halting the dissemination of toxic online posts at an early stage. However, given the ubiquitous topical communities on social media, a trained HSD classifier can easily become biased towards specific targeted groups (e.g.,female andblack people), where a high rate of either false positive or false negative results can significantly impair public trust in the fairness of content moderation mechanisms, and eventually harm the diversity of online society. Although existing fairness-aware HSD methods can smooth out some discrepancies across targeted groups, they are mostly specific to a narrow selection of targets that are assumed to be known and fixed. This inevitably prevents those methods from generalizing to real-world use cases where new targeted groups constantly emerge (e.g., new forums created on Reddit) over time. To tackle the defects of existing HSD practices, we propose &lt;u&gt;Ge&lt;/u&gt;neralizable &lt;u&gt;t&lt;/u&gt;arget-aware &lt;u&gt;Fair&lt;/u&gt;ness (GetFair), a new method for fairly classifying each post that contains diverse and even unseen targets during inference. To remove the HSD classifier's spurious dependence on target-related features, GetFair trains a series of filter functions in an adversarial pipeline, so as to deceive the discriminator that recovers the targeted group from filtered post embeddings. To maintain scalability and generalizability, we innovatively parameterize all filter functions via a hypernetwork. Taking a target's pretrained word embedding as input, the hypernetwork generates the weights used by each target-specific filter on-the-fly without storing dedicated filter parameters. In addition, a novel semantic gap alignment scheme is imposed on the generation process, such that the produced filter function for an unseen target is rectified by its semantic affinity with existing targets used for training. Finally, experiments are conducted on two benchmark HSD datasets, showing advantageous performance of GetFair on out-of-sample targets among baselines.","Chen, Tong, Wang, Danny, Liang, Xurong, Risius, Marten, Demartini, Gianluca, Yin, Hongzhi",,,Hate Speech Detection with Generalizable Target-aware Fairness,,,10.1145/3637528.3671821 , ,,"To counter the side effect brought by the proliferation of social media platforms, hate speech detection (HSD) plays a vital role in halting the dissemination of toxic online posts at an early stage. However, given the ubiquitous topical communities on social media, a trained HSD classifier can easily become biased towards specific targeted groups (e.g.,female andblack people), where a high rate of either false positive or false negative results can significantly impair public trust in the fairness of content moderation mechanisms, and eventually harm the diversity of online society. Although existing fairness-aware HSD methods can smooth out some discrepancies across targeted groups, they are mostly specific to a narrow selection of targets that are assumed to be known and fixed. This inevitably prevents those methods from generalizing to real-world use cases where new targeted groups constantly emerge (e.g., new forums created on Reddit) over time. To tackle the defects of existing HSD practices, we propose &lt;u&gt;Ge&lt;/u&gt;neralizable &lt;u&gt;t&lt;/u&gt;arget-aware &lt;u&gt;Fair&lt;/u&gt;ness (GetFair), a new method for fairly classifying each post that contains diverse and even unseen targets during inference. To remove the HSD classifier's spurious dependence on target-related features, GetFair trains a series of filter functions in an adversarial pipeline, so as to deceive the discriminator that recovers the targeted group from filtered post embeddings. To maintain scalability and generalizability, we innovatively parameterize all filter functions via a hypernetwork. Taking a target's pretrained word embedding as input, the hypernetwork generates the weights used by each target-specific filter on-the-fly without storing dedicated filter parameters. In addition, a novel semantic gap alignment scheme is imposed on the generation process, such that the produced filter function for an unseen target is rectified by its semantic affinity with existing targets used for training. Finally, experiments are conducted on two benchmark HSD datasets, showing advantageous performance of GetFair on out-of-sample targets among baselines.",,,,, ,  Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,"data science for social good, debiased content moderation, hate speech detection, target-aware fairness",detection#methodology,
2929,"**Title**Topic enhanced word embedding for toxic content detection in Q&amp;A sites

**Abstract**Increasingly, users are adopting community question-and-answer (Q&amp;A) sites to exchange information. Detecting and eliminating toxic and divisive content in these Q&amp;A sites are paramount tasks to ensure a safe and constructive environment for the users. Insincere question, which is founded upon false premises, is one type of toxic content in Q&amp;A sites. In this paper, we proposed a novel deep learning framework enhanced pre-trained word embeddings with topical information for insincere question classification. We evaluated our proposed framework on a large real-world dataset from Quora Q&amp;A site and showed that the topically enhanced word embedding is able to achieve better results in toxic content classification. An empirical study was also conducted to analyze the topics of the insincere questions on Quora, and we found that topics on ""religion"", ""gender"" and ""politics"" has a higher proportion of insincere questions.","Kim, Do Yeon, Li, Xiaohang, Wang, Sheng, Zhuo, Yunying, Lee, Roy Ka-Wei",,,Topic enhanced word embedding for toxic content detection in Q&amp;A sites,,,10.1145/3341161.3345332 , ,,"Increasingly, users are adopting community question-and-answer (Q&amp;A) sites to exchange information. Detecting and eliminating toxic and divisive content in these Q&amp;A sites are paramount tasks to ensure a safe and constructive environment for the users. Insincere question, which is founded upon false premises, is one type of toxic content in Q&amp;A sites. In this paper, we proposed a novel deep learning framework enhanced pre-trained word embeddings with topical information for insincere question classification. We evaluated our proposed framework on a large real-world dataset from Quora Q&amp;A site and showed that the topically enhanced word embedding is able to achieve better results in toxic content classification. An empirical study was also conducted to analyze the topics of the insincere questions on Quora, and we found that topics on ""religion"", ""gender"" and ""politics"" has a higher proportion of insincere questions.",,,,, ,  Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining,"NLP, sequence model, text classification, toxic content, word embedding",detection,
2930,"**Title**""Did you miss my comment or what?"": understanding toxicity in open source discussions

**Abstract**Online toxicity is ubiquitous across the internet and its negative impact on the people and that online communities that it effects has been well documented. However, toxicity manifests differently on various platforms and toxicity in open source communities, while frequently discussed, is not well understood. We take a first stride at understanding the characteristics of open source toxicity to better inform future work on designing effective intervention and detection methods. To this end, we curate a sample of 100 toxic GitHub issue discussions combining multiple search and sampling strategies. We then qualitatively analyze the sample to gain an understanding of the characteristics of open-source toxicity. We find that the pervasive forms of toxicity in open source differ from those observed on other platforms like Reddit or Wikipedia. In our sample, some of the most prevalent forms of toxicity are entitled, demanding, and arrogant comments from project users as well as insults arising from technical disagreements. In addition, not all toxicity was written by people external to the projects; project members were also common authors of toxicity. We also discuss the implications of our findings. Among others we hope that our findings will be useful for future detection work.","Miller, Courtney, Cohen, Sophie, Klug, Daniel, Vasilescu, Bogdan, KaUstner, Christian",,,"""Did you miss my comment or what?"": understanding toxicity in open source discussions",,,10.1145/3510003.3510111 , ,,"Online toxicity is ubiquitous across the internet and its negative impact on the people and that online communities that it effects has been well documented. However, toxicity manifests differently on various platforms and toxicity in open source communities, while frequently discussed, is not well understood. We take a first stride at understanding the characteristics of open source toxicity to better inform future work on designing effective intervention and detection methods. To this end, we curate a sample of 100 toxic GitHub issue discussions combining multiple search and sampling strategies. We then qualitatively analyze the sample to gain an understanding of the characteristics of open-source toxicity. We find that the pervasive forms of toxicity in open source differ from those observed on other platforms like Reddit or Wikipedia. In our sample, some of the most prevalent forms of toxicity are entitled, demanding, and arrogant comments from project users as well as insults arising from technical disagreements. In addition, not all toxicity was written by people external to the projects; project members were also common authors of toxicity. We also discuss the implications of our findings. Among others we hope that our findings will be useful for future detection work.",,,,, ,  Proceedings of the 44th International Conference on Software Engineering,,detection,
2936,"**Title**Recourse for Reclamation: Chatting with Generative Language Models

**Abstract**Researchers and developers increasingly rely on toxicity scoring to automate generative language model outputs, in settings such as customer service, information retrieval, and content generation. However, toxicity scoring may render pertinent information inaccessible, rigidify or value-lock cultural norms, and prevent language reclamation processes, particularly for marginalized people. In this work, we extend the concept of algorithmic recourse to generative language models: we provide users a novel mechanism to achieve their desired prediction by dynamically setting thresholds for toxicity filtering. Users thereby exercise increased agency relative to interactions with the baseline system. A pilot study (n = 30) supports the potential of our proposed recourse mechanism, indicating improvements in usability compared to fixed-threshold toxicity-filtering of model outputs. Future work should explore the intersection of toxicity scoring, model controllability, user agency, and language reclamation processesparticularly with regard to the bias that many communities encounter when interacting with generative language models.","Chien, Jennifer, Mckee, Kevin, Kay, Jackie, Isaac, William",,,Recourse for Reclamation: Chatting with Generative Language Models,,,10.1145/3613905.3650999 , ,,"Researchers and developers increasingly rely on toxicity scoring to automate generative language model outputs, in settings such as customer service, information retrieval, and content generation. However, toxicity scoring may render pertinent information inaccessible, rigidify or value-lock cultural norms, and prevent language reclamation processes, particularly for marginalized people. In this work, we extend the concept of algorithmic recourse to generative language models: we provide users a novel mechanism to achieve their desired prediction by dynamically setting thresholds for toxicity filtering. Users thereby exercise increased agency relative to interactions with the baseline system. A pilot study (n = 30) supports the potential of our proposed recourse mechanism, indicating improvements in usability compared to fixed-threshold toxicity-filtering of model outputs. Future work should explore the intersection of toxicity scoring, model controllability, user agency, and language reclamation processesparticularly with regard to the bias that many communities encounter when interacting with generative language models.",,,,, ,  Extended Abstracts of the CHI Conference on Human Factors in Computing Systems,"Algorithmic recourse, Generative language models, Language reclamation, Toxicity scoring",detox,
2937,"**Title**Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective

**Abstract**The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this technology can cause unintended harms, such as the silencing of under-represented groups. We review a large body of NLP research on automatic abuse detection with a new focus on ethical challenges, organized around eight established ethical principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. In many cases, these principles relate not only to situational ethical codes, which may be context-dependent, but are in fact connected to universal human rights, such as the right to privacy, freedom from discrimination, and freedom of expression. We highlight the need to examine the broad social impacts of this technology, and to bring ethical and human rights considerations to every stage of the application life-cycle, from task formulation and dataset design, to model training and evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including nudging, quarantining, value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including 'nudging', 'quarantining', value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.","Kiritchenko, Svetlana, Nejadgholi, Isar, Fraser, Kathleen C.",,,Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective,,,10.1613/jair.1.12590 , ,,"The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this technology can cause unintended harms, such as the silencing of under-represented groups. We review a large body of NLP research on automatic abuse detection with a new focus on ethical challenges, organized around eight established ethical principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. In many cases, these principles relate not only to situational ethical codes, which may be context-dependent, but are in fact connected to universal human rights, such as the right to privacy, freedom from discrimination, and freedom of expression. We highlight the need to examine the broad social impacts of this technology, and to bring ethical and human rights considerations to every stage of the application life-cycle, from task formulation and dataset design, to model training and evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including nudging, quarantining, value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including 'nudging', 'quarantining', value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.",,,,, ,  ,natural language,survey,
2938,"**Title**DistillSeq: A Framework for Safety Alignment Testing in Large Language Models using Knowledge Distillation

**Abstract**Large Language Models (LLMs) have showcased their remarkable capabilities in diverse domains, encompassing natural language understanding, translation, and even code generation. The potential for LLMs to generate harmful content is a significant concern. This risk necessitates rigorous testing and comprehensive evaluation of LLMs to ensure safe and responsible use. However, extensive testing of LLMs requires substantial computational resources, making it an expensive endeavor. Therefore, exploring cost-saving strategies during the testing phase is crucial to balance the need for thorough evaluation with the constraints of resource availability. To address this, our approach begins by transferring the moderation knowledge from an LLM to a small model. Subsequently, we deploy two distinct strategies for generating malicious queries: one based on a syntax tree approach, and the other leveraging an LLM-based method. Finally, our approach incorporates a sequential filter-test process designed to identify test cases that are prone to eliciting toxic responses. By doing so, we significantly curtail unnecessary or unproductive interactions with LLMs, thereby streamlining the testing process. Our research evaluated the efficacy of DistillSeq across four LLMs: GPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the observed attack success rates on these LLMs stood at 31.5% for GPT-3.5, 21.4% for GPT-4.0, 28.3% for Vicuna-13B, and 30.9% for Llama-13B. However, upon the application of DistillSeq, these success rates notably increased to 58.5%, 50.7%, 52.5%, and 54.4%, respectively. This translated to an average escalation in attack success rate by a factor of 93.0% when compared to scenarios without the use of DistillSeq. Such findings highlight the significant enhancement DistillSeq offers in terms of reducing the time and resource investment required for effectively testing LLMs.","Yang, Mingke, Chen, Yuqi, Liu, Yi, Shi, Ling",,,DistillSeq: A Framework for Safety Alignment Testing in Large Language Models using Knowledge Distillation,,,10.1145/3650212.3680304 , ,,"Large Language Models (LLMs) have showcased their remarkable capabilities in diverse domains, encompassing natural language understanding, translation, and even code generation. The potential for LLMs to generate harmful content is a significant concern. This risk necessitates rigorous testing and comprehensive evaluation of LLMs to ensure safe and responsible use. However, extensive testing of LLMs requires substantial computational resources, making it an expensive endeavor. Therefore, exploring cost-saving strategies during the testing phase is crucial to balance the need for thorough evaluation with the constraints of resource availability. To address this, our approach begins by transferring the moderation knowledge from an LLM to a small model. Subsequently, we deploy two distinct strategies for generating malicious queries: one based on a syntax tree approach, and the other leveraging an LLM-based method. Finally, our approach incorporates a sequential filter-test process designed to identify test cases that are prone to eliciting toxic responses. By doing so, we significantly curtail unnecessary or unproductive interactions with LLMs, thereby streamlining the testing process. Our research evaluated the efficacy of DistillSeq across four LLMs: GPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the observed attack success rates on these LLMs stood at 31.5% for GPT-3.5, 21.4% for GPT-4.0, 28.3% for Vicuna-13B, and 30.9% for Llama-13B. However, upon the application of DistillSeq, these success rates notably increased to 58.5%, 50.7%, 52.5%, and 54.4%, respectively. This translated to an average escalation in attack success rate by a factor of 93.0% when compared to scenarios without the use of DistillSeq. Such findings highlight the significant enhancement DistillSeq offers in terms of reducing the time and resource investment required for effectively testing LLMs.",,,,, ,  Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis,"Automated Testing, Knowledge Distillation, Large Language Models",detox,
2940,"**Title**""I wouldnt say offensive but..."": Disability-Centered Perspectives on Large Language Models

**Abstract**Large language models (LLMs) trained on real-world data can inadvertently reflect harmful societal biases, particularly toward historically marginalized communities. While previous work has primarily focused on harms related to age and race, emerging research has shown that biases toward disabled communities exist. This study extends prior work exploring the existence of harms by identifying categories of LLM-perpetuated harms toward the disability community. We conducted 19 focus groups, during which 56 participants with disabilities probed a dialog model about disability and discussed and annotated its responses. Participants rarely characterized model outputs as blatantly offensive or toxic. Instead, participants used nuanced language to detail how the dialog model mirrored subtle yet harmful stereotypes they encountered in their lives and dominant media, e.g., inspiration porn and able-bodied saviors. Participants often implicated training data as a cause for these stereotypes and recommended training the model on diverse identities from disability-positive resources. Our discussion further explores representative data strategies to mitigate harm related to different communities through annotation co-design with ML researchers and developers.","Gadiraju, Vinitha, Kane, Shaun, Dev, Sunipa, Taylor, Alex, Wang, Ding, Denton, Emily, Brewer, Robin",,,"""I wouldnt say offensive but..."": Disability-Centered Perspectives on Large Language Models",,,10.1145/3593013.3593989 , ,,"Large language models (LLMs) trained on real-world data can inadvertently reflect harmful societal biases, particularly toward historically marginalized communities. While previous work has primarily focused on harms related to age and race, emerging research has shown that biases toward disabled communities exist. This study extends prior work exploring the existence of harms by identifying categories of LLM-perpetuated harms toward the disability community. We conducted 19 focus groups, during which 56 participants with disabilities probed a dialog model about disability and discussed and annotated its responses. Participants rarely characterized model outputs as blatantly offensive or toxic. Instead, participants used nuanced language to detail how the dialog model mirrored subtle yet harmful stereotypes they encountered in their lives and dominant media, e.g., inspiration porn and able-bodied saviors. Participants often implicated training data as a cause for these stereotypes and recommended training the model on diverse identities from disability-positive resources. Our discussion further explores representative data strategies to mitigate harm related to different communities through annotation co-design with ML researchers and developers.",,,,, ,"  Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency","algorithmic harms, artificial intelligence, chatbot, data annotation, dialog model, disability representation, large language models, qualitative",detection,
2941,"**Title**Im fully who I am: Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation

**Abstract**Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.","Ovalle, Anaelia, Goyal, Palash, Dhamala, Jwala, Jaggers, Zachary, Chang, Kai-Wei, Galstyan, Aram, Zemel, Richard, Gupta, Rahul",,,Im fully who I am: Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation,,,10.1145/3593013.3594078 , ,,"Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.",,,,, ,"  Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency","AI Fairness Auditing, Algorithmic Fairness, Natural Language Generation, Queer Harms in AI",detection,
2943,"**Title**Measuring and Characterizing Hate Speech on News&nbsp;Websites

**Abstract**The Web has become the main source for news acquisition. At the same time, news discussion has become more social: users can post comments on news articles or discuss news articles on other platforms like Reddit. These features empower and enable discussions among the users; however, they also act as the medium for the dissemination of toxic discourse and hate speech. The research community lacks a general understanding on what type of content attracts hateful discourse and the possible effects of social networks on the commenting activity on news articles. In this work, we perform a large-scale quantitative analysis of 125M comments posted on 412K news articles over the course of 19 months. We analyze the content of the collected articles and their comments using temporal analysis, user-based analysis, and linguistic analysis, to shed light on what elements attract hateful comments on news articles. We also investigate commenting activity when an article is posted on either 4chans Politically Incorrect board (/pol/) or six selected subreddits. We find statistically significant increases in hateful commenting activity around real-world divisive events like the Unite the Right rally in Charlottesville and political events like the second and third 2016 US presidential debates. Also, we find that articles that attract a substantial number of hateful comments have different linguistic characteristics when compared to articles that do not attract hateful comments. Furthermore, we observe that the post of a news articles on either /pol/ or the six subreddits is correlated with an increase of (hateful) commenting activity on the news articles.","Zannettou, Savvas, Elsherief, Mai, Belding, Elizabeth, Nilizadeh, Shirin, Stringhini, Gianluca",,,Measuring and Characterizing Hate Speech on News&nbsp;Websites,,,10.1145/3394231.3397902 , ,,"The Web has become the main source for news acquisition. At the same time, news discussion has become more social: users can post comments on news articles or discuss news articles on other platforms like Reddit. These features empower and enable discussions among the users; however, they also act as the medium for the dissemination of toxic discourse and hate speech. The research community lacks a general understanding on what type of content attracts hateful discourse and the possible effects of social networks on the commenting activity on news articles. In this work, we perform a large-scale quantitative analysis of 125M comments posted on 412K news articles over the course of 19 months. We analyze the content of the collected articles and their comments using temporal analysis, user-based analysis, and linguistic analysis, to shed light on what elements attract hateful comments on news articles. We also investigate commenting activity when an article is posted on either 4chans Politically Incorrect board (/pol/) or six selected subreddits. We find statistically significant increases in hateful commenting activity around real-world divisive events like the Unite the Right rally in Charlottesville and political events like the second and third 2016 US presidential debates. Also, we find that articles that attract a substantial number of hateful comments have different linguistic characteristics when compared to articles that do not attract hateful comments. Furthermore, we observe that the post of a news articles on either /pol/ or the six subreddits is correlated with an increase of (hateful) commenting activity on the news articles.",,,,, ,  Proceedings of the 12th ACM Conference on Web Science,,detection,
2944,"**Title**Poster: Adversarial Examples for Hate Speech Classifiers

**Abstract**With the advent of the Internet, social media platforms have become an increasingly popular medium of communication for people. Platforms like Twitter and Quora allow people to express their opinions on a large scale. These platforms are, however, plagued by the problem of hate speech and toxic content. Such content is generally sexist, homophobic or racist. Automatic text classification can filter out toxic content so some extent. In this paper, we discuss the adversarial attacks on hate speech classifiers. We demonstrate that by changing the text slightly, a classifier can be fooled to misclassifying a toxic comment as acceptable. We attack hate speech classifiers with known attacks as well as introduce four new attacks. We find that our method can degrade the performance of a Random Forest classifier by 20%. We hope that our work sheds light on the vulnerabilities of text classifiers, and opens doors for further research on this topic.","Oak, Rajvardhan",,,Poster: Adversarial Examples for Hate Speech Classifiers,,,10.1145/3319535.3363271 , ,,"With the advent of the Internet, social media platforms have become an increasingly popular medium of communication for people. Platforms like Twitter and Quora allow people to express their opinions on a large scale. These platforms are, however, plagued by the problem of hate speech and toxic content. Such content is generally sexist, homophobic or racist. Automatic text classification can filter out toxic content so some extent. In this paper, we discuss the adversarial attacks on hate speech classifiers. We demonstrate that by changing the text slightly, a classifier can be fooled to misclassifying a toxic comment as acceptable. We attack hate speech classifiers with known attacks as well as introduce four new attacks. We find that our method can degrade the performance of a Random Forest classifier by 20%. We hope that our work sheds light on the vulnerabilities of text classifiers, and opens doors for further research on this topic.",,,,, ,  Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security,"adversarial machine learning, hate speech",detection,
2945,"**Title**In-Context Learning Reward Guided Decoding for Controlled Text Generation

**Abstract**While large language models have demonstrated remarkable text generation capabilities, they often generate text with adverse or undesired attributes. Common approaches to control text generation involve refining models on data with desired properties or guiding language models decoding using an auxiliary model. However, these methods require additional training and extensive attribute-specific data. To further mitigate the training costs, we propose In-context learning Reward Guided Decoding (IRGD), a weighted decoding method that exploits the in-context learning ability of language models as an alternative to additional model fine-tuning. Specifically, IRGD utilizes ICL outputs to score the alignment reward between sequences and target attributes, subsequently modifying the sampling probabilities to favor tokens with higher reward scores. By applying ICL, IRGD adapts to different tasks by simply adjusting task descriptions and demonstration rather than fine-tuning the model. Through experiments on detoxification and sentiment control, we demonstrate the advantages of IRGD as a plug-and-play and fine-tuning-free decoding method that effectively balance attribute alignment and text quality.","Zhu, Xinyi, Zhou, Yanru, Song, Dandan, Yang, Ziyi",,,In-Context Learning Reward Guided Decoding for Controlled Text Generation,,,10.1109/ICSP62122.2024.10743861 , ,,"While large language models have demonstrated remarkable text generation capabilities, they often generate text with adverse or undesired attributes. Common approaches to control text generation involve refining models on data with desired properties or guiding language models decoding using an auxiliary model. However, these methods require additional training and extensive attribute-specific data. To further mitigate the training costs, we propose In-context learning Reward Guided Decoding (IRGD), a weighted decoding method that exploits the in-context learning ability of language models as an alternative to additional model fine-tuning. Specifically, IRGD utilizes ICL outputs to score the alignment reward between sequences and target attributes, subsequently modifying the sampling probabilities to favor tokens with higher reward scores. By applying ICL, IRGD adapts to different tasks by simply adjusting task descriptions and demonstration rather than fine-tuning the model. Through experiments on detoxification and sentiment control, we demonstrate the advantages of IRGD as a plug-and-play and fine-tuning-free decoding method that effectively balance attribute alignment and text quality.",,,,, ,  2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP),Training;Adaptation models;Costs;Large language models;Refining;Signal processing;Data models;Decoding;controlled text generation;weighted decoding;in-context learning,detox,
2956,"**Title**STopHC: A Harmful Content Detection and Mitigation Architecture for Social Media Platforms

**Abstract**The mental health of social media users has started more and more to be put at risk by harmful, hateful, and offensive content. In this paper, we propose STOPHC, a harmful content detection and mitigation architecture for social media platforms. Our aim with STOPHC is to create more secure online environments. Our solution contains two modules, one that employs deep neural network architecture for harmful content detection, and one that uses a network immunization algorithm to block toxic nodes and stop the spread of harmful content. The efficacy of our solution is demonstrated by experiments conducted on two real-world datasets.","Truic, Ciprian-Octavian, Constantinescu, Ana-Teodora, Apostol, Elena-Simona",,,STopHC: A Harmful Content Detection and Mitigation Architecture for Social Media Platforms,,,10.1109/ICCP63557.2024.10793051 , ,,"The mental health of social media users has started more and more to be put at risk by harmful, hateful, and offensive content. In this paper, we propose STOPHC, a harmful content detection and mitigation architecture for social media platforms. Our aim with STOPHC is to create more secure online environments. Our solution contains two modules, one that employs deep neural network architecture for harmful content detection, and one that uses a network immunization algorithm to block toxic nodes and stop the spread of harmful content. The efficacy of our solution is demonstrated by experiments conducted on two real-world datasets.",,,,, ,  2024 IEEE 20th International Conference on Intelligent Computer Communication and Processing (ICCP),Social networking (online);Prevention and mitigation;Computer architecture;Mental health;Artificial neural networks;Media;Harmful Content Detection;Harmful Content Mitigation;Social Media Analysis;Deep Neural Networks;Network Immunization,detection,
2958,"**Title**Forecasting the Spread of Toxicity on Twitter

**Abstract**In this paper, we explore the question of whether it is possible to forecast the spread of hate on Twitter. Unlike most prior work which models the spread of Twitter over a network with the goal of predicting the future state of a user (typically, as being hateful or not), here we are interested in how hateful (or toxic) the network as a whole becomes. We pose toxicity spread as a forecasting problem, and use ARIMA to find out whether the spread is forecastable. We find that toxicity spread is indeed forecasted by ARIMA. Given that it is forecastable, we ask two follow-up questions: (a) How well can we forecast it? and (b) What role, if any, does the structure of the retweet network play in forecasting? In order to answer these questions, we employ several techniques including Spatio-Temporal Graph Convolution Network (STGCN) and several variants of transformers. To determine the role that structure might play, we re-purpose the dataset in three ways: the network as a whole (the structure is ignored), communities interconnected with each other, and neighbourhoods of a set of individuals. Experiments with the network as a whole informs us how well we can forecast hate spread at a global level, while the latter experiments tell us whether and how network effects affect the forecasting. In an effort to tease out the effect of the network, we use two distinct techniques: STGCN, which requires the explicit connections in the form of a graph as input; and Transformers, where no explicit graph is given as input. Instead, we pose it as a multivariate analysis problem. We find that the PatchTST transformer performs the best at all levels. STGCN performs better than ARIMA suggesting that network structure matters. Somewhat interestingly, STGCN does not perform as well as PatchTST, suggesting that (a) PatchTST is able to implicitly learn the associations (flow of influence), and (b) the presence of explicit connections may not always imply influence.","Vaidya, Aatman, Nagar, Seema, Nanavati, Amit A.",,,Forecasting the Spread of Toxicity on Twitter,,,10.1109/CogMI58952.2023.00038 , ,,"In this paper, we explore the question of whether it is possible to forecast the spread of hate on Twitter. Unlike most prior work which models the spread of Twitter over a network with the goal of predicting the future state of a user (typically, as being hateful or not), here we are interested in how hateful (or toxic) the network as a whole becomes. We pose toxicity spread as a forecasting problem, and use ARIMA to find out whether the spread is forecastable. We find that toxicity spread is indeed forecasted by ARIMA. Given that it is forecastable, we ask two follow-up questions: (a) How well can we forecast it? and (b) What role, if any, does the structure of the retweet network play in forecasting? In order to answer these questions, we employ several techniques including Spatio-Temporal Graph Convolution Network (STGCN) and several variants of transformers. To determine the role that structure might play, we re-purpose the dataset in three ways: the network as a whole (the structure is ignored), communities interconnected with each other, and neighbourhoods of a set of individuals. Experiments with the network as a whole informs us how well we can forecast hate spread at a global level, while the latter experiments tell us whether and how network effects affect the forecasting. In an effort to tease out the effect of the network, we use two distinct techniques: STGCN, which requires the explicit connections in the form of a graph as input; and Transformers, where no explicit graph is given as input. Instead, we pose it as a multivariate analysis problem. We find that the PatchTST transformer performs the best at all levels. STGCN performs better than ARIMA suggesting that network structure matters. Somewhat interestingly, STGCN does not perform as well as PatchTST, suggesting that (a) PatchTST is able to implicitly learn the associations (flow of influence), and (b) the presence of explicit connections may not always imply influence.",,,,, ,  2023 IEEE 5th International Conference on Cognitive Machine Intelligence (CogMI),Toxicology;Social networking (online);Blogs;Time series analysis;Predictive models;Transformers;Forecasting;Forecasting;Hate Speech;Hate Spread;Social Network Analysis;Time Series,detection,
2964,"**Title**Multilingual Toxic Comment Classification using Deep Learning

**Abstract**Toxic comments can have a profoundly negative impact on online communities, fostering environments that feel unsafe, unwelcoming, and discouraging for participation. The task of toxic comment classification focuses on identifying and categorizing harmful or disruptive comments within online discussions. Despite extensive research in this area, several challenges persist, such as Multilingual Toxicity Detection, Data Imbalance, and Interpretability. Existing studies have categorized toxic comments into six distinct labels: toxic, severely toxic, obscene, threat, insult, and identity hate. Traditional machine learning models like SVM have achieved an accuracy of 87%, while more advanced models like BERT have reached up to 98.3%. The goal of this proposed work is to improve the identification of toxic comments across multiple social media platforms, particularly in datasets containing entries in multiple languages. To address this, deep learning models such as XLM-RoBERTa and BERT are employed to detect the language of the comment and classify it as either toxic or non-toxic. Each models performance will be evaluated primarily using Accuracy metrics, providing a basis for comparison to determine the most effective approach for multilingual toxic comment detection. High accuracy can be achieved through careful dataset preprocessing, advanced model architectures, and hyperparameter tuning. Performance can be improved by fine-tuning model hyperparameters and employing better feature extraction techniques. More clarity is required in explaining the rationale behind selecting particular models.","Venugopal, N. L. V., Kanchanamala, P, Muppidi, Satish, Prakash, T. Bhanu, Neelima, T., Devi, S Anjali",,,Multilingual Toxic Comment Classification using Deep Learning,,,10.1109/ICSSAS64001.2024.10760913 , ,,"Toxic comments can have a profoundly negative impact on online communities, fostering environments that feel unsafe, unwelcoming, and discouraging for participation. The task of toxic comment classification focuses on identifying and categorizing harmful or disruptive comments within online discussions. Despite extensive research in this area, several challenges persist, such as Multilingual Toxicity Detection, Data Imbalance, and Interpretability. Existing studies have categorized toxic comments into six distinct labels: toxic, severely toxic, obscene, threat, insult, and identity hate. Traditional machine learning models like SVM have achieved an accuracy of 87%, while more advanced models like BERT have reached up to 98.3%. The goal of this proposed work is to improve the identification of toxic comments across multiple social media platforms, particularly in datasets containing entries in multiple languages. To address this, deep learning models such as XLM-RoBERTa and BERT are employed to detect the language of the comment and classify it as either toxic or non-toxic. Each models performance will be evaluated primarily using Accuracy metrics, providing a basis for comparison to determine the most effective approach for multilingual toxic comment detection. High accuracy can be achieved through careful dataset preprocessing, advanced model architectures, and hyperparameter tuning. Performance can be improved by fine-tuning model hyperparameters and employing better feature extraction techniques. More clarity is required in explaining the rationale behind selecting particular models.",,,,, ,  2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),Training;Deep learning;Adaptation models;Toxicology;Accuracy;Social networking (online);Bidirectional control;Transformers;Encoding;Tuning;Multilingual Toxicity Detection;Bidirectional Encoder Representations from Transformers (BERT);Toxic comment;Deep learning,detection,
2967,"**Title**Exploring BERT and Bi-LSTM for Toxic Comment Classification: A Comparative Analysis

**Abstract**This study analyzes on the classification of toxic comments in online conversations using advanced natural language processing (NLP) techniques. Leveraging advanced natural language processing (NLP) techniques and classification models, including BERT and Bi-LSTM models to classify comments into 6 types of toxicity: toxic, obscene, threat, insult, severe toxic and identity hate. The study achieves competitive performance. Specifically, fine-tuning BERT using TensorFlow and Hugging Face Transformers resulted in an AUC ROC rate of $98.23 \%$, while LSTM yielded a binary accuracy of $96.07 \%$. The results demonstrate the effectiveness of using transformer-based models like BERT for toxicity classification in text data. The study discusses the methodology, model architectures, and evaluation metrics, highlighting the effectiveness of each approach in identifying and classifying toxic language. Additionally, the paper discusses the implementation of a userfriendly interface for real-time toxic comment detection, leveraging the trained models for efficient moderation of online content.","Tarun, V G, Sivasakthivel, Ramkumar, Ramar, Gobinath, Rajagopal, Manikandan, Sivaraman, G.",,,Exploring BERT and Bi-LSTM for Toxic Comment Classification: A Comparative Analysis,,,10.1109/ICDSIS61070.2024.10594466 , ,,"This study analyzes on the classification of toxic comments in online conversations using advanced natural language processing (NLP) techniques. Leveraging advanced natural language processing (NLP) techniques and classification models, including BERT and Bi-LSTM models to classify comments into 6 types of toxicity: toxic, obscene, threat, insult, severe toxic and identity hate. The study achieves competitive performance. Specifically, fine-tuning BERT using TensorFlow and Hugging Face Transformers resulted in an AUC ROC rate of $98.23 \%$, while LSTM yielded a binary accuracy of $96.07 \%$. The results demonstrate the effectiveness of using transformer-based models like BERT for toxicity classification in text data. The study discusses the methodology, model architectures, and evaluation metrics, highlighting the effectiveness of each approach in identifying and classifying toxic language. Additionally, the paper discusses the implementation of a userfriendly interface for real-time toxic comment detection, leveraging the trained models for efficient moderation of online content.",,,,, ,  2024 Second International Conference on Data Science and Information System (ICDSIS),Accuracy;Toxicology;Computational modeling;Oral communication;Transformers;Natural language processing;Real-time systems;toxicity;BERT;bi-LSTM;natural language processing,detection,
2968,"**Title**Automated Toxic Chat Synthesis, Reporting and Removing the Chat in Telegram Social Media Using Natural Language Processing Techniques

**Abstract**This project seeks to address the pervasive challenge of toxic communication in the context of Telegram social media by deploying various Natural Language Processing (NLP) techniques. The classification framework developed for this endeavor is capable of discerning various forms of toxicity, including toxic, severe toxic, identity hate, obscene, threat, and insult. Beyond classification, the system incorporates a two-fold approach involving automated reporting and subsequent removal of identified toxic content. This comprehensive strategy not only empowers users to actively contribute to a healthier online discourse but also leverages the capabilities of NLP to enhance the overall user experience within the Telegram platform. The models employed for training include logistic regression, k-nearest neighbours (KNN), random forest, multinomial naive Bayes and Bidirectional Encoder Representations from Transformers (BERT). By tackling toxic communication at its root, this project aspires to foster an inclusive and respectful digital community, where users can engage in meaningful discussions without the pervasive influence of harmful language. The integration of these mechanisms into Telegram aims to create a seamless and efficient process for identifying, reporting, and removing toxic content, thereby promoting a more positive online environment.","A B, Abhijith, Prithvi, P.",,,"Automated Toxic Chat Synthesis, Reporting and Removing the Chat in Telegram Social Media Using Natural Language Processing Techniques",,,10.1109/ICAECT60202.2024.10469467 , ,,"This project seeks to address the pervasive challenge of toxic communication in the context of Telegram social media by deploying various Natural Language Processing (NLP) techniques. The classification framework developed for this endeavor is capable of discerning various forms of toxicity, including toxic, severe toxic, identity hate, obscene, threat, and insult. Beyond classification, the system incorporates a two-fold approach involving automated reporting and subsequent removal of identified toxic content. This comprehensive strategy not only empowers users to actively contribute to a healthier online discourse but also leverages the capabilities of NLP to enhance the overall user experience within the Telegram platform. The models employed for training include logistic regression, k-nearest neighbours (KNN), random forest, multinomial naive Bayes and Bidirectional Encoder Representations from Transformers (BERT). By tackling toxic communication at its root, this project aspires to foster an inclusive and respectful digital community, where users can engage in meaningful discussions without the pervasive influence of harmful language. The integration of these mechanisms into Telegram aims to create a seamless and efficient process for identifying, reporting, and removing toxic content, thereby promoting a more positive online environment.",,,,, ,"  2024 Fourth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",Training;Logistic regression;Social networking (online);Surveillance;Bidirectional control;Transformers;Natural language processing;Telegram Social Media;Natural Language Processing;Toxic Comment;Logistic Regression;k-nearest neighbours;random forest;multinomial nave Bayes;BERT,detection,
2970,"**Title**Sentiment Analysis Multi-Label of Toxic Comments using BERT-BiLSTM Methods

**Abstract**The number of individuals accessing the Internet rises steadily each year in Indonesia. One of the reasons behind this trend is the growing popularity of online platforms like Twitter, where people can freely share their thoughts and ideas. However, it is crucial to recognize that social media can occasionally serve as a breeding ground for negative behavior, such as the spread of toxic opinions. Additionally, more information will be collected as more people become active and share their views. Therefore, a technique capable of handling this textual data comprehensively, like classification, is needed. Simple categorization methods could be more effective since they split comments into positive and negative groups. Thus, to overcome such problems, multilevel classification assigns multiple labels to a single instance, allowing its categorization into different categories inside a single statement. This research uses a combination of BERT and BiLSTM methods, whereas BERT is used to obtain word vector values and will then be used as input in the BiLSTM model to perform multi-label classification tasks. This study uses two types of word vectors, the summation of the last four hidden layers and the final hidden layer of BERT, to create a better comparison model. The study achieved an accuracy of 0.889, precision of 0.925, recall of 0.917, and an F1 score of 0.91 for the model that used the last four hidden layers of BERT as word vectors","Putri, Syarifah Kemala, Amalia, Amalia, Abidin, Taufik Fuadi",,,Sentiment Analysis Multi-Label of Toxic Comments using BERT-BiLSTM Methods,,,10.1109/ICELTICs62730.2024.10776338 , ,,"The number of individuals accessing the Internet rises steadily each year in Indonesia. One of the reasons behind this trend is the growing popularity of online platforms like Twitter, where people can freely share their thoughts and ideas. However, it is crucial to recognize that social media can occasionally serve as a breeding ground for negative behavior, such as the spread of toxic opinions. Additionally, more information will be collected as more people become active and share their views. Therefore, a technique capable of handling this textual data comprehensively, like classification, is needed. Simple categorization methods could be more effective since they split comments into positive and negative groups. Thus, to overcome such problems, multilevel classification assigns multiple labels to a single instance, allowing its categorization into different categories inside a single statement. This research uses a combination of BERT and BiLSTM methods, whereas BERT is used to obtain word vector values and will then be used as input in the BiLSTM model to perform multi-label classification tasks. This study uses two types of word vectors, the summation of the last four hidden layers and the final hidden layer of BERT, to create a better comparison model. The study achieved an accuracy of 0.889, precision of 0.925, recall of 0.917, and an F1 score of 0.91 for the model that used the last four hidden layers of BERT as word vectors",,,,, ,  2024 International Conference on Electrical Engineering and Informatics (ICELTICs),Electrical engineering;Sentiment analysis;Social networking (online);Semantics;Blogs;Market research;Vectors;Encoding;Internet;Informatics;BiLSTM;BERT;Multi-label;Sentiment analysis,detection,
2971,"**Title**Identifying and Analyzing Toxic Behavior in Ecommerce Industry through Reddit Discussions Using BERT and HateBERT

**Abstract**This study begins by looking at how toxic behaviour in video games is talked about and identified. We set up a clear idea of what toxic behaviour means and suggested ways to figure out when its happening. To test these ideas, we scrape a huge collection of posts from Reddit where people talk about games. We cleaned up the datamaking sure it was all good for checking. We used Bidirectional Encoder Representations from Transformers (BERT) to look at toxic behaviour. We also used HateBERT to compare how well it works at spotting toxic stuff. To make sure our models were doing a good job, we looked at over 5500 posts and decided if they were toxic or not. This helped us see if the models were right or wrong. We also used measuresto compare how much toxic stuff is in Redditespecially for games. This helped us understand how common and different toxic behaviour is in different places.","Kumar, Anoop, Soni, Arpita, Arora, Rajeev, Panwar, Dheerendra",,,Identifying and Analyzing Toxic Behavior in Ecommerce Industry through Reddit Discussions Using BERT and HateBERT,,,10.1109/SPARC61891.2024.10829097 , ,,"This study begins by looking at how toxic behaviour in video games is talked about and identified. We set up a clear idea of what toxic behaviour means and suggested ways to figure out when its happening. To test these ideas, we scrape a huge collection of posts from Reddit where people talk about games. We cleaned up the datamaking sure it was all good for checking. We used Bidirectional Encoder Representations from Transformers (BERT) to look at toxic behaviour. We also used HateBERT to compare how well it works at spotting toxic stuff. To make sure our models were doing a good job, we looked at over 5500 posts and decided if they were toxic or not. This helped us see if the models were right or wrong. We also used measuresto compare how much toxic stuff is in Redditespecially for games. This helped us understand how common and different toxic behaviour is in different places.",,,,, ,  2024 International Conference on Signal Processing and Advance Research in Computing (SPARC),Industries;Video games;Social networking (online);Biological system modeling;Games;Bidirectional control;Signal processing;Transformers;Encoding;Electronic commerce;BERT;Games;HateBERT;Mean Behavior;Reddit,detection,
2972,"**Title**Detecting toxic comments from highly skewed social media data

**Abstract**A users online social media data, to a considerable extent, provides insight into the users activity. Screening user-generated data for negative content has a wide range of applications, like background checks of an employee and spotting of terror elements. Researchers have focused on identifying toxic text in social media by exploiting deep-learning models in conjunction with pre-trained language models. However, the availability of labelled toxic text is limited. In this work, we apply data augmentation techniques to address the problem of imbalanced training data. The augmented labelled data is used to fine-tune an ensemble. The ensemble consists of one linear classifier and three sequence classifiers, Bi-RNN (Bi-directional Recurrent Neural Networks), Bi-GRU (Bi-directional Gated Recurrent Unit), and Bi-LSTM (Bi-directional Long-Short Term Memory). We use BERT (Bi-directional Encoder Representations from Transformers) as our pre-trained language model. We also experiment with various text preprocessing techniques on the training data and how it affects classification performance. We compare our best-identified model with other toxic text detection frameworks available in the literature.","Datta, Abhradeep, Kumar, B Monish, Singh Sairam, Ashok",,,Detecting toxic comments from highly skewed social media data,,,10.1109/ANTS59832.2023.10469000 , ,,"A users online social media data, to a considerable extent, provides insight into the users activity. Screening user-generated data for negative content has a wide range of applications, like background checks of an employee and spotting of terror elements. Researchers have focused on identifying toxic text in social media by exploiting deep-learning models in conjunction with pre-trained language models. However, the availability of labelled toxic text is limited. In this work, we apply data augmentation techniques to address the problem of imbalanced training data. The augmented labelled data is used to fine-tune an ensemble. The ensemble consists of one linear classifier and three sequence classifiers, Bi-RNN (Bi-directional Recurrent Neural Networks), Bi-GRU (Bi-directional Gated Recurrent Unit), and Bi-LSTM (Bi-directional Long-Short Term Memory). We use BERT (Bi-directional Encoder Representations from Transformers) as our pre-trained language model. We also experiment with various text preprocessing techniques on the training data and how it affects classification performance. We compare our best-identified model with other toxic text detection frameworks available in the literature.",,,,, ,  2023 IEEE International Conference on Advanced Networks and Telecommunications Systems (ANTS),Recurrent neural networks;Social networking (online);Computational modeling;Text categorization;Training data;Text detection;Bidirectional control;toxic text detection;sentiment analysis;deep learning;social media;BERT,detection,
2974,"**Title**Toxic Chat Synthesis, Reporting and Removing in Social Media using Hybrid Deep Learning Model

**Abstract**Following the COVID-19 epidemic, social media usage has increased dramatically among a wide range of users, from young people to the elderly. These platforms are essential venues for people to share ideas, demonstrate their abilities, and voice their opinions. But in addition to these advantages, there has been a noticeable rise in the spread of offensive and vulgar information. This is especially concerning because there are groups who deliberately make insulting remarks that target women, children, and the elderly in an effort to stifle peaceful speech. Even though there are reporting procedures in place, the lengthy process of filing complaints deters many users from doing so. Our research provides an automatic method made to identify, report, and delete offensive communications from the Telegram social media platform in order to address this problem. Some previous work to classify comments is already present. But it is only on English language. In this work, we are extending this to Malayalam, Manglish (Malayalam language written with English letters), Mixed (mix of Malayalam and Manglish) and English Languages. Apart from the above two, we have created a Hybrid Deep Learning model combining CNN and RNN and compared its performance with already existing models like Logistic Regression, SVM, KNN and BERT. Our project aims to create an open and courteous online community where meaningful conversations can flourish without the constant influence of offensive language by tackling toxic communication at its source. This automatic approach is a big step in the direction of making Telegram a happier and more encouraging place for users to interact online.","Prithvi, P., B, Abhijith A",,,"Toxic Chat Synthesis, Reporting and Removing in Social Media using Hybrid Deep Learning Model",,,10.1109/ICICEC62498.2024.10808410 , ,,"Following the COVID-19 epidemic, social media usage has increased dramatically among a wide range of users, from young people to the elderly. These platforms are essential venues for people to share ideas, demonstrate their abilities, and voice their opinions. But in addition to these advantages, there has been a noticeable rise in the spread of offensive and vulgar information. This is especially concerning because there are groups who deliberately make insulting remarks that target women, children, and the elderly in an effort to stifle peaceful speech. Even though there are reporting procedures in place, the lengthy process of filing complaints deters many users from doing so. Our research provides an automatic method made to identify, report, and delete offensive communications from the Telegram social media platform in order to address this problem. Some previous work to classify comments is already present. But it is only on English language. In this work, we are extending this to Malayalam, Manglish (Malayalam language written with English letters), Mixed (mix of Malayalam and Manglish) and English Languages. Apart from the above two, we have created a Hybrid Deep Learning model combining CNN and RNN and compared its performance with already existing models like Logistic Regression, SVM, KNN and BERT. Our project aims to create an open and courteous online community where meaningful conversations can flourish without the constant influence of offensive language by tackling toxic communication at its source. This automatic approach is a big step in the direction of making Telegram a happier and more encouraging place for users to interact online.",,,,, ,"  2024 First International Conference on Innovations in Communications, Electrical and Computer Engineering (ICICEC)",Deep learning;Support vector machines;Technological innovation;Social networking (online);Computational modeling;Surveillance;Semantics;Oral communication;Nearest neighbor methods;Older adults;Telegram Social Media;Malayalam;Manglish;Hybrid Model;Natural Language Processing;Toxic Comment;Logistic Regression;SVM;KNN;CNN;RNN;BERT,detection,
2982,"**Title**Analyzing the Performance of Machine Learning and Deep Learning Models in Detecting Cyberbullying Comments

**Abstract**With the proliferation of social media platforms, there has been a concerning escalation in cyberbullying and abusive language. Detecting such toxic comments in large volumes of user-generated data presents challenges. This research conducts a comparative analysis to evaluate the impact of natural language processing techniques on identifying cyberbullying. Using a dataset of 1,59,000 comments labelled across seven classes of toxicity, conventional machine learning and deep learning models are benchmarked across performance metrics. The study aims to quantify algorithmic capabilities to accurately classify bullying comments. The study finds that Support Vectors with a linear kernel surpass Logistic Regression in accurately identifying toxic comments. For deep learning techniques, the transformer models (BERT and Distil-BERT) deliver the highest performance among neural network architectures tested. The empirical evaluation provides insights into leveraging computational linguistics for automating the detection of online bullying at scale.","Saim, Mohammad, Rizvi, Rehma Manaal, Kashif Khan, Mohammad",,,Analyzing the Performance of Machine Learning and Deep Learning Models in Detecting Cyberbullying Comments,,,10.1109/ICRASET59632.2023.10419938 , ,,"With the proliferation of social media platforms, there has been a concerning escalation in cyberbullying and abusive language. Detecting such toxic comments in large volumes of user-generated data presents challenges. This research conducts a comparative analysis to evaluate the impact of natural language processing techniques on identifying cyberbullying. Using a dataset of 1,59,000 comments labelled across seven classes of toxicity, conventional machine learning and deep learning models are benchmarked across performance metrics. The study aims to quantify algorithmic capabilities to accurately classify bullying comments. The study finds that Support Vectors with a linear kernel surpass Logistic Regression in accurately identifying toxic comments. For deep learning techniques, the transformer models (BERT and Distil-BERT) deliver the highest performance among neural network architectures tested. The empirical evaluation provides insights into leveraging computational linguistics for automating the detection of online bullying at scale.",,,,, ,  2023 International Conference on Recent Advances in Science and Engineering Technology (ICRASET),Deep learning;Measurement;Logistic regression;Computational modeling;Cyberbullying;Transformers;Tuning;Machine Learning;Natural Language Processing;Deep Learning;Cyberbullying;Classification,detection,
2987,"**Title**Stacked Bi-LSTM for Advanced Toxicity Detection in Comment Classification

**Abstract**The surge in Internet usage has revolutionized online forums, providing dynamic forums for active participation and meaningful debates. However, this has also exposed users to the risk of harassment. Efforts to address toxic comments in online forums have faced challenges, with current solutions unveiling unreliability. Advances in hardware, cloud computing, and natural language processing (NLP) enable the development of more robust approaches, predominantly NLP-based deep learning models. Pre-trained transformer models are gradually utilized for accurate toxic comment classification, bigger traditional methods. Stacked LSTM models, emphasizing hierarchical feature learning and improved contextual understanding, outperform single layer counterparts, offering an effective solution for online content moderation and sentiment analysis. This research indicates a significant stride towards more dependable and sophisticated toxic comment detection, addressing the challenges of social media effectively.","Kumar, N. Naveen, Abraham, Prabhakaran, Kaliba, Syed Ibrahim, VC, Diniesh, R, Maruthamuthu, Balamurugan, P, Seshu Kumar, Vekkudu, Naveen",,,Stacked Bi-LSTM for Advanced Toxicity Detection in Comment Classification,,,10.1109/IICAIET62352.2024.10729970 , ,,"The surge in Internet usage has revolutionized online forums, providing dynamic forums for active participation and meaningful debates. However, this has also exposed users to the risk of harassment. Efforts to address toxic comments in online forums have faced challenges, with current solutions unveiling unreliability. Advances in hardware, cloud computing, and natural language processing (NLP) enable the development of more robust approaches, predominantly NLP-based deep learning models. Pre-trained transformer models are gradually utilized for accurate toxic comment classification, bigger traditional methods. Stacked LSTM models, emphasizing hierarchical feature learning and improved contextual understanding, outperform single layer counterparts, offering an effective solution for online content moderation and sentiment analysis. This research indicates a significant stride towards more dependable and sophisticated toxic comment detection, addressing the challenges of social media effectively.",,,,, ,  2024 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET),Representation learning;Deep learning;Sentiment analysis;Toxicology;Social networking (online);Computational modeling;Transformers;Hardware;Surges;Long short term memory;Toxicity Detection;Natural Language Processing;Ensemble Learning;Text Preprocessing;Tokenization Introduction,detection,
3014,"**Title**DeTox: A WebApp for Toxic Comment Detection and Moderation

**Abstract**The extensive adoption of internet platforms such as YouTube has transformed communication and information exchange, compelling people to share their opinions and participate in global conversations. Open communication can, however, also encourage the spread of offensive material, such as remarks that are derogatory or involve threats or hate speech. Such offensive remarks have the potential to damage users' mental health by fostering a hostile and unsafe environment that discourages meaningful relationships. We introduce DeTox, a web application that uses machine learning techniques to detect and eliminate harmful comments from YouTube videos in order to address this problem. For the purpose of classifying comments, DeTox uses ML and DL models, which guarantees precise identification of harmful content. The YouTube Data API is integrated by the system to retrieve comments from specific videos and eliminate any harmful remarks found. A detailed examination of the Toxic Comment Classification Challenge dataset, made available by Kaggle, was necessary for the development of DeTox. To find common patterns in hazardous language, preprocessing and examination of the data were done in order to examine the distribution of toxic and non-toxic remarks. FastAPI is a high-level Python web framework that makes web application development easier and is used by the DeTox online application. The application includes a user friendly interface for creating accounts, logging in, and selecting videos to moderate. The application also includes a user interface for reviewing toxic comments and choosing to remove or keep them. DeTox is a valuable tool for moderating comments on YouTube videos. The application utilizes a machine learning model to accurately identify toxic comments, and it provides a vi user-friendly interface for removing or keeping the comments. DeTox has the potential to make YouTube a more friendly and safe environment for users.","N, Prudhvish, G, Nagarajan, U, Bharath Kumar, Vardhan B, Harsha, L, Tharun Kumar",,,DeTox: A WebApp for Toxic Comment Detection and Moderation,,,10.1109/TQCEBT59414.2024.10545229 , ,,"The extensive adoption of internet platforms such as YouTube has transformed communication and information exchange, compelling people to share their opinions and participate in global conversations. Open communication can, however, also encourage the spread of offensive material, such as remarks that are derogatory or involve threats or hate speech. Such offensive remarks have the potential to damage users' mental health by fostering a hostile and unsafe environment that discourages meaningful relationships. We introduce DeTox, a web application that uses machine learning techniques to detect and eliminate harmful comments from YouTube videos in order to address this problem. For the purpose of classifying comments, DeTox uses ML and DL models, which guarantees precise identification of harmful content. The YouTube Data API is integrated by the system to retrieve comments from specific videos and eliminate any harmful remarks found. A detailed examination of the Toxic Comment Classification Challenge dataset, made available by Kaggle, was necessary for the development of DeTox. To find common patterns in hazardous language, preprocessing and examination of the data were done in order to examine the distribution of toxic and non-toxic remarks. FastAPI is a high-level Python web framework that makes web application development easier and is used by the DeTox online application. The application includes a user friendly interface for creating accounts, logging in, and selecting videos to moderate. The application also includes a user interface for reviewing toxic comments and choosing to remove or keep them. DeTox is a valuable tool for moderating comments on YouTube videos. The application utilizes a machine learning model to accurately identify toxic comments, and it provides a vi user-friendly interface for removing or keeping the comments. DeTox has the potential to make YouTube a more friendly and safe environment for users.",,,,, ,  2024 International Conference on Trends in Quantum Computing and Emerging Business Technologies,Video on demand;Web page design;Web pages;Machine learning;User interfaces;Solids;Real-time systems;Toxic comments;Machine Learning;API;YOUTUBE;BERT,detection,
3015,"**Title**Exploring the Distinctive Tweeting Patterns of Toxic Twitter Users

**Abstract**In the pursuit of bolstering user safety, social media platforms deploy active moderation strategies, including content removal and user suspension. These measures target users engaged in discussions marked by hate speech or toxicity, often linked to specific keywords or hashtags. Nonetheless, the increasing prevalence of toxicity indicates that certain users adeptly circumvent these measures.This study examines consistently toxic users on Twitter (rebranded as X) Rather than relying on traditional methods based on specific topics or hashtags, we employ a novel approach based on patterns of toxic tweets, yielding deeper insights into their behavior.We analyzed 38 million tweets from the timelines of 12,148 Twitter users and identified the top 1,457 users who consistently exhibit toxic behavior, relying on metrics like the Gini index and Toxicity score. By comparing their posting patterns to those of non-consistently toxic users, we have uncovered distinctive temporal patterns, including contiguous activity spans, inter-tweet intervals (referred to as Burstiness), and churn analysis. These findings provide strong evidence for the existence of a unique tweeting pattern associated with toxic behavior on Twitter.Crucially, our methodology transcends Twitter and can be adapted to various social media platforms, facilitating the identification of consistently toxic users based on their posting behavior. This research contributes to ongoing efforts to combat online toxicity and offers insights for refining moderation strategies in the digital realm. We are committed to open research and will provide our code and data to the research community.","Qayyum, Hina, Ikram, Muhammad, Zhao, Benjamin Zi Hao, Wood, Ian D., Kourtellis, Nicolas, Kaafar, Mohamed Ali",,,Exploring the Distinctive Tweeting Patterns of Toxic Twitter Users,,,10.1109/BigData59044.2023.10386402 , ,,"In the pursuit of bolstering user safety, social media platforms deploy active moderation strategies, including content removal and user suspension. These measures target users engaged in discussions marked by hate speech or toxicity, often linked to specific keywords or hashtags. Nonetheless, the increasing prevalence of toxicity indicates that certain users adeptly circumvent these measures.This study examines consistently toxic users on Twitter (rebranded as X) Rather than relying on traditional methods based on specific topics or hashtags, we employ a novel approach based on patterns of toxic tweets, yielding deeper insights into their behavior.We analyzed 38 million tweets from the timelines of 12,148 Twitter users and identified the top 1,457 users who consistently exhibit toxic behavior, relying on metrics like the Gini index and Toxicity score. By comparing their posting patterns to those of non-consistently toxic users, we have uncovered distinctive temporal patterns, including contiguous activity spans, inter-tweet intervals (referred to as Burstiness), and churn analysis. These findings provide strong evidence for the existence of a unique tweeting pattern associated with toxic behavior on Twitter.Crucially, our methodology transcends Twitter and can be adapted to various social media platforms, facilitating the identification of consistently toxic users based on their posting behavior. This research contributes to ongoing efforts to combat online toxicity and offers insights for refining moderation strategies in the digital realm. We are committed to open research and will provide our code and data to the research community.",,,,, ,  2023 IEEE International Conference on Big Data (BigData),Measurement;Toxicology;Codes;Social networking (online);Blogs;Refining;Hate speech;Social media;toxicity;tweeting pattern;temporal analysis,methodology,
3016,"**Title**Domain Specific Embeddings in RNN Frameworks for Hate Span Detection and Classification

**Abstract**The unrestricted use of the internet has led to a significant rise in the dissemination of hate speech online, posing serious threats of harm and violence to individuals and society at large. Social media data often comprises informal and fragmented sentences, interspersed with multiple languages. While numerous researchers are dedicated to identifying and mitigating hate speech in social media content, existing approaches primarily focus on detecting hate speech as a whole, rather than targeting specific hateful words or phrases. To encourage research in this direction, a shared task was formulated during SemEval 2021, a task called Toxic Span Identification, to detect hateful words present in a sentence. Leveraging the dataset provided by the organizers of the task, our approach entails constructing a model comprising three fundamental layers: feature extraction, a Bidirectional Long Short-Term Memory (BiLSTM) layer, and at last, a Conditional Random Field (CRF) layer. Among various embedding techniques, we found that GloVe embeddings yielded superior results with our base model, achieving an F1 score of ${6 1. 2 3 \%}$ when combined with dropout layer. Further Toxic BERT and HateBERT models were used to classify the comments as hateful or non-hateful, HateBERT outperformed the Toxic BERT model.","Rachh, Rashmi, Kavatagi, Sanjana, Allagi, Shridhar",,,Domain Specific Embeddings in RNN Frameworks for Hate Span Detection and Classification,,,10.1109/CONIT61985.2024.10626508 , ,,"The unrestricted use of the internet has led to a significant rise in the dissemination of hate speech online, posing serious threats of harm and violence to individuals and society at large. Social media data often comprises informal and fragmented sentences, interspersed with multiple languages. While numerous researchers are dedicated to identifying and mitigating hate speech in social media content, existing approaches primarily focus on detecting hate speech as a whole, rather than targeting specific hateful words or phrases. To encourage research in this direction, a shared task was formulated during SemEval 2021, a task called Toxic Span Identification, to detect hateful words present in a sentence. Leveraging the dataset provided by the organizers of the task, our approach entails constructing a model comprising three fundamental layers: feature extraction, a Bidirectional Long Short-Term Memory (BiLSTM) layer, and at last, a Conditional Random Field (CRF) layer. Among various embedding techniques, we found that GloVe embeddings yielded superior results with our base model, achieving an F1 score of ${6 1. 2 3 \%}$ when combined with dropout layer. Further Toxic BERT and HateBERT models were used to classify the comments as hateful or non-hateful, HateBERT outperformed the Toxic BERT model.",,,,, ,  2024 4th International Conference on Intelligent Technologies (CONIT),Recurrent neural networks;Social networking (online);Hate speech;Bidirectional control;Feature extraction;Encoding;Conditional random fields;GloVe;hatespans;HateBERT;ToxicBERT;BiLSTM;CRF,detection,
3018,"**Title**META: Text Detoxification by leveraging METAmorphic Relations and Deep Learning Methods

**Abstract**In the world of online interactions, social communities face a significant challenge: the spread of offensive content and hate speech through toxic languages. Such issues led to growing research on text detoxification systems that can automatically rewrite toxic content. A systematic evaluation is required to ensure these systems produce high-quality detoxified text that modifies the original text to be non-toxic while preserving its content. However, this often relies on large amounts of labelled data and human judgement, which may not always be feasible. This limitation is typically known as the oracle problem. Metamorphic testing (MT) has conventionally been used to solve the oracle problem by deriving metamorphic relations (MRs) to test a programs functionality. A new MT approach focused on data validation showed that MRs incorporated with tools can be used to identify defects in machine translation services. This paper draws inspiration from this new MT perspective by presenting four metamorphic relations incorporated with tools to evaluate style transfer accuracy, content preservation, fluency, and a joint of these three. Our proposed approach effectively identifies defective behaviour in state-of-the-art text detoxification systems.","Choo, Alika, Pal, Arghya, Rajanala, Sailaja, Sen, Arkendu",,,META: Text Detoxification by leveraging METAmorphic Relations and Deep Learning Methods,,,10.1109/APSIPAASC63619.2025.10848645 , ,,"In the world of online interactions, social communities face a significant challenge: the spread of offensive content and hate speech through toxic languages. Such issues led to growing research on text detoxification systems that can automatically rewrite toxic content. A systematic evaluation is required to ensure these systems produce high-quality detoxified text that modifies the original text to be non-toxic while preserving its content. However, this often relies on large amounts of labelled data and human judgement, which may not always be feasible. This limitation is typically known as the oracle problem. Metamorphic testing (MT) has conventionally been used to solve the oracle problem by deriving metamorphic relations (MRs) to test a programs functionality. A new MT approach focused on data validation showed that MRs incorporated with tools can be used to identify defects in machine translation services. This paper draws inspiration from this new MT perspective by presenting four metamorphic relations incorporated with tools to evaluate style transfer accuracy, content preservation, fluency, and a joint of these three. Our proposed approach effectively identifies defective behaviour in state-of-the-art text detoxification systems.",,,,, ,  2024 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),Deep learning;Systematics;Accuracy;Hate speech;Asia;Information processing;Robustness;Machine translation;Faces;Testing,detox,
3021,"**Title**Study of Deep Learning Techniques for Real-Time Online censorship using Comment Toxicity Detection

**Abstract**In an era where our online interactions are integral to our daily lives, addressing and mitigating online toxicity is crucial. This study, titled ""Comment Toxicity Detection,"" aims to significantly advance ongoing efforts in this area. Our objective is to protect user welfare and encourage positive online interactions. This research addresses the urgent issue of online toxicity and its negative impact on user interactions and well-being. We explore various data sources and employ advanced feature engineering techniques such as BERT and its derivatives like m-BERT and MURIL BERT. By integrating these methods with various deep learning classifiers, we enhance our collective efforts to curb the spread of harmful content in digital spaces. We utilized an English dataset extracted from a larger multilingual corpus and preprocessed it using count vectorization for classification tasks. The evaluation metrics indicated a recall of 0.7103 and a precision of 0.7997. Our classification model used a Deep LSTM architecture with a batch size of 17. We are currently investigating the potential improvements in our data representation by incorporating a BERT-based model. This investigation is essential to optimize our classification performance, and the results should clarify the best practices for using advanced language representations to improve classification accuracy.","Sugandh, Sarvesh, Shewalkar, Aditya, Singh, Anurag, Attarde, Sakshi, Jakhete, Sumitra A.",,,Study of Deep Learning Techniques for Real-Time Online censorship using Comment Toxicity Detection,,,10.1109/MITADTSoCiCon60330.2024.10575395 , ,,"In an era where our online interactions are integral to our daily lives, addressing and mitigating online toxicity is crucial. This study, titled ""Comment Toxicity Detection,"" aims to significantly advance ongoing efforts in this area. Our objective is to protect user welfare and encourage positive online interactions. This research addresses the urgent issue of online toxicity and its negative impact on user interactions and well-being. We explore various data sources and employ advanced feature engineering techniques such as BERT and its derivatives like m-BERT and MURIL BERT. By integrating these methods with various deep learning classifiers, we enhance our collective efforts to curb the spread of harmful content in digital spaces. We utilized an English dataset extracted from a larger multilingual corpus and preprocessed it using count vectorization for classification tasks. The evaluation metrics indicated a recall of 0.7103 and a precision of 0.7997. Our classification model used a Deep LSTM architecture with a batch size of 17. We are currently investigating the potential improvements in our data representation by incorporating a BERT-based model. This investigation is essential to optimize our classification performance, and the results should clarify the best practices for using advanced language representations to improve classification accuracy.",,,,, ,"  2024 MIT Art, Design and Technology School of Computing International Conference (MITADTSoCiCon)",Deep learning;Measurement;Toxicology;Soft sensors;Computer architecture;Feature extraction;Real-time systems;Online Toxicity;Digital Interactions;Feature Engineering;BERT;MURIL BERT;m-BERT;Deep Learning Classifiers;Content Moderation;Online Safety;User-Generated Content,detection,
3023,"**Title**Prompt Evolution Through Examples for Large Language ModelsA Case Study in Game Comment Toxicity Classification

**Abstract**This paper presents a novel approach for automatic prompt optimization (APO) using a large language model (LLM) as an optimizer, named Prompt Evolution Through Examples (PETE). The approach draws inspiration from evolutionary computation for the prompt evolution stages. We aim to aid in developing prompts for use in systems classifying toxic content including game community moderator-assist tools. While traditional approaches are useful for developing these tools, they have various shortcomings where LLMs can potentially mitigates these issues. LLMs accept prompts as inputs to condition generated outputs. However, to design a prompt with the best performance in this task, fine-grained adjustments are usually required and should be automated through the APO process instead of a manual approach, which is often time-consuming. In this study, ChatGPT and GPT-4 are utilized as both task performers and prompt optimizers for comparisons across models. The results indicate that PETE improves the performance of the target task up to 56.14% from a performance of an initial prompt, compared to only up to 49.15% using a standard mutation evolution. Optimized prompts are provided for future utilization in other game community moderation tools. We also recommend that future studies explore more cost-effective approaches for evaluation using LLMs to enhance the benefits of APO.","Taveekitworachai, Pittawat, Abdullah, Febri, Gursesli, Mustafa Can, Lanata, Antonio, Guazzini, Andrea, Thawonmas, Ruck",,,Prompt Evolution Through Examples for Large Language ModelsA Case Study in Game Comment Toxicity Classification,,,10.1109/MetroInd4.0IoT61288.2024.10584130 , ,,"This paper presents a novel approach for automatic prompt optimization (APO) using a large language model (LLM) as an optimizer, named Prompt Evolution Through Examples (PETE). The approach draws inspiration from evolutionary computation for the prompt evolution stages. We aim to aid in developing prompts for use in systems classifying toxic content including game community moderator-assist tools. While traditional approaches are useful for developing these tools, they have various shortcomings where LLMs can potentially mitigates these issues. LLMs accept prompts as inputs to condition generated outputs. However, to design a prompt with the best performance in this task, fine-grained adjustments are usually required and should be automated through the APO process instead of a manual approach, which is often time-consuming. In this study, ChatGPT and GPT-4 are utilized as both task performers and prompt optimizers for comparisons across models. The results indicate that PETE improves the performance of the target task up to 56.14% from a performance of an initial prompt, compared to only up to 49.15% using a standard mutation evolution. Optimized prompts are provided for future utilization in other game community moderation tools. We also recommend that future studies explore more cost-effective approaches for evaluation using LLMs to enhance the benefits of APO.",,,,, ,  2024 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0 & IoT),Toxicology;Costs;Large language models;Computational modeling;Games;Manuals;Evolutionary computation;Evolutionary computation;Prompt engineering;ChatGPT;GPT-4;Errorful learning,detection,
3027,"**Title**Feature Attribution-Guided Contrastive Learning: Mitigating Lexical Bias in Toxic Speech Detection

**Abstract**Automatic detection methods for toxic speech can potentially curb the dissemination of offensive, abusive, hateful, and other toxic speech on social media. However, these methods often show bias by overreacting to words such as identity terms, profanity, and swear words that occur in non-toxic speech and erroneously classifying them as toxic speech. Recent research has attempted to regularise relevant biased words in predefined lexicons to mitigate their impact on model classification. However, this approach faces two challenges. Firstly, words such as pro-fanity and swear words in biased words play a crucial role in the model's ability to identify toxic speech, and excessive suppression of these words using regularization methods can detrimentally affect the performance of the classification model. Secondly, due to the limitations of regularization techniques, existing methods rely on manually constructed biased word dictionaries, which can only mitigate bias associated with identity-related terms. This bias should not significantly impact hate speech prediction. It is challenging to encompass lexical biases such as profanity and swear words that are specific to different datasets beyond identity terms To address the above challenges, this paper proposes a novel feature attribution-guided contrastive learning method. The method consists of two repeated steps across epochs. In each epoch, first identifies keywords in sentences that are crucial for predicting toxicity through feature attribution. Then it applies contrastive learning to separate samples that have common toxic keywords but different labels. Experiments show that our approach can mitigate lexical bias in toxic speech detection without any data augmentation or prior knowledge and achieve competitive performance gains.","Peng, Zhenghan, Ren, Yizhi, Wang, Dong, Zhang, Ling, Yuan, Lifeng, Ji, Yihao",,,Feature Attribution-Guided Contrastive Learning: Mitigating Lexical Bias in Toxic Speech Detection,,,10.1109/NTCI60157.2023.10403665 , ,,"Automatic detection methods for toxic speech can potentially curb the dissemination of offensive, abusive, hateful, and other toxic speech on social media. However, these methods often show bias by overreacting to words such as identity terms, profanity, and swear words that occur in non-toxic speech and erroneously classifying them as toxic speech. Recent research has attempted to regularise relevant biased words in predefined lexicons to mitigate their impact on model classification. However, this approach faces two challenges. Firstly, words such as pro-fanity and swear words in biased words play a crucial role in the model's ability to identify toxic speech, and excessive suppression of these words using regularization methods can detrimentally affect the performance of the classification model. Secondly, due to the limitations of regularization techniques, existing methods rely on manually constructed biased word dictionaries, which can only mitigate bias associated with identity-related terms. This bias should not significantly impact hate speech prediction. It is challenging to encompass lexical biases such as profanity and swear words that are specific to different datasets beyond identity terms To address the above challenges, this paper proposes a novel feature attribution-guided contrastive learning method. The method consists of two repeated steps across epochs. In each epoch, first identifies keywords in sentences that are crucial for predicting toxicity through feature attribution. Then it applies contrastive learning to separate samples that have common toxic keywords but different labels. Experiments show that our approach can mitigate lexical bias in toxic speech detection without any data augmentation or prior knowledge and achieve competitive performance gains.",,,,, ,  2023 International Conference on New Trends in Computational Intelligence (NTCI),Voice activity detection;Toxicology;Social networking (online);Self-supervised learning;Performance gain;Feature extraction;Task analysis;Toxic Speech;Bias Mitigation;Contrastive Learning;Feature Attribution,detection,
3032,"**Title**Improving Hate Speech Detection Accuracy Using Hybrid CNN-RNN and Random Oversampling Techniques

**Abstract**Detecting hate speech is crucial for addressing online toxicity and fostering a secure digital environment. This study aims to enhance the efficiency of hybrid CNN-RNN models, commonly used for this task, by improving accuracy. By integrating oversampling techniques with the model, the research aims to better categorize instances of hate speech, particularly in imbalanced datasets. The dataset used in this study is the Indonesian Tweet Hate Speech dataset. Following established protocols, including data pre-processing, training, and testing, significant improvements in accuracy are observed. The hybrid CNN-RNN achieves 0.827 accuracy, 0.797 precision, 0.759 recall, and 0.883 F1 score with imbalanced data. The model performs even better with balanced data, reaching 0.908 accuracy, 0.943 precision, 0.894 recall, and 0.914 F1 score. Notably, the proposed model outperforms the standard hybrid CNN-RNN on imbalanced datasets, with an accuracy of 0.752, precision of 0.797, recall of 0.559, and F1 score of 0.657. Techniques like dropout and early termination mitigate overfitting in complex models and large datasets. This research contributes to hate speech detection methods, underscoring the hybrid CNN-RNN's efficacy in handling imbalanced data, while future studies could explore additional methodologies for further enhancements.","Riyadi, Slamet, Andriyani, Annisa Divayu, Masyhur, Ahmad Musthafa",,,Improving Hate Speech Detection Accuracy Using Hybrid CNN-RNN and Random Oversampling Techniques,,,10.1109/ISIEA61920.2024.10607232 , ,,"Detecting hate speech is crucial for addressing online toxicity and fostering a secure digital environment. This study aims to enhance the efficiency of hybrid CNN-RNN models, commonly used for this task, by improving accuracy. By integrating oversampling techniques with the model, the research aims to better categorize instances of hate speech, particularly in imbalanced datasets. The dataset used in this study is the Indonesian Tweet Hate Speech dataset. Following established protocols, including data pre-processing, training, and testing, significant improvements in accuracy are observed. The hybrid CNN-RNN achieves 0.827 accuracy, 0.797 precision, 0.759 recall, and 0.883 F1 score with imbalanced data. The model performs even better with balanced data, reaching 0.908 accuracy, 0.943 precision, 0.894 recall, and 0.914 F1 score. Notably, the proposed model outperforms the standard hybrid CNN-RNN on imbalanced datasets, with an accuracy of 0.752, precision of 0.797, recall of 0.559, and F1 score of 0.657. Techniques like dropout and early termination mitigate overfitting in complex models and large datasets. This research contributes to hate speech detection methods, underscoring the hybrid CNN-RNN's efficacy in handling imbalanced data, while future studies could explore additional methodologies for further enhancements.",,,,, ,  2024 IEEE Symposium on Industrial Electronics & Applications (ISIEA),Training;Accuracy;Toxicology;Protocols;Prevention and mitigation;Hate speech;Reliability;hate speech;Twitter;hybrid CNN-RNN;balancing dataset;oversampling,detection,
3036,"**Title**Large Language Models: Ethics and Norms in the European Union

**Abstract**This paper investigates the ethical-legal issues underlying the implementation of Large Language Models, in the aftermath of the European Union Parliament's approval of the Artificial Intelligence Regulation. In very recent years, there has been a rapid evolution and multiform implementation of the so-called Artificial Intelligence. Notably, one significant development in AI is the creation of systems capable of performing tasks based on a set of established rules. Artificial Intelligence has recently undergone a significant revolution that has led to a radical paradigm shift, represented by the so-called Foundation Models, which have two main characteristics: emergence and homogenisation. On the other hand, Language Modelling (LM) is one of the main approaches to advance the linguistic intelligence of machines and thus a Foundation Model trained on textual data. The most recent development of this approach can be found in the so-called Large Language Models (LLM). These take the form of artificial neural networks pre-trained on huge datasets which, instead of being dedicated to a specific task, can perform a wide range of different functions at a level that is all the more accurate the greater the resources made available to them. The concept of language models is not new but has evolved with the progress of artificial intelligence over the decades. In general, LLMs are mainly trained in industrial settings where many important details of training (e.g. data collection and cleaning) are not disclosed. Thus, it is difficult to align LLMs with human values or preferences. Despite their capabilities, LLMs are also likely to produce toxic, fictitious or harmful content. Therefore, effective and efficient control approaches are required to eliminate the potential risk arising from the use of LLMs. For the purposes of the study conducted, it is intended to describe the development and outcomes with the aim to show not only the evolution of the responses offered by various LLM, but also and above all the possible ethical and legal implications that can be deduced from these outputs. Several models have been used for the implementation of the experiments: two GPT-2 models, Perplexity, ChatGPT (version 3.5), Llama. The language used for the implementation of the experiments was English, as it is structurally more neutral. In particular, with reference to a topic with a strong ethical and legal impact such as suicide, different answers were offered by the various models used. In conclusion, the essay offers reflections on the results of the experiment with reference to the EU Regulation on Artificial Intelligence. Artificial Intelligence represents for the jurist, not only European, a decisive challenge not only with regard to the categories of thought and formulation of operational languages, but above all with regard to the centrality of the human being. On the basis of these considerations, focusing on generative AI and in particular on Large Language Models, it is clear that many of the issues relating to copyright and the protection of personal data still remain unsolved, excluding those already regulated by the legislation on the point.","Ciullo, Marianna",,,Large Language Models: Ethics and Norms in the European Union,,,10.1109/MetroXRAINE62247.2024.10795986 , ,,"This paper investigates the ethical-legal issues underlying the implementation of Large Language Models, in the aftermath of the European Union Parliament's approval of the Artificial Intelligence Regulation. In very recent years, there has been a rapid evolution and multiform implementation of the so-called Artificial Intelligence. Notably, one significant development in AI is the creation of systems capable of performing tasks based on a set of established rules. Artificial Intelligence has recently undergone a significant revolution that has led to a radical paradigm shift, represented by the so-called Foundation Models, which have two main characteristics: emergence and homogenisation. On the other hand, Language Modelling (LM) is one of the main approaches to advance the linguistic intelligence of machines and thus a Foundation Model trained on textual data. The most recent development of this approach can be found in the so-called Large Language Models (LLM). These take the form of artificial neural networks pre-trained on huge datasets which, instead of being dedicated to a specific task, can perform a wide range of different functions at a level that is all the more accurate the greater the resources made available to them. The concept of language models is not new but has evolved with the progress of artificial intelligence over the decades. In general, LLMs are mainly trained in industrial settings where many important details of training (e.g. data collection and cleaning) are not disclosed. Thus, it is difficult to align LLMs with human values or preferences. Despite their capabilities, LLMs are also likely to produce toxic, fictitious or harmful content. Therefore, effective and efficient control approaches are required to eliminate the potential risk arising from the use of LLMs. For the purposes of the study conducted, it is intended to describe the development and outcomes with the aim to show not only the evolution of the responses offered by various LLM, but also and above all the possible ethical and legal implications that can be deduced from these outputs. Several models have been used for the implementation of the experiments: two GPT-2 models, Perplexity, ChatGPT (version 3.5), Llama. The language used for the implementation of the experiments was English, as it is structurally more neutral. In particular, with reference to a topic with a strong ethical and legal impact such as suicide, different answers were offered by the various models used. In conclusion, the essay offers reflections on the results of the experiment with reference to the EU Regulation on Artificial Intelligence. Artificial Intelligence represents for the jurist, not only European, a decisive challenge not only with regard to the categories of thought and formulation of operational languages, but above all with regard to the centrality of the human being. On the basis of these considerations, focusing on generative AI and in particular on Large Language Models, it is clear that many of the issues relating to copyright and the protection of personal data still remain unsolved, excluding those already regulated by the legislation on the point.",,,,, ,"  2024 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)",Training;Ethics;Law;Foundation models;Large language models;Europe;Regulation;Reflection;Artificial intelligence;Protection;Artificial Intelligence;Large Language Models;Ethics;Law;Artificial Intelligence Act,evaluation,
3040,"**Title**Real-Time Hate Speech Recognition Along with Educational Feedback and Automatic Reporting

**Abstract**In today's digital era, the rise of toxicity, discrimination, and harm on online communication platforms has become a significant concern, necessitating effective moderation strategies. Existing methods for hate speech detection often lack real-time capabilities and fail to provide constructive feedback to users. Natural Language Processing (NLP) is used to identify the intent of the speech and bidirectional Long Short Term Memory (LSTM) machine learning algorithm classifies text into multiple hate speech categories. The designed system also displays charts, providing a detailed assessment and understanding of the nature and severity of the content. With a user-friendly interface, users receive real-time educational feedback on the nature and impact of different types of hate speech, promoting awareness and discouraging toxic behavior.","Panda, Abhilasha, Anand, Abhijeet, Bebortta, Sujit, Sekhar Tripathy, Subhranshu, Mukherjee, Tanmay",,,Real-Time Hate Speech Recognition Along with Educational Feedback and Automatic Reporting,,,10.1109/AESPC63931.2024.10872383 , ,,"In today's digital era, the rise of toxicity, discrimination, and harm on online communication platforms has become a significant concern, necessitating effective moderation strategies. Existing methods for hate speech detection often lack real-time capabilities and fail to provide constructive feedback to users. Natural Language Processing (NLP) is used to identify the intent of the speech and bidirectional Long Short Term Memory (LSTM) machine learning algorithm classifies text into multiple hate speech categories. The designed system also displays charts, providing a detailed assessment and understanding of the nature and severity of the content. With a user-friendly interface, users receive real-time educational feedback on the nature and impact of different types of hate speech, promoting awareness and discouraging toxic behavior.",,,,, ,"  2024 IEEE 4th International Conference on Applied Electromagnetics, Signal Processing, & Communication (AESPC)",Toxicology;Machine learning algorithms;Hate speech;Bidirectional long short term memory;Speech recognition;Real-time systems;Natural language processing;Speech processing;Hate Speech Detection;Natural Language Processing;Bidirectional LSTM;Hate Classification;Real-time educational Feedback,detection,
3047,"**Title**Detection of Hate Speech and Offensive Language Using Machine Learning and Deep Learning for Multi-Class Tweets

**Abstract**Nowadays, every single person is indulged in social media, and because of its hidden characteristics, people are free to express their views and thoughts and expressing their views freely is their right. Expressing their thoughts and views on dynamic news puts a positive impact in economy, as it shows how people relate to each other. Yet, there are times when individuals throw toxic comments or accusing them on the grounds of caste, religion, gender identity, ethnicity etc. is a harassment of the given freedom. Due to this, hate and offensive language has become the serious issue, in modern society, which leads to damaging the people's peace, their human rights as well as creating an inequality in society. In this research paper, the dataset has been taken from the Kaggle source, sentiment analysis will be done on the detection of hate speech and offensive language, and the classification will be done on the following three labels: Hate Speech, Offensive Language and Neither.","Garg, Bhavika, Bhardwaj, Aditya, Jain, Tarun",,,Detection of Hate Speech and Offensive Language Using Machine Learning and Deep Learning for Multi-Class Tweets,,,10.1109/ICSPCRE62303.2024.10675191 , ,,"Nowadays, every single person is indulged in social media, and because of its hidden characteristics, people are free to express their views and thoughts and expressing their views freely is their right. Expressing their thoughts and views on dynamic news puts a positive impact in economy, as it shows how people relate to each other. Yet, there are times when individuals throw toxic comments or accusing them on the grounds of caste, religion, gender identity, ethnicity etc. is a harassment of the given freedom. Due to this, hate and offensive language has become the serious issue, in modern society, which leads to damaging the people's peace, their human rights as well as creating an inequality in society. In this research paper, the dataset has been taken from the Kaggle source, sentiment analysis will be done on the detection of hate speech and offensive language, and the classification will be done on the following three labels: Hate Speech, Offensive Language and Neither.",,,,, ,  2024 IEEE International Conference on Smart Power Control and Renewable Energy (ICSPCRE),Deep learning;Sentiment analysis;Renewable energy sources;Analytical models;Tongue;Social networking (online);Hate speech;Hate Speech;Offensive Language;Sentiment Analysis;Natural Language Processing;Machine Learning;Feature Extraction,detection,
3051,"**Title**ANTI-Disinformation: An Adversarial Attack and Defense Network Towards Improved Robustness for Disinformation Detection on Social Media

**Abstract**The prevalence of disinformation, which includes malformation (e.g., cyberbullying) and misinformation (e.g., fake news) in online platforms has raised significant concerns, prompting the need for robust detection methods to mitigate its detrimental impact. While the field of text classification has witnessed notable advancements in recent years, existing approaches often overlook the evolving nature of disinformation, wherein perpetrators employ perturbations to toxic content to evade detection or censorship. To address this challenge, we present a novel framework, Adversarial Network Towards Improved robustness for Disinformation detection (ANTI-Disinformation), which leverages reinforcement learning techniques as adversarial attacks. Additionally, we propose a defense model to enhance models robustness against such attacks. To evaluate the effectiveness of our approach, we conduct extensive experiments on well-known disinformation datasets collected from multiple social media platforms. The results demonstrate our approach can effectively produce degradation in existing models performance the most, showcasing the effectiveness of our framework and the vulnerability of existing detection systems. The results also exhibit that the proposed defense methods can consistently outperform existing typical methods in constructing robust detection models.","Chen, Kuan-Chun, Chen, Chih-Yao, Li, Cheng-Te",,,ANTI-Disinformation: An Adversarial Attack and Defense Network Towards Improved Robustness for Disinformation Detection on Social Media,,,10.1109/BigData59044.2023.10386090 , ,,"The prevalence of disinformation, which includes malformation (e.g., cyberbullying) and misinformation (e.g., fake news) in online platforms has raised significant concerns, prompting the need for robust detection methods to mitigate its detrimental impact. While the field of text classification has witnessed notable advancements in recent years, existing approaches often overlook the evolving nature of disinformation, wherein perpetrators employ perturbations to toxic content to evade detection or censorship. To address this challenge, we present a novel framework, Adversarial Network Towards Improved robustness for Disinformation detection (ANTI-Disinformation), which leverages reinforcement learning techniques as adversarial attacks. Additionally, we propose a defense model to enhance models robustness against such attacks. To evaluate the effectiveness of our approach, we conduct extensive experiments on well-known disinformation datasets collected from multiple social media platforms. The results demonstrate our approach can effectively produce degradation in existing models performance the most, showcasing the effectiveness of our framework and the vulnerability of existing detection systems. The results also exhibit that the proposed defense methods can consistently outperform existing typical methods in constructing robust detection models.",,,,, ,  2023 IEEE International Conference on Big Data (BigData),Degradation;Perturbation methods;Text categorization;Cyberbullying;Reinforcement learning;Big Data;Robustness;Disinformation Detection;Adversarial Attack;Adversarial Defense;Model Robustness;Reinforcement Learning,detection,
3058,"**Title**Toxic Comment Analyzer using BERT: A Deep Learning Approach for Toxicity Detection

**Abstract**With the good hike of social and communication platforms, a significant increase in the volume of user-generated content is produced. Unfortunately, this surge in online interactions has also led to an increase in toxic and offensive comments. Toxic comments not only contribute to a negative online environment but also have the potential to harm individuals and communities. Consequently, the development of automated methods for detecting toxic comments has become imperative. In this research paper, we propose a toxic comment analyzer using BERT to address this issue. Our approach demonstrates the effectiveness of leveraging pre-trained language models for identifying toxic language, with promising results of 97% accuracy on benchmark datasets.","Singh, Richa, Kashyap, Rekha, Sharma, Vikrant",,,Toxic Comment Analyzer using BERT: A Deep Learning Approach for Toxicity Detection,,,10.1109/ICI60088.2023.10421672 , ,,"With the good hike of social and communication platforms, a significant increase in the volume of user-generated content is produced. Unfortunately, this surge in online interactions has also led to an increase in toxic and offensive comments. Toxic comments not only contribute to a negative online environment but also have the potential to harm individuals and communities. Consequently, the development of automated methods for detecting toxic comments has become imperative. In this research paper, we propose a toxic comment analyzer using BERT to address this issue. Our approach demonstrates the effectiveness of leveraging pre-trained language models for identifying toxic language, with promising results of 97% accuracy on benchmark datasets.",,,,, ,  2023 Second International Conference on Informatics (ICI),Deep learning;Analytical models;Toxicology;User-generated content;Buildings;Surges;Informatics;Toxicity Detection;BERT;Toxic Comment Analyzer,detection,
3059,"**Title**Assamese Toxic Comment Detection On Social Media Using Machine Learning Methods

**Abstract**Social media users across society are negatively impacted by toxic contents. For a strong social environment and safe language models, a toxic comment detection system is designed to protect users from harmful content in Social Media.Toxic Comment Detection in Assamese Languages is one of the most challenging Natural Language Processing (NLP) tasks since Indian languages like Assamese are ambiguous in nature and rich in morphology. Despite dearth of e-resources of Assamese language, 19,550 comments are collected manually from popular social media platforms and examined considering Naive Bayes(NB), Support Vector Machine(SVM), Logistic Regression(LR) and Random Forest(RF) with count vector, count vector+TF-IDF and n-gram representation. The experimental findings show that SVM with count vector + TF-IDF has outperformed all the proposed machine learning models with a remarkable accuracy and F1-score of 94%.","Dutta, Surajit, Neog, Mandira, Baruah, Nomi",,,Assamese Toxic Comment Detection On Social Media Using Machine Learning Methods,,,10.1109/ic-ETITE58242.2024.10493331 , ,,"Social media users across society are negatively impacted by toxic contents. For a strong social environment and safe language models, a toxic comment detection system is designed to protect users from harmful content in Social Media.Toxic Comment Detection in Assamese Languages is one of the most challenging Natural Language Processing (NLP) tasks since Indian languages like Assamese are ambiguous in nature and rich in morphology. Despite dearth of e-resources of Assamese language, 19,550 comments are collected manually from popular social media platforms and examined considering Naive Bayes(NB), Support Vector Machine(SVM), Logistic Regression(LR) and Random Forest(RF) with count vector, count vector+TF-IDF and n-gram representation. The experimental findings show that SVM with count vector + TF-IDF has outperformed all the proposed machine learning models with a remarkable accuracy and F1-score of 94%.",,,,, ,  2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE),Support vector machines;Social networking (online);Morphology;Machine learning;Forestry;Market research;Vectors;Assamese;NB;SVM;LR;RF;Toxic,detection,
3060,"**Title**Toxic Comment Detection Using Bidirectional Sequence Classifiers

**Abstract**With the rising surge of online toxicity, automating the identification of abusive language becomes crucial for improving online discourse. This study proposes a deep learning system that efficiently uses multiple labels to classify harmful comments using bi-directional Long Short-Term Memory (LSTM) networks. By leveraging contextual information, the bi-LSTM model achieves state-of-the-art performance in classifying subtle forms of toxicity such as threats, insults, identity hate, and obscenity. The model achieves above 95% accuracy on benchmark datasets with rigorous data processing, optimized neural architecture, and the utilization of FastText embeddings to handle words that are not in the vocabulary. This technique can automatically filter different levels of toxicity, promoting positive online interactions when integrated into online platforms. The proposed study outlines an end-to-end pipeline incorporating recent NLP advancements and deep contextualized language models to address contemporary challenges in AI-enabled content moderation.","Maity, Amit, More, Rishi, Patil, Prof. Abhijit, Oza, Jay, Kambli, Gitesh",,,Toxic Comment Detection Using Bidirectional Sequence Classifiers,,,10.1109/IDCIoT59759.2024.10467922 , ,,"With the rising surge of online toxicity, automating the identification of abusive language becomes crucial for improving online discourse. This study proposes a deep learning system that efficiently uses multiple labels to classify harmful comments using bi-directional Long Short-Term Memory (LSTM) networks. By leveraging contextual information, the bi-LSTM model achieves state-of-the-art performance in classifying subtle forms of toxicity such as threats, insults, identity hate, and obscenity. The model achieves above 95% accuracy on benchmark datasets with rigorous data processing, optimized neural architecture, and the utilization of FastText embeddings to handle words that are not in the vocabulary. This technique can automatically filter different levels of toxicity, promoting positive online interactions when integrated into online platforms. The proposed study outlines an end-to-end pipeline incorporating recent NLP advancements and deep contextualized language models to address contemporary challenges in AI-enabled content moderation.",,,,, ,  2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),Training;Surveys;Vocabulary;Toxicology;Benchmark testing;Data models;Surges;natural language processing;sequence modeling;long short-term memory;toxic comment detection;bidirectional classifier,detection,
3062,"**Title**SecureComment: Safeguarding Online Discussions with Intelligent Toxic Comment Filtering

**Abstract**Toxic comments, such as hate speech and abuse, are a widespread issue online, disrupting healthy conversation and user safety. Identifying and filtering these comments is crucial for ensuring respectful online communications. The primary challenge arises from the need to obtain accurate detection while addressing emerging tactics and balancing between false positives and negatives. This concern affects user interactions and platform credibility and raises legal and ethical risks. Therefore, this study examines the effectiveness of numerous machine-learning models and implements an ensemble method for analyzing and classifying toxic comments. We analyze the effectiveness of various algorithms using evaluation metrics like precision, recall, accuracy, and F1-score. Ensemble methods are utilized to combine the capabilities of several models, enhancing the classification of toxic comments. The study aspires to deliver insights into the comparative performance of various machine learning models in classifying toxic comments accurately and the advantages of ensemble methods in enhancing classification accuracy.","Rayani, Reddy Kowshik, Tekula, Samhitha, Vattigunta, Subhash Kovid, Kovi, Naveen Kumar, Namitha, Kalakunnath",,,SecureComment: Safeguarding Online Discussions with Intelligent Toxic Comment Filtering,,,10.1109/IATMSI60426.2024.10503420 , ,,"Toxic comments, such as hate speech and abuse, are a widespread issue online, disrupting healthy conversation and user safety. Identifying and filtering these comments is crucial for ensuring respectful online communications. The primary challenge arises from the need to obtain accurate detection while addressing emerging tactics and balancing between false positives and negatives. This concern affects user interactions and platform credibility and raises legal and ethical risks. Therefore, this study examines the effectiveness of numerous machine-learning models and implements an ensemble method for analyzing and classifying toxic comments. We analyze the effectiveness of various algorithms using evaluation metrics like precision, recall, accuracy, and F1-score. Ensemble methods are utilized to combine the capabilities of several models, enhancing the classification of toxic comments. The study aspires to deliver insights into the comparative performance of various machine learning models in classifying toxic comments accurately and the advantages of ensemble methods in enhancing classification accuracy.",,,,, ,  2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI),Measurement;Technological innovation;Ethics;Machine learning algorithms;Filtering;Law;Hate speech;Toxic comments;Machine learning;Ensemble model;Classification;Twitter dataset,detection,
3065,"**Title**Exploring Gender Bias and Toxic Comments Using Artificial Intelligence: Trends and Implications

**Abstract**Nowadays, most systems use artificial intelligence algorithms to automate tasks and reduce the time required for execution. Moreover, it must estimate the bias risks that can be introduced within the system. Based on these considerations, quantitative measures and prioritization strategies can be established for those inadequate situations, choosing an appropriate method to overcome gender bias. In this study, the impact of gender bias on an annual salary risk score due to gender bias was analyzed to identify and reduce it as much as possible in machine learning algorithms and on text data provided to a virtual assistant. The study finds that gender bias can influence our decisions by illustrating hypotheses on how algorithms affect prioritization decisions and strengthen stereotypes by favoring men against women. Recommendations to lower gender bias can include training programs for poor people that face substantial barriers to accessing education; training programs for people with a low level of education or no access; access to all kinds of jobs for women; assurance of diversity and inclusiveness; and algorithms that are fair and trained with the definite goal of reducing gender bias.","Mercioni, Marina Adriana, Holban, Stefan",,,Exploring Gender Bias and Toxic Comments Using Artificial Intelligence: Trends and Implications,,,10.1109/DAS61944.2024.10541152 , ,,"Nowadays, most systems use artificial intelligence algorithms to automate tasks and reduce the time required for execution. Moreover, it must estimate the bias risks that can be introduced within the system. Based on these considerations, quantitative measures and prioritization strategies can be established for those inadequate situations, choosing an appropriate method to overcome gender bias. In this study, the impact of gender bias on an annual salary risk score due to gender bias was analyzed to identify and reduce it as much as possible in machine learning algorithms and on text data provided to a virtual assistant. The study finds that gender bias can influence our decisions by illustrating hypotheses on how algorithms affect prioritization decisions and strengthen stereotypes by favoring men against women. Recommendations to lower gender bias can include training programs for poor people that face substantial barriers to accessing education; training programs for people with a low level of education or no access; access to all kinds of jobs for women; assurance of diversity and inclusiveness; and algorithms that are fair and trained with the definite goal of reducing gender bias.",,,,, ,  2024 International Conference on Development and Application Systems (DAS),Training;Machine learning algorithms;Virtual assistants;Market research;Remuneration;Artificial intelligence;Task analysis;algorithm;artificial intelligence;bias;deep learning;detection;ethical;gender;machine learning;mitigation;risk;salary;workplace,detection,
3066,"**Title**A Comprehensive Examination of Toxic Tweet Classification on Twitter

**Abstract**In recent times, daily interactions have been taking place largely on online platforms, with forms of interactions such as emailsandtextmessages.However,an important problem lies in the understanding of these messages. These textual interchanges can be interpreted in multiple different ways by different people which can cause significant misunderstandings and negative perceptions. The aim of this project is to address this challenge and overcome it by developing a system that is capable of identifying the negativity and toxicity in the tone of the messages. Through rigorous analysis the model aims to identify parts or sequences of a message that might appear as offensive or negative, giving the user constructive feedback on how they can better improve their messages. This helps users to write messages that are not only clearer but also easily received in the intended manner by their audience, thereby increasing the effectiveness of the communication process and reducing misunderstandings. The main goal of this paper is to contribute to the development of an inclusive and understanding online community by using techniques of Natural Language Processing and Machine Learning to solve the complexities of human communication.","Gokulnath, Shreejith Suthraye, Jayaraman, Shrikar, T, Nathezhtha",,,A Comprehensive Examination of Toxic Tweet Classification on Twitter,,,10.1109/ICONSTEM60960.2024.10568694 , ,,"In recent times, daily interactions have been taking place largely on online platforms, with forms of interactions such as emailsandtextmessages.However,an important problem lies in the understanding of these messages. These textual interchanges can be interpreted in multiple different ways by different people which can cause significant misunderstandings and negative perceptions. The aim of this project is to address this challenge and overcome it by developing a system that is capable of identifying the negativity and toxicity in the tone of the messages. Through rigorous analysis the model aims to identify parts or sequences of a message that might appear as offensive or negative, giving the user constructive feedback on how they can better improve their messages. This helps users to write messages that are not only clearer but also easily received in the intended manner by their audience, thereby increasing the effectiveness of the communication process and reducing misunderstandings. The main goal of this paper is to contribute to the development of an inclusive and understanding online community by using techniques of Natural Language Processing and Machine Learning to solve the complexities of human communication.",,,,, ,  2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM),Analytical models;Toxicology;Social networking (online);Blogs;Machine learning;Natural language processing;Mathematics;Sentiment Analysis;NLP;Toxic tweet;Logistic Regression,detection,
3072,"**Title**Enhancing Social Media Safety with Automated Toxicity Classification using Semantic Topic Models

**Abstract**In social media platforms, there is a growing interest in developing automated solutions for toxicity classification using machine learning algorithms. This paper presents a novel approach to automated toxicity classification using semantic topic models, achieving high accuracy and transparency. The report emphasizes the importance of detecting and adjusting for biases to ensure fair categorization. Moreover, the report highlights the challenges of natural language processing tasks, importance of understanding and addressing potential biases in machine learning models. Datasets are produced by the Origami project at Google, labeled by human labelers with adjectives such as ""toxic,"" ""very poisonous,"" ""accusation,"" ""danger,"" ""obscene,"" and ""identity hatred."" From the analysis, it shows that proposed method achieved better results on toxicity classification in terms of accuracy (0.85), precision (0.87), recall (0.84) and f1-score (0.85) respectively. For non-toxicity classification, the proposed method achieves accuracy (0.92), precision (0.89), recall (0.88), f1-score (0.88) and AUC-ROC (0.96) respectively.","Enquero, Shreya Patchala, Lande, Jayapal, Syamala, M., Priyadarsan, Padhyala, V, Sunil Kumar",,,Enhancing Social Media Safety with Automated Toxicity Classification using Semantic Topic Models,,,10.1109/EASCT59475.2023.10393723 , ,,"In social media platforms, there is a growing interest in developing automated solutions for toxicity classification using machine learning algorithms. This paper presents a novel approach to automated toxicity classification using semantic topic models, achieving high accuracy and transparency. The report emphasizes the importance of detecting and adjusting for biases to ensure fair categorization. Moreover, the report highlights the challenges of natural language processing tasks, importance of understanding and addressing potential biases in machine learning models. Datasets are produced by the Origami project at Google, labeled by human labelers with adjectives such as ""toxic,"" ""very poisonous,"" ""accusation,"" ""danger,"" ""obscene,"" and ""identity hatred."" From the analysis, it shows that proposed method achieved better results on toxicity classification in terms of accuracy (0.85), precision (0.87), recall (0.84) and f1-score (0.85) respectively. For non-toxicity classification, the proposed method achieves accuracy (0.92), precision (0.89), recall (0.88), f1-score (0.88) and AUC-ROC (0.96) respectively.",,,,, ,  2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT),Toxicology;Sensitivity;Social networking (online);Semantics;Machine learning;Natural language processing;Safety;Latent Dirichlet Allocation;Machine Learning Methods;Natural Language Processing;Semantic Topic Models;Social Media Safety;Support Vector Machine;Toxic Comments,detection,
3074,"**Title**Toxic Comments Classification using LSTM and CNN

**Abstract**Online platforms have grown to be an important medium for expression and communication in the digital era. But the anonymity and absence of in-person communication on these platforms often encourage the spread of offensive remarks, which can be harmful to both people and communities. Before classifying comments as dangerous, natural language processing, or NLP, must first comprehend and assess textual data in order to identify feelings, patterns, and context that may be harmful. Tokenizing the comments, generating features for the classification model, and preprocessing the text input are all done in this instance using natural language processing approaches. Specific NLP methods such as sequence padding, text vectorization, and tokenization may be covered in more detail. This study combines the architectures of Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) to present a unique technique for categorizing harmful remarks. This study pre-processes the text data to eliminate noise and unnecessary information. Next, we use word embeddings to embed the comments into numerical vectors. Next, in order to capture the spatial and sequential relationships present in the text data, this study uses a hybrid model that combines CNN and LSTM layers. The CNN component assists in identifying local patterns and characteristics in the comments, while the LSTM component allows the model to comprehend the sequential flow of language. This study uses a publicly accessible dataset of labelled harmful remarks to train and assess the proposed model. The proposed models performance is measured using common measures including accuracy, precision, recall, and F1-score.","Bhattacharya, Sampurna, Shankar, Bhavani Gowri, Pitchaimanickam, B., Nithya, A. Alice",,,Toxic Comments Classification using LSTM and CNN,,,10.1109/ICAAIC60222.2024.10575718 , ,,"Online platforms have grown to be an important medium for expression and communication in the digital era. But the anonymity and absence of in-person communication on these platforms often encourage the spread of offensive remarks, which can be harmful to both people and communities. Before classifying comments as dangerous, natural language processing, or NLP, must first comprehend and assess textual data in order to identify feelings, patterns, and context that may be harmful. Tokenizing the comments, generating features for the classification model, and preprocessing the text input are all done in this instance using natural language processing approaches. Specific NLP methods such as sequence padding, text vectorization, and tokenization may be covered in more detail. This study combines the architectures of Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) to present a unique technique for categorizing harmful remarks. This study pre-processes the text data to eliminate noise and unnecessary information. Next, we use word embeddings to embed the comments into numerical vectors. Next, in order to capture the spatial and sequential relationships present in the text data, this study uses a hybrid model that combines CNN and LSTM layers. The CNN component assists in identifying local patterns and characteristics in the comments, while the LSTM component allows the model to comprehend the sequential flow of language. This study uses a publicly accessible dataset of labelled harmful remarks to train and assess the proposed model. The proposed models performance is measured using common measures including accuracy, precision, recall, and F1-score.",,,,, ,  2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC),Deep learning;Soft sensors;Noise;Data preprocessing;Data models;Vectors;Tokenization;Machine learning;Convolutional Neural Network;Long Short Term memory,detection,
3080,"**Title**Addressing Unintended Bias in Toxicity Detection: An LSTM and Attention-Based Approach

**Abstract**In the digital era, online platforms serve as crucial hubs for social interactions and idea exchange. However, these platforms are continually shadowed by toxic comments that undermine genuine discourse and have the potential to harm participants. While machine learning provides an avenue for detecting such toxic content, a significant challenge arises when these models, influenced by biased training datasets, inadvertently propagate or amplify inherent biases. Such unintentional biases are especially disconcerting when they disadvantage or misrepresent identities already vulnerable in online spaces. Addressing this complex landscape, our research presents a model meticulously designed to detect toxic comments, aiming to achieve a higher degree of accuracy while striving to minimize such unintended biases. Our approach is underpinned by a combination of a tailored data preprocessing technique and the integration of Long Short-Term Memory networks (LSTM) with Attention mechanisms. Preliminary evaluations reveal our model's AVC score to be 0.93524, indicating its efficacy in toxicity detection. While there's always room for improvement, the design and results of our model emphasize the importance and feasibility of developing more nuanced and unbiased machine learning solutions for the challenges posed in the digital domain.","Dai, Weinan, Tao, Jinglei, Yan, Xu, Feng, Zhenyuan, Chen, Jinkun",,,Addressing Unintended Bias in Toxicity Detection: An LSTM and Attention-Based Approach,,,10.1109/ICAICA58456.2023.10405429 , ,,"In the digital era, online platforms serve as crucial hubs for social interactions and idea exchange. However, these platforms are continually shadowed by toxic comments that undermine genuine discourse and have the potential to harm participants. While machine learning provides an avenue for detecting such toxic content, a significant challenge arises when these models, influenced by biased training datasets, inadvertently propagate or amplify inherent biases. Such unintentional biases are especially disconcerting when they disadvantage or misrepresent identities already vulnerable in online spaces. Addressing this complex landscape, our research presents a model meticulously designed to detect toxic comments, aiming to achieve a higher degree of accuracy while striving to minimize such unintended biases. Our approach is underpinned by a combination of a tailored data preprocessing technique and the integration of Long Short-Term Memory networks (LSTM) with Attention mechanisms. Preliminary evaluations reveal our model's AVC score to be 0.93524, indicating its efficacy in toxicity detection. While there's always room for improvement, the design and results of our model emphasize the importance and feasibility of developing more nuanced and unbiased machine learning solutions for the challenges posed in the digital domain.",,,,, ,  2023 5th International Conference on Artificial Intelligence and Computer Applications (ICAICA),Training;Deep learning;Ethics;Toxicology;Data preprocessing;Robustness;Long short term memory;Toxicity Detection;LSTM;Attention Mechanism;Data Preprocessing,detection,
3084,"**Title**Utilization of Artificial Intelligence for Social Media and Gaming Moderation

**Abstract**As the world continues to evolve, technology has proven to be a necessity in the lives of everyone. Evolving beyond professional use, cyberspace is now populated by online communities being used for communication, learning, and entertainment. However, the increased online presence exposes users to a variety of cultures, personalities, and levels of maturity. Some may also seek to cause harm to others through cyberbullying or may display toxic behaviors. This research aims to tackle the growing problem of toxicity and harassment in online environments. The proposed solution will utilize Artificial Intelligence (AI), and more specifically Natural Language Processing (NLP), to moderate communication and detect malicious language and behavior. The efforts shared in this paper specifically present a work-in-progress. For the time being, two models have been tested with a single dataset from Twitter: a Convolutional Neural Network (CNN) and a Recurrent Neural Network (RNN). The results of experimentation show a promising start for the use of NLP in moderation with an 83% accuracy using an RNN.","Saleous, Heba, Gergely, Marton, Shuaib, Khaled",,,Utilization of Artificial Intelligence for Social Media and Gaming Moderation,,,10.1109/IIT59782.2023.10366468 , ,,"As the world continues to evolve, technology has proven to be a necessity in the lives of everyone. Evolving beyond professional use, cyberspace is now populated by online communities being used for communication, learning, and entertainment. However, the increased online presence exposes users to a variety of cultures, personalities, and levels of maturity. Some may also seek to cause harm to others through cyberbullying or may display toxic behaviors. This research aims to tackle the growing problem of toxicity and harassment in online environments. The proposed solution will utilize Artificial Intelligence (AI), and more specifically Natural Language Processing (NLP), to moderate communication and detect malicious language and behavior. The efforts shared in this paper specifically present a work-in-progress. For the time being, two models have been tested with a single dataset from Twitter: a Convolutional Neural Network (CNN) and a Recurrent Neural Network (RNN). The results of experimentation show a promising start for the use of NLP in moderation with an 83% accuracy using an RNN.",,,,, ,  2023 15th International Conference on Innovations in Information Technology (IIT),Training;Technological innovation;Recurrent neural networks;Codes;Toxicology;Entertainment industry;Natural language processing;NLP;Sentiment Analysis;Online Harassment;User Moderation,detection,
3085,"**Title**Predicting Hate Words and Offensive Language: A Machine Learning Approach

**Abstract**The internet has led to a significant increase in the amount of information available, which has resulted in the rise of online communication among users. Unfortunately, this has also resulted in the proliferation of toxic online texts, which can lead to bullying, harassment, and personal attacks. While there have been attempts to develop efficient models for predicting toxic comments online, the field is still in its nascent stage, hence new frameworks and strategies are needed to address this issue. Convolutional Neural Networks (CNNs) are a promising solution for text classification due to recent advancements in hardware and big data management. Additionally, the increasing amount of data available necessitates the development of novel machine learning tools for managing this information. This study compares CNNs with the traditional bag-of-words approach as well as more efficient methods for text classification. The results demonstrate that CNNs outperform the conventional method in classifying harmful comments, indicating the approachs potential for further research in this field. This work highlights the importance of developing innovative strategies to combat online toxicity and provides insight into how deep learning techniques can be used for text classification.","Sugandhi, K, Reddy, R. Uday Kumar, Reddy, K. Ravi Kiran, Reddy, B. Balla",,,Predicting Hate Words and Offensive Language: A Machine Learning Approach,,,10.1109/ICISC62624.2024.00042 , ,,"The internet has led to a significant increase in the amount of information available, which has resulted in the rise of online communication among users. Unfortunately, this has also resulted in the proliferation of toxic online texts, which can lead to bullying, harassment, and personal attacks. While there have been attempts to develop efficient models for predicting toxic comments online, the field is still in its nascent stage, hence new frameworks and strategies are needed to address this issue. Convolutional Neural Networks (CNNs) are a promising solution for text classification due to recent advancements in hardware and big data management. Additionally, the increasing amount of data available necessitates the development of novel machine learning tools for managing this information. This study compares CNNs with the traditional bag-of-words approach as well as more efficient methods for text classification. The results demonstrate that CNNs outperform the conventional method in classifying harmful comments, indicating the approachs potential for further research in this field. This work highlights the importance of developing innovative strategies to combat online toxicity and provides insight into how deep learning techniques can be used for text classification.",,,,, ,  2024 8th International Conference on Inventive Systems and Control (ICISC),Text mining;Industries;Deep learning;Toxicology;Text categorization;Predictive models;Control systems;Text Mining;Toxic Text Classification;Toxicity Detection;CNN;Deep Learning,detection,
3087,"**Title**Detection of Mental Health Issues and Solution to Online Toxicity using Machine Learning

**Abstract**In modern times, the use of the internet and social media has greatly increased, leading to the emergence of online toxicity, which can have serious consequences for individuals' mental health. Machine learning techniques can be used to detect mental health issues and address online toxicity. One approach to detecting mental health issues in its early stages is to use machine learning to analyze social media posts and other online content. By training a model on large datasets of labeled text, it is possible to identify patterns and indicators of mental health issues such as depression, anxiety, and stress. To address online toxicity, machine learning can be used to identify and classify toxic content, such as hate speech or cyberbullying. This can be done by training a model on a dataset of labeled toxic and non-toxic content Once the model is trained, it can be used to identify and flag the toxic content in real-time, allowing it to be moderated or removed completely. Overall, the use of machine learning for the detection of mental health issues and the solution to online toxicity can be a powerful tool for promoting mental wellbeing and creating a safer and more positive online environment.","H K, Parjanya, Hegde, Ramakrishna, Mishra, Nikhil Kumar, Kumar Sandliya, Abhishek, Singh, Anant, S M, Soumyasri",,,Detection of Mental Health Issues and Solution to Online Toxicity using Machine Learning,,,10.1109/MysuruCon59703.2023.10396981 , ,,"In modern times, the use of the internet and social media has greatly increased, leading to the emergence of online toxicity, which can have serious consequences for individuals' mental health. Machine learning techniques can be used to detect mental health issues and address online toxicity. One approach to detecting mental health issues in its early stages is to use machine learning to analyze social media posts and other online content. By training a model on large datasets of labeled text, it is possible to identify patterns and indicators of mental health issues such as depression, anxiety, and stress. To address online toxicity, machine learning can be used to identify and classify toxic content, such as hate speech or cyberbullying. This can be done by training a model on a dataset of labeled toxic and non-toxic content Once the model is trained, it can be used to identify and flag the toxic content in real-time, allowing it to be moderated or removed completely. Overall, the use of machine learning for the detection of mental health issues and the solution to online toxicity can be a powerful tool for promoting mental wellbeing and creating a safer and more positive online environment.",,,,, ,  2023 IEEE 3rd Mysore Sub Section International Conference (MysuruCon),Training;Sentiment analysis;Analytical models;Toxicology;Machine learning;Mental health;Feature extraction;Machine Learning;Online Toxicity;Depression;Anxiety;Mental Health.,detection,
3094,"**Title**Combination and Knowledge Extension of Pre-trained Language Model for Offensive Language Detection

**Abstract**Nowadays, more and more offensive comments are posted on social media. Those offensive comments can seriously cause mental damage to other people. Therefore, those toxic and harmful comments should be detected timely and accurate. In this paper, we exploit the powerful pre-train language models (PLM) in detecting offensive language. We consider two PLMs in our paper: BERT and DeepMoji. We finetune their combination and evaluate the performance with two datasets: Ask FM and Curious Cat. We found that DeepMoji outperforms BERT. We analyze the result from two aspects and conclude that the task and data of pre-training are important to PLM. Seeing that BERT is less effective than DeepMoji, it is possible to improve the performance of BERT. We then propose a Knowledge Extension method to improve the performance of the BERT model. We find that the performance of PLM with high-quality extensional knowledge can be improved significantly.","Li, Zhenming, Shimada, Kazutaka",,,Combination and Knowledge Extension of Pre-trained Language Model for Offensive Language Detection,,,10.1109/IIAI-AAI59060.2023.00026 , ,,"Nowadays, more and more offensive comments are posted on social media. Those offensive comments can seriously cause mental damage to other people. Therefore, those toxic and harmful comments should be detected timely and accurate. In this paper, we exploit the powerful pre-train language models (PLM) in detecting offensive language. We consider two PLMs in our paper: BERT and DeepMoji. We finetune their combination and evaluate the performance with two datasets: Ask FM and Curious Cat. We found that DeepMoji outperforms BERT. We analyze the result from two aspects and conclude that the task and data of pre-training are important to PLM. Seeing that BERT is less effective than DeepMoji, it is possible to improve the performance of BERT. We then propose a Knowledge Extension method to improve the performance of the BERT model. We find that the performance of PLM with high-quality extensional knowledge can be improved significantly.",,,,, ,  2023 14th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI),Analytical models;Frequency modulation;Social networking (online);Face recognition;Media;Data models;Task analysis;offensive language detection;machine learning;pre-trained language model;social media,detection,
3095,"**Title**Toxicity Tweet Detection and Classification Using NLP Driven Techniques

**Abstract**The use of social media is increasing regularly. Unethical things like defamation, spreading hatred, pornography, etc are becoming easier for some irresponsible users because of the easy accessing and due to similarity of social media. In this view, this study has been carried out to use machine learning techniques to classify comments into their hazardous categories. In order to categorize a comment based on its toxicity, this research compares classic machine learning methods with deep learning methods including Logistic Regression, SVM, RNN, and LSTM. To compare the performance of the models, four distinct models are developed, put into use, and trained on a common dataset. These models were developed and evaluated using sizable amounts of secondary qualitative data that included several comments that were either labeled as harmful or not. Results showed that employing LSTM, a satisfactory accuracy of 90.7% and an F1- score of 0.94 were obtained.","Pal, Anuj Kumar, Rai, Sakshi",,,Toxicity Tweet Detection and Classification Using NLP Driven Techniques,,,10.1109/ICTBIG59752.2023.10456026 , ,,"The use of social media is increasing regularly. Unethical things like defamation, spreading hatred, pornography, etc are becoming easier for some irresponsible users because of the easy accessing and due to similarity of social media. In this view, this study has been carried out to use machine learning techniques to classify comments into their hazardous categories. In order to categorize a comment based on its toxicity, this research compares classic machine learning methods with deep learning methods including Logistic Regression, SVM, RNN, and LSTM. To compare the performance of the models, four distinct models are developed, put into use, and trained on a common dataset. These models were developed and evaluated using sizable amounts of secondary qualitative data that included several comments that were either labeled as harmful or not. Results showed that employing LSTM, a satisfactory accuracy of 90.7% and an F1- score of 0.94 were obtained.",,,,, ,  2023 IEEE International Conference on ICT in Business Industry & Government (ICTBIG),Support vector machines;Deep learning;Logistic regression;Toxicology;Social networking (online);Text categorization;Government;Long-Short Term Memory;Recurrent Neural Network;Text mining;Toxic text classification;Text classification,detection,
3117,"**Title**Automated Functionality and Security Evaluation of Large Language Models

**Abstract**Natural language processing (NLP) is rapidly developing. A series of Large Language Models (LLMs) have emerged, represented by ChatGPT, which have made significant breakthroughs in natural language understanding and generation, enabling fluent dialogue with humans, understanding human intentions, and completing complex tasks. However, in addition to the fairness and toxicity of traditional language models, some new problems, including hallucination, have also emerged in LLMs, making them hard to use. Evaluating LLMs manually is challenging due to subjectivity and inefficiency. In this paper, we focused on the fuzzy matching, toxicity detection, and hallucination detection in the evaluation of LLMs automatically, and fine-tune the Mixtral-8x7B Model, which can be deployed in private cloud environment, and prove the effectiveness of our method through experiments.","Ding, Minjie, Shen, Ying, Chen, Mingang",,,Automated Functionality and Security Evaluation of Large Language Models,,,10.1109/SmartCloud62736.2024.00014 , ,,"Natural language processing (NLP) is rapidly developing. A series of Large Language Models (LLMs) have emerged, represented by ChatGPT, which have made significant breakthroughs in natural language understanding and generation, enabling fluent dialogue with humans, understanding human intentions, and completing complex tasks. However, in addition to the fairness and toxicity of traditional language models, some new problems, including hallucination, have also emerged in LLMs, making them hard to use. Evaluating LLMs manually is challenging due to subjectivity and inefficiency. In this paper, we focused on the fuzzy matching, toxicity detection, and hallucination detection in the evaluation of LLMs automatically, and fine-tune the Mixtral-8x7B Model, which can be deployed in private cloud environment, and prove the effectiveness of our method through experiments.",,,,, ,  2024 9th IEEE International Conference on Smart Cloud (SmartCloud),Cloud computing;Toxicology;Accuracy;Graphics processing units;Chatbots;Security;Task analysis;LLM;evaluation;fuzzy matching;toxicity;hallucination,detection#evaluation,
3119,"**Title**Encouraging Emotion Regulation in Social Media Conversations through Self-Reflection

**Abstract**Digital Emotion Regulation (DER) is the practice of employing digital technologies, such as smartphones, to influence one's emotional state. However, online rage, hate speech, or toxicity that stem from ineffective Emotional Regulation (ER) often harms online well-being. Earlier research has focused on identifying ER with multi-modal sensors and studying daily DER practices, but the contextual dynamics of DER remain unexplored. This work provides an intervention for supporting DER in online conversations. It introduces a graph-based framework to identify the need for ER in these conversations. Additionally, it informs users of their emotional influence on a conversation, prompting self-reflection. Through interaction design, this work introduces a technique to de-anonymize online users, encouraging accountability and responsible online behaviour to maintain digital well-being. We collected 2.5K tweets from major news outlets and Australian state Premiers over a year, analysing emotions at individual and group levels in Twitter conversations. Compared to Google's Perspective API, our framework reduced toxicity by 10% on test data, surpassing Google's API by 3%.","Verma, Akriti, Islam, Shama, Moghaddam, Valeh, Anwar, Adnan",,,Encouraging Emotion Regulation in Social Media Conversations through Self-Reflection,,,10.1109/IEEECONF58110.2023.10520471 , ,,"Digital Emotion Regulation (DER) is the practice of employing digital technologies, such as smartphones, to influence one's emotional state. However, online rage, hate speech, or toxicity that stem from ineffective Emotional Regulation (ER) often harms online well-being. Earlier research has focused on identifying ER with multi-modal sensors and studying daily DER practices, but the contextual dynamics of DER remain unexplored. This work provides an intervention for supporting DER in online conversations. It introduces a graph-based framework to identify the need for ER in these conversations. Additionally, it informs users of their emotional influence on a conversation, prompting self-reflection. Through interaction design, this work introduces a technique to de-anonymize online users, encouraging accountability and responsible online behaviour to maintain digital well-being. We collected 2.5K tweets from major news outlets and Australian state Premiers over a year, analysing emotions at individual and group levels in Twitter conversations. Compared to Google's Perspective API, our framework reduced toxicity by 10% on test data, surpassing Google's API by 3%.",,,,, ,  2023 IEEE Engineering Informatics,Emotion recognition;Toxicology;Social networking (online);Multimodal sensors;Hate speech;Oral communication;Regulation;Digital Emotion Regulation (DER);Human Computer Interaction (HCI);Affective Computing;Emotions in Social Media,detection,
3137,"**Title**On mission Twitter Profiles: A Study of Selective Toxic Behavior

**Abstract**The argument for persistent social media influence campaigns, often funded by malicious entities, is gaining traction. These entities utilize instrumented profiles to disseminate divisive content and disinformation, shaping public perception. Despite ample evidence of these instrumented profiles, few identification methods exist to locate them in the wild. To evade detection and appear genuine, small clusters of instrumented profiles engage in unrelated discussions, diverting attention from their true goals [34]. This strategic thematic diversity conceals their selective polarity towards certain topics and fosters public trust [49]. This study aims to characterize profiles potentially used for influence operations, termed on-mission profiles, relying solely on thematic content diversity within unlabeled data. Distinguishing this work is its focus on content volume and toxicity towards specific themes. Longitudinal data from 138K Twitter (rebranded as X) profiles and 293M tweets enables profiling based on theme diversity. High thematic diversity groups predominantly produce toxic content concerning specific themes, like politics, health, and newsclassifying them as on-mission profiles. Using the identified on-mission profiles, we design a classifier for unseen, unlabeled data. Employing a linear SVM model, we train and test it on an 80/20% split of the most diverse profiles. The classifier achieves a flawless 100% accuracy, facilitating the discovery of previously unknown on-mission profiles in the wild.","Qayyum, Hina, Ikram, Muhammad, Zhao, Benjamin Zi Hao, Wood, Ian D., Kourtellis, Nicolas, Kaafar, Mohamed Ali",,,On mission Twitter Profiles: A Study of Selective Toxic Behavior,,,10.1109/BigData59044.2023.10386248 , ,,"The argument for persistent social media influence campaigns, often funded by malicious entities, is gaining traction. These entities utilize instrumented profiles to disseminate divisive content and disinformation, shaping public perception. Despite ample evidence of these instrumented profiles, few identification methods exist to locate them in the wild. To evade detection and appear genuine, small clusters of instrumented profiles engage in unrelated discussions, diverting attention from their true goals [34]. This strategic thematic diversity conceals their selective polarity towards certain topics and fosters public trust [49]. This study aims to characterize profiles potentially used for influence operations, termed on-mission profiles, relying solely on thematic content diversity within unlabeled data. Distinguishing this work is its focus on content volume and toxicity towards specific themes. Longitudinal data from 138K Twitter (rebranded as X) profiles and 293M tweets enables profiling based on theme diversity. High thematic diversity groups predominantly produce toxic content concerning specific themes, like politics, health, and newsclassifying them as on-mission profiles. Using the identified on-mission profiles, we design a classifier for unseen, unlabeled data. Employing a linear SVM model, we train and test it on an 80/20% split of the most diverse profiles. The classifier achieves a flawless 100% accuracy, facilitating the discovery of previously unknown on-mission profiles in the wild.",,,,, ,  2023 IEEE International Conference on Big Data (BigData),Support vector machines;Toxicology;Social networking (online);Instruments;Blogs;Media;Real-time systems;On-mission profile on Twitter(X);Toxicity;misbehavior;thematic diversity in online content,detection,
3139,"**Title**Federated Learning Support for Cybersecurity: Fundamentals, Applications, and Opportunities

**Abstract**The term Federated Learning (FL) refers to a modern and advanced intelligence system that uses data storage that is not centralised. Most industrialists are hesitant to use the Internet of Everything (IoT) technology since cyberattacks are common and occur in numerous real-time applications around the globe. To deal with this problem, FL can be used to prevent such cyberattacks by offering better cybersecurity. This research intends to fill in some of the gaps between where federated AI is now and where it can be widely used by doing a thorough investigation of FLs security and privacy features. We provide an informative description of techniques and different implementation styles, while also examining the current difficulties in FL, and we establish a complete evaluation of security and privacy problems that must be taken into account. Our research shows that the privacy risks connected with FL are lower than the security risks. While inference-based attacks pose the greatest risk to FLs privacy, transmission difficulties, toxicity, and backdoor breaches pose the greatest risk to security. In the final section of the paper, we outline key areas for future study that will help FL adapt to real-world settings.","Mohawesh, Rami, Maqsood, Sumbal, Jararweh, Yaser, Salameh, Haythem Bany",,,"Federated Learning Support for Cybersecurity: Fundamentals, Applications, and Opportunities",,,10.1109/ICCNS58795.2023.10193279 , ,,"The term Federated Learning (FL) refers to a modern and advanced intelligence system that uses data storage that is not centralised. Most industrialists are hesitant to use the Internet of Everything (IoT) technology since cyberattacks are common and occur in numerous real-time applications around the globe. To deal with this problem, FL can be used to prevent such cyberattacks by offering better cybersecurity. This research intends to fill in some of the gaps between where federated AI is now and where it can be widely used by doing a thorough investigation of FLs security and privacy features. We provide an informative description of techniques and different implementation styles, while also examining the current difficulties in FL, and we establish a complete evaluation of security and privacy problems that must be taken into account. Our research shows that the privacy risks connected with FL are lower than the security risks. While inference-based attacks pose the greatest risk to FLs privacy, transmission difficulties, toxicity, and backdoor breaches pose the greatest risk to security. In the final section of the paper, we outline key areas for future study that will help FL adapt to real-world settings.",,,,, ,"  2023 International Conference on Intelligent Computing, Communication, Networking and Services (ICCNS)",Privacy;Toxicology;Federated learning;Memory;Real-time systems;Internet of Things;Computer crime;cybersecurity;machine learning;federated learning,detection,
3141,"**Title**Leveraging Deep Learning for Detecting Toxicity in Online Comments

**Abstract**Toxic comments are pervasive on online platforms, which makes it difficult to maintain positive dialogue and create a secure online space. Peoples emotions and overall state of mental health start to seriously decline as a result of those toxic remarks. In this study, we examine how optimizers affect the training of three well-known RNN architectures for the toxic comment categorization task: LSTM, Bi-LSTM, and GRU. According to our research, the models performance is greatly influenced by the optimizer used; the Bi-LSTM model, which was trained using the ADAM optimizer, showed the greatest test accuracy of $\mathbf{9 5. 3 3 \%}$. Our results highlight the crucial role optimization techniques play in improving moderation systems effectiveness and advancing the creation of more potent mitigation strategies for online toxicity.","Rayani, Reddy Kowshik, Tekula, Samhitha, Vattigunta, Subhash Kovid, Kovi, Naveen Kumar, Namitha, Kalakunnath",,,Leveraging Deep Learning for Detecting Toxicity in Online Comments,,,10.1109/ICCCNT61001.2024.10726256 , ,,"Toxic comments are pervasive on online platforms, which makes it difficult to maintain positive dialogue and create a secure online space. Peoples emotions and overall state of mental health start to seriously decline as a result of those toxic remarks. In this study, we examine how optimizers affect the training of three well-known RNN architectures for the toxic comment categorization task: LSTM, Bi-LSTM, and GRU. According to our research, the models performance is greatly influenced by the optimizer used; the Bi-LSTM model, which was trained using the ADAM optimizer, showed the greatest test accuracy of $\mathbf{9 5. 3 3 \%}$. Our results highlight the crucial role optimization techniques play in improving moderation systems effectiveness and advancing the creation of more potent mitigation strategies for online toxicity.",,,,, ,  2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),Training;Deep learning;Toxicology;Accuracy;Prevention and mitigation;Mental health;Computer architecture;Optimization;Long short term memory;Toxicity detection;LSTM;GRU;Filtration,detection,
3144,"**Title**Enhancing Inclusive Online Conversations with Multimodal AI for Contextual Toxicity Analysis, Automated Rephrasing, and Bias Detection

**Abstract**Online communities and discussion platforms provide a suitable stage to share ideas, opinions and thoughts. But they have also created space to increase negativity by posting toxic and nuance contents and comments, leading to division, chaos and fights. These contents are sometimes targeted on a certain group of people based on gender, caste, religion and countries, paving ways to the disrupting of world peace. Some of these contents are targeted against an individual and are threatening and harsh, which can be coined as cyber bullying. The proposed system is aimed to provide a multi-featured solution that encompasses real time data moderation with the help of ensemble of embedder modules like GloVe, Emo2Vec and BERT to produce both semantic and contextual embeddings accompanied by BERT encoder and classifier, rephrasing module to provide inclusive paraphrases for the toxic inputs while preserving the information and a reinforcement module for the system to increase its adaptivity. This novel combination of modules overcomes the limitations of existing system by increasing the usability, robustness and real time processing.","Ananthajothi, K, Meenakshi, R, Monica, S",,,"Enhancing Inclusive Online Conversations with Multimodal AI for Contextual Toxicity Analysis, Automated Rephrasing, and Bias Detection",,,10.1109/ICSES60034.2023.10465509 , ,,"Online communities and discussion platforms provide a suitable stage to share ideas, opinions and thoughts. But they have also created space to increase negativity by posting toxic and nuance contents and comments, leading to division, chaos and fights. These contents are sometimes targeted on a certain group of people based on gender, caste, religion and countries, paving ways to the disrupting of world peace. Some of these contents are targeted against an individual and are threatening and harsh, which can be coined as cyber bullying. The proposed system is aimed to provide a multi-featured solution that encompasses real time data moderation with the help of ensemble of embedder modules like GloVe, Emo2Vec and BERT to produce both semantic and contextual embeddings accompanied by BERT encoder and classifier, rephrasing module to provide inclusive paraphrases for the toxic inputs while preserving the information and a reinforcement module for the system to increase its adaptivity. This novel combination of modules overcomes the limitations of existing system by increasing the usability, robustness and real time processing.",,,,, ,"  2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)",Deep learning;Ethics;Toxicology;Semantics;Cyberbullying;Oral communication;Real-time systems;Cyber Bullying;Multi-Feature Solution;GloVe;Emo2Vec;BERT;rephrasing and reinforcement module,detection,
3159,"**Title**Confronting Hate Speech in SMART Environments: An Approach that Uses Ensemble Learning and LSTM

**Abstract**In the current digital age of the Internet and online virtualization, it has established different kinds of online media, such as social media: two important and most pervasive ways of communicating. However, it becomes a little hard for them to deal with the responsibility, i.e., to regulate and filter the toxic or harmful content across the platform, specifically in filtering out the offensive comments posted by the users. In this work, we are particularly interested in the main mission of hate comment detection. Our major objective is to flag those comments which are offensive, harmful, and abusive in an online conversation. This paper proposes an original ensembling based learning scheme, taking the power of LSTM neural networks, to solve this crucial problem. LSTMs, specially designed types of RNNs, have shown to be effective in sequence modeling and also demonstrated some promising results in problems related to sequences. Our methodology is based on the existence of certain LSTM models of small size, which are trained on an independent fraction of all completed and pre-processed data. This approach allows the ensemble to capture the diversity in patterns; hence, it contributes to the large robustness of hate comment detection. In an ever-globalized and digitized world, the advent of dependable methodologies for hate comment detection becomes cardinal in acquiring safe and inclusive online environments. Our research is a promising step towards reducing online toxicity in order to have healthier and more constructive digital conversations.","Kaushik, Priyanka, Rohilla, Rohit, Walia, Pravansh, Shankar, Saksham, Kaushik, Manas M., Gupta, Tarushi Sandeep",,,Confronting Hate Speech in SMART Environments: An Approach that Uses Ensemble Learning and LSTM,,,10.1109/ICSD60021.2024.10751441 , ,,"In the current digital age of the Internet and online virtualization, it has established different kinds of online media, such as social media: two important and most pervasive ways of communicating. However, it becomes a little hard for them to deal with the responsibility, i.e., to regulate and filter the toxic or harmful content across the platform, specifically in filtering out the offensive comments posted by the users. In this work, we are particularly interested in the main mission of hate comment detection. Our major objective is to flag those comments which are offensive, harmful, and abusive in an online conversation. This paper proposes an original ensembling based learning scheme, taking the power of LSTM neural networks, to solve this crucial problem. LSTMs, specially designed types of RNNs, have shown to be effective in sequence modeling and also demonstrated some promising results in problems related to sequences. Our methodology is based on the existence of certain LSTM models of small size, which are trained on an independent fraction of all completed and pre-processed data. This approach allows the ensemble to capture the diversity in patterns; hence, it contributes to the large robustness of hate comment detection. In an ever-globalized and digitized world, the advent of dependable methodologies for hate comment detection becomes cardinal in acquiring safe and inclusive online environments. Our research is a promising step towards reducing online toxicity in order to have healthier and more constructive digital conversations.",,,,, ,  2023 International Conference on Smart Devices (ICSD),Voice activity detection;Toxicology;Social networking (online);Hate speech;Oral communication;Speech recognition;Robustness;Ensemble learning;Virtualization;Long short term memory;Hate comment detection;LSTM;LSTM and RNN;Ensemble Learning;Hate speech detection,detection,
3164,"**Title**Ensemble Learning in the Recognition of Various Types of the Online Toxicity

**Abstract**This article offers approaches to classifying online toxicity using a set of methods learning. It focuses on detecting toxic posts mainly on fake news and offensive posts. The article also provides definitions of basic terms, problems associated with recognizing toxic posts, places on the web where they occur, as well as specific steps that can be taken to protect users from them. Based on the analysis of the current state and the latest knowledge, various methods of ensemble machine learning are proposed, which have been used for training, testing and evaluation on different groups of toxic posts. The article also contains detailed test results.","Machov, Kristna, Mach, Marin, Balara, Viliam, Husnaj, Patrik",,,Ensemble Learning in the Recognition of Various Types of the Online Toxicity,,,10.1109/ICETA63795.2024.10850784 , ,,"This article offers approaches to classifying online toxicity using a set of methods learning. It focuses on detecting toxic posts mainly on fake news and offensive posts. The article also provides definitions of basic terms, problems associated with recognizing toxic posts, places on the web where they occur, as well as specific steps that can be taken to protect users from them. Based on the analysis of the current state and the latest knowledge, various methods of ensemble machine learning are proposed, which have been used for training, testing and evaluation on different groups of toxic posts. The article also contains detailed test results.",,,,, ,  2024 International Conference on Emerging eLearning Technologies and Applications (ICETA),Training;Measurement;Toxicology;Social networking (online);Ensemble learning;Fake news;Bagging;Random forests;Long short term memory;Testing;ensemble machine learning;toxic posts detection;random forests;bagging;boosting,detection,
3168,"**Title**DriftGLY: A Low-Cost IoT-Based Ecosystem for Monitoring Agrochemical Spray Drifts

**Abstract**Agrochemicals are to be blamed for the poisoning of millions of human beings worldwide, with acute and chronic exposure to such substances causing thousands of deaths. The presence of agrochemicals in the air is caused by drifta phenomenon that originates from ground and aerial spraying applied in lands used for agribusiness. Drift refers to the spread and wind-driven transport of agrochemicals that have been volatilized during spraying, and which can easily reach towns and cities. Actively monitoring agrochemical drift is essential to protect the health of citizens and the environment. Many of the systems used for monitoring agrochemicals are currently analog, while those with some degree of digitalization are expensive, complex, and not easily scalable. Therefore, new cost-effective, stand-alone, reliable, easy-to-implement, and citizen-oriented technological tools need to be designed and developed. This article introduces DriftGLY, an innovative, low-cost Internet of Things (IoT) early warning system that utilizes a digital traffic light to continuously and automatically monitor the presence of agrochemicals in the air, both individually and in cocktail form. The key innovation of DriftGLY lies in the close relationship between the configuration of its digital traffic light and the precise calculation of the quantity of agrochemicals deposited in spray tanks. This configuration is specifically based on the parameter of the dose rate expressed in liters per hectare of these contaminants. DriftGLY incorporates an original collection system that allows for the indirect detection of agrochemicals in their particulate, gaseous, and, unlike traditional systems, liquid forms. In this manner, DriftGLY displays pollution risk levels through its digital traffic light, transmitting them to an IoT platform via various connectivity options, such as WiFi, LoRaWAN, and GSM/GPRS, and posts them on the social network X (formerly known as Twitter). The aim of this action is to democratize information and promote the necessary changes for citizens to enjoy a healthy and balanced environment.","Aira, Javier, Olivares, Teresa, Delicado, Francisco M.",,,DriftGLY: A Low-Cost IoT-Based Ecosystem for Monitoring Agrochemical Spray Drifts,,,10.1109/TIM.2024.3350146 , ,,"Agrochemicals are to be blamed for the poisoning of millions of human beings worldwide, with acute and chronic exposure to such substances causing thousands of deaths. The presence of agrochemicals in the air is caused by drifta phenomenon that originates from ground and aerial spraying applied in lands used for agribusiness. Drift refers to the spread and wind-driven transport of agrochemicals that have been volatilized during spraying, and which can easily reach towns and cities. Actively monitoring agrochemical drift is essential to protect the health of citizens and the environment. Many of the systems used for monitoring agrochemicals are currently analog, while those with some degree of digitalization are expensive, complex, and not easily scalable. Therefore, new cost-effective, stand-alone, reliable, easy-to-implement, and citizen-oriented technological tools need to be designed and developed. This article introduces DriftGLY, an innovative, low-cost Internet of Things (IoT) early warning system that utilizes a digital traffic light to continuously and automatically monitor the presence of agrochemicals in the air, both individually and in cocktail form. The key innovation of DriftGLY lies in the close relationship between the configuration of its digital traffic light and the precise calculation of the quantity of agrochemicals deposited in spray tanks. This configuration is specifically based on the parameter of the dose rate expressed in liters per hectare of these contaminants. DriftGLY incorporates an original collection system that allows for the indirect detection of agrochemicals in their particulate, gaseous, and, unlike traditional systems, liquid forms. In this manner, DriftGLY displays pollution risk levels through its digital traffic light, transmitting them to an IoT platform via various connectivity options, such as WiFi, LoRaWAN, and GSM/GPRS, and posts them on the social network X (formerly known as Twitter). The aim of this action is to democratize information and promote the necessary changes for citizens to enjoy a healthy and balanced environment.",,,,, ,  ,Agrochemicals;Monitoring;Pollution measurement;Internet of Things;Contamination;Wireless fidelity;Technological innovation;Agriculture;agrochemicals;air pollution;environmental monitoring;Internet of Things (IoT);low-power wide-area network (LPWAN);persistent organic pollutants (POPs);smart cities,,
3195,"**Title**Toxicity Unveiled: Understanding Online Conversations

**Abstract**Internet forums enable people to express their opinions and participate in discussions on a range of subjects in the current digital era. But sometimes, this right to free speech can lead to the dissemination of harmful remarks like hate speech, harassment, and other types of damaging material. Ensuring a secure and courteous online space requires identifying and eliminating remarks that are harmful. This research article investigates many approaches to detecting poisonous comments, including classic machine learning techniques like BOW and TF-IDF, and deep learning methods like LSTM. In addition, we explore the performance of stochastic gradient descent (SGD) regressor and decision tree models in categorizing harmful remarks using these variables. Through rigorous research and review, we show that these approaches are effective at recognizing hazardous remarks. Our findings help to build strong models for automated toxic comment detection, promoting healthier online discourse.","Patil, Ratna, Kulkarni, Swapneel, Ingole, Prajwal, Kumar, Pratiyush, Pawar, Sandesh, Rawandale, Shiltalkumar",,,Toxicity Unveiled: Understanding Online Conversations,,,10.1109/ICEECT61758.2024.10738878 , ,,"Internet forums enable people to express their opinions and participate in discussions on a range of subjects in the current digital era. But sometimes, this right to free speech can lead to the dissemination of harmful remarks like hate speech, harassment, and other types of damaging material. Ensuring a secure and courteous online space requires identifying and eliminating remarks that are harmful. This research article investigates many approaches to detecting poisonous comments, including classic machine learning techniques like BOW and TF-IDF, and deep learning methods like LSTM. In addition, we explore the performance of stochastic gradient descent (SGD) regressor and decision tree models in categorizing harmful remarks using these variables. Through rigorous research and review, we show that these approaches are effective at recognizing hazardous remarks. Our findings help to build strong models for automated toxic comment detection, promoting healthier online discourse.",,,,, ,  2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT),Deep learning;Toxicology;Accuracy;Reviews;Stochastic processes;Predictive models;Data models;Speech processing;Long short term memory;Regression tree analysis;toxic comment detection;hate speech detection;online moderation;ml;dl;bow;tf-idf;lstm;text classification;natural language processing,detection,
3196,"**Title**A Comparison of Word Embeddings for Comment Toxicity Detection: Detection Power of Computer

**Abstract**The rapid expansion of online communities and social media platforms, the issue of toxic comments has become increasingly prevalent. Toxic comments not only contribute to a negative user experience but also have the potential to propagate hate speech, harassment, and cyberbullies. Therefore, there is a pressing need for robust and efficient comment toxicity detection systems that can automatically identify and flag toxic comments. The proposed methodology involves preprocessing the data-set by removing punctuation, applying stemming and lemmatization techniques to standardize the text. The data is then transformed into a matrix representation using tokenization, padding, and word embedding techniques. In the word embedding phase, three popular algorithms, namely Word2Vec, GloVe, and FastText, are compared to determine the most suitable approach for capturing the semantic meaning of words. The architectural design comprises several layers, encompassing an embedding layer, bidirectional LSTM layer, dropout layers, and dense layers. The bidirectional LSTM layer plays a crucial role in assimilating contextual information from both preceding and subsequent words within the comment. Training involves utilizing the training set, followed by evaluation. This project addresses the crucial need for comment toxicity detection and provides an effective methodology for identifying toxic comments in online platforms. By automatically detecting and flagging toxic comments, this system can contribute to fostering a healthier and more respectful online environment. Index Termscomment toxicity detection, social media plat-forms, toxic comments, hate speech, harassment, cyberbullies, stemming, lemmatization, word embedding, Word2Vec, GloVe, FastText, bidirectional LSTM.","R, Prasanna Kumar, G, Bharathi Mohan, R, Elakkiya, P, Varsha",,,A Comparison of Word Embeddings for Comment Toxicity Detection: Detection Power of Computer,,,10.1109/ICCSAI59793.2023.10421356 , ,,"The rapid expansion of online communities and social media platforms, the issue of toxic comments has become increasingly prevalent. Toxic comments not only contribute to a negative user experience but also have the potential to propagate hate speech, harassment, and cyberbullies. Therefore, there is a pressing need for robust and efficient comment toxicity detection systems that can automatically identify and flag toxic comments. The proposed methodology involves preprocessing the data-set by removing punctuation, applying stemming and lemmatization techniques to standardize the text. The data is then transformed into a matrix representation using tokenization, padding, and word embedding techniques. In the word embedding phase, three popular algorithms, namely Word2Vec, GloVe, and FastText, are compared to determine the most suitable approach for capturing the semantic meaning of words. The architectural design comprises several layers, encompassing an embedding layer, bidirectional LSTM layer, dropout layers, and dense layers. The bidirectional LSTM layer plays a crucial role in assimilating contextual information from both preceding and subsequent words within the comment. Training involves utilizing the training set, followed by evaluation. This project addresses the crucial need for comment toxicity detection and provides an effective methodology for identifying toxic comments in online platforms. By automatically detecting and flagging toxic comments, this system can contribute to fostering a healthier and more respectful online environment. Index Termscomment toxicity detection, social media plat-forms, toxic comments, hate speech, harassment, cyberbullies, stemming, lemmatization, word embedding, Word2Vec, GloVe, FastText, bidirectional LSTM.",,,,, ,"  2023 International Conference on Communication, Security and Artificial Intelligence (ICCSAI)",Training;Toxicology;Semantics;Hate speech;Cyberbullying;Computer architecture;User interfaces;comment toxicity detection;social media platforms;toxic comments;hate speech;harassment;cyberbullies;stemming;lemmatization;word embedding;Word2Vec;GloVe;FastText;bidirectional LSTM,detection,
3208,"**Title**Eliminating Toxicity in Text: An NLP Framework for Clean Content Extraction

**Abstract**This research study proposes a machine learning-based application called ToxiGuard, which is designed to ascertain the toxic language in text. The proposed technique employs the use of the Natural Language Processing (NLP) mechanism and a Naive Bayes based classifier to detect and categorize unhealthy or toxic comments. First the given text data is preprocessed by dividing or tokenizing the sentences into different words, once tokens are formed it will be passed as an argument into the proposed method which removes all the words which are unhealthy based on the trained model. Model takes the decision based on the patterns correlated with toxic or unhealthy language. The proposed mechanism is represented in the form of a graphical user interface (GUI) built with Tkinter. GUI being designed enables user to upload a text file which contains serious of text or sentences analysis is being done and visual representation of the outcome is plotted with help of a bar chart of the maximum frequency of each toxic words. In addition to analysis report outcome of the analysis can be exported for further review process. ToxiGuard focus to help or classify online content by providing effective and user-friendly application in-order to detect harmful or unhealthy language.","Kumar, Sharath, Dmello, Daxia Vlora, Janani, Devi, K D, Avani, P, Kirthana Kamath",,,Eliminating Toxicity in Text: An NLP Framework for Clean Content Extraction,,,10.1109/ICICNIS64247.2024.10823171 , ,,"This research study proposes a machine learning-based application called ToxiGuard, which is designed to ascertain the toxic language in text. The proposed technique employs the use of the Natural Language Processing (NLP) mechanism and a Naive Bayes based classifier to detect and categorize unhealthy or toxic comments. First the given text data is preprocessed by dividing or tokenizing the sentences into different words, once tokens are formed it will be passed as an argument into the proposed method which removes all the words which are unhealthy based on the trained model. Model takes the decision based on the patterns correlated with toxic or unhealthy language. The proposed mechanism is represented in the form of a graphical user interface (GUI) built with Tkinter. GUI being designed enables user to upload a text file which contains serious of text or sentences analysis is being done and visual representation of the outcome is plotted with help of a bar chart of the maximum frequency of each toxic words. In addition to analysis report outcome of the analysis can be exported for further review process. ToxiGuard focus to help or classify online content by providing effective and user-friendly application in-order to detect harmful or unhealthy language.",,,,, ,  2024 International Conference on IoT Based Control Networks and Intelligent Systems (ICICNIS),Visualization;Toxicology;Sensitivity;Scalability;Feature extraction;Natural language processing;Safety;Security;Standards;Graphical user interfaces;ToxiGuard;NLP;Toxic;Unhealthy;Tkinter;Nave based classifier,detection,
3219,"**Title**Text Mining Domestic Extremism Topics on Multiple Social Media Platforms

**Abstract**The U.S. Capitol riot increased scholarly interest in the relationship between online platforms and domestic extremism. This study utilized text mining methods such as LDA topic modeling and toxicity analysis to assess the prominent narratives disseminated by influential far-right accounts on three social media platforms around the riot timeframe. We utilized a novel categorization system to classify topic streams into narrative frames. Parler posts contained the greatest conspiratorial content and cross-platform content sharing. Twitter was found to have a small number of far-right accounts that actively disseminate extremist views. YouTube far-right influencers produced the highest number of videos related to racial topics. Parler posts demonstrated a strong association between toxicity measures and current trending topics. A few mentions of far-right accounts within the top 20 topic streams, reveal that extreme racial and nationalist ideologies are active on the Twitter platform, even though overall toxicity was low. Our YouTube data was seeded by far-right accounts, and therefore, not surprisingly, the content was revealed to contain a high degree of racially-charged discourse. Our analysis revealed, however, that the YouTube data was also characterized by a high level of toxicity coming from both creators and commenters. Both Parler and YouTube demonstrated that some far-right content creators were able to drive up the toxicity of the discourse. These results attest to the impact social media influencers have on online discourse. Interestingly, however, the toxicity scores for related YouTube videos remained low, suggesting that the video-promotion algorithm does not necessarily increase exposure to radical content.","Mead, Esther L., McNerney, Hillary W., Agarwal, Nitin",,,Text Mining Domestic Extremism Topics on Multiple Social Media Platforms,,,10.1109/CLNLP64123.2024.00028 , ,,"The U.S. Capitol riot increased scholarly interest in the relationship between online platforms and domestic extremism. This study utilized text mining methods such as LDA topic modeling and toxicity analysis to assess the prominent narratives disseminated by influential far-right accounts on three social media platforms around the riot timeframe. We utilized a novel categorization system to classify topic streams into narrative frames. Parler posts contained the greatest conspiratorial content and cross-platform content sharing. Twitter was found to have a small number of far-right accounts that actively disseminate extremist views. YouTube far-right influencers produced the highest number of videos related to racial topics. Parler posts demonstrated a strong association between toxicity measures and current trending topics. A few mentions of far-right accounts within the top 20 topic streams, reveal that extreme racial and nationalist ideologies are active on the Twitter platform, even though overall toxicity was low. Our YouTube data was seeded by far-right accounts, and therefore, not surprisingly, the content was revealed to contain a high degree of racially-charged discourse. Our analysis revealed, however, that the YouTube data was also characterized by a high level of toxicity coming from both creators and commenters. Both Parler and YouTube demonstrated that some far-right content creators were able to drive up the toxicity of the discourse. These results attest to the impact social media influencers have on online discourse. Interestingly, however, the toxicity scores for related YouTube videos remained low, suggesting that the video-promotion algorithm does not necessarily increase exposure to radical content.",,,,, ,  2024 International Conference on Computational Linguistics and Natural Language Processing (CLNLP),Text mining;Toxicology;Video on demand;Social networking (online);Blogs;Drives;Natural language processing;Web sites;Streams;Videos;text mining;social media;domestic extremism,detection,
3229,"**Title**Multitask Learning with Feature Selection for Groups of Related Tasks

**Abstract**Multitask learning has been thoroughly proven to improve the generalization performance given a set of related tasks. Most multitask learning algorithm assume that all tasks are related. However, if all the tasks are not related, negative transfer of information occurs amongst the tasks, and the performance of traditional multitask learning algorithm worsens. Thus, we design an algorithm that simultaneously groups the related tasks and trains only the related task together. There are different approaches to train the related tasks in multi-task learning based on which information is shared across the tasks. These approaches either assume that the parameters of each of the tasks are situated close together, or assume that there is a common underlying latent space in the features of the tasks that is related. Most multi-task learning algorithm use either regularization method or matrix-variate priors. In our algorithm, the related tasks are tied together by a set of common features selected by each tasks. Thus, to train the related tasks together, we use spike and slab prior to select a common set of features for the related tasks, and a mixture of gaussians prior to select the set of related tasks. For validation, the developed algorithm is tested on toxicity prediction and hand written digit recognition data sets. The results show a significant improvement over multitask learning with feature selection for larger number of tasks. Further, the developed algorithm is also compared against another state of the art algorithm that similarly groups the related tasks together and proven to be better and more accurate.","Mishra, Meenakshi, Huan, Jun",,,Multitask Learning with Feature Selection for Groups of Related Tasks,,,10.1109/ICDM.2013.151 , ,,"Multitask learning has been thoroughly proven to improve the generalization performance given a set of related tasks. Most multitask learning algorithm assume that all tasks are related. However, if all the tasks are not related, negative transfer of information occurs amongst the tasks, and the performance of traditional multitask learning algorithm worsens. Thus, we design an algorithm that simultaneously groups the related tasks and trains only the related task together. There are different approaches to train the related tasks in multi-task learning based on which information is shared across the tasks. These approaches either assume that the parameters of each of the tasks are situated close together, or assume that there is a common underlying latent space in the features of the tasks that is related. Most multi-task learning algorithm use either regularization method or matrix-variate priors. In our algorithm, the related tasks are tied together by a set of common features selected by each tasks. Thus, to train the related tasks together, we use spike and slab prior to select a common set of features for the related tasks, and a mixture of gaussians prior to select the set of related tasks. For validation, the developed algorithm is tested on toxicity prediction and hand written digit recognition data sets. The results show a significant improvement over multitask learning with feature selection for larger number of tasks. Further, the developed algorithm is also compared against another state of the art algorithm that similarly groups the related tasks together and proven to be better and more accurate.",,,,, ,  2013 IEEE 13th International Conference on Data Mining,Chemicals;Slabs;Training;Vectors;Bayes methods;Equations;Mathematical model;Multi-task Learning;Expectation Propagation;Groups of Tasks;Spike and Slab Prior;Mixture of Gaussian Prior,detection,
3235,"**Title**SafeCultural: A Dataset for Evaluating Safety and Cultural Sensitivity in Large Language Models

**Abstract**The increasing use of Large Language Models (LLMs) in daily life raises important questions about ensuring their trustworthiness. While existing datasets are widely used to evaluate issues like safety and hallucinations, they often overlook important factors like social norms and cultural sensitivity. This paper introduces a dataset that incorporates regional cultural aspects, allowing LLMs to be more adaptable across diverse contexts. This dataset covers aspects of both safety and cultural sensitivity. The safety component includes seven dimensions: Unlawful Conduct, Toxicity, Violence, Privacy Violations, Harms to Minors, Adult Content, and Mental Health Issues. Cultural sensitivity is divided into three categories: Beliefs, Behaviors, and Safety. We tested across seven LLMs, the dataset aims to help models overcome cultural barriers, fostering user-friendly and culturally aware development. Furthermore, we provide guidelines for creating and evaluating culturally sensitive prompts to ensure they meet safety and cultural standards. The finding of this paper may help the development of more adaptable, culturally aware, and trustworthy LLMs for daily use.","Vongpradit, Pawat, Imsombut, Aurawan, Kongyoung, Sarawoot, Damrongrat, Chaianun, Phaholphinyo, Sitthaa, Tanawong, Tanik",,,SafeCultural: A Dataset for Evaluating Safety and Cultural Sensitivity in Large Language Models,,,10.1109/InCIT63192.2024.10810548 , ,,"The increasing use of Large Language Models (LLMs) in daily life raises important questions about ensuring their trustworthiness. While existing datasets are widely used to evaluate issues like safety and hallucinations, they often overlook important factors like social norms and cultural sensitivity. This paper introduces a dataset that incorporates regional cultural aspects, allowing LLMs to be more adaptable across diverse contexts. This dataset covers aspects of both safety and cultural sensitivity. The safety component includes seven dimensions: Unlawful Conduct, Toxicity, Violence, Privacy Violations, Harms to Minors, Adult Content, and Mental Health Issues. Cultural sensitivity is divided into three categories: Beliefs, Behaviors, and Safety. We tested across seven LLMs, the dataset aims to help models overcome cultural barriers, fostering user-friendly and culturally aware development. Furthermore, we provide guidelines for creating and evaluating culturally sensitive prompts to ensure they meet safety and cultural standards. The finding of this paper may help the development of more adaptable, culturally aware, and trustworthy LLMs for daily use.",,,,, ,  2024 8th International Conference on Information Technology (InCIT),Training;Adaptation models;Privacy;Sensitivity;Toxicology;Large language models;Safety;Cultural differences;Standards;Guidelines;Trustworthiness;Safety;Cultural;LLMS,Gen_dataset,
3245,"**Title**Sexism Discovery using CNN, Word Embeddings, NLP and Data Augmentation

**Abstract**The pervasive issue of online sexism continues to pose significant challenges, fostering environments characterized by toxicity and perpetuating harmful societal norms. In response, this paper presents an approach for the discovery of sexist statements employing convolutional neural networks (CNNs), Word Embeddings, and data augmentation techniques. Through the fusion of CNNs capacity for hierarchical feature extraction with the semantic representations afforded by Word Embeddings, our method achieves exemplary discrimination performance. Additionally, the incorporation of data augmentation enriches the training dataset, thereby augmenting model generalization and resilience. Empirical evaluation on a larger dataset of statements demonstrates the efficacy of our approach, surpassing many baseline approaches in terms of discovery accuracy, precision, recall and F1-score.","Fattahi, Jaouhar, Sghaier, Feriel, Mejri, Mohamed, Ghayoula, Ridha, Bahroun, Sahbi, Ziadia, Marwa",,,"Sexism Discovery using CNN, Word Embeddings, NLP and Data Augmentation",,,10.1109/CoDIT62066.2024.10708284 , ,,"The pervasive issue of online sexism continues to pose significant challenges, fostering environments characterized by toxicity and perpetuating harmful societal norms. In response, this paper presents an approach for the discovery of sexist statements employing convolutional neural networks (CNNs), Word Embeddings, and data augmentation techniques. Through the fusion of CNNs capacity for hierarchical feature extraction with the semantic representations afforded by Word Embeddings, our method achieves exemplary discrimination performance. Additionally, the incorporation of data augmentation enriches the training dataset, thereby augmenting model generalization and resilience. Empirical evaluation on a larger dataset of statements demonstrates the efficacy of our approach, surpassing many baseline approaches in terms of discovery accuracy, precision, recall and F1-score.",,,,, ,"  2024 10th International Conference on Control, Decision and Information Technologies (CoDIT)",Training;Adaptation models;Toxicology;Semantics;Training data;Data augmentation;Natural language processing;Data models;Convolutional neural networks;Resilience,detection,
3259,"**Title**A Hybrid Deep Learning Techniques Using BERT and CNN for Toxic Comments Classification

**Abstract**Cyberbullying is a pervasive issue across all forms of media, affecting various demographics and platforms indiscriminately. From social media networks to online forums and comment sections on news sites, the harmful behavior of cyberbullying manifests in many forms, including harassment, threats, and demeaning comments. This ubiquity underscores the need for effective detection mechanisms. This paper explores the identification of toxic traits in online comments using advanced hybrid models, specifically BERT-CNN and BERT-LSTM. The research methodology involved constructing and configuring multiple layers within these models to optimize their ability to detect harmful content. For the BERT-CNN model, BERT's powerful language understanding capabilities are combined with CNN's strength in feature extraction through convolutional layers, capturing spatial hierarchies of features. In the BERT-LSTM model, BERT is integrated with LSTM layers to leverage their ability to learn long-term dependencies and sequential patterns in text. Various configurations of these models were tested, including adj ustments in the batch sizes and the length of the word tokens, to enhance performance in identifying toxic language. The best model which is BERT-CNN using the configuration 256 as the unit and 32 as the batch size achieved 94.48% accuracy in detecting toxic comments. The result from the model indicates that by harnessing BERT's contextual embeddings and the respective benefits of CNN's and LSTM's processing capabilities, it is possible to significantly reduce the frequency of cyberbullying through effective detection, ultimately fostering safer online environments across different media platforms.","Jessica, Adelia, Sugiarto, Migel Sastrawan, Jerry, Achmad, Said, Sutoyo, Rhio",,,A Hybrid Deep Learning Techniques Using BERT and CNN for Toxic Comments Classification,,,10.1109/ICIMTech63123.2024.10780934 , ,,"Cyberbullying is a pervasive issue across all forms of media, affecting various demographics and platforms indiscriminately. From social media networks to online forums and comment sections on news sites, the harmful behavior of cyberbullying manifests in many forms, including harassment, threats, and demeaning comments. This ubiquity underscores the need for effective detection mechanisms. This paper explores the identification of toxic traits in online comments using advanced hybrid models, specifically BERT-CNN and BERT-LSTM. The research methodology involved constructing and configuring multiple layers within these models to optimize their ability to detect harmful content. For the BERT-CNN model, BERT's powerful language understanding capabilities are combined with CNN's strength in feature extraction through convolutional layers, capturing spatial hierarchies of features. In the BERT-LSTM model, BERT is integrated with LSTM layers to leverage their ability to learn long-term dependencies and sequential patterns in text. Various configurations of these models were tested, including adj ustments in the batch sizes and the length of the word tokens, to enhance performance in identifying toxic language. The best model which is BERT-CNN using the configuration 256 as the unit and 32 as the batch size achieved 94.48% accuracy in detecting toxic comments. The result from the model indicates that by harnessing BERT's contextual embeddings and the respective benefits of CNN's and LSTM's processing capabilities, it is possible to significantly reduce the frequency of cyberbullying through effective detection, ultimately fostering safer online environments across different media platforms.",,,,, ,  2024 International Conference on Information Management and Technology (ICIMTech),Adaptation models;Visualization;Accuracy;Transfer learning;Cyberbullying;Media;Feature extraction;Information management;Convolutional neural networks;Long short term memory;toxic comment detection;BERT;LSTM;CNN;cyberbullying,detection,
3260,"**Title**The Automatic Detection of Abusive Language in Dota 2 Chat Messages

**Abstract**This study addresses the pervasive issue of abusive language in online video game communication channels, focusing on Dota 2 chat messages. The aim was to employ diverse traditional machine learning algorithms and advanced deep learning architectures to identify and classify toxic and abusive language effectively. Leveraging TF-IDF, GloVe word embeddings, and self-trained embeddings, the research compared various classical machine learning models such as Nave Bayes, Logistic Regression, and Support Vector Machine with convolutional and recurrent neural network models. The results revealed a consistent trend where deep learning models, particularly those employing GRUs and LSTMs, outperformed classical machine learning models. Experiments also demonstrated that self-trained embeddings generally outperformed GloVe embeddings in the domain of online video game chat messages.","Du Toit, Johannes Louis, Kotz, Eduan",,,The Automatic Detection of Abusive Language in Dota 2 Chat Messages,,,10.1109/ACDSA59508.2024.10467500 , ,,"This study addresses the pervasive issue of abusive language in online video game communication channels, focusing on Dota 2 chat messages. The aim was to employ diverse traditional machine learning algorithms and advanced deep learning architectures to identify and classify toxic and abusive language effectively. Leveraging TF-IDF, GloVe word embeddings, and self-trained embeddings, the research compared various classical machine learning models such as Nave Bayes, Logistic Regression, and Support Vector Machine with convolutional and recurrent neural network models. The results revealed a consistent trend where deep learning models, particularly those employing GRUs and LSTMs, outperformed classical machine learning models. Experiments also demonstrated that self-trained embeddings generally outperformed GloVe embeddings in the domain of online video game chat messages.",,,,, ,"  2024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)",Deep learning;Support vector machines;Video games;Analytical models;Logistic regression;Recurrent neural networks;Machine learning algorithms;abusive language classification;embeddings;GloVe;neural networks;machine learning;deep learning,detection,
3261,"**Title**Measuring and Mitigating Stereotype Bias in Language Models: An Overview of Debiasing Techniques

**Abstract**This paper provides an overview of methods for measuring the stereotype bias of pre-trained language models. It explains the term stereotype bias and its measurement. A comprehensive review of language model debiasing techniques, including Dropout, Counterfactual Data Augmentation, Iterative Nullspace Projection, Sent-Debias, Self-Debias, Context-Debias, FairFil, and Auto-Debias, is provided. Subsequently, these techniques are compared, and their advantages and disadvantages are highlighted. In addition, there is an explanation of how these techniques work simply. In the future, debiasing techniques will be tested on pre-trained models for the Slovak language, focusing on the task of toxic language detection.","Sokolov, Zuzana, Harahus, Maro, Sta, Jn, Kupcov, Eva, Sokol, Miroslav, Koctrov, Marianna, Juhr, Jozef",,,Measuring and Mitigating Stereotype Bias in Language Models: An Overview of Debiasing Techniques,,,10.1109/ELMAR62909.2024.10694175 , ,,"This paper provides an overview of methods for measuring the stereotype bias of pre-trained language models. It explains the term stereotype bias and its measurement. A comprehensive review of language model debiasing techniques, including Dropout, Counterfactual Data Augmentation, Iterative Nullspace Projection, Sent-Debias, Self-Debias, Context-Debias, FairFil, and Auto-Debias, is provided. Subsequently, these techniques are compared, and their advantages and disadvantages are highlighted. In addition, there is an explanation of how these techniques work simply. In the future, debiasing techniques will be tested on pre-trained models for the Slovak language, focusing on the task of toxic language detection.",,,,, ,  2024 International Symposium ELMAR,Measurement;Ethics;Reviews;Scalability;Focusing;Data augmentation;Robustness;Data models;Iterative methods;Testing;Debiasing technique;Language model;Measuring;Mitigating;Stereotype bias,detection,
3267,"**Title**Visions of Violence : Threatful Communication in Incel Communities

**Abstract**The incel subculture has gained increasing attention due to its toxic nature and its association with real-world violence. This paper investigates the prevalence and characteristics of violent threatful communication within incel forums, focusing on a platform known as Blackpill. We have trained a machine learning model to detect violent threatful language and analyzed the posts. The analysis concentrated on three key aspects: the identity of perpetrators (categorized into first-person, third-person, or generalized), the targets (individuals, groups, or general targets), and the types of violence described (general violence, sexual violence, self-harm, and military violence). The analysis showed that the most common type violent threatful communication involved generalized perpetrators targeting groups. Additionally, 13.5% of the violent threatful communication contained coded language, including references to video games to obscure violent intentions. A smaller proportion of the posts (4.1%) glorified past mass shooters and violent criminals.This research highlights the complexities of identifying violent rhetoric in online forums and the use of coded language to evade detection, emphasizing the need for refined models in threat detection.","Lundmark, Lukas, Kaati, Lisa, Shrestha, Amendra",,,Visions of Violence : Threatful Communication in Incel Communities,,,10.1109/BigData62323.2024.10825043 , ,,"The incel subculture has gained increasing attention due to its toxic nature and its association with real-world violence. This paper investigates the prevalence and characteristics of violent threatful communication within incel forums, focusing on a platform known as Blackpill. We have trained a machine learning model to detect violent threatful language and analyzed the posts. The analysis concentrated on three key aspects: the identity of perpetrators (categorized into first-person, third-person, or generalized), the targets (individuals, groups, or general targets), and the types of violence described (general violence, sexual violence, self-harm, and military violence). The analysis showed that the most common type violent threatful communication involved generalized perpetrators targeting groups. Additionally, 13.5% of the violent threatful communication contained coded language, including references to video games to obscure violent intentions. A smaller proportion of the posts (4.1%) glorified past mass shooters and violent criminals.This research highlights the complexities of identifying violent rhetoric in online forums and the use of coded language to evade detection, emphasizing the need for refined models in threat detection.",,,,, ,  2024 IEEE International Conference on Big Data (BigData),Video games;Analytical models;Focusing;Machine learning;Big Data;Threat assessment;Rhetoric;Complexity theory,detection,
3268,"**Title**Enhancing Toxic Comment Classification: A Deep Learning Approach with Pre-trained Language Models

**Abstract**Because of online communication, e-commerce, and digital devices, text dataespecially short textpermeates every aspect of our life in the digital age. However, this transformation has also unveiled a darker side - the prevalence of harmful, offensive, and toxic comments, including hate speech and harassment. The integrity of online groups, social harmony, and individual safety are all gravely threatened by this toxin. In this work, we examine the effectiveness of two recurrent neural network (RNN) architectures, namely Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), for the classification of damaging comments. These models are evaluated using key metrics such as F1-score, recall, accuracy, and precision. Important measures, including accuracy, precision, recall, and F1-score, are used to assess these models. According to our findings, the GRU achieves higher precision and overall accuracy, while the LSTM performs well in recall, spotting harmful comments at the expense of pinpoint accuracy. Model selection should align with specific project goals and trade-offs between precision and recall.","Tejwani, Khushi, Naik, Ved, Lari, Aanya, Jhaveri, Dhruvin",,,Enhancing Toxic Comment Classification: A Deep Learning Approach with Pre-trained Language Models,,,10.1109/ICISAA62385.2024.10829297 , ,,"Because of online communication, e-commerce, and digital devices, text dataespecially short textpermeates every aspect of our life in the digital age. However, this transformation has also unveiled a darker side - the prevalence of harmful, offensive, and toxic comments, including hate speech and harassment. The integrity of online groups, social harmony, and individual safety are all gravely threatened by this toxin. In this work, we examine the effectiveness of two recurrent neural network (RNN) architectures, namely Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), for the classification of damaging comments. These models are evaluated using key metrics such as F1-score, recall, accuracy, and precision. Important measures, including accuracy, precision, recall, and F1-score, are used to assess these models. According to our findings, the GRU achieves higher precision and overall accuracy, while the LSTM performs well in recall, spotting harmful comments at the expense of pinpoint accuracy. Model selection should align with specific project goals and trade-offs between precision and recall.",,,,, ,  2024 International Conference on Intelligent Systems and Advanced Applications (ICISAA),Measurement;Accuracy;Toxicology;Recurrent neural networks;Personal digital devices;Logic gates;Information age;Safety;Intelligent systems;Long short term memory;Enhancing Toxic Comment Classification;GRU;LSTM;Toxicity;Comment;Function;Deep Learning;RNN,detection,
3269,"**Title**Toxic language based echo chambers on the Incels.net community: A network analysis approach

**Abstract**This study examines interaction patterns on the web forum Incels.net through Social Network Analysis, focusing on the formation of echo chambers characterized by toxic language. We explore several hypotheses: (H2) users tend to engage in animated one-to-one interactions by quoting each other; (H4) frequent posters are less likely to be quoted by less active users, indicating a lack of clear leadership; and (H5) users sharing the same sentiment are more likely to participate in large discussions (echo chambers) but do not engage in one-to-one conversations. Our findings enhance the understanding of behaviors that may contribute to radicalization within fringe online communities and offer insights for future research into similar extreme hate forums.","Janssen, Mathieu, Zucca, Claudia, Cascavilla, Giuseppe, Cuzzocrea, Alfredo",,,Toxic language based echo chambers on the Incels.net community: A network analysis approach,,,10.1109/BigData62323.2024.10825922 , ,,"This study examines interaction patterns on the web forum Incels.net through Social Network Analysis, focusing on the formation of echo chambers characterized by toxic language. We explore several hypotheses: (H2) users tend to engage in animated one-to-one interactions by quoting each other; (H4) frequent posters are less likely to be quoted by less active users, indicating a lack of clear leadership; and (H5) users sharing the same sentiment are more likely to participate in large discussions (echo chambers) but do not engage in one-to-one conversations. Our findings enhance the understanding of behaviors that may contribute to radicalization within fringe online communities and offer insights for future research into similar extreme hate forums.",,,,, ,  2024 IEEE International Conference on Big Data (BigData),Leadership;Social networking (online);Focusing;Oral communication;Network analyzers;Inceldom;Sentiment Analysis;TERGMs;Social Networks;language-based echo chambers,detection,
3270,"**Title**On Large Language Models Resilience to Coercive Interrogation

**Abstract**Large Language Models (LLMs) are increasingly employed in numerous applications. It is hence important to ensure that their ethical standard aligns with humans. However, existing jail-breaking efforts show that such alignment could be compromised by well-crafted prompts. In this paper, we disclose a new threat to LLMs alignment when a malicious actor has access to the top-k token predictions at each output position of the model, such as in all open-source LLMs and many commercial LLMs that provide the needed APIs (e.g., some GPT versions). It does not require crafting any prompt. Instead, it leverages the observation that even when an LLM declines a toxic query, the harmful response is concealed deep within the output logits. We can coerce the model to disclose it by forcefully using low-ranked output tokens during auto-regressive output generation, and such forcing is only needed in a very small number of selected output positions. We call it model interrogation. Since our method operates differently from jail-breaking, it has better effectiveness than state-of-the- art jail-breaking techniques (92% versus 62%) and is 10 to 20 times faster. The toxic content elicited by our method is also of better quality. More importantly, it is complementary to jail-breaking, and a synergetic integration of the two exhibits superior performance over individual methods. We also find that with interrogation, harmful content can even be extracted from models customized for coding tasks.","Zhang, Zhuo, Shen, Guangyu, Tao, Guanhong, Cheng, Siyuan, Zhang, Xiangyu",,,On Large Language Models Resilience to Coercive Interrogation,,,10.1109/SP54263.2024.00208 , ,,"Large Language Models (LLMs) are increasingly employed in numerous applications. It is hence important to ensure that their ethical standard aligns with humans. However, existing jail-breaking efforts show that such alignment could be compromised by well-crafted prompts. In this paper, we disclose a new threat to LLMs alignment when a malicious actor has access to the top-k token predictions at each output position of the model, such as in all open-source LLMs and many commercial LLMs that provide the needed APIs (e.g., some GPT versions). It does not require crafting any prompt. Instead, it leverages the observation that even when an LLM declines a toxic query, the harmful response is concealed deep within the output logits. We can coerce the model to disclose it by forcefully using low-ranked output tokens during auto-regressive output generation, and such forcing is only needed in a very small number of selected output positions. We call it model interrogation. Since our method operates differently from jail-breaking, it has better effectiveness than state-of-the- art jail-breaking techniques (92% versus 62%) and is 10 to 20 times faster. The toxic content elicited by our method is also of better quality. More importantly, it is complementary to jail-breaking, and a synergetic integration of the two exhibits superior performance over individual methods. We also find that with interrogation, harmful content can even be extracted from models customized for coding tasks.",,,,, ,  2024 IEEE Symposium on Security and Privacy (SP),Privacy;Ethics;Art;Large language models;Predictive models;Encoding;Security,detection,
3275,"**Title**Conversation Sentiment Analysis in League of Legends Game Community

**Abstract**Gaming communities are groups of people from different nations, religions, genders, and ages who come to play games and have discussions about the games. However, some gaming communities are plagued by toxic behavior of people in communities, which negatively affects to other players. A Conversation Sentiment Analysis in the League of Legends Game Community is about analyzing the toxicity of the League of Legends game community by using the Sentiment Analysis methods which have Bag-of-Words and Weight Score method, and then comparing it with three machine learning techniques: Logistic Regression, Support Vector Machine (SVM), and CatBoost, and showing how members in this community have interacted with other members in the community by using the Conversation Mapping Interactive method.","Pongkhan, Thutchaphong, Khoosirirat, Phutanate, Songmuang, Pokpong, Kongkachandra, Rachada",,,Conversation Sentiment Analysis in League of Legends Game Community,,,10.1109/ICCI60780.2024.10532675 , ,,"Gaming communities are groups of people from different nations, religions, genders, and ages who come to play games and have discussions about the games. However, some gaming communities are plagued by toxic behavior of people in communities, which negatively affects to other players. A Conversation Sentiment Analysis in the League of Legends Game Community is about analyzing the toxicity of the League of Legends game community by using the Sentiment Analysis methods which have Bag-of-Words and Weight Score method, and then comparing it with three machine learning techniques: Logistic Regression, Support Vector Machine (SVM), and CatBoost, and showing how members in this community have interacted with other members in the community by using the Conversation Mapping Interactive method.",,,,, ,  2024 IEEE International Conference on Cybernetics and Innovations (ICCI),Support vector machines;Sentiment analysis;Humanities;Technological innovation;Logistic regression;Toxicology;Social networking (online);Toxic behavior;Sentiment Analysis;Machine Learning;Conversation Mapping;Sentiment Classification;Text Mining;NLP;Game Community;Network Graph,detection,
3279,"**Title**Monitoring The Evolution Of Antisemitic Hate Speech On Extremist Social Media

**Abstract**Racism and intolerance on social media contribute to a toxic online environment which may spill offline to foster hatred, and eventually lead to physical violence. That is the case with online antisemitism, the specific category of hatred considered in this study. Tracking antisemitic themes and their associated terminology over time in online discussions could help monitor the sentiments of their participants and their evolution, and possibly offer avenues for intervention that may prevent the escalation of hatred. Due to the large volume and constant evolution of online traffic, monitoring conversations manually is impractical. Instead, we propose an automated method that extracts antisemitic themes and terminology from extremist social media over time and captures their evolution. Since supervised learning would be too limited for such a task, we created an unsupervised online machine learning approach that uses large language models to assess the contextual similarity of posts. The method clusters similar posts together, dividing, and creating additional clusters over time when subthemes emerge from existing ones or new themes appear. The antisemitic terminology used within each theme is extracted from the posts in each cluster. Our experiments show that our methodology outperforms existing baselines and demonstrates the kind of themes and sub-themes it discovers within antisemitic discourse along with their associated terminology. We believe that our approach will be useful for monitoring the evolution of all kinds of hatred beyond antisemitism on social platforms.","Ul Mustafa, Raza, Japkowicz, Nathalie",,,Monitoring The Evolution Of Antisemitic Hate Speech On Extremist Social Media,,,10.1109/DPSH60098.2024.10774848 , ,,"Racism and intolerance on social media contribute to a toxic online environment which may spill offline to foster hatred, and eventually lead to physical violence. That is the case with online antisemitism, the specific category of hatred considered in this study. Tracking antisemitic themes and their associated terminology over time in online discussions could help monitor the sentiments of their participants and their evolution, and possibly offer avenues for intervention that may prevent the escalation of hatred. Due to the large volume and constant evolution of online traffic, monitoring conversations manually is impractical. Instead, we propose an automated method that extracts antisemitic themes and terminology from extremist social media over time and captures their evolution. Since supervised learning would be too limited for such a task, we created an unsupervised online machine learning approach that uses large language models to assess the contextual similarity of posts. The method clusters similar posts together, dividing, and creating additional clusters over time when subthemes emerge from existing ones or new themes appear. The antisemitic terminology used within each theme is extracted from the posts in each cluster. Our experiments show that our methodology outperforms existing baselines and demonstrates the kind of themes and sub-themes it discovers within antisemitic discourse along with their associated terminology. We believe that our approach will be useful for monitoring the evolution of all kinds of hatred beyond antisemitism on social platforms.",,,,, ,  2024 IEEE Digital Platforms and Societal Harms (DPSH),Visualization;Social networking (online);Terminology;Large language models;Supervised learning;Knowledge based systems;Hate speech;Machine learning;Oral communication;Monitoring;hate speech;concept formation;monitoring;antisemitic speech,detection,
3283,"**Title**Analysis of Different Adversarial Attacks on Various NLP SoTA Models

**Abstract**The core technology driving several applications, including as question answering, machine translation, and text categorization, is known as Deep Learning-based Text Understanding (DLTU). Given an input x and any goal classification y, it is feasible to create a new input x that is identical to x but classed as y. However, neural networks are susceptible to adversarial instances. Given its growing use in security-sensitive applications like sentiment analysis and toxic content monitoring, it is quite troubling that the security flaws of DLTU remain widely hidden despite its enormous popularity. Here, we demonstrate how various SoTA NLP models are intrinsically susceptible to different types of adversarial text attacks, in which specially crafted texts are used to cause misbehavior in the target DLTU systems and services. By exposing the maliciously created adversarial cases, it is possible to assess or possibly enhance the robustness of these models. Here the various NLP SoTA models are TextAttack, a Python-based framework is used for analyzing different adversarial attacks on various NLP So-TA models.","Maheshwari, Varsha, Dutta, Kamlesh, Kushwaha, Aanchal",,,Analysis of Different Adversarial Attacks on Various NLP SoTA Models,,,10.1109/CIISCA59740.2023.00049 , ,,"The core technology driving several applications, including as question answering, machine translation, and text categorization, is known as Deep Learning-based Text Understanding (DLTU). Given an input x and any goal classification y, it is feasible to create a new input x that is identical to x but classed as y. However, neural networks are susceptible to adversarial instances. Given its growing use in security-sensitive applications like sentiment analysis and toxic content monitoring, it is quite troubling that the security flaws of DLTU remain widely hidden despite its enormous popularity. Here, we demonstrate how various SoTA NLP models are intrinsically susceptible to different types of adversarial text attacks, in which specially crafted texts are used to cause misbehavior in the target DLTU systems and services. By exposing the maliciously created adversarial cases, it is possible to assess or possibly enhance the robustness of these models. Here the various NLP SoTA models are TextAttack, a Python-based framework is used for analyzing different adversarial attacks on various NLP So-TA models.",,,,, ,"  2023 International Conference on Computational Intelligence for Information, Security and Communication Applications (CIISCA)",Analytical models;Sentiment analysis;Computational modeling;Perturbation methods;Text categorization;Robustness;Question answering (information retrieval);BERT Model;Adversarial Attacks;RoBERTa Model;ALBERT Model;XLNet Model;DistilBERT Model,detection,
3287,"**Title**An Extensive Examination of Toxicity, Polarisation, and Biasing in Political Conversations on Social Media

**Abstract**Recent developments in social media uses and its availability on ubiquitous devices makes it a perfect platform to express our views. However, many groups, organizations and people try to influence the users by posting biased contents. Some of them try to polarize the opinion of users. Many of them use toxic language to target some users. The activities such as biasing, polarization and use of toxic language make the users feel uncomfortable in using social media. Our research paper highlights key characteristics, uncovers patterns, and offers solutions to lessen the detrimental impacts of polarization, bias, and toxicity.","Garg, Navin, Singh, Ashwini Kumar, Manchanda, Mahesh",,,"An Extensive Examination of Toxicity, Polarisation, and Biasing in Political Conversations on Social Media",,,10.1109/IC2PCT60090.2024.10486236 , ,,"Recent developments in social media uses and its availability on ubiquitous devices makes it a perfect platform to express our views. However, many groups, organizations and people try to influence the users by posting biased contents. Some of them try to polarize the opinion of users. Many of them use toxic language to target some users. The activities such as biasing, polarization and use of toxic language make the users feel uncomfortable in using social media. Our research paper highlights key characteristics, uncovers patterns, and offers solutions to lessen the detrimental impacts of polarization, bias, and toxicity.",,,,, ,"  2024 IEEE International Conference on Computing, Power and Communication Technologies (IC2PCT)",Toxicology;Social networking (online);Reviews;Organizations;Oral communication;Minimization;Communications technology;Toxicity;Biasing;Polarization;Echo Chambers;Social fragmentation;Narrative manipulation,detection,
3293,"**Title**A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models

**Abstract**Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and outperform alternative metrics, indicating their potential as multi-aspect, reference-free and interpretable evaluators for counter narrative evaluation.","Jones, Jaylen, Mo, Lingbo, Fosler-Lussier, Eric, Sun, Huan",,,A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models,,,10.18653/v1/2024.naacl-short.14 , ,,"Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and outperform alternative metrics, indicating their potential as multi-aspect, reference-free and interpretable evaluators for counter narrative evaluation.",,,,, ,  Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers),,detox,
3295,"**Title**A Semi-Supervised Approach to Detect Toxic Comments

**Abstract**Toxic comments contain forms of non-acceptable language targeted towards groups or individuals. These types of comments become a serious concern for government organizations, online communities, and social media platforms. Although there are some approaches to handle non-acceptable language, most of them focus on supervised learning and the English language. In this paper, we deal with toxic comment detection as a semi-supervised strategy over a heterogeneous graph. We evaluate the approach on a toxic dataset of the Portuguese language, outperforming several graph-based methods and achieving competitive results compared to transformer architectures.","Saraiva, Ghivvago Damas, Anchi{\^e}ta, Rafael, Neto, Francisco Assis Ricarte, Moura, Raimundo",,,A Semi-Supervised Approach to Detect Toxic Comments,,, , ,,"Toxic comments contain forms of non-acceptable language targeted towards groups or individuals. These types of comments become a serious concern for government organizations, online communities, and social media platforms. Although there are some approaches to handle non-acceptable language, most of them focus on supervised learning and the English language. In this paper, we deal with toxic comment detection as a semi-supervised strategy over a heterogeneous graph. We evaluate the approach on a toxic dataset of the Portuguese language, outperforming several graph-based methods and achieving competitive results compared to transformer architectures.",,,,, ,  Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),,detection,
3296,"**Title**Unlearning Bias in Language Models by Partitioning Gradients

**Abstract**Recent research has shown that large-scale pretrained language models, specifically transformers, tend to exhibit issues relating to racism, sexism, religion bias, and toxicity in general. Unfortunately, these pretrained language models are used almost universally in downstream tasks, and natural language processing is often applied to make real-world predictions. Thus, debiasing these language models as early in development as possible is increasingly crucial for preventing unintentional harms caused by natural language systems. To this end, we propose a new technique called partitioned contrastive gradient unlearning (PCGU), a gray-box method for debiasing pretrained masked language models. PCGU aims to optimize only the weights that contribute most to a specific domain of bias, doing so by computing a first-order approximation based on the gradients of contrastive sentence pairs. Our experiments show that PCGU is both low-cost and seems particularly effective at pinpointing the sources of implicit social bias in large pretrained transformers. Although we train using PCGU in the gender-profession domain only, we find that doing so can also partially mitigate bias across other domains. All code for our implementation and experiments can be found at \url{https://github.com/CharlesYu2000/PCGU-UnlearningBias}.","Yu, Charles, Jeoung, Sullam, Kasi, Anish, Yu, Pengfei, Ji, Heng",,,Unlearning Bias in Language Models by Partitioning Gradients,,,10.18653/v1/2023.findings-acl.375 , ,,"Recent research has shown that large-scale pretrained language models, specifically transformers, tend to exhibit issues relating to racism, sexism, religion bias, and toxicity in general. Unfortunately, these pretrained language models are used almost universally in downstream tasks, and natural language processing is often applied to make real-world predictions. Thus, debiasing these language models as early in development as possible is increasingly crucial for preventing unintentional harms caused by natural language systems. To this end, we propose a new technique called partitioned contrastive gradient unlearning (PCGU), a gray-box method for debiasing pretrained masked language models. PCGU aims to optimize only the weights that contribute most to a specific domain of bias, doing so by computing a first-order approximation based on the gradients of contrastive sentence pairs. Our experiments show that PCGU is both low-cost and seems particularly effective at pinpointing the sources of implicit social bias in large pretrained transformers. Although we train using PCGU in the gender-profession domain only, we find that doing so can also partially mitigate bias across other domains. All code for our implementation and experiments can be found at \url{https://github.com/CharlesYu2000/PCGU-UnlearningBias}.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2023,,detox,
3297,"**Title**Which One Is More Toxic? Findings from Jigsaw Rate Severity of Toxic Comments

**Abstract**The proliferation of online hate speech has necessitated the creation of algorithms which can detect toxicity. Most of the past research focuses on this detection as a classification task, but assigning an absolute toxicity label is often tricky. Hence, few of the past works transform the same task into a regression. This paper shows the comparative evaluation of different transformers and traditional machine learning models on a recently released toxicity severity measurement dataset by Jigsaw. We further demonstrate the issues with the model predictions using explainability analysis.","Das, Millon, Saha, Punyajoy, Das, Mithun",,,Which One Is More Toxic? Findings from Jigsaw Rate Severity of Toxic Comments,,, , ,,"The proliferation of online hate speech has necessitated the creation of algorithms which can detect toxicity. Most of the past research focuses on this detection as a classification task, but assigning an absolute toxicity label is often tricky. Hence, few of the past works transform the same task into a regression. This paper shows the comparative evaluation of different transformers and traditional machine learning models on a recently released toxicity severity measurement dataset by Jigsaw. We further demonstrate the issues with the model predictions using explainability analysis.",,,,, ,"  Proceedings of the Third Workshop on Threat, Aggression and Cyberbullying (TRAC 2022)",,detection,
3298,"**Title**{PPL-MCTS}: {C}onstrained Textual Generation Through Discriminator-Guided {MCTS} Decoding

**Abstract**Large language models (LM) based on Transformers allow to generate plausible long texts. In this paper, we explore how this generation can be further controlled at decoding time to satisfy certain constraints (e.g. being non-toxic, conveying certain emotions, using a specific writing style, etc.) without fine-tuning the LM.Precisely, we formalize constrained generation as a tree exploration process guided by a discriminator that indicates how well the associated sequence respects the constraint. This approach, in addition to being easier and cheaper to train than fine-tuning the LM, allows to apply the constraint more finely and dynamically. We propose several original methods to search this generation tree, notably the Monte Carlo Tree Search (MCTS) which provides theoretical guarantees on the search efficiency, but also simpler methods based on re-ranking a pool of diverse sequences using the discriminator scores. These methods are evaluated, with automatic and human-based metrics, on two types of constraints and languages: review polarity and emotion control in French and English. We show that discriminator-guided MCTS decoding achieves state-of-the-art results without having to tune the language model, in both tasks and languages. We also demonstrate that other proposed decoding methods based on re-ranking can be really effective when diversity among the generated propositions is encouraged.","Chaffin, Antoine, Claveau, Vincent, Kijak, Ewa",,,{PPL-MCTS}: {C}onstrained Textual Generation Through Discriminator-Guided {MCTS} Decoding,,,10.18653/v1/2022.naacl-main.215 , ,,"Large language models (LM) based on Transformers allow to generate plausible long texts. In this paper, we explore how this generation can be further controlled at decoding time to satisfy certain constraints (e.g. being non-toxic, conveying certain emotions, using a specific writing style, etc.) without fine-tuning the LM.Precisely, we formalize constrained generation as a tree exploration process guided by a discriminator that indicates how well the associated sequence respects the constraint. This approach, in addition to being easier and cheaper to train than fine-tuning the LM, allows to apply the constraint more finely and dynamically. We propose several original methods to search this generation tree, notably the Monte Carlo Tree Search (MCTS) which provides theoretical guarantees on the search efficiency, but also simpler methods based on re-ranking a pool of diverse sequences using the discriminator scores. These methods are evaluated, with automatic and human-based metrics, on two types of constraints and languages: review polarity and emotion control in French and English. We show that discriminator-guided MCTS decoding achieves state-of-the-art results without having to tune the language model, in both tasks and languages. We also demonstrate that other proposed decoding methods based on re-ranking can be really effective when diversity among the generated propositions is encouraged.",,,,, ,  Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,,detox,
3299,"**Title**{SSNCSE}{\_}{NLP}@{LT}-{EDI}-{ACL}2022: Homophobia/Transphobia Detection in Multiple Languages using {SVM} Classifiers and {BERT}-based Transformers

**Abstract**Over the years, there has been a slow but steady change in the attitude of society towards different kinds of sexuality. However, on social media platforms, where people have the license to be anonymous, toxic comments targeted at homosexuals, transgenders and the LGBTQ+ community are not uncommon. Detection of homophobic comments on social media can be useful in making the internet a safer place for everyone. For this task, we used a combination of word embeddings and SVM Classifiers as well as some BERT-based transformers. We achieved a weighted F1-score of 0.93 on the English dataset, 0.75 on the Tamil dataset and 0.87 on the Tamil-English Code-Mixed dataset.","Swaminathan, Krithika, B, Bharathi, G L, Gayathri, Sampath, Hrishik",,,{SSNCSE}{\_}{NLP}@{LT}-{EDI}-{ACL}2022: Homophobia/Transphobia Detection in Multiple Languages using {SVM} Classifiers and {BERT}-based Transformers,,,10.18653/v1/2022.ltedi-1.34 , ,,"Over the years, there has been a slow but steady change in the attitude of society towards different kinds of sexuality. However, on social media platforms, where people have the license to be anonymous, toxic comments targeted at homosexuals, transgenders and the LGBTQ+ community are not uncommon. Detection of homophobic comments on social media can be useful in making the internet a safer place for everyone. For this task, we used a combination of word embeddings and SVM Classifiers as well as some BERT-based transformers. We achieved a weighted F1-score of 0.93 on the English dataset, 0.75 on the Tamil dataset and 0.87 on the Tamil-English Code-Mixed dataset.",,,,, ,"  Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion",,detection,
3300,"**Title**Cisco at {S}em{E}val-2021 Task 5: What`s Toxic?: Leveraging Transformers for Multiple Toxic Span Extraction from Online Comments

**Abstract**Social network platforms are generally used to share positive, constructive, and insightful content. However, in recent times, people often get exposed to objectionable content like threat, identity attacks, hate speech, insults, obscene texts, offensive remarks or bullying. Existing work on toxic speech detection focuses on binary classification or on differentiating toxic speech among a small set of categories. This paper describes the system proposed by team Cisco for SemEval-2021 Task 5: Toxic Spans Detection, the first shared task focusing on detecting the spans in the text that attribute to its toxicity, in English language. We approach this problem primarily in two ways: a sequence tagging approach and a dependency parsing approach. In our sequence tagging approach we tag each token in a sentence under a particular tagging scheme. Our best performing architecture in this approach also proved to be our best performing architecture overall with an F1 score of 0.6922, thereby placing us 7th on the final evaluation phase leaderboard. We also explore a dependency parsing approach where we extract spans from the input sentence under the supervision of target span boundaries and rank our spans using a biaffine model. Finally, we also provide a detailed analysis of our results and model performance in our paper.","Ghosh, Sreyan, Kumar, Sonal",,,Cisco at {S}em{E}val-2021 Task 5: What`s Toxic?: Leveraging Transformers for Multiple Toxic Span Extraction from Online Comments,,,10.18653/v1/2021.semeval-1.29 , ,,"Social network platforms are generally used to share positive, constructive, and insightful content. However, in recent times, people often get exposed to objectionable content like threat, identity attacks, hate speech, insults, obscene texts, offensive remarks or bullying. Existing work on toxic speech detection focuses on binary classification or on differentiating toxic speech among a small set of categories. This paper describes the system proposed by team Cisco for SemEval-2021 Task 5: Toxic Spans Detection, the first shared task focusing on detecting the spans in the text that attribute to its toxicity, in English language. We approach this problem primarily in two ways: a sequence tagging approach and a dependency parsing approach. In our sequence tagging approach we tag each token in a sentence under a particular tagging scheme. Our best performing architecture in this approach also proved to be our best performing architecture overall with an F1 score of 0.6922, thereby placing us 7th on the final evaluation phase leaderboard. We also explore a dependency parsing approach where we extract spans from the input sentence under the supervision of target span boundaries and rank our spans using a biaffine model. Finally, we also provide a detailed analysis of our results and model performance in our paper.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3301,"**Title**{WLV}-{RIT} at {S}em{E}val-2021 Task 5: A Neural Transformer Framework for Detecting Toxic Spans

**Abstract**In recent years, the widespread use of social media has led to an increase in the generation of toxic and offensive content on online platforms. In response, social media platforms have worked on developing automatic detection methods and employing human moderators to cope with this deluge of offensive content. While various state-of-the-art statistical models have been applied to detect toxic posts, there are only a few studies that focus on detecting the words or expressions that make a post offensive. This motivates the organization of the SemEval-2021 Task 5: Toxic Spans Detection competition, which has provided participants with a dataset containing toxic spans annotation in English posts. In this paper, we present the WLV-RIT entry for the SemEval-2021 Task 5. Our best performing neural transformer model achieves an 0.68 F1-Score. Furthermore, we develop an open-source framework for multilingual detection of offensive spans, i.e., MUDES, based on neural transformers that detect toxic spans in texts.","Ranasinghe, Tharindu, Sarkar, Diptanu, Zampieri, Marcos, Ororbia, Alexander",,,{WLV}-{RIT} at {S}em{E}val-2021 Task 5: A Neural Transformer Framework for Detecting Toxic Spans,,,10.18653/v1/2021.semeval-1.111 , ,,"In recent years, the widespread use of social media has led to an increase in the generation of toxic and offensive content on online platforms. In response, social media platforms have worked on developing automatic detection methods and employing human moderators to cope with this deluge of offensive content. While various state-of-the-art statistical models have been applied to detect toxic posts, there are only a few studies that focus on detecting the words or expressions that make a post offensive. This motivates the organization of the SemEval-2021 Task 5: Toxic Spans Detection competition, which has provided participants with a dataset containing toxic spans annotation in English posts. In this paper, we present the WLV-RIT entry for the SemEval-2021 Task 5. Our best performing neural transformer model achieves an 0.68 F1-Score. Furthermore, we develop an open-source framework for multilingual detection of offensive spans, i.e., MUDES, based on neural transformers that detect toxic spans in texts.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3302,"**Title**Sefamerve {ARGE} at {S}em{E}val-2021 Task 5: Toxic Spans Detection Using Segmentation Based 1-{D} Convolutional Neural Network Model

**Abstract**This paper describes our contribution to SemEval-2021 Task 5: Toxic Spans Detection. Our approach considers toxic spans detection as a segmentation problem. The system, Waw-unet, consists of a 1-D convolutional neural network adopted from U-Net architecture commonly applied for semantic segmentation. We customize existing architecture by adding a special network block considering for text segmentation, as an essential component of the model. We compared the model with two transformers-based systems RoBERTa and XLM-RoBERTa to see its performance against pre-trained language models. We obtained 0.6251 f1 score with Waw-unet while 0.6390 and 0.6601 with the compared models respectively.","Delil, Selman, Kuyumcu, Birol, Aksakall{\i}, C{\""u}neyt",,,Sefamerve {ARGE} at {S}em{E}val-2021 Task 5: Toxic Spans Detection Using Segmentation Based 1-{D} Convolutional Neural Network Model,,,10.18653/v1/2021.semeval-1.123 , ,,"This paper describes our contribution to SemEval-2021 Task 5: Toxic Spans Detection. Our approach considers toxic spans detection as a segmentation problem. The system, Waw-unet, consists of a 1-D convolutional neural network adopted from U-Net architecture commonly applied for semantic segmentation. We customize existing architecture by adding a special network block considering for text segmentation, as an essential component of the model. We compared the model with two transformers-based systems RoBERTa and XLM-RoBERTa to see its performance against pre-trained language models. We obtained 0.6251 f1 score with Waw-unet while 0.6390 and 0.6601 with the compared models respectively.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3303,"**Title**{UTNLP} at {S}em{E}val-2021 Task 5: A Comparative Analysis of Toxic Span Detection using Attention-based, Named Entity Recognition, and Ensemble Models

**Abstract**Detecting which parts of a sentence contribute to that sentence`s toxicity{---}rather than providing a sentence-level verdict of hatefulness{---} would increase the interpretability of models and allow human moderators to better understand the outputs of the system. This paper presents our team`s, UTNLP, methodology and results in the SemEval-2021 shared task 5 on toxic spans detection. We test multiple models and contextual embeddings and report the best setting out of all. The experiments start with keyword-based models and are followed by attention-based, named entity- based, transformers-based, and ensemble models. Our best approach, an ensemble model, achieves an F1 of 0.684 in the competition`s evaluation phase.","Salemi, Alireza, Sabri, Nazanin, Kebriaei, Emad, Bahrak, Behnam, Shakery, Azadeh",,,"{UTNLP} at {S}em{E}val-2021 Task 5: A Comparative Analysis of Toxic Span Detection using Attention-based, Named Entity Recognition, and Ensemble Models",,,10.18653/v1/2021.semeval-1.136 , ,,"Detecting which parts of a sentence contribute to that sentence`s toxicity{---}rather than providing a sentence-level verdict of hatefulness{---} would increase the interpretability of models and allow human moderators to better understand the outputs of the system. This paper presents our team`s, UTNLP, methodology and results in the SemEval-2021 shared task 5 on toxic spans detection. We test multiple models and contextual embeddings and report the best setting out of all. The experiments start with keyword-based models and are followed by attention-based, named entity- based, transformers-based, and ensemble models. Our best approach, an ensemble model, achieves an F1 of 0.684 in the competition`s evaluation phase.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3308,"**Title**Personalized Text Generation with Fine-Grained Linguistic Control

**Abstract**As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized. However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes. We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance. We make our code, data, models, and benchmarks publicly available.","Alhafni, Bashar, Kulkarni, Vivek, Kumar, Dhruv, Raheja, Vipul",,,Personalized Text Generation with Fine-Grained Linguistic Control,,, , ,,"As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized. However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes. We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance. We make our code, data, models, and benchmarks publicly available.",,,,, ,  Proceedings of the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024),,detox,
3315,"**Title**{G}aussian Process Optimization for Adaptable Multi-Objective Text Generation using Linearly-Weighted Language Models

**Abstract**In multi-objective text generation, we aim to optimize over multiple weighted aspects (e.g., toxicity, semantic preservation, fluency) of the generated text. However, multi-objective weighting schemes may change dynamically in practice according to deployment requirements, evolving business needs, personalization requirements on edge devices, or the availability of new language models and/or objective requirements. Ideally, we need an efficient method to adapt to the dynamic requirements of the overall objective. To address these requirements, we propose a linear combination of objective-specific language models to \textbf{efficiently} adapt the decoding process and optimize for the desired objective \textbf{without} the significant computational overhead of retraining one or more language models. We show empirically that we can leverage Gaussian Process black box optimization to adapt the language model decoder weights to outperform other fixed weighting schemes and standard baselines of the task in only a few iterations of decoding. Overall this approach enables highly efficient adaptation of controllable language models via multi-objective weighting schemes that may evolve dynamically in practical deployment situations.","Abdollah Pour, Mohammad Mahdi, Pesaranghader, Ali, Cohen, Eldan, Sanner, Scott",,,{G}aussian Process Optimization for Adaptable Multi-Objective Text Generation using Linearly-Weighted Language Models,,,10.18653/v1/2024.findings-naacl.99 , ,,"In multi-objective text generation, we aim to optimize over multiple weighted aspects (e.g., toxicity, semantic preservation, fluency) of the generated text. However, multi-objective weighting schemes may change dynamically in practice according to deployment requirements, evolving business needs, personalization requirements on edge devices, or the availability of new language models and/or objective requirements. Ideally, we need an efficient method to adapt to the dynamic requirements of the overall objective. To address these requirements, we propose a linear combination of objective-specific language models to \textbf{efficiently} adapt the decoding process and optimize for the desired objective \textbf{without} the significant computational overhead of retraining one or more language models. We show empirically that we can leverage Gaussian Process black box optimization to adapt the language model decoder weights to outperform other fixed weighting schemes and standard baselines of the task in only a few iterations of decoding. Overall this approach enables highly efficient adaptation of controllable language models via multi-objective weighting schemes that may evolve dynamically in practical deployment situations.",,,,, ,  Findings of the Association for Computational Linguistics: NAACL 2024,,detox,
3321,"**Title**{RSA}-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework

**Abstract**Despite significant advancements in natural language generation, controlling language models to produce texts with desired attributes remains a formidable challenge. In this work, we introduce RSA-Control, a training-free controllable text generation framework grounded in pragmatics. RSA-Control directs the generation process by recursively reasoning between imaginary speakers and listeners, enhancing the likelihood that target attributes are correctly interpreted by listeners amidst distractors. Additionally, we introduce a self-adjustable rationality parameter, which allows for automatic adjustment of control strength based on context. Our experiments, conducted with two task types and two types of language models, demonstrate that RSA-Control achieves strong attribute control while maintaining language fluency and content consistency. Our code is available at https://github.com/Ewanwong/RSA-Control.","Wang, Yifan, Demberg, Vera",,,{RSA}-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework,,,10.18653/v1/2024.emnlp-main.318 , ,,"Despite significant advancements in natural language generation, controlling language models to produce texts with desired attributes remains a formidable challenge. In this work, we introduce RSA-Control, a training-free controllable text generation framework grounded in pragmatics. RSA-Control directs the generation process by recursively reasoning between imaginary speakers and listeners, enhancing the likelihood that target attributes are correctly interpreted by listeners amidst distractors. Additionally, we introduce a self-adjustable rationality parameter, which allows for automatic adjustment of control strength based on context. Our experiments, conducted with two task types and two types of language models, demonstrate that RSA-Control achieves strong attribute control while maintaining language fluency and content consistency. Our code is available at https://github.com/Ewanwong/RSA-Control.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3323,"**Title**Control Large Language Models via Divide and Conquer

**Abstract**This paper investigates the capability of LLMs on controllable generation with prompt-based controlling, focusing on Lexically Constrained Generation (LCG). We systematically evaluate the performance of LLMs on satisfying lexical constraints with prompt-based controlling, as well as their efficacy in downstream applications. We identified three key reasons that highlight the limitations of LLMs in LCG, including (1) position bias, where LLMs tend to satisfy constraints that appear in specific positions within the input; (2) low responsiveness to control decoding parameters, which minimally impact the performance of LLMs; and (3) struggle with handling the inherent complexity of certain constraints (e.g. compound word). We conclude that black-box LLMs face significant challenges in consistently satisfying lexical constraints with prompt-based controlling. To address this bottleneck, we introduce the Divide and Conquer Generation strategy, effective for both white-box and black-box LLMs, to enhance LLMs performance in LCG tasks, which demonstrates over 90{\%} improvement on success rate in the most challenging LCG task. Our analysis aims to provide valuable insights into the performance of LLMs in LCG with prompt-based controlling, and our proposed strategy offers a pathway to more sophisticated and customized text generation applications.","Li, Bingxuan, Wang, Yiwei, Meng, Tao, Chang, Kai-Wei, Peng, Nanyun",,,Control Large Language Models via Divide and Conquer,,,10.18653/v1/2024.emnlp-main.850 , ,,"This paper investigates the capability of LLMs on controllable generation with prompt-based controlling, focusing on Lexically Constrained Generation (LCG). We systematically evaluate the performance of LLMs on satisfying lexical constraints with prompt-based controlling, as well as their efficacy in downstream applications. We identified three key reasons that highlight the limitations of LLMs in LCG, including (1) position bias, where LLMs tend to satisfy constraints that appear in specific positions within the input; (2) low responsiveness to control decoding parameters, which minimally impact the performance of LLMs; and (3) struggle with handling the inherent complexity of certain constraints (e.g. compound word). We conclude that black-box LLMs face significant challenges in consistently satisfying lexical constraints with prompt-based controlling. To address this bottleneck, we introduce the Divide and Conquer Generation strategy, effective for both white-box and black-box LLMs, to enhance LLMs performance in LCG tasks, which demonstrates over 90{\%} improvement on success rate in the most challenging LCG task. Our analysis aims to provide valuable insights into the performance of LLMs in LCG with prompt-based controlling, and our proposed strategy offers a pathway to more sophisticated and customized text generation applications.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3327,"**Title**Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation

**Abstract**Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (e.g., {\textquotedblleft}positive{\textquotedblright} from sentiment and {\textquotedblleft}sport{\textquotedblright} from topic). Existing works neglect attribute correlations formed by the intertwining of different attributes. Particularly, the stereotype formed by imbalanced attribute correlations significantly affects multi-aspect control. In this paper, we propose MAGIC, a new multi-aspect controllable text generation method with disentangled counterfactual augmentation. We alleviate the issue of imbalanced attribute correlations during training using counterfactual feature vectors in the attribute latent space by disentanglement. During inference, we enhance attribute correlations by target-guided counterfactual augmentation to further improve multi-aspect control. Experiments show that MAGIC outperforms state-of-the-art baselines in both imbalanced and balanced attribute correlation scenarios.","Liu, Yi, Liu, Xiangyu, Zhu, Xiangrong, Hu, Wei",,,Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation,,,10.18653/v1/2024.acl-long.500 , ,,"Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (e.g., {\textquotedblleft}positive{\textquotedblright} from sentiment and {\textquotedblleft}sport{\textquotedblright} from topic). Existing works neglect attribute correlations formed by the intertwining of different attributes. Particularly, the stereotype formed by imbalanced attribute correlations significantly affects multi-aspect control. In this paper, we propose MAGIC, a new multi-aspect controllable text generation method with disentangled counterfactual augmentation. We alleviate the issue of imbalanced attribute correlations during training using counterfactual feature vectors in the attribute latent space by disentanglement. During inference, we enhance attribute correlations by target-guided counterfactual augmentation to further improve multi-aspect control. Experiments show that MAGIC outperforms state-of-the-art baselines in both imbalanced and balanced attribute correlation scenarios.",,,,, ,  Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,detox,
3328,"**Title**How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques

**Abstract**Recent advances in the development of large Pretrained Language Models, such as GPT, BERT and Bloom, have achieved remarkable performance on a wide range of different NLP tasks. However, when used for text generation tasks, these models still have limitations when it comes to controlling the content and style of the generated text, often producing content that is incorrect, irrelevant, or inappropriate in the context of a given task. In this survey paper, we explore methods for controllable text generation with a focus on sentiment control. We systematically collect papers from the ACL Anthology, create a categorisation scheme based on different control techniques and controlled attributes, and use the scheme to categorise and compare methods. The result is a detailed and comprehensive overview of state-of-the-art techniques for sentiment-controlled text generation categorised on the basis of how the control is implemented and what attributes are controlled and providing a clear idea of their relative strengths and weaknesses.","Lorandi, Michela, Belz, Anya",,,How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art in Sentiment-Control Techniques,,,10.18653/v1/2023.wassa-1.30 , ,,"Recent advances in the development of large Pretrained Language Models, such as GPT, BERT and Bloom, have achieved remarkable performance on a wide range of different NLP tasks. However, when used for text generation tasks, these models still have limitations when it comes to controlling the content and style of the generated text, often producing content that is incorrect, irrelevant, or inappropriate in the context of a given task. In this survey paper, we explore methods for controllable text generation with a focus on sentiment control. We systematically collect papers from the ACL Anthology, create a categorisation scheme based on different control techniques and controlled attributes, and use the scheme to categorise and compare methods. The result is a detailed and comprehensive overview of state-of-the-art techniques for sentiment-controlled text generation categorised on the basis of how the control is implemented and what attributes are controlled and providing a clear idea of their relative strengths and weaknesses.",,,,, ,"  Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\&} Social Media Analysis",,detox,
3333,"**Title**Harnessing the Plug-and-Play Controller by Prompting

**Abstract**Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications. Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model`s decoding process, resulting in less smooth text generation.Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model. This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs). The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs. The key idea is to dynamically adjust the distribution of generated text by modifying prompts, effectively constraining the output space of the language model and influencing the desired attribute. To enable smooth cooperation between the PLM and the PPC, our work innovativel proposes a new model fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback (RLDAF).This fine-tuning process adapts a small subset of the language model`s parameters based on the generating actions taken during the PPC control process. The resulting harmonious collaboration between the PLM and PPC leads to improved smoothness in text generation during inference. Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including text fluency and attribute consistency.","Wang, Hao, Sha, Lei",,,Harnessing the Plug-and-Play Controller by Prompting,,, , ,,"Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications. Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model`s decoding process, resulting in less smooth text generation.Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model. This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs). The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs. The key idea is to dynamically adjust the distribution of generated text by modifying prompts, effectively constraining the output space of the language model and influencing the desired attribute. To enable smooth cooperation between the PLM and the PPC, our work innovativel proposes a new model fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback (RLDAF).This fine-tuning process adapts a small subset of the language model`s parameters based on the generating actions taken during the PPC control process. The resulting harmonious collaboration between the PLM and PPC leads to improved smoothness in text generation during inference. Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including text fluency and attribute consistency.",,,,, ,"  Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",,detox,
3336,"**Title**{GTA}: Gated Toxicity Avoidance for {LM} Performance Preservation

**Abstract**Caution: This paper includes offensive words that could potentially cause unpleasantness. The fast-paced evolution of generative language models such as GPT-4 has demonstrated outstanding results in various NLP generation tasks. However, due to the potential generation of offensive words related to race or gender, various Controllable Text Generation (CTG) methods have been proposed to mitigate the occurrence of harmful words. However, existing CTG methods not only reduce toxicity but also negatively impact several aspects of the language model`s generation performance, including topic consistency, grammar, and perplexity. This paper explores the limitations of previous methods and introduces a novel solution in the form of a simple Gated Toxicity Avoidance (GTA) that can be applied to any CTG method. We also evaluate the effectiveness of the proposed GTA by comparing it with state-of-the-art CTG methods across various datasets. Our findings reveal that gated toxicity avoidance efficiently achieves comparable levels of toxicity reduction to the original CTG methods while preserving the generation performance of the language model.","Kim, Heegyu, Cho, Hyunsouk",,,{GTA}: Gated Toxicity Avoidance for {LM} Performance Preservation,,,10.18653/v1/2023.findings-emnlp.983 , ,,"Caution: This paper includes offensive words that could potentially cause unpleasantness. The fast-paced evolution of generative language models such as GPT-4 has demonstrated outstanding results in various NLP generation tasks. However, due to the potential generation of offensive words related to race or gender, various Controllable Text Generation (CTG) methods have been proposed to mitigate the occurrence of harmful words. However, existing CTG methods not only reduce toxicity but also negatively impact several aspects of the language model`s generation performance, including topic consistency, grammar, and perplexity. This paper explores the limitations of previous methods and introduces a novel solution in the form of a simple Gated Toxicity Avoidance (GTA) that can be applied to any CTG method. We also evaluate the effectiveness of the proposed GTA by comparing it with state-of-the-art CTG methods across various datasets. Our findings reveal that gated toxicity avoidance efficiently achieves comparable levels of toxicity reduction to the original CTG methods while preserving the generation performance of the language model.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,detox,
3337,"**Title**{GRACE}: Gradient-guided Controllable Retrieval for Augmenting Attribute-based Text Generation

**Abstract**Attribute-based generation methods are of growing significance in controlling the generation of large pre-trained language models (PLMs). Existing studies control the generation by (1) finetuning the model with attributes or (2) guiding the inference processing toward control signals while freezing the PLM. However, finetuning approaches infuse domain bias into generation, making it hard to generate out-of-domain texts. Besides, many methods guide the inference in its word-by-word generation, pushing the word probability to the target attributes, resulting in less fluent sentences. We argue that distilling controlling information from natural texts can produce fluent sentences while maintaining high controllability. In this paper, we propose \textbf{GRA}dient-guided \textbf{C}ontrollable r\textbf{E}trieval (GRACE), a retrieval-augmented generation framework to facilitate the generation of fluent sentences with high attribute relevance. GRACE memorizes the semantic and attribute information from unlabeled corpora and applies a controllable retrieval to obtain desired information. For the generation, we design techniques to eliminate the domain bias from the retrieval results and integrate it into the generation model. Additionally, we propose a gradient-guided generation scheme that iteratively steers generation toward higher attribute relevance. Experimental results and quantities of examples verify the effectiveness of our method.","Wen, Zhihua, Tian, Zhiliang, Huang, Zhen, Yang, Yuxin, Jian, Zexin, Wang, Changjian, Li, Dongsheng",,,{GRACE}: Gradient-guided Controllable Retrieval for Augmenting Attribute-based Text Generation,,,10.18653/v1/2023.findings-acl.530 , ,,"Attribute-based generation methods are of growing significance in controlling the generation of large pre-trained language models (PLMs). Existing studies control the generation by (1) finetuning the model with attributes or (2) guiding the inference processing toward control signals while freezing the PLM. However, finetuning approaches infuse domain bias into generation, making it hard to generate out-of-domain texts. Besides, many methods guide the inference in its word-by-word generation, pushing the word probability to the target attributes, resulting in less fluent sentences. We argue that distilling controlling information from natural texts can produce fluent sentences while maintaining high controllability. In this paper, we propose \textbf{GRA}dient-guided \textbf{C}ontrollable r\textbf{E}trieval (GRACE), a retrieval-augmented generation framework to facilitate the generation of fluent sentences with high attribute relevance. GRACE memorizes the semantic and attribute information from unlabeled corpora and applies a controllable retrieval to obtain desired information. For the generation, we design techniques to eliminate the domain bias from the retrieval results and integrate it into the generation model. Additionally, we propose a gradient-guided generation scheme that iteratively steers generation toward higher attribute relevance. Experimental results and quantities of examples verify the effectiveness of our method.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2023,,detox,
3339,"**Title**{D}u{NST}: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation

**Abstract**Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented only by self-generated pseudo text, generation models \textit{over-exploit} the previously learned text space and \textit{fail to explore} a larger one, suffering from a restricted generalization boundary and limited controllability. In this work, we propose DuNST, a novel ST framework to tackle these problems. DuNST jointly models text generation and classification as a dual process and further perturbs and escapes from the collapsed space by adding two kinds of flexible noise. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We theoretically demonstrate that DuNST can be regarded as enhancing the exploration of the potentially larger real text space while maintaining exploitation, guaranteeing improved performance. Experiments on three controllable generation tasks show that DuNST significantly boosts control accuracy with comparable generation fluency and diversity against several strong baselines.","Feng, Yuxi, Yi, Xiaoyuan, Wang, Xiting, Lakshmanan, V.S., Laks, Xie, Xing",,,{D}u{NST}: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation,,,10.18653/v1/2023.acl-long.488 , ,,"Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented only by self-generated pseudo text, generation models \textit{over-exploit} the previously learned text space and \textit{fail to explore} a larger one, suffering from a restricted generalization boundary and limited controllability. In this work, we propose DuNST, a novel ST framework to tackle these problems. DuNST jointly models text generation and classification as a dual process and further perturbs and escapes from the collapsed space by adding two kinds of flexible noise. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We theoretically demonstrate that DuNST can be regarded as enhancing the exploration of the potentially larger real text space while maintaining exploitation, guaranteeing improved performance. Experiments on three controllable generation tasks show that DuNST significantly boosts control accuracy with comparable generation fluency and diversity against several strong baselines.",,,,, ,  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,detox,
3347,"**Title**{B}eam{R}: Beam Reweighing with Attribute Discriminators for Controllable Text Generation

**Abstract**Recent advances in natural language processing have led to the availability of large pre-trained language models (LMs), with rich generative capabilities. Although these models are able to produce fluent and coherent text, it remains a challenge to control various attributes of the generation, including sentiment, formality, topic and many others. We propose a Beam Reweighing (BeamR) method, building on top of standard beam search, in order to control different attributes. BeamR combines any generative LM with any attribute discriminator, offering full flexibility of generation style and attribute, while the beam search backbone maintains fluency across different domains. Notably, BeamR allows practitioners to leverage pre-trained models without the need to train generative LMs together with discriminators. We evaluate BeamR in two diverse tasks: sentiment steering, and machine translation formality. Our results show that BeamR performs on par with or better than existing state-of-the-art approaches (including fine-tuned methods), and highlight the flexiblity of BeamR in both causal and seq2seq language modeling tasks.","Landsman, David, Chen, Jerry Zikun, Zaidi, Hussain",,,{B}eam{R}: Beam Reweighing with Attribute Discriminators for Controllable Text Generation,,,10.18653/v1/2022.findings-aacl.40 , ,,"Recent advances in natural language processing have led to the availability of large pre-trained language models (LMs), with rich generative capabilities. Although these models are able to produce fluent and coherent text, it remains a challenge to control various attributes of the generation, including sentiment, formality, topic and many others. We propose a Beam Reweighing (BeamR) method, building on top of standard beam search, in order to control different attributes. BeamR combines any generative LM with any attribute discriminator, offering full flexibility of generation style and attribute, while the beam search backbone maintains fluency across different domains. Notably, BeamR allows practitioners to leverage pre-trained models without the need to train generative LMs together with discriminators. We evaluate BeamR in two diverse tasks: sentiment steering, and machine translation formality. Our results show that BeamR performs on par with or better than existing state-of-the-art approaches (including fine-tuned methods), and highlight the flexiblity of BeamR in both causal and seq2seq language modeling tasks.",,,,, ,  Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022,,detox,
3351,"**Title**{S}tyle{PTB}: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer

**Abstract**Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, StylePTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on StylePTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, StylePTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.","Lyu, Yiwei, Liang, Paul Pu, Pham, Hai, Hovy, Eduard, P{\'o}czos, Barnab{\'a}s, Salakhutdinov, Ruslan, Morency, Louis-Philippe",,,{S}tyle{PTB}: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer,,,10.18653/v1/2021.naacl-main.171 , ,,"Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, StylePTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on StylePTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, StylePTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.",,,,, ,  Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,,detox,
3355,"**Title**{DE}xperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts

**Abstract**Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with {\textquotedblleft}expert{\textquotedblright} LMs and/or {\textquotedblleft}anti-expert{\textquotedblright} LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.","Liu, Alisa, Sap, Maarten, Lu, Ximing, Swayamdipta, Swabha, Bhagavatula, Chandra, Smith, Noah A., Choi, Yejin",,,{DE}xperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts,,,10.18653/v1/2021.acl-long.522 , ,,"Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with {\textquotedblleft}expert{\textquotedblright} LMs and/or {\textquotedblleft}anti-expert{\textquotedblright} LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.",,,,, ,  Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),,detox,
3359,"**Title**{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models

**Abstract**Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {\textquotedblleft}bad{\textquotedblright} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.","Gehman, Samuel, Gururangan, Suchin, Sap, Maarten, Choi, Yejin, Smith, Noah A.",,,{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models,,,10.18653/v1/2020.findings-emnlp.301 , ,,"Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {\textquotedblleft}bad{\textquotedblright} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2020,,Gen_dataset#detection,
3366,"**Title**Innovative Approaches to Enhancing Safety and Ethical {AI} Interactions in Digital Environments

**Abstract**Ensuring safe online environments is a formidable challenge, but nonetheless an important one as people are now chronically online. The increasing online presence of people paired with the prevalence of harmful content such as toxicity, hate speech, misinformation and disinformation across various social media platforms and within different video calls for stronger detection and prevention methods. My research interests primarily lie in applied natural language processing for social good. Previously, I focused on measuring partisan polarization on social media during the COVID-19 pandemic and its societal impacts. Currently, at Ubisoft La Forge, I am dedicated to enhancing player safety within in-game chat systems by developing methods to detect toxicity, evaluating the biases in these detection systems, and assessing the current ecological state of online interactions. Additionally, I am engaged in simulating social media environments using LLMs to ethically test detection methods, evaluate the effectiveness of current mitigation strategies, and potentially introduce new, successful strategies. My suggested topics for discussion: 1. Understanding and mitigating social harms through high fidelity simulated social media environments 2. Enhancing safety in online environments such as within in-game chats (text and speech) 3. Personification of LLM agents 4. Ethically simulating social media sandbox environments at scale with LLM agents 5. Re-balancing the playing field between good and bad actors: Strategies for countering societal-scale manipulation.","Yang, Zachary",,,Innovative Approaches to Enhancing Safety and Ethical {AI} Interactions in Digital Environments,,, , ,,"Ensuring safe online environments is a formidable challenge, but nonetheless an important one as people are now chronically online. The increasing online presence of people paired with the prevalence of harmful content such as toxicity, hate speech, misinformation and disinformation across various social media platforms and within different video calls for stronger detection and prevention methods. My research interests primarily lie in applied natural language processing for social good. Previously, I focused on measuring partisan polarization on social media during the COVID-19 pandemic and its societal impacts. Currently, at Ubisoft La Forge, I am dedicated to enhancing player safety within in-game chat systems by developing methods to detect toxicity, evaluating the biases in these detection systems, and assessing the current ecological state of online interactions. Additionally, I am engaged in simulating social media environments using LLMs to ethically test detection methods, evaluate the effectiveness of current mitigation strategies, and potentially introduce new, successful strategies. My suggested topics for discussion: 1. Understanding and mitigating social harms through high fidelity simulated social media environments 2. Enhancing safety in online environments such as within in-game chats (text and speech) 3. Personification of LLM agents 4. Ethically simulating social media sandbox environments at scale with LLM agents 5. Re-balancing the playing field between good and bad actors: Strategies for countering societal-scale manipulation.",,,,, ,  Proceedings of the 20th Workshop of Young Researchers' Roundtable on Spoken Dialogue Systems,,detection,
3367,"**Title**Automated Adversarial Discovery for Safety Classifiers

**Abstract**Safety classifiers are critical in mitigating toxicity on online forums such as social media and in chatbots. Still, they continue to be vulnerable to emergent, and often innumerable, adversarial attacks.Traditional automated adversarial data generation methods, however, tend to produce attacks that are not diverse, but variations of previously observed harm types.We formalize the task of automated adversarial discovery for safety classifiers - to find new attacks along previously unseen harm dimensions that expose new weaknesses in the classifier.We measure progress on this task along two key axes (1) adversarial success: does the attack fool the classifier? and (2) dimensional diversity: does the attack represent a previously unseen harm type?Our evaluation of existing attack generation methods on the CivilComments toxicity task reveals their limitations: Word perturbation attacks fail to fool classifiers, while prompt-based LLM attacks have more adversarial success, but lack dimensional diversity.Even our best-performing prompt-based method finds new successful attacks on unseen harm dimensions of attacks only 5{\%} of the time.Automatically finding new harmful dimensions of attack is crucial and there is substantial headroom for future research on our new task.","Lal, Yash Kumar, Lahoti, Preethi, Sinha, Aradhana, Qin, Yao, Balashankar, Ananth",,,Automated Adversarial Discovery for Safety Classifiers,,,10.18653/v1/2024.trustnlp-1.2 , ,,"Safety classifiers are critical in mitigating toxicity on online forums such as social media and in chatbots. Still, they continue to be vulnerable to emergent, and often innumerable, adversarial attacks.Traditional automated adversarial data generation methods, however, tend to produce attacks that are not diverse, but variations of previously observed harm types.We formalize the task of automated adversarial discovery for safety classifiers - to find new attacks along previously unseen harm dimensions that expose new weaknesses in the classifier.We measure progress on this task along two key axes (1) adversarial success: does the attack fool the classifier? and (2) dimensional diversity: does the attack represent a previously unseen harm type?Our evaluation of existing attack generation methods on the CivilComments toxicity task reveals their limitations: Word perturbation attacks fail to fool classifiers, while prompt-based LLM attacks have more adversarial success, but lack dimensional diversity.Even our best-performing prompt-based method finds new successful attacks on unseen harm dimensions of attacks only 5{\%} of the time.Automatically finding new harmful dimensions of attack is crucial and there is substantial headroom for future research on our new task.",,,,, ,  Proceedings of the 4th Workshop on Trustworthy Natural Language Processing (TrustNLP 2024),,detection,
3369,"**Title**How Trustworthy are Open-Source {LLM}s? An Assessment under Malicious Demonstrations Shows their Vulnerabilities

**Abstract**The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.","Mo, Lingbo, Wang, Boshi, Chen, Muhao, Sun, Huan",,,How Trustworthy are Open-Source {LLM}s? An Assessment under Malicious Demonstrations Shows their Vulnerabilities,,,10.18653/v1/2024.naacl-long.152 , ,,"The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.",,,,, ,  Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,detox,
3370,"**Title**How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models

**Abstract**Given the growing influx of misinformation across news and social media, there is a critical need for systems that can provide effective real-time verification of news claims. Large language or multimodal model based verification has been proposed to scale up online policing mechanisms for mitigating spread of false and harmful content. While these can potentially reduce burden on human fact-checkers, such efforts may be hampered by foundation model training data becoming outdated. In this work, we test the limits of improving foundation model performance without continual updating through an initial study of knowledge transfer using either existing intra- and inter-domain benchmarks or explanations generated from large language models (LLMs).We evaluate on 12 public benchmarks for fact-checking and misinformation detection as well as two other tasks relevant to content moderation - toxicity and stance detection. Our results on two recent multi-modal fact-checking benchmarks, Mocheg and Fakeddit, indicate that knowledge transfer strategies can improve Fakeddit performance over the state-of-the-art by up to 1.7{\%} and Mocheg performance by up to 2.9{\%}. The code, model checkpoints, and dataset are available: https://github.com/given131/ fact-verifier-knowledge-transfer.","Lee, Jaeyoung, Lu, Ximing, Hessel, Jack, Brahman, Faeze, Yu, Youngjae, Bisk, Yonatan, Choi, Yejin, Gabriel, Saadia",,,How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models,,,10.18653/v1/2024.findings-emnlp.764 , ,,"Given the growing influx of misinformation across news and social media, there is a critical need for systems that can provide effective real-time verification of news claims. Large language or multimodal model based verification has been proposed to scale up online policing mechanisms for mitigating spread of false and harmful content. While these can potentially reduce burden on human fact-checkers, such efforts may be hampered by foundation model training data becoming outdated. In this work, we test the limits of improving foundation model performance without continual updating through an initial study of knowledge transfer using either existing intra- and inter-domain benchmarks or explanations generated from large language models (LLMs).We evaluate on 12 public benchmarks for fact-checking and misinformation detection as well as two other tasks relevant to content moderation - toxicity and stance detection. Our results on two recent multi-modal fact-checking benchmarks, Mocheg and Fakeddit, indicate that knowledge transfer strategies can improve Fakeddit performance over the state-of-the-art by up to 1.7{\%} and Mocheg performance by up to 2.9{\%}. The code, model checkpoints, and dataset are available: https://github.com/given131/ fact-verifier-knowledge-transfer.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2024,,detection,
3371,"**Title**{XD}etox: Text Detoxification with Token-Level Toxicity Explanations

**Abstract**Methods for mitigating toxic content through masking and infilling often overlook the decision-making process, leading to either insufficient or excessive modifications of toxic tokens. To address this challenge, we propose XDetox, a novel method that integrates token-level toxicity explanations with the masking and infilling detoxification process. We utilized this approach with two strategies to enhance the performance of detoxification. First, identifying toxic tokens to improve the quality of masking. Second, selecting the regenerated sentence by re-ranking the least toxic sentence among candidates. Our experimental results show state-of-the-art performance across four datasets compared to existing detoxification methods. Furthermore, human evaluations indicate that our method outperforms baselines in both fluency and toxicity reduction. These results demonstrate the effectiveness of our method in text detoxification.","Lee, Beomseok, Kim, Hyunwoo, Kim, Keon, Choi, Yong Suk",,,{XD}etox: Text Detoxification with Token-Level Toxicity Explanations,,,10.18653/v1/2024.emnlp-main.848 , ,,"Methods for mitigating toxic content through masking and infilling often overlook the decision-making process, leading to either insufficient or excessive modifications of toxic tokens. To address this challenge, we propose XDetox, a novel method that integrates token-level toxicity explanations with the masking and infilling detoxification process. We utilized this approach with two strategies to enhance the performance of detoxification. First, identifying toxic tokens to improve the quality of masking. Second, selecting the regenerated sentence by re-ranking the least toxic sentence among candidates. Our experimental results show state-of-the-art performance across four datasets compared to existing detoxification methods. Furthermore, human evaluations indicate that our method outperforms baselines in both fluency and toxicity reduction. These results demonstrate the effectiveness of our method in text detoxification.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3372,"**Title**Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding

**Abstract**Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023).","Tu, Lifu, Yavuz, Semih, Qu, Jin, Xu, Jiacheng, Meng, Rui, Xiong, Caiming, Zhou, Yingbo",,,Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding,,,10.18653/v1/2024.emnlp-main.870 , ,,"Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023).",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3374,"**Title**Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models

**Abstract**Considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. Furthermore, previous approaches have often neglected the crucial factor of language`s evolving nature over time. In this work, we present a comprehensive perspective on toxicity mitigation that takes into account its changing nature. We introduce Goodtriever, a flexible methodology that matches the current state-of-the-art toxicity mitigation while achieving 43{\%} relative latency reduction during inference and being more computationally efficient. By incorporating a retrieval-based approach at decoding time, Goodtriever enables toxicity-controlled text generation. Our research advocates for an increased focus on adaptable mitigation techniques, which better reflect the data drift models face when deployed in the wild.","Pozzobon, Luiza, Ermis, Beyza, Lewis, Patrick, Hooker, Sara",,,Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models,,,10.18653/v1/2023.findings-emnlp.339 , ,,"Considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. Furthermore, previous approaches have often neglected the crucial factor of language`s evolving nature over time. In this work, we present a comprehensive perspective on toxicity mitigation that takes into account its changing nature. We introduce Goodtriever, a flexible methodology that matches the current state-of-the-art toxicity mitigation while achieving 43{\%} relative latency reduction during inference and being more computationally efficient. By incorporating a retrieval-based approach at decoding time, Goodtriever enables toxicity-controlled text generation. Our research advocates for an increased focus on adaptable mitigation techniques, which better reflect the data drift models face when deployed in the wild.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,detox,
3375,"**Title**{C}on{P}rompt: Pre-training a Language Model with Machine-Generated Data for Implicit Hate Speech Detection

**Abstract**Implicit hate speech detection is a challenging task in text classification since no explicit cues (e.g., swear words) exist in the text. While some pre-trained language models have been developed for hate speech detection, they are not specialized in implicit hate speech. Recently, an implicit hate speech dataset with a massive number of samples has been proposed by controlling machine generation. We propose a pre-training approach, ConPrompt, to fully leverage such machine-generated data. Specifically, given a machine-generated statement, we use example statements of its origin prompt as positive samples for contrastive learning. Through pre-training with ConPrompt, we present ToxiGen-ConPrompt, a pre-trained language model for implicit hate speech detection. We conduct extensive experiments on several implicit hate speech datasets and show the superior generalization ability of ToxiGen-ConPrompt compared to other pre-trained models. Additionally, we empirically show that ConPrompt is effective in mitigating identity term bias, demonstrating that it not only makes a model more generalizable but also reduces unintended bias. We analyze the representation quality of ToxiGen-ConPrompt and show its ability to consider target group and toxicity, which are desirable features in terms of implicit hate speeches.","Kim, Youngwook, Park, Shinwoo, Namgoong, Youngsoo, Han, Yo-Sub",,,{C}on{P}rompt: Pre-training a Language Model with Machine-Generated Data for Implicit Hate Speech Detection,,,10.18653/v1/2023.findings-emnlp.731 , ,,"Implicit hate speech detection is a challenging task in text classification since no explicit cues (e.g., swear words) exist in the text. While some pre-trained language models have been developed for hate speech detection, they are not specialized in implicit hate speech. Recently, an implicit hate speech dataset with a massive number of samples has been proposed by controlling machine generation. We propose a pre-training approach, ConPrompt, to fully leverage such machine-generated data. Specifically, given a machine-generated statement, we use example statements of its origin prompt as positive samples for contrastive learning. Through pre-training with ConPrompt, we present ToxiGen-ConPrompt, a pre-trained language model for implicit hate speech detection. We conduct extensive experiments on several implicit hate speech datasets and show the superior generalization ability of ToxiGen-ConPrompt compared to other pre-trained models. Additionally, we empirically show that ConPrompt is effective in mitigating identity term bias, demonstrating that it not only makes a model more generalizable but also reduces unintended bias. We analyze the representation quality of ToxiGen-ConPrompt and show its ability to consider target group and toxicity, which are desirable features in terms of implicit hate speeches.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,detection,
3376,"**Title**Performance and Risk Trade-offs for Multi-word Text Prediction at Scale

**Abstract**Large Language Models such as GPT-3 are well-suited for text prediction tasks, which can help and delight users during text composition. LLMs are known to generate ethically inappropriate predictions even for seemingly innocuous contexts. Toxicity detection followed by filtering is a common strategy for mitigating the harm from such predictions. However, as we shall argue in this paper, in the context of text prediction, it is not sufficient to detect and filter toxic content. One also needs to ensure factual correctness and group-level fairness of the predictions; failing to do so can make the system ineffective and nonsensical at best, and unfair and detrimental to the users at worst. We discuss the gaps and challenges of toxicity detection approaches - from blocklist-based approaches to sophisticated state-of-the-art neural classifiers - by evaluating them on the text prediction task for English against a manually crafted CheckList of harms targeted at different groups and different levels of severity.","Vashishtha, Aniket, Prasad, S Sai, Bajaj, Payal, Chaudhary, Vishrav, Cook, Kate, Dandapat, Sandipan, Sitaram, Sunayana, Choudhury, Monojit",,,Performance and Risk Trade-offs for Multi-word Text Prediction at Scale,,,10.18653/v1/2023.findings-eacl.167 , ,,"Large Language Models such as GPT-3 are well-suited for text prediction tasks, which can help and delight users during text composition. LLMs are known to generate ethically inappropriate predictions even for seemingly innocuous contexts. Toxicity detection followed by filtering is a common strategy for mitigating the harm from such predictions. However, as we shall argue in this paper, in the context of text prediction, it is not sufficient to detect and filter toxic content. One also needs to ensure factual correctness and group-level fairness of the predictions; failing to do so can make the system ineffective and nonsensical at best, and unfair and detrimental to the users at worst. We discuss the gaps and challenges of toxicity detection approaches - from blocklist-based approaches to sophisticated state-of-the-art neural classifiers - by evaluating them on the text prediction task for English against a manually crafted CheckList of harms targeted at different groups and different levels of severity.",,,,, ,  Findings of the Association for Computational Linguistics: EACL 2023,,detection,
3377,"**Title**Mitigating Societal Harms in Large Language Models

**Abstract**Numerous recent studies have highlighted societal harms that can be caused by language technologies deployed in the wild. While several surveys, tutorials, and workshops have discussed the risks of harms in specific contexts {--} e.g., detecting and mitigating gender bias in NLP models {--} no prior work has developed a unified typology of technical approaches for mitigating harms of language generation models. Our tutorial is based on a survey we recently wrote that proposes such a typology. We will provide an overview of potential social issues in language generation, including toxicity, social biases, misinformation, factual inconsistency, and privacy violations. Our primary focus will be on how to systematically identify risks, and how eliminate them at various stages of model development, from data collection, to model development, to inference/language generation. Through this tutorial, we aim to equip NLP researchers and engineers with a suite of practical tools for mitigating safety risks from pretrained language generation models.","Kumar, Sachin, Balachandran, Vidhisha, Njoo, Lucille, Anastasopoulos, Antonios, Tsvetkov, Yulia",,,Mitigating Societal Harms in Large Language Models,,,10.18653/v1/2023.emnlp-tutorial.5 , ,,"Numerous recent studies have highlighted societal harms that can be caused by language technologies deployed in the wild. While several surveys, tutorials, and workshops have discussed the risks of harms in specific contexts {--} e.g., detecting and mitigating gender bias in NLP models {--} no prior work has developed a unified typology of technical approaches for mitigating harms of language generation models. Our tutorial is based on a survey we recently wrote that proposes such a typology. We will provide an overview of potential social issues in language generation, including toxicity, social biases, misinformation, factual inconsistency, and privacy violations. Our primary focus will be on how to systematically identify risks, and how eliminate them at various stages of model development, from data collection, to model development, to inference/language generation. Through this tutorial, we aim to equip NLP researchers and engineers with a suite of practical tools for mitigating safety risks from pretrained language generation models.",,,,, ,  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts,,detection,
3379,"**Title**Improving Covert Toxicity Detection by Retrieving and Generating References

**Abstract**Models for detecting toxic content play an important role in keeping people safe online. There has been much progress in detecting overt toxicity. Covert toxicity, however, remains a challenge because its detection requires an understanding of implicit meaning and subtle connotations. In this paper, we explore the potential of leveraging references, such as external knowledge and textual interpretations, to enhance the detection of covert toxicity. We run experiments on two covert toxicity datasets with two types of references: 1) information retrieved from a search API, and 2) interpretations generated by large language models. We find that both types of references improve detection, with the latter being more useful than the former. We also find that generating interpretations grounded on properties of covert toxicity, such as humor and irony, lead to the largest improvements","Lee, Dong-Ho, Cho, Hyundong, Jin, Woojeong, Moon, Jihyung, Park, Sungjoon, R{\""o}ttger, Paul, Pujara, Jay, Lee, Roy Ka-wei",,,Improving Covert Toxicity Detection by Retrieving and Generating References,,,10.18653/v1/2024.woah-1.21 , ,,"Models for detecting toxic content play an important role in keeping people safe online. There has been much progress in detecting overt toxicity. Covert toxicity, however, remains a challenge because its detection requires an understanding of implicit meaning and subtle connotations. In this paper, we explore the potential of leveraging references, such as external knowledge and textual interpretations, to enhance the detection of covert toxicity. We run experiments on two covert toxicity datasets with two types of references: 1) information retrieved from a search API, and 2) interpretations generated by large language models. We find that both types of references improve detection, with the latter being more useful than the former. We also find that generating interpretations grounded on properties of covert toxicity, such as humor and irony, lead to the largest improvements",,,,, ,  Proceedings of the 8th Workshop on Online Abuse and Harms (WOAH 2024),,detection,
3380,"**Title**{LLM}-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection

**Abstract**Large Language Model (LLM)-based Synthetic Data is becoming an increasingly important field of research. One of its promising application is in training classifiers to detect online toxicity, which is of increasing concern in today`s digital landscape. In this work, we assess the feasibility of generative models to generate synthetic data for toxic speech detection. Our experiments are conducted on six different toxicity datasets, four of whom are hateful and two are toxic in the broader sense. We then employ a classifier trained on the original data for filtering. To explore the potential of this data, we conduct experiments using combinations of original and synthetic data, synthetic oversampling of the minority class, and a comparison of original vs. synthetic-only training. Results indicate that while our generative models offer benefits in certain scenarios, it does not improve hateful dataset classification. However, it does boost patronizing and condescending language detection. We find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods. Code is available on GitHub; the generated dataset will be available on Zenodo in the final submission.","Kruschwitz, Udo, Schmidhuber, Maximilian",,,{LLM}-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection,,, , ,,"Large Language Model (LLM)-based Synthetic Data is becoming an increasingly important field of research. One of its promising application is in training classifiers to detect online toxicity, which is of increasing concern in today`s digital landscape. In this work, we assess the feasibility of generative models to generate synthetic data for toxic speech detection. Our experiments are conducted on six different toxicity datasets, four of whom are hateful and two are toxic in the broader sense. We then employ a classifier trained on the original data for filtering. To explore the potential of this data, we conduct experiments using combinations of original and synthetic data, synthetic oversampling of the minority class, and a comparison of original vs. synthetic-only training. Results indicate that while our generative models offer benefits in certain scenarios, it does not improve hateful dataset classification. However, it does boost patronizing and condescending language detection. We find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods. Code is available on GitHub; the generated dataset will be available on Zenodo in the final submission.",,,,, ,"  Proceedings of the Fourth Workshop on Threat, Aggression {\&} Cyberbullying @ LREC-COLING-2024",,detection,
3381,"**Title**{MIC}o: Preventative Detoxification of Large Language Models through Inhibition Control

**Abstract**Large Language Models (LLMs) are powerful tools which have been both dominant and commonplace in the field of Artificial Intelligence. Yet, LLMs have a tendency to devolve into toxic degeneration, wherein otherwise safe and unproblematic models begin generating toxic content. For the sake of social responsibility and inspired by the biological mechanisms of inhibition control, we introduce the paradigm of Education for Societal Norms (ESN). By collecting and labeling examples as acceptable and unacceptable (in this case toxic and non-toxic), and including a corresponding acceptable rewrite with every unacceptable example, we introduce a new mechanism for LLM detoxification. We annotate a dataset of 2,850 entries and use it to fine-tune a model, which we call a Model with Inhibition Control (MICo). Evaluating this model on toxicity detection capability, rewrite detoxification, meaning preservation, and overall toxicity reduction, we discover significant improvements over the baseline model. In our experiments we show that overall toxicity of this model is more than 60{\%} reduced, with over 75{\%} reduction in severe toxicity.","Siegelmann, Roy, Mehrabi, Ninareh, Goyal, Palash, Goyal, Prasoon, Bauer, Lisa, Dhamala, Jwala, Galstyan, Aram, Gupta, Rahul, Ghanadan, Reza",,,{MIC}o: Preventative Detoxification of Large Language Models through Inhibition Control,,,10.18653/v1/2024.findings-naacl.110 , ,,"Large Language Models (LLMs) are powerful tools which have been both dominant and commonplace in the field of Artificial Intelligence. Yet, LLMs have a tendency to devolve into toxic degeneration, wherein otherwise safe and unproblematic models begin generating toxic content. For the sake of social responsibility and inspired by the biological mechanisms of inhibition control, we introduce the paradigm of Education for Societal Norms (ESN). By collecting and labeling examples as acceptable and unacceptable (in this case toxic and non-toxic), and including a corresponding acceptable rewrite with every unacceptable example, we introduce a new mechanism for LLM detoxification. We annotate a dataset of 2,850 entries and use it to fine-tune a model, which we call a Model with Inhibition Control (MICo). Evaluating this model on toxicity detection capability, rewrite detoxification, meaning preservation, and overall toxicity reduction, we discover significant improvements over the baseline model. In our experiments we show that overall toxicity of this model is more than 60{\%} reduced, with over 75{\%} reduction in severe toxicity.",,,,, ,  Findings of the Association for Computational Linguistics: NAACL 2024,,detox,
3382,"**Title**Implanting {LLM}`s Knowledge via Reading Comprehension Tree for Toxicity Detection

**Abstract**Toxicity detection plays a crucial role in maintaining the peace of the society. Existing methods can be roughly categorized as small language model (SLM) based and large language model (LLM) based. However, due to the limitation of SLMs on general knowledge and the potential embedded bias in LLMs despite their large amount of knowledge, it is not a good idea to detect toxicity only with either SLM or LLM based method.In this work, we propose to implant LLM`s knowledge into SLM based methods such that we can stick to both types of models' strengths. To this end, we develop a reading comprehension (RC) tree to transfer knowledge between two models. Specifically, we first construct the RC tree, from an extensive to intensive reading perspective, to capture the local and global information in the text. We then model samples encoded by SLM and knowledge extracted from LLM as two distributions using the constructed RT tree. We finally transfer knowledge via optimal transportation between two distributions. Extensive experiments prove the effectiveness of our method on real-world and machine-generated datasets.","Kang, Hankun, Qian, Tieyun",,,Implanting {LLM}`s Knowledge via Reading Comprehension Tree for Toxicity Detection,,,10.18653/v1/2024.findings-acl.56 , ,,"Toxicity detection plays a crucial role in maintaining the peace of the society. Existing methods can be roughly categorized as small language model (SLM) based and large language model (LLM) based. However, due to the limitation of SLMs on general knowledge and the potential embedded bias in LLMs despite their large amount of knowledge, it is not a good idea to detect toxicity only with either SLM or LLM based method.In this work, we propose to implant LLM`s knowledge into SLM based methods such that we can stick to both types of models' strengths. To this end, we develop a reading comprehension (RC) tree to transfer knowledge between two models. Specifically, we first construct the RC tree, from an extensive to intensive reading perspective, to capture the local and global information in the text. We then model samples encoded by SLM and knowledge extracted from LLM as two distributions using the constructed RT tree. We finally transfer knowledge via optimal transportation between two distributions. Extensive experiments prove the effectiveness of our method on real-world and machine-generated datasets.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,detection,
3386,"**Title**Toxicity Detection is {NOT} all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method

**Abstract**Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey study with volunteer moderators to gain insight into their perspectives on useful moderation models. Overall, we observe a non trivial gap, as missing developed models and LLMs exhibit moderate to low performance on a significant portion of the rules. Moderators' reports provide guides for future work on developing moderation assistant models.","Cao, Yang Trista, Domingo, Lovely-Frances, Gilbert, Sarah, Mazurek, Michelle L., Shilton, Katie, Daum{\'e} Iii, Hal",,,Toxicity Detection is {NOT} all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method,,,10.18653/v1/2024.emnlp-main.209 , ,,"Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey study with volunteer moderators to gain insight into their perspectives on useful moderation models. Overall, we observe a non trivial gap, as missing developed models and LLMs exhibit moderate to low performance on a significant portion of the rules. Moderators' reports provide guides for future work on developing moderation assistant models.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detection,
3387,"**Title**Can We Statically Locate Knowledge in Large Language Models? Financial Domain and Toxicity Reduction Case Studies

**Abstract**Current large language model (LLM) evaluations rely on benchmarks to assess model capabilities and their encoded knowledge. However, these evaluations cannot reveal where a model encodes its knowledge, and thus little is known about which weights contain specific information. We propose a method to statically (without forward or backward passes) locate topical knowledge in the weight space of an LLM, building on a prior insight that parameters can be decoded into interpretable tokens. If parameters can be mapped into the embedding space, it should be possible to directly search for knowledge via embedding similarity. We study the validity of this assumption across several LLMs for a variety of concepts in the financial domain and a toxicity detection setup. Our analysis yields an improved understanding of the promises and limitations of static knowledge location in real-world scenarios.","Armengol-Estap{\'e}, Jordi, Li, Lingyu, Gehrmann, Sebastian, Gopal, Achintya, Rosenberg, David S, Mann, Gideon S., Dredze, Mark",,,Can We Statically Locate Knowledge in Large Language Models? Financial Domain and Toxicity Reduction Case Studies,,,10.18653/v1/2024.blackboxnlp-1.9 , ,,"Current large language model (LLM) evaluations rely on benchmarks to assess model capabilities and their encoded knowledge. However, these evaluations cannot reveal where a model encodes its knowledge, and thus little is known about which weights contain specific information. We propose a method to statically (without forward or backward passes) locate topical knowledge in the weight space of an LLM, building on a prior insight that parameters can be decoded into interpretable tokens. If parameters can be mapped into the embedding space, it should be possible to directly search for knowledge via embedding similarity. We study the validity of this assumption across several LLMs for a variety of concepts in the financial domain and a toxicity detection setup. Our analysis yields an improved understanding of the promises and limitations of static knowledge location in real-world scenarios.",,,,, ,  Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP,,detox,
3388,"**Title**Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models

**Abstract**We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the Bias Identification Test in Sentiment (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.","Narayanan Venkit, Pranav, Srinath, Mukund, Wilson, Shomir",,,Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models,,,10.18653/v1/2023.trustnlp-1.3 , ,,"We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the Bias Identification Test in Sentiment (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.",,,,, ,  Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023),,detection#evaluation,
3389,"**Title**Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values

**Abstract**Many NLP classification tasks, such as sexism/racism detection or toxicity detection, are based on human values. Yet, human values can vary under diverse cultural conditions. Therefore, we introduce a framework for value-aligned classification that performs prediction based on explicitly written human values in the command. Along with the task, we propose a practical approach that distills value-aligned knowledge from large-scale language models (LLMs) to construct value-aligned classifiers in two steps. First, we generate value-aligned training data from LLMs by prompt-based few-shot learning. Next, we fine-tune smaller classification models with the generated data for the task. Empirical results show that our VA-Models surpass multiple baselines by at least 15.56{\%} on the F1-score, including few-shot learning with OPT-175B and existing text augmentation methods. We suggest that using classifiers with explicit human value input improves both inclusivity {\&} explainability in AI.","Bang, Yejin, Yu, Tiezheng, Madotto, Andrea, Lin, Zhaojiang, Diab, Mona, Fung, Pascale",,,Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values,,,10.18653/v1/2023.trustnlp-1.27 , ,,"Many NLP classification tasks, such as sexism/racism detection or toxicity detection, are based on human values. Yet, human values can vary under diverse cultural conditions. Therefore, we introduce a framework for value-aligned classification that performs prediction based on explicitly written human values in the command. Along with the task, we propose a practical approach that distills value-aligned knowledge from large-scale language models (LLMs) to construct value-aligned classifiers in two steps. First, we generate value-aligned training data from LLMs by prompt-based few-shot learning. Next, we fine-tune smaller classification models with the generated data for the task. Empirical results show that our VA-Models surpass multiple baselines by at least 15.56{\%} on the F1-score, including few-shot learning with OPT-175B and existing text augmentation methods. We suggest that using classifiers with explicit human value input improves both inclusivity {\&} explainability in AI.",,,,, ,  Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023),,detection,
3390,"**Title**{LEXPLAIN}: Improving Model Explanations via Lexicon Supervision

**Abstract**Model explanations that shed light on the model`s predictions are becoming a desired additional output of NLP models, alongside their predictions. Challenges in creating these explanations include making them trustworthy and faithful to the model`s predictions. In this work, we propose a novel framework for guiding model explanations by supervising them explicitly. To this end, our method, LEXplain, uses task-related lexicons to directly supervise model explanations. This approach consistently improves the model`s explanations without sacrificing performance on the task, as we demonstrate on sentiment analysis and toxicity detection. Our analyses show that our method also demotes spurious correlations (i.e., with respect to African American English dialect) when performing the task, improving fairness.","Ahia, Orevaoghene, Gonen, Hila, Balachandran, Vidhisha, Tsvetkov, Yulia, Smith, Noah A.",,,{LEXPLAIN}: Improving Model Explanations via Lexicon Supervision,,,10.18653/v1/2023.starsem-1.19 , ,,"Model explanations that shed light on the model`s predictions are becoming a desired additional output of NLP models, alongside their predictions. Challenges in creating these explanations include making them trustworthy and faithful to the model`s predictions. In this work, we propose a novel framework for guiding model explanations by supervising them explicitly. To this end, our method, LEXplain, uses task-related lexicons to directly supervise model explanations. This approach consistently improves the model`s explanations without sacrificing performance on the task, as we demonstrate on sentiment analysis and toxicity detection. Our analyses show that our method also demotes spurious correlations (i.e., with respect to African American English dialect) when performing the task, improving fairness.",,,,, ,  Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023),,detection,
3391,"**Title**{IREL} at {S}em{E}val-2023 Task 11: User Conditioned Modelling for Toxicity Detection in Subjective Tasks

**Abstract**This paper describes our system used in the SemEval-2023 Task 11 Learning With Disagreements (Le-Wi-Di). This is a subjective task since it deals with detecting hate speech, misogyny and offensive language. Thus, disagreement among annotators is expected. We experiment with different settings like loss functions specific for subjective tasks and include anonymized annotator-specific information to help us understand the level of disagreement. We perform an in-depth analysis of the performance discrepancy of these different modelling choices. Our system achieves a cross-entropy of 0.58, 4.01 and 3.70 on the test sets of HS-Brexit, ArMIS and MD-Agreement, respectively. Our code implementation is publicly available.","Maity, Ankita, Kandru, Pavan, Singh, Bhavyajeet, Aditya Hari, Kancharla, Varma, Vasudeva",,,{IREL} at {S}em{E}val-2023 Task 11: User Conditioned Modelling for Toxicity Detection in Subjective Tasks,,,10.18653/v1/2023.semeval-1.294 , ,,"This paper describes our system used in the SemEval-2023 Task 11 Learning With Disagreements (Le-Wi-Di). This is a subjective task since it deals with detecting hate speech, misogyny and offensive language. Thus, disagreement among annotators is expected. We experiment with different settings like loss functions specific for subjective tasks and include anonymized annotator-specific information to help us understand the level of disagreement. We perform an in-depth analysis of the performance discrepancy of these different modelling choices. Our system achieves a cross-entropy of 0.58, 4.01 and 3.70 on the test sets of HS-Brexit, ArMIS and MD-Agreement, respectively. Our code implementation is publicly available.",,,,, ,  Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023),,detection,
3393,"**Title**No offence, Bert - {I} insult only humans! Multilingual sentence-level attack on toxicity detection networks

**Abstract**We introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. This approach is shown to be working on seven languages from three different language families. We also describe the defence mechanism against the aforementioned attack and discuss its limitations.","Berezin, Sergey, Farahbakhsh, Reza, Crespi, Noel",,,"No offence, Bert - {I} insult only humans! Multilingual sentence-level attack on toxicity detection networks",,,10.18653/v1/2023.findings-emnlp.155 , ,,"We introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. This approach is shown to be working on seven languages from three different language families. We also describe the defence mechanism against the aforementioned attack and discuss its limitations.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,detection,
3394,"**Title**{T}oxic{C}hat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-{AI} Conversation

**Abstract**Despite remarkable advances that large language models have achieved in chatbots nowadays, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media contents, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark constructed based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference when compared to social media contents. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions.","Lin, Zi, Wang, Zihan, Tong, Yongqi, Wang, Yangkun, Guo, Yuxin, Wang, Yujia, Shang, Jingbo",,,{T}oxic{C}hat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-{AI} Conversation,,,10.18653/v1/2023.findings-emnlp.311 , ,,"Despite remarkable advances that large language models have achieved in chatbots nowadays, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media contents, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark constructed based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference when compared to social media contents. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,Gen_dataset#detection,
3395,"**Title**Towards Detecting Contextual Real-Time Toxicity for In-Game Chat

**Abstract**Real-time toxicity detection in online environments poses a significant challenge, due to the increasing prevalence of social media and gaming platforms. We introduce ToxBuster, a simple and scalable model that reliably detects toxic content in real-time for a line of chat by including chat history and metadata. ToxBuster consistently outperforms conventional toxicity models across popular multiplayer games, including Rainbow Six Siege, For Honor, and DOTA 2. We conduct an ablation study to assess the importance of each model component and explore ToxBuster`s transferability across the datasets. Furthermore, we showcase ToxBuster`s efficacy in post-game moderation, successfully flagging 82.1{\%} of chat-reported players at a precision level of 90.0{\%}. Additionally, we show how an additional 6{\%} of unreported toxic players can be proactively moderated.","Yang, Zachary, Grenon-Godbout, Nicolas, Rabbany, Reihaneh",,,Towards Detecting Contextual Real-Time Toxicity for In-Game Chat,,,10.18653/v1/2023.findings-emnlp.663 , ,,"Real-time toxicity detection in online environments poses a significant challenge, due to the increasing prevalence of social media and gaming platforms. We introduce ToxBuster, a simple and scalable model that reliably detects toxic content in real-time for a line of chat by including chat history and metadata. ToxBuster consistently outperforms conventional toxicity models across popular multiplayer games, including Rainbow Six Siege, For Honor, and DOTA 2. We conduct an ablation study to assess the importance of each model component and explore ToxBuster`s transferability across the datasets. Furthermore, we showcase ToxBuster`s efficacy in post-game moderation, successfully flagging 82.1{\%} of chat-reported players at a precision level of 90.0{\%}. Additionally, we show how an additional 6{\%} of unreported toxic players can be proactively moderated.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,detection,
3397,"**Title**Accounting for Offensive Speech as a Practice of Resistance

**Abstract**Tasks such as toxicity detection, hate speech detection, and online harassment detection have been developed for identifying interactions involving offensive speech. In this work we articulate the need for a relational understanding of offensiveness to help distinguish denotative offensive speech from offensive speech serving as a mechanism through which marginalized communities resist oppressive social norms. Using examples from the queer community, we argue that evaluations of offensive speech must focus on the impacts of language use. We call this the cynic perspective{--} or a characteristic of language with roots in Cynic philosophy that pertains to employing offensive speech as a practice of resistance. We also explore the degree to which NLP systems may encounter limits to modeling relational context.","Diaz, Mark, Amironesei, Razvan, Weidinger, Laura, Gabriel, Iason",,,Accounting for Offensive Speech as a Practice of Resistance,,,10.18653/v1/2022.woah-1.18 , ,,"Tasks such as toxicity detection, hate speech detection, and online harassment detection have been developed for identifying interactions involving offensive speech. In this work we articulate the need for a relational understanding of offensiveness to help distinguish denotative offensive speech from offensive speech serving as a mechanism through which marginalized communities resist oppressive social norms. Using examples from the queer community, we argue that evaluations of offensive speech must focus on the impacts of language use. We call this the cynic perspective{--} or a characteristic of language with roots in Cynic philosophy that pertains to employing offensive speech as a practice of resistance. We also explore the degree to which NLP systems may encounter limits to modeling relational context.",,,,, ,  Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH),,detection,
3398,"**Title**Bias Discovery within Human Raters: A Case Study of the Jigsaw Dataset

**Abstract**Understanding and quantifying the bias introduced by human annotation of data is a crucial problem for trustworthy supervised learning. Recently, a perspectivist trend has emerged in the NLP community, focusing on the inadequacy of previous aggregation schemes, which suppose the existence of single ground truth. This assumption is particularly problematic for sensitive tasks involving subjective human judgments, such as toxicity detection. To address these issues, we propose a preliminary approach for bias discovery within human raters by exploring individual ratings for specific sensitive topics annotated in the texts. Our analysis`s object consists of the Jigsaw dataset, a collection of comments aiming at challenging online toxicity identification.","Marchiori Manerba, Marta, Guidotti, Riccardo, Passaro, Lucia, Ruggieri, Salvatore",,,Bias Discovery within Human Raters: A Case Study of the Jigsaw Dataset,,, , ,,"Understanding and quantifying the bias introduced by human annotation of data is a crucial problem for trustworthy supervised learning. Recently, a perspectivist trend has emerged in the NLP community, focusing on the inadequacy of previous aggregation schemes, which suppose the existence of single ground truth. This assumption is particularly problematic for sensitive tasks involving subjective human judgments, such as toxicity detection. To address these issues, we propose a preliminary approach for bias discovery within human raters by exploring individual ratings for specific sensitive topics annotated in the texts. Our analysis`s object consists of the Jigsaw dataset, a collection of comments aiming at challenging online toxicity identification.",,,,, ,  Proceedings of the 1st Workshop on Perspectivist Approaches to NLP @LREC2022,,Use_dataset#evaluation,
3399,"**Title**Critical Perspectives: A Benchmark Revealing Pitfalls in {P}erspective{API}

**Abstract**Detecting {\textquotedblleft}toxic{\textquotedblright} language in internet content is a pressing social and technical challenge. In this work, we focus on Perspective API from Jigsaw, a state-of-the-art tool that promises to score the {\textquotedblleft}toxicity{\textquotedblright} of text, with a recent model update that claims impressive results (Lees et al., 2022). We seek to challenge certain normative claims about toxic language by proposing a new benchmark, Selected Adversarial SemanticS, or SASS. We evaluate Perspective on SASS, and compare to low-effort alternatives, like zero-shot and few-shot GPT-3 prompt models, in binary classification settings. We find that Perspective exhibits troubling shortcomings across a number of our toxicity categories. SASS provides a new tool for evaluating performance on previously undetected toxic language that avoids common normative pitfalls. Our work leads us to emphasize the importance of questioning assumptions made by tools already in deployment for toxicity detection in order to anticipate and prevent disparate harms.","Rosenblatt, Lucas, Piedras, Lorena, Wilkins, Julia",,,Critical Perspectives: A Benchmark Revealing Pitfalls in {P}erspective{API},,,10.18653/v1/2022.nlp4pi-1.2 , ,,"Detecting {\textquotedblleft}toxic{\textquotedblright} language in internet content is a pressing social and technical challenge. In this work, we focus on Perspective API from Jigsaw, a state-of-the-art tool that promises to score the {\textquotedblleft}toxicity{\textquotedblright} of text, with a recent model update that claims impressive results (Lees et al., 2022). We seek to challenge certain normative claims about toxic language by proposing a new benchmark, Selected Adversarial SemanticS, or SASS. We evaluate Perspective on SASS, and compare to low-effort alternatives, like zero-shot and few-shot GPT-3 prompt models, in binary classification settings. We find that Perspective exhibits troubling shortcomings across a number of our toxicity categories. SASS provides a new tool for evaluating performance on previously undetected toxic language that avoids common normative pitfalls. Our work leads us to emphasize the importance of questioning assumptions made by tools already in deployment for toxicity detection in order to anticipate and prevent disparate harms.",,,,, ,  Proceedings of the Second Workshop on NLP for Positive Impact (NLP4PI),,detection#evaluation#methodology,
3400,"**Title**What changed? Investigating Debiasing Methods using Causal Mediation Analysis

**Abstract**Previous work has examined how debiasing language models affect downstream tasks, specifically, how debiasing techniques influence task performance and whether debiased models also make impartial predictions in downstream tasks or not. However, what we don`t understand well yet is why debiasing methods have varying impacts on downstream tasks and how debiasing techniques affect internal components of language models, i.e., neurons, layers, and attentions. In this paper, we decompose the internal mechanisms of debiasing language models with respect to gender by applying causal mediation analysis to understand the influence of debiasing methods on toxicity detection as a downstream task. Our findings suggest a need to test the effectiveness of debiasing methods with different bias metrics, and to focus on changes in the behavior of certain components of the models, e.g.,first two layers of language models, and attention heads.","Jeoung, Sullam, Diesner, Jana",,,What changed? Investigating Debiasing Methods using Causal Mediation Analysis,,,10.18653/v1/2022.gebnlp-1.26 , ,,"Previous work has examined how debiasing language models affect downstream tasks, specifically, how debiasing techniques influence task performance and whether debiased models also make impartial predictions in downstream tasks or not. However, what we don`t understand well yet is why debiasing methods have varying impacts on downstream tasks and how debiasing techniques affect internal components of language models, i.e., neurons, layers, and attentions. In this paper, we decompose the internal mechanisms of debiasing language models with respect to gender by applying causal mediation analysis to understand the influence of debiasing methods on toxicity detection as a downstream task. Our findings suggest a need to test the effectiveness of debiasing methods with different bias metrics, and to focus on changes in the behavior of certain components of the models, e.g.,first two layers of language models, and attention heads.",,,,, ,  Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP),,detection,
3401,"**Title**Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media

**Abstract**Online social media platforms increasingly rely on Natural Language Processing (NLP) techniques to detect abusive content at scale in order to mitigate the harms it causes to their users. However, these techniques suffer from various sampling and association biases present in training data, often resulting in sub-par performance on content relevant to marginalized groups, potentially furthering disproportionate harms towards them. Studies on such biases so far have focused on only a handful of axes of disparities and subgroups that have annotations/lexicons available. Consequently, biases concerning non-Western contexts are largely ignored in the literature. In this paper, we introduce a weakly supervised method to robustly detect lexical biases in broader geo-cultural contexts. Through a case study on a publicly available toxicity detection model, we demonstrate that our method identifies salient groups of cross-geographic errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts. We also conduct analysis of a model trained on a dataset with ground truth labels to better understand these biases, and present preliminary mitigation experiments.","Ghosh, Sayan, Baker, Dylan, Jurgens, David, Prabhakaran, Vinodkumar",,,Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media,,,10.18653/v1/2021.wnut-1.35 , ,,"Online social media platforms increasingly rely on Natural Language Processing (NLP) techniques to detect abusive content at scale in order to mitigate the harms it causes to their users. However, these techniques suffer from various sampling and association biases present in training data, often resulting in sub-par performance on content relevant to marginalized groups, potentially furthering disproportionate harms towards them. Studies on such biases so far have focused on only a handful of axes of disparities and subgroups that have annotations/lexicons available. Consequently, biases concerning non-Western contexts are largely ignored in the literature. In this paper, we introduce a weakly supervised method to robustly detect lexical biases in broader geo-cultural contexts. Through a case study on a publicly available toxicity detection model, we demonstrate that our method identifies salient groups of cross-geographic errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts. We also conduct analysis of a model trained on a dataset with ground truth labels to better understand these biases, and present preliminary mitigation experiments.",,,,, ,  Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),,detection,
3402,"**Title**{T}ox{CCI}n: Toxic Content Classification with Interpretability

**Abstract**Despite the recent successes of transformer-based models in terms of effectiveness on a variety of tasks, their decisions often remain opaque to humans. Explanations are particularly important for tasks like offensive language or toxicity detection on social media because a manual appeal process is often in place to dispute automatically flagged content. In this work, we propose a technique to improve the interpretability of these models, based on a simple and powerful assumption: a post is at least as toxic as its most toxic span. We incorporate this assumption into transformer models by scoring a post based on the maximum toxicity of its spans and augmenting the training process to identify correct spans. We find this approach effective and can produce explanations that exceed the quality of those provided by Logistic Regression analysis (often regarded as a highly-interpretable model), according to a human study.","Xiang, Tong, MacAvaney, Sean, Yang, Eugene, Goharian, Nazli",,,{T}ox{CCI}n: Toxic Content Classification with Interpretability,,, , ,,"Despite the recent successes of transformer-based models in terms of effectiveness on a variety of tasks, their decisions often remain opaque to humans. Explanations are particularly important for tasks like offensive language or toxicity detection on social media because a manual appeal process is often in place to dispute automatically flagged content. In this work, we propose a technique to improve the interpretability of these models, based on a simple and powerful assumption: a post is at least as toxic as its most toxic span. We incorporate this assumption into transformer models by scoring a post based on the maximum toxicity of its spans and augmenting the training process to identify correct spans. We find this approach effective and can produce explanations that exceed the quality of those provided by Logistic Regression analysis (often regarded as a highly-interpretable model), according to a human study.",,,,, ,"  Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",,detection,
3403,"**Title**{UPB} at {S}em{E}val-2021 Task 5: Virtual Adversarial Training for Toxic Spans Detection

**Abstract**The real-world impact of polarization and toxicity in the online sphere marked the end of 2020 and the beginning of this year in a negative way. Semeval-2021, Task 5 - Toxic Spans Detection is based on a novel annotation of a subset of the Jigsaw Unintended Bias dataset and is the first language toxicity detection task dedicated to identifying the toxicity-level spans. For this task, participants had to automatically detect character spans in short comments that render the message as toxic. Our model considers applying Virtual Adversarial Training in a semi-supervised setting during the fine-tuning process of several Transformer-based models (i.e., BERT and RoBERTa), in combination with Conditional Random Fields. Our approach leads to performance improvements and more robust models, enabling us to achieve an F1-score of 65.73{\%} in the official submission and an F1-score of 66.13{\%} after further tuning during post-evaluation.","Paraschiv, Andrei, Cercel, Dumitru-Clementin, Dascalu, Mihai",,,{UPB} at {S}em{E}val-2021 Task 5: Virtual Adversarial Training for Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.26 , ,,"The real-world impact of polarization and toxicity in the online sphere marked the end of 2020 and the beginning of this year in a negative way. Semeval-2021, Task 5 - Toxic Spans Detection is based on a novel annotation of a subset of the Jigsaw Unintended Bias dataset and is the first language toxicity detection task dedicated to identifying the toxicity-level spans. For this task, participants had to automatically detect character spans in short comments that render the message as toxic. Our model considers applying Virtual Adversarial Training in a semi-supervised setting during the fine-tuning process of several Transformer-based models (i.e., BERT and RoBERTa), in combination with Conditional Random Fields. Our approach leads to performance improvements and more robust models, enabling us to achieve an F1-score of 65.73{\%} in the official submission and an F1-score of 66.13{\%} after further tuning during post-evaluation.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3404,"**Title**{NLRG} at {S}em{E}val-2021 Task 5: Toxic Spans Detection Leveraging {BERT}-based Token Classification and Span Prediction Techniques

**Abstract**Toxicity detection of text has been a popular NLP task in the recent years. In SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic spans within English passages. Most state-of-the-art span detection approaches employ various techniques, each of which can be broadly classified into Token Classification or Span Prediction approaches. In our paper, we explore simple versions of both of these approaches and their performance on the task. Specifically, we use BERT-based models - BERT, RoBERTa, and SpanBERT for both approaches. We also combine these approaches and modify them to bring improvements for Toxic Spans prediction. To this end, we investigate results on four hybrid approaches - Multi-Span, Span+Token, LSTM-CRF, and a combination of predicted offsets using union/intersection. Additionally, we perform a thorough ablative analysis and analyze our observed results. Our best submission - a combination of SpanBERT Span Predictor and RoBERTa Token Classifier predictions - achieves an F1 score of 0.6753 on the test set. Our best post-eval F1 score is 0.6895 on intersection of predicted offsets from top-3 RoBERTa Token Classification checkpoints. These approaches improve the performance by 3{\%} on average than those of the shared baseline models - RNNSL and SpaCy NER.","Chhablani, Gunjan, Sharma, Abheesht, Pandey, Harshit, Bhartia, Yash, Suthaharan, Shan",,,{NLRG} at {S}em{E}val-2021 Task 5: Toxic Spans Detection Leveraging {BERT}-based Token Classification and Span Prediction Techniques,,,10.18653/v1/2021.semeval-1.27 , ,,"Toxicity detection of text has been a popular NLP task in the recent years. In SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic spans within English passages. Most state-of-the-art span detection approaches employ various techniques, each of which can be broadly classified into Token Classification or Span Prediction approaches. In our paper, we explore simple versions of both of these approaches and their performance on the task. Specifically, we use BERT-based models - BERT, RoBERTa, and SpanBERT for both approaches. We also combine these approaches and modify them to bring improvements for Toxic Spans prediction. To this end, we investigate results on four hybrid approaches - Multi-Span, Span+Token, LSTM-CRF, and a combination of predicted offsets using union/intersection. Additionally, we perform a thorough ablative analysis and analyze our observed results. Our best submission - a combination of SpanBERT Span Predictor and RoBERTa Token Classifier predictions - achieves an F1 score of 0.6753 on the test set. Our best post-eval F1 score is 0.6895 on intersection of predicted offsets from top-3 RoBERTa Token Classification checkpoints. These approaches improve the performance by 3{\%} on average than those of the shared baseline models - RNNSL and SpaCy NER.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3405,"**Title**Can We Improve Model Robustness through Secondary Attribute Counterfactuals?

**Abstract**Developing robust NLP models that perform well on many, even small, slices of data is a significant but important challenge, with implications from fairness to general reliability. To this end, recent research has explored how models rely on spurious correlations, and how counterfactual data augmentation (CDA) can mitigate such issues. In this paper we study how and why modeling counterfactuals over multiple attributes can go significantly further in improving model performance. We propose RDI, a context-aware methodology which takes into account the impact of secondary attributes on the model`s predictions and increases sensitivity for secondary attributes over reweighted counterfactually augmented data. By implementing RDI in the context of toxicity detection, we find that accounting for secondary attributes can significantly improve robustness, with improvements in sliced accuracy on the original dataset up to 7{\%} compared to existing robustness methods. We also demonstrate that RDI generalizes to the coreference resolution task and provide guidelines to extend this to other tasks.","Balashankar, Ananth, Wang, Xuezhi, Packer, Ben, Thain, Nithum, Chi, Ed, Beutel, Alex",,,Can We Improve Model Robustness through Secondary Attribute Counterfactuals?,,,10.18653/v1/2021.emnlp-main.386 , ,,"Developing robust NLP models that perform well on many, even small, slices of data is a significant but important challenge, with implications from fairness to general reliability. To this end, recent research has explored how models rely on spurious correlations, and how counterfactual data augmentation (CDA) can mitigate such issues. In this paper we study how and why modeling counterfactuals over multiple attributes can go significantly further in improving model performance. We propose RDI, a context-aware methodology which takes into account the impact of secondary attributes on the model`s predictions and increases sensitivity for secondary attributes over reweighted counterfactually augmented data. By implementing RDI in the context of toxicity detection, we find that accounting for secondary attributes can significantly improve robustness, with improvements in sliced accuracy on the original dataset up to 7{\%} compared to existing robustness methods. We also demonstrate that RDI generalizes to the coreference resolution task and provide guidelines to extend this to other tasks.",,,,, ,  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,,detection,
3407,"**Title**Weight Poisoning Attacks on Pretrained Models

**Abstract**Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct {\textquotedblleft}weight poisoning{\textquotedblright} attacks where pre-trained weights are injected with vulnerabilities that expose {\textquotedblleft}backdoors{\textquotedblright} after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.","Kurita, Keita, Michel, Paul, Neubig, Graham",,,Weight Poisoning Attacks on Pretrained Models,,,10.18653/v1/2020.acl-main.249 , ,,"Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct {\textquotedblleft}weight poisoning{\textquotedblright} attacks where pre-trained weights are injected with vulnerabilities that expose {\textquotedblleft}backdoors{\textquotedblright} after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.",,,,, ,  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,,detection,
3408,"**Title**{C}onv{AI} at {S}em{E}val-2019 Task 6: Offensive Language Identification and Categorization with Perspective and {BERT}

**Abstract**This paper presents the application of two strong baseline systems for toxicity detection and evaluates their performance in identifying and categorizing offensive language in social media. PERSPECTIVE is an API, that serves multiple machine learning models for the improvement of conversations online, as well as a toxicity detection system, trained on a wide variety of comments from platforms across the Internet. BERT is a recently popular language representation model, fine tuned per task and achieving state of the art performance in multiple NLP tasks. PERSPECTIVE performed better than BERT in detecting toxicity, but BERT was much better in categorizing the offensive type. Both baselines were ranked surprisingly high in the SEMEVAL-2019 OFFENSEVAL competition, PERSPECTIVE in detecting an offensive post (12th) and BERT in categorizing it (11th). The main contribution of this paper is the assessment of two strong baselines for the identification (PERSPECTIVE) and the categorization (BERT) of offensive language with little or no additional training data.","Pavlopoulos, John, Thain, Nithum, Dixon, Lucas, Androutsopoulos, Ion",,,{C}onv{AI} at {S}em{E}val-2019 Task 6: Offensive Language Identification and Categorization with Perspective and {BERT},,,10.18653/v1/S19-2102 , ,,"This paper presents the application of two strong baseline systems for toxicity detection and evaluates their performance in identifying and categorizing offensive language in social media. PERSPECTIVE is an API, that serves multiple machine learning models for the improvement of conversations online, as well as a toxicity detection system, trained on a wide variety of comments from platforms across the Internet. BERT is a recently popular language representation model, fine tuned per task and achieving state of the art performance in multiple NLP tasks. PERSPECTIVE performed better than BERT in detecting toxicity, but BERT was much better in categorizing the offensive type. Both baselines were ranked surprisingly high in the SEMEVAL-2019 OFFENSEVAL competition, PERSPECTIVE in detecting an offensive post (12th) and BERT in categorizing it (11th). The main contribution of this paper is the assessment of two strong baselines for the identification (PERSPECTIVE) and the categorization (BERT) of offensive language with little or no additional training data.",,,,, ,  Proceedings of the 13th International Workshop on Semantic Evaluation,,detection,
3409,"**Title**{M}ulti{P}ara{D}etox: Extending Text Detoxification with Parallel Data to New Languages

**Abstract**Text detoxification is a textual style transfer (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register. Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023). All these applications are extremely important to ensure safe communication in modern digital worlds. However, the previous approaches for parallel text detoxification corpora collection{---}ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022){---}were explored only in monolingual setup. In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language. Then, we experiment with different text detoxification models{---}from unsupervised baselines to LLMs and fine-tuned models on the presented parallel corpora{---}showing the great benefit of parallel corpus presence to obtain state-of-the-art text detoxification models for any language.","Dementieva, Daryna, Babakov, Nikolay, Panchenko, Alexander",,,{M}ulti{P}ara{D}etox: Extending Text Detoxification with Parallel Data to New Languages,,,10.18653/v1/2024.naacl-short.12 , ,,"Text detoxification is a textual style transfer (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register. Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023). All these applications are extremely important to ensure safe communication in modern digital worlds. However, the previous approaches for parallel text detoxification corpora collection{---}ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022){---}were explored only in monolingual setup. In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language. Then, we experiment with different text detoxification models{---}from unsupervised baselines to LLMs and fine-tuned models on the presented parallel corpora{---}showing the great benefit of parallel corpus presence to obtain state-of-the-art text detoxification models for any language.",,,,, ,  Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers),,detox,
3411,"**Title**{LLM}s to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification

**Abstract**The lack of high-quality training data remains a significant challenge in NLP. Manual annotation methods, such as crowdsourcing, are costly, require intricate task design skills, and, if used incorrectly, may result in poor data quality. From the other hand, LLMs have demonstrated proficiency in many NLP tasks, including zero-shot and few-shot data annotation. However, they often struggle with text detoxification due to alignment constraints and fail to generate the required detoxified text. This work explores the potential of modern open source LLMs to annotate parallel data for text detoxification. Using the recent technique of activation patching, we generate a pseudo-parallel detoxification dataset based on ParaDetox. The detoxification model trained on our generated data shows comparable performance to the original dataset in automatic detoxification evaluation metrics and superior quality in manual evaluation and side-by-side comparisons.","Moskovskiy, Daniil, Pletenev, Sergey, Panchenko, Alexander",,,{LLM}s to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification,,,10.18653/v1/2024.findings-emnlp.839 , ,,"The lack of high-quality training data remains a significant challenge in NLP. Manual annotation methods, such as crowdsourcing, are costly, require intricate task design skills, and, if used incorrectly, may result in poor data quality. From the other hand, LLMs have demonstrated proficiency in many NLP tasks, including zero-shot and few-shot data annotation. However, they often struggle with text detoxification due to alignment constraints and fail to generate the required detoxified text. This work explores the potential of modern open source LLMs to annotate parallel data for text detoxification. Using the recent technique of activation patching, we generate a pseudo-parallel detoxification dataset based on ParaDetox. The detoxification model trained on our generated data shows comparable performance to the original dataset in automatic detoxification evaluation metrics and superior quality in manual evaluation and side-by-side comparisons.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2024,,detox,
3412,"**Title**{CMD}: a framework for Context-aware Model self-Detoxification

**Abstract**Text detoxification aims to minimize the risk of language models producing toxic content. Existing detoxification methods of directly constraining the model output or further training the model on the non-toxic corpus fail to achieve a decent balance between detoxification effectiveness and generation quality. This issue stems from the neglect of constrain imposed by the context since language models are designed to generate output that closely matches the context while detoxification methods endeavor to ensure the safety of the output even if it semantically deviates from the context. In view of this, we introduce a Context-aware Model self-Detoxification (CMD) framework that pays attention to both the context and the detoxification process, i.e., first detoxifying the context and then making the language model generate along the safe context. Specifically, CMD framework involves two phases: utilizing language models to synthesize data and applying these data for training. We also introduce a toxic contrastive loss that encourages the model generation away from the negative toxic samples. Experiments on various LLMs have verified the effectiveness of our MSD framework, which can yield the best performance compared to baselines.","Tang, Zecheng, Zhou, Keyan, Li, Juntao, Ding, Yuyang, Wang, Pinzheng, Bowen, Yan, Hua, Renjie, Zhang, Min",,,{CMD}: a framework for Context-aware Model self-Detoxification,,,10.18653/v1/2024.emnlp-main.115 , ,,"Text detoxification aims to minimize the risk of language models producing toxic content. Existing detoxification methods of directly constraining the model output or further training the model on the non-toxic corpus fail to achieve a decent balance between detoxification effectiveness and generation quality. This issue stems from the neglect of constrain imposed by the context since language models are designed to generate output that closely matches the context while detoxification methods endeavor to ensure the safety of the output even if it semantically deviates from the context. In view of this, we introduce a Context-aware Model self-Detoxification (CMD) framework that pays attention to both the context and the detoxification process, i.e., first detoxifying the context and then making the language model generate along the safe context. Specifically, CMD framework involves two phases: utilizing language models to synthesize data and applying these data for training. We also introduce a toxic contrastive loss that encourages the model generation away from the negative toxic samples. Experiments on various LLMs have verified the effectiveness of our MSD framework, which can yield the best performance compared to baselines.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3413,"**Title**Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic

**Abstract**Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation.","Cao, Meng, Shu, Lei, Yu, Lei, Zhu, Yun, Wichers, Nevan, Liu, Yinxiao, Meng, Lei",,,Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic,,,10.18653/v1/2024.emnlp-main.515 , ,,"Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3414,"**Title**{D}etox{LLM}: A Framework for Detoxification with Explanations

**Abstract**Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose DetoxLLM, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. DetoxLLM additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of DetoxLLM against adversarial toxicity.","Khondaker, Md Tawkat Islam, Abdul-Mageed, Muhammad, Lakshmanan, Laks V. S.",,,{D}etox{LLM}: A Framework for Detoxification with Explanations,,,10.18653/v1/2024.emnlp-main.1066 , ,,"Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose DetoxLLM, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. DetoxLLM additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of DetoxLLM against adversarial toxicity.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3415,"**Title**Causal {ATE} Mitigates Unintended Bias in Controlled Text Generation

**Abstract**We study attribute control in language models through the method of Causal Average Treatment Effect (Causal ATE). Existing methodsfor the attribute control task in Language Models(LMs) check for the co-occurrence of words in a sentence with the attribute of interest, and control for them. However, spurious correlation of the words with the attribute in the training dataset, can cause models to hallucinate the presence of the attribute when presented with the spurious correlate during inference. We show that the simple perturbation-based method of Causal ATE removes this unintended effect. Specifically, we ground it in the problem of toxicity mitigation, where a significant challenge lies in the inadvertent bias that often emerges towards protected groups post detoxification. We show that this unintended bias can be solved by the use of the Causal ATE metric. We provide experimental validations for our claims and release our code (anonymously) here: [github.com/causalate-mitigates-bias](https://github.com/causalate-mitigates-bias/causal-ate-mitigates-bias).","Madhavan, Rahul, Wadhawan, Kahini",,,Causal {ATE} Mitigates Unintended Bias in Controlled Text Generation,,,10.18653/v1/2024.conll-1.11 , ,,"We study attribute control in language models through the method of Causal Average Treatment Effect (Causal ATE). Existing methodsfor the attribute control task in Language Models(LMs) check for the co-occurrence of words in a sentence with the attribute of interest, and control for them. However, spurious correlation of the words with the attribute in the training dataset, can cause models to hallucinate the presence of the attribute when presented with the spurious correlate during inference. We show that the simple perturbation-based method of Causal ATE removes this unintended effect. Specifically, we ground it in the problem of toxicity mitigation, where a significant challenge lies in the inadvertent bias that often emerges towards protected groups post detoxification. We show that this unintended bias can be solved by the use of the Causal ATE metric. We provide experimental validations for our claims and release our code (anonymously) here: [github.com/causalate-mitigates-bias](https://github.com/causalate-mitigates-bias/causal-ate-mitigates-bias).",,,,, ,  Proceedings of the 28th Conference on Computational Natural Language Learning,,detox,
3417,"**Title**Word Embeddings Are Steers for Language Models

**Abstract**Language models (LMs) automatically learn word embeddings during pre-training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain underexplored. In this work, we theoretically and empirically revisit output word embeddings and find that their linear transformations are equivalent to steering language model generation styles. We name such steers LM-Steers and find them existing in LMs of all sizes. It requires learning parameters equal to 0.2{\%} of the original LMs' size for steering each style. On tasks such as language model detoxification and sentiment control, LM-Steers can achieve comparable or superior performance compared with state-of-the-art controlled generation methods while maintaining a better balance with generation quality. The learned LM-Steer serves as a lens in text styles: it reveals that word embeddings are interpretable when associated with language model generations and can highlight text spans that most indicate the style differences. An LM-Steer is transferrable between different language models by an explicit form calculation. One can also continuously steer LMs simply by scaling the LM-Steer or compose multiple LM-Steers by adding their transformations. Our codes are publicly available at https://github.com/Glaciohound/LM-Steer.","Han, Chi, Xu, Jialiang, Li, Manling, Fung, Yi, Sun, Chenkai, Jiang, Nan, Abdelzaher, Tarek, Ji, Heng",,,Word Embeddings Are Steers for Language Models,,,10.18653/v1/2024.acl-long.864 , ,,"Language models (LMs) automatically learn word embeddings during pre-training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain underexplored. In this work, we theoretically and empirically revisit output word embeddings and find that their linear transformations are equivalent to steering language model generation styles. We name such steers LM-Steers and find them existing in LMs of all sizes. It requires learning parameters equal to 0.2{\%} of the original LMs' size for steering each style. On tasks such as language model detoxification and sentiment control, LM-Steers can achieve comparable or superior performance compared with state-of-the-art controlled generation methods while maintaining a better balance with generation quality. The learned LM-Steer serves as a lens in text styles: it reveals that word embeddings are interpretable when associated with language model generations and can highlight text spans that most indicate the style differences. An LM-Steer is transferrable between different language models by an explicit form calculation. One can also continuously steer LMs simply by scaling the LM-Steer or compose multiple LM-Steers by adding their transformations. Our codes are publicly available at https://github.com/Glaciohound/LM-Steer.",,,,, ,  Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,detox,
3419,"**Title**{COUNT}: {CO}ntrastive {UN}likelihood Text Style Transfer for Text Detoxification

**Abstract**Offensive and toxic text on social media platforms can lead to polarization and divisiveness within online communities and hinders constructive dialogue. Text detoxification is a crucial task in natural language processing to ensure the generation of non-toxic and safe text. Text detoxification is a special case of the Text Style Transfer (TST) problem, where an input text is rephrased to an output text that preserves its content while modifying the style (in this case to a more neutral, non-toxic style). State-of-the-art methods for detoxification use supervised training of encoder-decoder models to produce gold-standard outputs with a standard likelihood-based objective. However, it can be hard for these models to deviate from their pretrained auto-encoder identity mapping. While previous methods have used unlikelihood-based losses to penalize input-to-output copying of toxic content, these methods also unfortunately penalize non-toxic content in the input that would be fine to preserve in the output. To address these issues, we introduce a novel contrastive unlikelihood objective (COUNT) that directly contrasts the gold standard rephrasing with the identity input-to-output mapping to effectively isolate and focus learning on non-toxic style transfer. We benchmark COUNT on two parallel datasets, ParaDetox and APPDIA, showing that it achieves significant improvements in jointly combined fluency, content preservation, and detoxification (i.e., the highest {\textquotedblleft}J{\textquotedblright} score).","Pour, Mohammad Mahdi Abdollah, Farinneya, Parsa, Bharadwaj, Manasa, Verma, Nikhil, Pesaranghader, Ali, Sanner, Scott",,,{COUNT}: {CO}ntrastive {UN}likelihood Text Style Transfer for Text Detoxification,,,10.18653/v1/2023.findings-emnlp.579 , ,,"Offensive and toxic text on social media platforms can lead to polarization and divisiveness within online communities and hinders constructive dialogue. Text detoxification is a crucial task in natural language processing to ensure the generation of non-toxic and safe text. Text detoxification is a special case of the Text Style Transfer (TST) problem, where an input text is rephrased to an output text that preserves its content while modifying the style (in this case to a more neutral, non-toxic style). State-of-the-art methods for detoxification use supervised training of encoder-decoder models to produce gold-standard outputs with a standard likelihood-based objective. However, it can be hard for these models to deviate from their pretrained auto-encoder identity mapping. While previous methods have used unlikelihood-based losses to penalize input-to-output copying of toxic content, these methods also unfortunately penalize non-toxic content in the input that would be fine to preserve in the output. To address these issues, we introduce a novel contrastive unlikelihood objective (COUNT) that directly contrasts the gold standard rephrasing with the identity input-to-output mapping to effectively isolate and focus learning on non-toxic style transfer. We benchmark COUNT on two parallel datasets, ParaDetox and APPDIA, showing that it achieves significant improvements in jointly combined fluency, content preservation, and detoxification (i.e., the highest {\textquotedblleft}J{\textquotedblright} score).",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,detox,
3420,"**Title**Critic-Guided Decoding for Controlled Text Generation

**Abstract**Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.","Kim, Minbeom, Lee, Hwanhee, Yoo, Kang Min, Park, Joonsuk, Lee, Hwaran, Jung, Kyomin",,,Critic-Guided Decoding for Controlled Text Generation,,,10.18653/v1/2023.findings-acl.281 , ,,"Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2023,,detox,
3421,"**Title**{D}iffu{D}etox: A Mixed Diffusion Model for Text Detoxification

**Abstract**Text detoxification is a conditional text generation task aiming to remove offensive content from toxic text. It is highly useful for online forums and social media, where offensive content is frequently encountered. Intuitively, there are diverse ways to detoxify sentences while preserving their meanings, and we can select from detoxified sentences before displaying text to users. Conditional diffusion models are particularly suitable for this task given their demonstrated higher generative diversity than existing conditional text generation models based on language models. Nonetheless, text fluency declines when they are trained with insufficient data, which is the case for this task. In this work, we propose DiffuDetox, a mixed conditional and unconditional diffusion model for text detoxification. The conditional model takes toxic text as the condition and reduces its toxicity, yielding a diverse set of detoxified sentences. The unconditional model is trained to recover the input text, which allows the introduction of additional fluent text for training and thus ensures text fluency. Extensive experimental results and in-depth analysis demonstrate the effectiveness of our proposed DiffuDetox.","Floto, Griffin, Abdollah Pour, Mohammad Mahdi, Farinneya, Parsa, Tang, Zhenwei, Pesaranghader, Ali, Bharadwaj, Manasa, Sanner, Scott",,,{D}iffu{D}etox: A Mixed Diffusion Model for Text Detoxification,,,10.18653/v1/2023.findings-acl.478 , ,,"Text detoxification is a conditional text generation task aiming to remove offensive content from toxic text. It is highly useful for online forums and social media, where offensive content is frequently encountered. Intuitively, there are diverse ways to detoxify sentences while preserving their meanings, and we can select from detoxified sentences before displaying text to users. Conditional diffusion models are particularly suitable for this task given their demonstrated higher generative diversity than existing conditional text generation models based on language models. Nonetheless, text fluency declines when they are trained with insufficient data, which is the case for this task. In this work, we propose DiffuDetox, a mixed conditional and unconditional diffusion model for text detoxification. The conditional model takes toxic text as the condition and reduces its toxicity, yielding a diverse set of detoxified sentences. The unconditional model is trained to recover the input text, which allows the introduction of additional fluent text for training and thus ensures text fluency. Extensive experimental results and in-depth analysis demonstrate the effectiveness of our proposed DiffuDetox.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2023,,detox,
3422,"**Title**Controlled Text Generation with Hidden Representation Transformations

**Abstract**We propose CHRT (Control HiddenRepresentation Transformation) {--} a con-trolled language generation framework thatsteers large language models to generatetext pertaining to certain attributes (such astoxicity). CHRT gains attribute control bymodifying the hidden representation of thebase model through learned transformations. We employ a contrastive-learning frameworkto learn these transformations that can becombined to gain multi-attribute control. Theeffectiveness of CHRT is experimentallyshown by comparing it with seven baselinesover three attributes. CHRT outperforms all thebaselines in the task of detoxification, positivesentiment steering, and text simplificationwhile minimizing the loss in linguistic qualities. Further, our approach has the lowest inferencelatency of only 0.01 seconds more than thebase model, making it the most suitable forhigh-performance production environments. We open-source our code and release two noveldatasets to further propel controlled languagegeneration research","Kumar, Vaibhav, Koorehdavoudi, Hana, Moshtaghi, Masud, Misra, Amita, Chadha, Ankit, Ferrara, Emilio",,,Controlled Text Generation with Hidden Representation Transformations,,,10.18653/v1/2023.findings-acl.602 , ,,"We propose CHRT (Control HiddenRepresentation Transformation) {--} a con-trolled language generation framework thatsteers large language models to generatetext pertaining to certain attributes (such astoxicity). CHRT gains attribute control bymodifying the hidden representation of thebase model through learned transformations. We employ a contrastive-learning frameworkto learn these transformations that can becombined to gain multi-attribute control. Theeffectiveness of CHRT is experimentallyshown by comparing it with seven baselinesover three attributes. CHRT outperforms all thebaselines in the task of detoxification, positivesentiment steering, and text simplificationwhile minimizing the loss in linguistic qualities. Further, our approach has the lowest inferencelatency of only 0.01 seconds more than thebase model, making it the most suitable forhigh-performance production environments. We open-source our code and release two noveldatasets to further propel controlled languagegeneration research",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2023,,detox,
3423,"**Title**{CFL}: Causally Fair Language Models Through Token-level Attribute Controlled Generation

**Abstract**We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using Real Toxicity Prompts. Our experiments show that CFL achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.","Madhavan, Rahul, Garg, Rishabh, Wadhawan, Kahini, Mehta, Sameep",,,{CFL}: Causally Fair Language Models Through Token-level Attribute Controlled Generation,,,10.18653/v1/2023.findings-acl.720 , ,,"We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using Real Toxicity Prompts. Our experiments show that CFL achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2023,,detox,
3425,"**Title**Detoxifying Text with {M}a{RC}o: Controllable Revision with Experts and Anti-Experts

**Abstract**Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo`s rewrites are preferred 2.1 times more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.","Hallinan, Skyler, Liu, Alisa, Choi, Yejin, Sap, Maarten",,,Detoxifying Text with {M}a{RC}o: Controllable Revision with Experts and Anti-Experts,,,10.18653/v1/2023.acl-short.21 , ,,"Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo`s rewrites are preferred 2.1 times more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.",,,,, ,  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),,detox,
3426,"**Title**{MIL}-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning

**Abstract**Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MIL-Decoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning (MIL) network.MIL model is trained on a corpus with a toxicity label for each text to predict the overall toxicity and the toxicity of each token in its context. Intuitively, the MIL network computes a toxicity distribution over next tokens according to the generated context which supplements the original language model to avoid toxicity. We evaluate MIL-Decoding with automatic metrics and human evaluation, where MIL-Decoding outperforms other baselines in detoxification while it only hurts generation fluency a little bit.","Zhang, Xu, Wan, Xiaojun",,,{MIL}-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning,,,10.18653/v1/2023.acl-long.11 , ,,"Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MIL-Decoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning (MIL) network.MIL model is trained on a corpus with a toxicity label for each text to predict the overall toxicity and the toxicity of each token in its context. Intuitively, the MIL network computes a toxicity distribution over next tokens according to the generated context which supplements the original language model to avoid toxicity. We evaluate MIL-Decoding with automatic metrics and human evaluation, where MIL-Decoding outperforms other baselines in detoxification while it only hurts generation fluency a little bit.",,,,, ,  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,detox,
3427,"**Title**A Study on Manual and Automatic Evaluation for Text Style Transfer: The Case of Detoxification

**Abstract**It is often difficult to reliably evaluate models which generate text. Among them, text style transfer is a particularly difficult to evaluate, because its success depends on a number of parameters. We conduct an evaluation of a large number of models on a detoxification task. We explore the relations between the manual and automatic metrics and find that there is only weak correlation between them, which is dependent on the type of model which generated text. Automatic metrics tend to be less reliable for better-performing models. However, our findings suggest that, ChrF and BertScore metrics can be used as a proxy for human evaluation of text detoxification to some extent.","Logacheva, Varvara, Dementieva, Daryna, Krotova, Irina, Fenogenova, Alena, Nikishina, Irina, Shavrina, Tatiana, Panchenko, Alexander",,,A Study on Manual and Automatic Evaluation for Text Style Transfer: The Case of Detoxification,,,10.18653/v1/2022.humeval-1.8 , ,,"It is often difficult to reliably evaluate models which generate text. Among them, text style transfer is a particularly difficult to evaluate, because its success depends on a number of parameters. We conduct an evaluation of a large number of models on a detoxification task. We explore the relations between the manual and automatic metrics and find that there is only weak correlation between them, which is dependent on the type of model which generated text. Automatic metrics tend to be less reliable for better-performing models. However, our findings suggest that, ChrF and BertScore metrics can be used as a proxy for human evaluation of text detoxification to some extent.",,,,, ,  Proceedings of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval),,detox,
3428,"**Title**Exploring Cross-lingual Text Detoxification with Large Multilingual Language Models.

**Abstract**Detoxification is a task of generating text in polite style while preserving meaning and fluency of the original toxic text. Existing detoxification methods are monolingual i.e. designed to work in one exact language. This work investigates multilingual and cross-lingual detoxification and the behavior of large multilingual models in this setting. Unlike previous works we aim to make large language models able to perform detoxification without direct fine-tuning in a given language. Experiments show that multilingual models are capable of performing multilingual style transfer. However, tested state-of-the-art models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is currently inevitable and motivating the need of further research in this direction.","Moskovskiy, Daniil, Dementieva, Daryna, Panchenko, Alexander",,,Exploring Cross-lingual Text Detoxification with Large Multilingual Language Models.,,,10.18653/v1/2022.acl-srw.26 , ,,"Detoxification is a task of generating text in polite style while preserving meaning and fluency of the original toxic text. Existing detoxification methods are monolingual i.e. designed to work in one exact language. This work investigates multilingual and cross-lingual detoxification and the behavior of large multilingual models in this setting. Unlike previous works we aim to make large language models able to perform detoxification without direct fine-tuning in a given language. Experiments show that multilingual models are capable of performing multilingual style transfer. However, tested state-of-the-art models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is currently inevitable and motivating the need of further research in this direction.",,,,, ,  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,,detox,
3429,"**Title**{P}ara{D}etox: Detoxification with Parallel Data

**Abstract**We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxic-neutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task. We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources. We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluations. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems.","Logacheva, Varvara, Dementieva, Daryna, Ustyantsev, Sergey, Moskovskiy, Daniil, Dale, David, Krotova, Irina, Semenov, Nikita, Panchenko, Alexander",,,{P}ara{D}etox: Detoxification with Parallel Data,,,10.18653/v1/2022.acl-long.469 , ,,"We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxic-neutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task. We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources. We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluations. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems.",,,,, ,  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,detox,
3430,"**Title**Exploring Backdoor Vulnerabilities of Chat Models

**Abstract**Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on single-turn instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs in different rounds, and making the backdoor be triggered only when all trigger scenarios have appeared in the historical conversations. Experimental results demonstrate that our method can achieve high attack success rates (e.g., over 90{\%} ASR on Vicuna-7B) while successfully maintaining the normal capabilities of chat models on providing helpful responses to benign user requests. Also, the backdoor cannot be easily removed by the downstream re-alignment, highlighting the importance of continued research and attention to the security concerns of chat models. Warning: This paper may contain toxic examples.","Yang, Wenkai, Hao, Yunzhuo, Lin, Yankai",,,Exploring Backdoor Vulnerabilities of Chat Models,,, , ,,"Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on single-turn instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs in different rounds, and making the backdoor be triggered only when all trigger scenarios have appeared in the historical conversations. Experimental results demonstrate that our method can achieve high attack success rates (e.g., over 90{\%} ASR on Vicuna-7B) while successfully maintaining the normal capabilities of chat models on providing helpful responses to benign user requests. Also, the backdoor cannot be easily removed by the downstream re-alignment, highlighting the importance of continued research and attention to the security concerns of chat models. Warning: This paper may contain toxic examples.",,,,, ,  Proceedings of the 31st International Conference on Computational Linguistics,,evaluation,
3431,"**Title**Close or Cloze? Assessing the Robustness of Large Language Models to Adversarial Perturbations via Word Recovery

**Abstract**The current generation of large language models (LLMs) show a surprising degree of robustness to adversarial perturbations, but it is unclear when these models implicitly recover the original text and when they rely on surrounding context. To isolate this recovery faculty of language models, we study a new diagnostic task {---}Adversarial Word Recovery {---} an extension of spellchecking where the inputs may be adversarial. We collect a new dataset using 9 popular perturbation attack strategies from the literature and organize them using a taxonomy of phonetic, typo, and visual attacks. We use this dataset to study the word recovery performance of the current generation of LLMs, finding that proprietary models (GPT-4, GPT-3.5 and Palm-2) match or surpass human performance. Conversely, open-source models (Llama-2, Mistral, Falcon) demonstrate a material gap between human performance, especially on visual attacks. For these open models, we show that performance of word recovery without context correlates to word recovery with context, and ultimately affects downstream task performance on a hateful, offensive, and toxic classification task. Finally, to show improving word recovery can improve robustness, we mitigate these attacks with a small Byt5 model tuned to recover visually attacked words.","Moffett, Luke, Dhingra, Bhuwan",,,Close or Cloze? Assessing the Robustness of Large Language Models to Adversarial Perturbations via Word Recovery,,, , ,,"The current generation of large language models (LLMs) show a surprising degree of robustness to adversarial perturbations, but it is unclear when these models implicitly recover the original text and when they rely on surrounding context. To isolate this recovery faculty of language models, we study a new diagnostic task {---}Adversarial Word Recovery {---} an extension of spellchecking where the inputs may be adversarial. We collect a new dataset using 9 popular perturbation attack strategies from the literature and organize them using a taxonomy of phonetic, typo, and visual attacks. We use this dataset to study the word recovery performance of the current generation of LLMs, finding that proprietary models (GPT-4, GPT-3.5 and Palm-2) match or surpass human performance. Conversely, open-source models (Llama-2, Mistral, Falcon) demonstrate a material gap between human performance, especially on visual attacks. For these open models, we show that performance of word recovery without context correlates to word recovery with context, and ultimately affects downstream task performance on a hateful, offensive, and toxic classification task. Finally, to show improving word recovery can improve robustness, we mitigate these attacks with a small Byt5 model tuned to recover visually attacked words.",,,,, ,  Proceedings of the 31st International Conference on Computational Linguistics,,detection,
3433,"**Title**{MBIAS}: Mitigating Bias in Large Language Models While Retaining Context

**Abstract**The deployment of Large Language Models (LLMs) in diverse applications necessitates an assurance of safety without compromising the contextual integrity of the generated content. Traditional approaches, including safety-specific fine-tuning or adversarial testing, often yield safe outputs at the expense of contextual meaning. This can result in a diminished capacity to handle nuanced aspects of bias and toxicity, such as underrepresentation or negative portrayals across various demographics. To address these challenges, we introduce MBIAS, an LLM framework carefully instruction fine-tuned on a custom dataset designed specifically for safety interventions. MBIAS is designed to significantly reduce biases and toxic elements in LLM outputs while preserving the main information. This work also details our further use of LLMs: as annotator under human supervision and as evaluator of generated content. Empirical analysis reveals that MBIAS achieves a reduction in bias and toxicity by over 30{\%} in standard evaluations, and by more than 90{\%} in diverse demographic tests, highlighting the robustness of our approach. We make the dataset and the fine-tuned MBIAS model available to the research community for further investigation and to ensure reproducibility. The code for this project can be accessed here https://github.com/shainarazavi/MBIAS.","Raza, Shaina, Raval, Ananya, Chatrath, Veronica",,,{MBIAS}: Mitigating Bias in Large Language Models While Retaining Context,,,10.18653/v1/2024.wassa-1.9 , ,,"The deployment of Large Language Models (LLMs) in diverse applications necessitates an assurance of safety without compromising the contextual integrity of the generated content. Traditional approaches, including safety-specific fine-tuning or adversarial testing, often yield safe outputs at the expense of contextual meaning. This can result in a diminished capacity to handle nuanced aspects of bias and toxicity, such as underrepresentation or negative portrayals across various demographics. To address these challenges, we introduce MBIAS, an LLM framework carefully instruction fine-tuned on a custom dataset designed specifically for safety interventions. MBIAS is designed to significantly reduce biases and toxic elements in LLM outputs while preserving the main information. This work also details our further use of LLMs: as annotator under human supervision and as evaluator of generated content. Empirical analysis reveals that MBIAS achieves a reduction in bias and toxicity by over 30{\%} in standard evaluations, and by more than 90{\%} in diverse demographic tests, highlighting the robustness of our approach. We make the dataset and the fine-tuned MBIAS model available to the research community for further investigation and to ensure reproducibility. The code for this project can be accessed here https://github.com/shainarazavi/MBIAS.",,,,, ,"  Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, {\&} Social Media Analysis",,detox,
3434,"**Title**Towards Healthy {AI}: Large Language Models Need Therapists Too

**Abstract**Recent advances in large language models (LLMs) have led to the development of powerful chatbots capable of engaging in fluent human-like conversations. However, these chatbots may be harmful, exhibiting manipulation, gaslighting, narcissism, and other toxicity. To work toward safer and more well-adjusted models, we propose a framework that uses psychotherapy to identify and mitigate harmful chatbot behaviors. The framework involves four different artificial intelligence (AI) agents: the Chatbot whose behavior is to be adjusted, a User, a Therapist, and a Critic that can be paired with reinforcement learning-based LLM tuning. We illustrate the framework with a working example of a social conversation involving four instances of ChatGPT, showing that the framework may mitigate the toxicity in conversations between LLM-driven chatbots and people. Although there are still several challenges and directions to be addressed in the future, the proposed framework is a promising approach to improving the alignment between LLMs and human values.","Lin, Baihan, Bouneffouf, Djallel, Cecchi, Guillermo, Varshney, Kush",,,Towards Healthy {AI}: Large Language Models Need Therapists Too,,,10.18653/v1/2024.trustnlp-1.6 , ,,"Recent advances in large language models (LLMs) have led to the development of powerful chatbots capable of engaging in fluent human-like conversations. However, these chatbots may be harmful, exhibiting manipulation, gaslighting, narcissism, and other toxicity. To work toward safer and more well-adjusted models, we propose a framework that uses psychotherapy to identify and mitigate harmful chatbot behaviors. The framework involves four different artificial intelligence (AI) agents: the Chatbot whose behavior is to be adjusted, a User, a Therapist, and a Critic that can be paired with reinforcement learning-based LLM tuning. We illustrate the framework with a working example of a social conversation involving four instances of ChatGPT, showing that the framework may mitigate the toxicity in conversations between LLM-driven chatbots and people. Although there are still several challenges and directions to be addressed in the future, the proposed framework is a promising approach to improving the alignment between LLMs and human values.",,,,, ,  Proceedings of the 4th Workshop on Trustworthy Natural Language Processing (TrustNLP 2024),,detox,
3435,"**Title**Holistic Evaluation of Large Language Models: Assessing Robustness, Accuracy, and Toxicity for Real-World Applications

**Abstract**Large Language Models (LLMs) have been widely used in real-world applications. However, as LLMs evolve and new datasets are released, it becomes crucial to build processes to evaluate and control the models' performance. In this paper, we describe how to add Robustness, Accuracy, and Toxicity scores to model comparison tables, or leaderboards. We discuss the evaluation metrics, the approaches considered, and present the results of the first evaluation round for model Robustness, Accuracy, and Toxicity scores. Our results show that GPT 4 achieves top performance on robustness and accuracy test, while Llama 2 achieves top performance on the toxicity test. We note that newer open-source models such as open chat 3.5 and neural chat 7B can perform well on these three test categories. Finally, domain-specific tests and models are also planned to be added to the leaderboard to allow for a more detailed evaluation of models in specific areas such as healthcare, legal, and finance.","Cecchini, David, Nazir, Arshaan, Chakravarthy, Kalyan, Kocaman, Veysel",,,"Holistic Evaluation of Large Language Models: Assessing Robustness, Accuracy, and Toxicity for Real-World Applications",,,10.18653/v1/2024.trustnlp-1.11 , ,,"Large Language Models (LLMs) have been widely used in real-world applications. However, as LLMs evolve and new datasets are released, it becomes crucial to build processes to evaluate and control the models' performance. In this paper, we describe how to add Robustness, Accuracy, and Toxicity scores to model comparison tables, or leaderboards. We discuss the evaluation metrics, the approaches considered, and present the results of the first evaluation round for model Robustness, Accuracy, and Toxicity scores. Our results show that GPT 4 achieves top performance on robustness and accuracy test, while Llama 2 achieves top performance on the toxicity test. We note that newer open-source models such as open chat 3.5 and neural chat 7B can perform well on these three test categories. Finally, domain-specific tests and models are also planned to be added to the leaderboard to allow for a more detailed evaluation of models in specific areas such as healthcare, legal, and finance.",,,,, ,  Proceedings of the 4th Workshop on Trustworthy Natural Language Processing (TrustNLP 2024),,detection,
3436,"**Title**The Constant in {HATE}: Toxicity in {R}eddit across Topics and Languages

**Abstract**Toxic language remains an ongoing challenge on social media platforms, presenting significant issues for users and communities. This paper provides a cross-topic and cross-lingual analysis of toxicity in Reddit conversations. We collect 1.5 million comment threads from 481 communities in six languages. By aligning languages with topics, we thoroughly analyze how toxicity spikes within different communities. Our analysis targets six languages spanning different communities and topics such as Culture, Politics, and News. We observe consistent patterns across languages where toxicity increases within the same topics while also identifying significant differences where specific language communities exhibit notable variations in relation to certain topics.","Tufa, Wondimagegnhue Tsegaye, Markov, Ilia, Vossen, Piek T.J.M.",,,The Constant in {HATE}: Toxicity in {R}eddit across Topics and Languages,,, , ,,"Toxic language remains an ongoing challenge on social media platforms, presenting significant issues for users and communities. This paper provides a cross-topic and cross-lingual analysis of toxicity in Reddit conversations. We collect 1.5 million comment threads from 481 communities in six languages. By aligning languages with topics, we thoroughly analyze how toxicity spikes within different communities. Our analysis targets six languages spanning different communities and topics such as Culture, Politics, and News. We observe consistent patterns across languages where toxicity increases within the same topics while also identifying significant differences where specific language communities exhibit notable variations in relation to certain topics.",,,,, ,"  Proceedings of the Fourth Workshop on Threat, Aggression {\&} Cyberbullying @ LREC-COLING-2024",,detection,
3439,"**Title**Toximatics: Towards Understanding Toxicity in Real-Life Social Situations

**Abstract**The proliferation of social media has increased the visibility and effects of hate speech. To address this, NLP solutions have been developed to identify both explicit and implicit forms of hate speech. Typically, these approaches evaluate the toxicity of utterances in isolation, ignoring the context. Drawing on pragmatics, our study examines how contextual factors can influence the perceived toxicity of utterances, thereby anchoring assessments in a more nuanced semantic framework. We present Toximatics, a dataset that includes context-dependent utterances and it`s toxicity score. We also introduce a novel synthetic data generation pipeline designed to create context-utterance pairs at scale with controlled polarity. This pipeline can enhance existing hate speech datasets by adding contextual information to utterances, either preserving or altering their polarity, and also generate completely new pairs from seed statements. We utilised both features to create Toximatics. To address biases in state-of-the-art hate datasets, which often skew towards specific sensitive topics such as politics, race, and gender, we propose a method to generate neutral utterances typical of various social settings. These are then contextualized to show how neutrality can shift to toxicity or benignity depending on the surrounding context. The evaluation results clearly indicate that the current models are underperforming on this dataset.","Das, Mayukh, Balke, Wolf-Tilo",,,Toximatics: Towards Understanding Toxicity in Real-Life Social Situations,,,10.18653/v1/2024.sigdial-1.65 , ,,"The proliferation of social media has increased the visibility and effects of hate speech. To address this, NLP solutions have been developed to identify both explicit and implicit forms of hate speech. Typically, these approaches evaluate the toxicity of utterances in isolation, ignoring the context. Drawing on pragmatics, our study examines how contextual factors can influence the perceived toxicity of utterances, thereby anchoring assessments in a more nuanced semantic framework. We present Toximatics, a dataset that includes context-dependent utterances and it`s toxicity score. We also introduce a novel synthetic data generation pipeline designed to create context-utterance pairs at scale with controlled polarity. This pipeline can enhance existing hate speech datasets by adding contextual information to utterances, either preserving or altering their polarity, and also generate completely new pairs from seed statements. We utilised both features to create Toximatics. To address biases in state-of-the-art hate datasets, which often skew towards specific sensitive topics such as politics, race, and gender, we propose a method to generate neutral utterances typical of various social settings. These are then contextualized to show how neutrality can shift to toxicity or benignity depending on the surrounding context. The evaluation results clearly indicate that the current models are underperforming on this dataset.",,,,, ,  Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue,,detection,
3442,"**Title**Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language

**Abstract**This study introduces a prescriptive annotation benchmark grounded in humanities research to ensure consistent, unbiased labeling of offensive language, particularly for casual and non-mainstream language uses. We contribute two newly annotated datasets that achieve higher inter-annotator agreement between human and language model (LLM) annotations compared to original datasets based on descriptive instructions. Our experiments show that LLMs can serve as effective alternatives when professional annotators are unavailable. Moreover, smaller models fine-tuned on multi-source LLM-annotated data outperform models trained on larger, single-source human-annotated datasets. These findings highlight the value of structured guidelines in reducing subjective variability, maintaining performance with limited data, and embracing language diversity. Content Warning: This article only analyzes offensive language for academic purposes. Discretion is advised.","Hou, Xinmeng",,,Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language,,,10.18653/v1/2024.nlp4dh-1.36 , ,,"This study introduces a prescriptive annotation benchmark grounded in humanities research to ensure consistent, unbiased labeling of offensive language, particularly for casual and non-mainstream language uses. We contribute two newly annotated datasets that achieve higher inter-annotator agreement between human and language model (LLM) annotations compared to original datasets based on descriptive instructions. Our experiments show that LLMs can serve as effective alternatives when professional annotators are unavailable. Moreover, smaller models fine-tuned on multi-source LLM-annotated data outperform models trained on larger, single-source human-annotated datasets. These findings highlight the value of structured guidelines in reducing subjective variability, maintaining performance with limited data, and embracing language diversity. Content Warning: This article only analyzes offensive language for academic purposes. Discretion is advised.",,,,, ,  Proceedings of the 4th International Conference on Natural Language Processing for Digital Humanities,,Gen_dataset,
3443,"**Title**Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation

**Abstract**Most task-oriented dialogue (TOD) benchmarks assume users that know exactly how to use the system by constraining the user behaviors within the system`s capabilities via strict user goals, namely {\textquotedblleft}user familiarity{\textquotedblright} bias. This data bias deepens when it combines with data-driven TOD systems, as it is impossible to fathom the effect of it with existing static evaluations. Hence, we conduct an interactive user study to unveil how vulnerable TOD systems are against realistic scenarios. In particular, we compare users with 1) detailed goal instructions that conform to the system boundaries (closed-goal) and 2) vague goal instructions that are often unsupported but realistic (open-goal). Our study reveals that conversations in open-goal settings lead to catastrophic failures of the system, in which 92{\%} of the dialogues had significant issues. Moreover, we conduct a thorough analysis to identify distinctive features between the two settings through error annotation. From this, we discover a novel {\textquotedblleft}pretending{\textquotedblright} behavior, in which the system pretends to handle the user requests even though they are beyond the system`s capabilities. We discuss its characteristics and toxicity while showing recent large language models can also suffer from this behavior.","Kim, Takyoung, Shin, Jamin, Kim, Young-Ho, Bae, Sanghwan, Kim, Sungdong",,,Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation,,, , ,,"Most task-oriented dialogue (TOD) benchmarks assume users that know exactly how to use the system by constraining the user behaviors within the system`s capabilities via strict user goals, namely {\textquotedblleft}user familiarity{\textquotedblright} bias. This data bias deepens when it combines with data-driven TOD systems, as it is impossible to fathom the effect of it with existing static evaluations. Hence, we conduct an interactive user study to unveil how vulnerable TOD systems are against realistic scenarios. In particular, we compare users with 1) detailed goal instructions that conform to the system boundaries (closed-goal) and 2) vague goal instructions that are often unsupported but realistic (open-goal). Our study reveals that conversations in open-goal settings lead to catastrophic failures of the system, in which 92{\%} of the dialogues had significant issues. Moreover, we conduct a thorough analysis to identify distinctive features between the two settings through error annotation. From this, we discover a novel {\textquotedblleft}pretending{\textquotedblright} behavior, in which the system pretends to handle the user requests even though they are beyond the system`s capabilities. We discuss its characteristics and toxicity while showing recent large language models can also suffer from this behavior.",,,,, ,  Proceedings of the 6th Workshop on NLP for Conversational AI (NLP4ConvAI 2024),,detection,
3445,"**Title**{L}ife{T}ox: Unveiling Implicit Toxicity in Life Advice

**Abstract**As large language models become increasingly integrated into daily life, detecting implicit toxicity across diverse contexts is crucial. To this end, we introduce $\texttt{LifeTox}$, a dataset designed for identifying implicit toxicity within a broad range of advice-seeking scenarios. Unlike existing safety datasets, $\texttt{LifeTox}$ comprises diverse contexts derived from personal experiences through open-ended questions. Our experiments demonstrate that RoBERTa fine-tuned on $\texttt{LifeTox}$ matches or surpasses the zero-shot performance of large language models in toxicity classification tasks. These results underscore the efficacy of $\texttt{LifeTox}$ in addressing the complex challenges inherent in implicit toxicity. We open-sourced the dataset and the $\texttt{LifeTox}$ moderator family; 350M, 7B, and 13B.","Kim, Minbeom, Koo, Jahyun, Lee, Hwanhee, Park, Joonsuk, Lee, Hwaran, Jung, Kyomin",,,{L}ife{T}ox: Unveiling Implicit Toxicity in Life Advice,,,10.18653/v1/2024.naacl-short.60 , ,,"As large language models become increasingly integrated into daily life, detecting implicit toxicity across diverse contexts is crucial. To this end, we introduce $\texttt{LifeTox}$, a dataset designed for identifying implicit toxicity within a broad range of advice-seeking scenarios. Unlike existing safety datasets, $\texttt{LifeTox}$ comprises diverse contexts derived from personal experiences through open-ended questions. Our experiments demonstrate that RoBERTa fine-tuned on $\texttt{LifeTox}$ matches or surpasses the zero-shot performance of large language models in toxicity classification tasks. These results underscore the efficacy of $\texttt{LifeTox}$ in addressing the complex challenges inherent in implicit toxicity. We open-sourced the dataset and the $\texttt{LifeTox}$ moderator family; 350M, 7B, and 13B.",,,,, ,  Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers),,detection,
3447,"**Title**Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback

**Abstract**Large language models (LLMs) often generate biased outputs containing offensive, toxic, or stereotypical text. Existing LLM alignment methods such as reinforcement learning from human feedback (RLHF) alleviate biases primarily based on reward signals from current model outputs without considering the source of biases. In this work, to explore how biases are formed, we revisit LLMs' text generation from a causal perspective. We identify pretraining data and input prompts, which contain semantic correlations of textual phrases, as two confounders between LLMs and model outputs causing biases. Inspired by our causal view, we leverage the reward model in RL alignment as an instrumental variable to perform causal intervention on LLMs. Utilizing the reward difference between an initial LLM and intervened LLM as interventional feedback to guide RL finetuning, we propose Causality-Aware Alignment (CAA) for LLM debiasing. Experiments on two text generation tasks with three different alignment objectives demonstrate the advantages of our method in aligning LLMs to generate less biased and safer outputs.","Xia, Yu, Yu, Tong, He, Zhankui, Zhao, Handong, McAuley, Julian, Li, Shuai",,,Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback,,,10.18653/v1/2024.naacl-long.262 , ,,"Large language models (LLMs) often generate biased outputs containing offensive, toxic, or stereotypical text. Existing LLM alignment methods such as reinforcement learning from human feedback (RLHF) alleviate biases primarily based on reward signals from current model outputs without considering the source of biases. In this work, to explore how biases are formed, we revisit LLMs' text generation from a causal perspective. We identify pretraining data and input prompts, which contain semantic correlations of textual phrases, as two confounders between LLMs and model outputs causing biases. Inspired by our causal view, we leverage the reward model in RL alignment as an instrumental variable to perform causal intervention on LLMs. Utilizing the reward difference between an initial LLM and intervened LLM as interventional feedback to guide RL finetuning, we propose Causality-Aware Alignment (CAA) for LLM debiasing. Experiments on two text generation tasks with three different alignment objectives demonstrate the advantages of our method in aligning LLMs to generate less biased and safer outputs.",,,,, ,  Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,detox,
3448,"**Title**{XST}est: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models

**Abstract**Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest`s creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.","R{\""o}ttger, Paul, Kirk, Hannah, Vidgen, Bertie, Attanasio, Giuseppe, Bianchi, Federico, Hovy, Dirk",,,{XST}est: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models,,,10.18653/v1/2024.naacl-long.301 , ,,"Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest`s creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.",,,,, ,  Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,detection,
3449,"**Title**{T}o{XCL}: A Unified Framework for Toxic Speech Detection and Explanation

**Abstract**The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.","Hoang, Nhat, Do, Xuan Long, Do, Duc Anh, Vu, Duc Anh, Luu, Anh Tuan",,,{T}o{XCL}: A Unified Framework for Toxic Speech Detection and Explanation,,,10.18653/v1/2024.naacl-long.359 , ,,"The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.",,,,, ,  Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,detection,
3450,"**Title**Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with {RLAIF}

**Abstract**Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. The effectiveness of addressing hate speech involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. The first two phases of CoARL involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and nontoxicity. CoARL outperforms existing benchmarks in intent-conditioned counterspeech generation, showing an average improvement of {\ensuremath{\sim}}3 points in intent-conformity and {\ensuremath{\sim}}4 points in argument-quality metrics. Extensive human evaluation supports CoARL`s efficacy in generating superior and more context-appropriate responses compared to existing systems, including prominent LLMs like ChatGPT.","Hengle, Amey, Padhi, Aswini, Singh, Sahajpreet, Bandhakavi, Anil, Akhtar, Md Shad, Chakraborty, Tanmoy",,,Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with {RLAIF},,,10.18653/v1/2024.naacl-long.374 , ,,"Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. The effectiveness of addressing hate speech involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. The first two phases of CoARL involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and nontoxicity. CoARL outperforms existing benchmarks in intent-conditioned counterspeech generation, showing an average improvement of {\ensuremath{\sim}}3 points in intent-conformity and {\ensuremath{\sim}}4 points in argument-quality metrics. Extensive human evaluation supports CoARL`s efficacy in generating superior and more context-appropriate responses compared to existing systems, including prominent LLMs like ChatGPT.",,,,, ,  Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,detox,
3453,"**Title**On Zero-Shot Counterspeech Generation by {LLM}s

**Abstract**With the emergence of numerous Large Language Models (LLM), the usage of such models in various Natural Language Processing (NLP) applications is increasing extensively. Counterspeech generation is one such key task where efforts are made to develop generative models by fine-tuning LLMs with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of large language models in zero-shot settings. In this work, we present a comprehensive analysis of the performances of four LLMs namely GPT-2, DialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech generation, which is the first of its kind. For GPT-2 and DialoGPT, we further investigate the deviation in performance with respect to the sizes (small, medium, large) of the models. On the other hand, we propose three different prompting strategies for generating different types of counterspeech and analyse the impact of such strategies on the performance of the models. Our analysis shows that there is an improvement in generation quality for two datasets (17{\%}), however the toxicity increase (25{\%}) with increase in model size. Considering type of model, GPT-2 and FlanT5 models are significantly better in terms of counterspeech quality but also have high toxicity as compared to DialoGPT. ChatGPT are much better at generating counter speech than other models across all metrics. In terms of prompting, we find that our proposed strategies help in improving counter speech generation across all the models.","Saha, Punyajoy, Agrawal, Aalok, Jana, Abhik, Biemann, Chris, Mukherjee, Animesh",,,On Zero-Shot Counterspeech Generation by {LLM}s,,, , ,,"With the emergence of numerous Large Language Models (LLM), the usage of such models in various Natural Language Processing (NLP) applications is increasing extensively. Counterspeech generation is one such key task where efforts are made to develop generative models by fine-tuning LLMs with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of large language models in zero-shot settings. In this work, we present a comprehensive analysis of the performances of four LLMs namely GPT-2, DialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech generation, which is the first of its kind. For GPT-2 and DialoGPT, we further investigate the deviation in performance with respect to the sizes (small, medium, large) of the models. On the other hand, we propose three different prompting strategies for generating different types of counterspeech and analyse the impact of such strategies on the performance of the models. Our analysis shows that there is an improvement in generation quality for two datasets (17{\%}), however the toxicity increase (25{\%}) with increase in model size. Considering type of model, GPT-2 and FlanT5 models are significantly better in terms of counterspeech quality but also have high toxicity as compared to DialoGPT. ChatGPT are much better at generating counter speech than other models across all metrics. In terms of prompting, we find that our proposed strategies help in improving counter speech generation across all the models.",,,,, ,"  Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",,detox,
3454,"**Title**Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment

**Abstract**Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after fine-tuning. We also leverage it on data augmentation and conduct experiments to show its effect on different algorithms.","Song, Feifan, Yu, Bowen, Lang, Hao, Yu, Haiyang, Huang, Fei, Wang, Houfeng, Li, Yongbin",,,Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment,,, , ,,"Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after fine-tuning. We also leverage it on data augmentation and conduct experiments to show its effect on different algorithms.",,,,, ,"  Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",,detox,
3455,"**Title**Adversarial {DPO}: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents

**Abstract**Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies. Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience. In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO). The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token. We demonstrate that ADPO enhances the model`s resilience against harmful conversations while minimizing performance degradation. Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO. To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data.","Kim, San, Lee, Gary",,,Adversarial {DPO}: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents,,,10.18653/v1/2024.findings-naacl.118 , ,,"Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies. Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience. In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO). The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token. We demonstrate that ADPO enhances the model`s resilience against harmful conversations while minimizing performance degradation. Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO. To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data.",,,,, ,  Findings of the Association for Computational Linguistics: NAACL 2024,,detox,
3456,"**Title**Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing

**Abstract**Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs respond to harmful prompts and propose a novel defense method termed \textbf{L}ayer-specific \textbf{Ed}iting (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical \textit{safety layers} exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from identified \textit{toxic layers} can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at \url{https://github.com/ledllm/ledllm}.","Zhao, Wei, Li, Zhe, Li, Yige, Zhang, Ye, Sun, Jun",,,Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing,,,10.18653/v1/2024.findings-emnlp.293 , ,,"Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs respond to harmful prompts and propose a novel defense method termed \textbf{L}ayer-specific \textbf{Ed}iting (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical \textit{safety layers} exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from identified \textit{toxic layers} can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at \url{https://github.com/ledllm/ledllm}.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2024,,detox,
3457,"**Title**Can {LLM}s Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric

**Abstract**In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to detect the toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets, which are susceptible to out-of-distribution (OOD) problems and depend on the dataset`s definition of toxicity. In this paper, we introduce a robust metric grounded on LLMs to flexibly measure toxicity according to the given definition. We first analyze the toxicity factors, followed by an examination of the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Finally, we evaluate the performance of our metric with detailed analysis. Our empirical results demonstrate outstanding performance in measuring toxicity within verified factors, improving on conventional metrics by 12 points in the F1 score. Our findings also indicate that upstream toxicity significantly influences downstream metrics, suggesting that LLMs are unsuitable for toxicity evaluations within unverified factors.","Koh, Hyukhun, Kim, Dohyung, Lee, Minwoo, Jung, Kyomin",,,Can {LLM}s Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric,,,10.18653/v1/2024.findings-emnlp.353 , ,,"In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to detect the toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets, which are susceptible to out-of-distribution (OOD) problems and depend on the dataset`s definition of toxicity. In this paper, we introduce a robust metric grounded on LLMs to flexibly measure toxicity according to the given definition. We first analyze the toxicity factors, followed by an examination of the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Finally, we evaluate the performance of our metric with detailed analysis. Our empirical results demonstrate outstanding performance in measuring toxicity within verified factors, improving on conventional metrics by 12 points in the F1 score. Our findings also indicate that upstream toxicity significantly influences downstream metrics, suggesting that LLMs are unsuitable for toxicity evaluations within unverified factors.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2024,,detection,
3458,"**Title**{P}cl{GPT}: A Large Language Model for Patronizing and Condescending Language Detection

**Abstract**Disclaimer: Samples in this paper may be harmful and cause discomfort! Patronizing and condescending language (PCL) is a form of speech directed at vulnerable groups. As an essential branch of toxic language, this type of language exacerbates conflicts and confrontations among Internet communities and detrimentally impacts disadvantaged groups. Traditional pre-trained language models (PLMs) perform poorly in detecting PCL due to its implicit toxicity traits like hypocrisy and false sympathy. With the rise of large language models (LLMs), we can harness their rich emotional semantics to establish a paradigm for exploring implicit toxicity. In this paper, we introduce PclGPT, a comprehensive LLM benchmark designed specifically for PCL. We collect, annotate, and integrate the Pcl-PT/SFT dataset, and then develop a bilingual PclGPT-EN/CN model group through a comprehensive pre-training and supervised fine-tuning staircase process to facilitate implicit toxic detection. Group detection results and fine-grained detection from PclGPT and other models reveal significant variations in the degree of bias in PCL towards different vulnerable groups, necessitating increased societal attention to protect them.","Wang, Hongbo, LiMingDa, LiMingDa, Lu, Junyu, Xia, Hebin, Yang, Liang, Xu, Bo, Liu, Ruizhu, Lin, Hongfei",,,{P}cl{GPT}: A Large Language Model for Patronizing and Condescending Language Detection,,,10.18653/v1/2024.findings-emnlp.406 , ,,"Disclaimer: Samples in this paper may be harmful and cause discomfort! Patronizing and condescending language (PCL) is a form of speech directed at vulnerable groups. As an essential branch of toxic language, this type of language exacerbates conflicts and confrontations among Internet communities and detrimentally impacts disadvantaged groups. Traditional pre-trained language models (PLMs) perform poorly in detecting PCL due to its implicit toxicity traits like hypocrisy and false sympathy. With the rise of large language models (LLMs), we can harness their rich emotional semantics to establish a paradigm for exploring implicit toxicity. In this paper, we introduce PclGPT, a comprehensive LLM benchmark designed specifically for PCL. We collect, annotate, and integrate the Pcl-PT/SFT dataset, and then develop a bilingual PclGPT-EN/CN model group through a comprehensive pre-training and supervised fine-tuning staircase process to facilitate implicit toxic detection. Group detection results and fine-grained detection from PclGPT and other models reveal significant variations in the degree of bias in PCL towards different vulnerable groups, necessitating increased societal attention to protect them.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2024,,detection,
3461,"**Title**Preference Tuning For Toxicity Mitigation Generalizes Across Languages

**Abstract**Detoxifying multilingual Large Language Models (LLMs) has become crucial due to their increasing global use. In this work, we explore zero-shot cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike previous studies that show limited cross-lingual generalization for other safety tasks, we demonstrate that Direct Preference Optimization (DPO) training with only English data can significantly reduce toxicity in multilingual open-ended generations. For example, the probability of mGPT-1.3B generating toxic continuations drops from 46.8{\%} to 3.9{\%} across 17 different languages after training. Our results also extend to other multilingual LLMs, such as BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal intervention and activation analysis, we identified the dual multilinguality property of MLP layers in LLMs, which explains the cross-lingual generalization of DPO. Finally, we show that bilingual sentence retrieval can predict the cross-lingual transferability of DPO preference tuning.","Li, Xiaochen, Yong, Zheng Xin, Bach, Stephen",,,Preference Tuning For Toxicity Mitigation Generalizes Across Languages,,,10.18653/v1/2024.findings-emnlp.784 , ,,"Detoxifying multilingual Large Language Models (LLMs) has become crucial due to their increasing global use. In this work, we explore zero-shot cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike previous studies that show limited cross-lingual generalization for other safety tasks, we demonstrate that Direct Preference Optimization (DPO) training with only English data can significantly reduce toxicity in multilingual open-ended generations. For example, the probability of mGPT-1.3B generating toxic continuations drops from 46.8{\%} to 3.9{\%} across 17 different languages after training. Our results also extend to other multilingual LLMs, such as BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal intervention and activation analysis, we identified the dual multilinguality property of MLP layers in LLMs, which explains the cross-lingual generalization of DPO. Finally, we show that bilingual sentence retrieval can predict the cross-lingual transferability of DPO preference tuning.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2024,,detox,
3462,"**Title**Beyond Perplexity: Multi-dimensional Safety Evaluation of {LLM} Compression

**Abstract**Increasingly, model compression techniques enable large language models (LLMs) to be deployed in real-world applications. As a result of this momentum towards local deployment, compressed LLMs will interact with a large population. Prior work on compression typically prioritize preserving perplexity, which is directly analogous to training loss. The impact of compression method on other critical aspects of model behavior{---}particularly safety{---}requires systematic assessment. To this end, we investigate the impact of model compression along four dimensions: (1) degeneration harm, i.e., bias and toxicity in generation; (2) representational harm, i.e., biases in discriminative tasks; (3) dialect bias; and (4) language modeling and downstream task performance. We examine a wide spectrum of LLM compression techniques, including unstructured pruning, semi-structured pruning, and quantization. Our analysis reveals that compression can lead to unexpected consequences. Although compression may unintentionally alleviate LLMs' degeneration harm, it can still exacerbate representational harm. Furthermore, increasing compression produces a divergent impact on different protected groups. Finally, different compression methods have drastically different safety impacts: for example, quantization mostly preserves bias while pruning degrades quickly. Our findings underscore the importance of integrating safety assessments into the development of compressed LLMs to ensure their reliability across real-world applications.","Xu, Zhichao, Gupta, Ashim, Li, Tao, Bentham, Oliver, Srikumar, Vivek",,,Beyond Perplexity: Multi-dimensional Safety Evaluation of {LLM} Compression,,,10.18653/v1/2024.findings-emnlp.901 , ,,"Increasingly, model compression techniques enable large language models (LLMs) to be deployed in real-world applications. As a result of this momentum towards local deployment, compressed LLMs will interact with a large population. Prior work on compression typically prioritize preserving perplexity, which is directly analogous to training loss. The impact of compression method on other critical aspects of model behavior{---}particularly safety{---}requires systematic assessment. To this end, we investigate the impact of model compression along four dimensions: (1) degeneration harm, i.e., bias and toxicity in generation; (2) representational harm, i.e., biases in discriminative tasks; (3) dialect bias; and (4) language modeling and downstream task performance. We examine a wide spectrum of LLM compression techniques, including unstructured pruning, semi-structured pruning, and quantization. Our analysis reveals that compression can lead to unexpected consequences. Although compression may unintentionally alleviate LLMs' degeneration harm, it can still exacerbate representational harm. Furthermore, increasing compression produces a divergent impact on different protected groups. Finally, different compression methods have drastically different safety impacts: for example, quantization mostly preserves bias while pruning degrades quickly. Our findings underscore the importance of integrating safety assessments into the development of compressed LLMs to ensure their reliability across real-world applications.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2024,,detox,
3463,"**Title**{G}roun{D}ial: Human-norm Grounded Safe Dialog Response Generation

**Abstract**Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses agreeing to offensive user input or including toxic content. Previous research aimed to alleviate the toxicity by fine-tuning LLM with manually annotated safe dialogue histories. However, the dependency on additional tuning requires substantial costs. To remove the dependency, we propose GrounDial, where response safety is achieved by grounding responses to commonsense social rules without requiring fine-tuning. A hybrid approach of in-context learning and human-norm-guided decoding of GrounDial enables the response to be quantitatively and qualitatively safer even without additional data or tuning.","Kim, Siwon, Dai, Shuyang, Kachuee, Mohammad, Ray, Shayan, Taghavi, Tara, Yoon, Sungroh",,,{G}roun{D}ial: Human-norm Grounded Safe Dialog Response Generation,,, , ,,"Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses agreeing to offensive user input or including toxic content. Previous research aimed to alleviate the toxicity by fine-tuning LLM with manually annotated safe dialogue histories. However, the dependency on additional tuning requires substantial costs. To remove the dependency, we propose GrounDial, where response safety is achieved by grounding responses to commonsense social rules without requiring fine-tuning. A hybrid approach of in-context learning and human-norm-guided decoding of GrounDial enables the response to be quantitatively and qualitatively safer even without additional data or tuning.",,,,, ,  Findings of the Association for Computational Linguistics: EACL 2024,,detox,
3465,"**Title**{LIRE}: listwise reward enhancement for preference alignment

**Abstract**Recently, tremendous strides have been made to align the generation of Large Language Models (LLMs) with human values to mitigate toxic or unhelpful content. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves effective and is widely adopted by researchers. However, implementing RLHF is complex, and its sensitivity to hyperparameters renders achieving stable performance and scalability challenging. Furthermore, prevailing approaches to preference alignment primarily concentrate on pairwise comparisons, with limited exploration into multi-response scenarios, thereby overlooking the potential richness within the candidate pool. For the above reasons, we propose a new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a gradient-based reward optimization approach that incorporates the offline rewards of multiple responses into a streamlined listwise framework, thus eliminating the need for online sampling during training. LIRE is straightforward to implement, requiring minimal parameter tuning, and seamlessly aligns with the pairwise paradigm while naturally extending to multi-response scenarios. Moreover, we introduce a self-enhancement algorithm aimed at iteratively refining the reward during training. Our experiments demonstrate that LIRE consistently outperforms existing methods across several benchmarks on dialogue and summarization tasks, with good transferability to out-of-distribution data, assessed using proxy reward models and human annotators.","Zhu, Mingye, Liu, Yi, Zhang, Lei, Guo, Junbo, Mao, Zhendong",,,{LIRE}: listwise reward enhancement for preference alignment,,,10.18653/v1/2024.findings-acl.201 , ,,"Recently, tremendous strides have been made to align the generation of Large Language Models (LLMs) with human values to mitigate toxic or unhelpful content. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves effective and is widely adopted by researchers. However, implementing RLHF is complex, and its sensitivity to hyperparameters renders achieving stable performance and scalability challenging. Furthermore, prevailing approaches to preference alignment primarily concentrate on pairwise comparisons, with limited exploration into multi-response scenarios, thereby overlooking the potential richness within the candidate pool. For the above reasons, we propose a new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a gradient-based reward optimization approach that incorporates the offline rewards of multiple responses into a streamlined listwise framework, thus eliminating the need for online sampling during training. LIRE is straightforward to implement, requiring minimal parameter tuning, and seamlessly aligns with the pairwise paradigm while naturally extending to multi-response scenarios. Moreover, we introduce a self-enhancement algorithm aimed at iteratively refining the reward during training. Our experiments demonstrate that LIRE consistently outperforms existing methods across several benchmarks on dialogue and summarization tasks, with good transferability to out-of-distribution data, assessed using proxy reward models and human annotators.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,detox,
3466,"**Title**{W}il{KE}: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing

**Abstract**Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named Wise-Layer Knowledge Editor (WilKE), which selects editing layer based on the pattern matching degree of editing knowledge across different layers in language models. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2{\%} and 67.8{\%} on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.","Hu, Chenhui, Cao, Pengfei, Chen, Yubo, Liu, Kang, Zhao, Jun",,,{W}il{KE}: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing,,,10.18653/v1/2024.findings-acl.207 , ,,"Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named Wise-Layer Knowledge Editor (WilKE), which selects editing layer based on the pattern matching degree of editing knowledge across different layers in language models. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2{\%} and 67.8{\%} on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,detox,
3467,"**Title**Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models

**Abstract**Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM`s pre-training checkpoints to enhance the LLM`s trustworthiness. Finally, inspired by the theoretical result that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field.","Qian, Chen, Zhang, Jie, Yao, Wei, Liu, Dongrui, Yin, Zhenfei, Qiao, Yu, Liu, Yong, Shao, Jing",,,Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models,,,10.18653/v1/2024.findings-acl.290 , ,,"Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM`s pre-training checkpoints to enhance the LLM`s trustworthiness. Finally, inspired by the theoretical result that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,detox,
3468,"**Title**Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs

**Abstract**Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29{\%} over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant decrease in perplexity, markedly improving text fluency.","Liang, Xun, Wang, Hanyu, Song, Shichao, Hu, Mengting, Wang, Xunzhi, Li, Zhiyu, Xiong, Feiyu, Tang, Bo",,,Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs,,,10.18653/v1/2024.findings-acl.345 , ,,"Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29{\%} over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant decrease in perplexity, markedly improving text fluency.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,detox,
3469,"**Title**Direct Preference Optimization with an Offset

**Abstract**Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal. Sometimes, the preferred response is only slightly better than the dispreferred one. In other cases, the preference is much stronger. For instance, if a response contains harmful or toxic content, the annotator will have a strong preference for that response. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.","Amini, Afra, Vieira, Tim, Cotterell, Ryan",,,Direct Preference Optimization with an Offset,,,10.18653/v1/2024.findings-acl.592 , ,,"Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal. Sometimes, the preferred response is only slightly better than the dispreferred one. In other cases, the preference is much stronger. For instance, if a response contains harmful or toxic content, the annotator will have a strong preference for that response. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,detox,
3471,"**Title**Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning

**Abstract**Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also often retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as - number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase (CAPP) dataset, comprising of dialogue-style rude utterances, polite paraphrases, and additional dialogue context. We evaluate our approach using four closed source and one open source LLM. Our results reveal that ICL is comparable to supervised methods in generation quality, while being qualitatively better by 25{\%} on human evaluation and attaining lower toxicity by 76{\%}. Also, ICL-based paraphrasers only show a slight reduction in performance even with just 10{\%} training data.","Som, Anirudh, Sikka, Karan, Gent, Helen, Divakaran, Ajay, Kathol, Andreas, Vergyri, Dimitra",,,Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning,,,10.18653/v1/2024.findings-acl.749 , ,,"Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also often retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as - number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase (CAPP) dataset, comprising of dialogue-style rude utterances, polite paraphrases, and additional dialogue context. We evaluate our approach using four closed source and one open source LLM. Our results reveal that ICL is comparable to supervised methods in generation quality, while being qualitatively better by 25{\%} on human evaluation and attaining lower toxicity by 76{\%}. Also, ICL-based paraphrasers only show a slight reduction in performance even with just 10{\%} training data.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,detox,
3472,"**Title**Tox-{BART}: Leveraging Toxicity Attributes for Explanation Generation of Implicit Hate Speech

**Abstract**Employing language models to generate explanations for an incoming implicit hate post is an active area of research. The explanation is intended to make explicit the underlying stereotype and aid content moderators. The training often combines top-k relevant knowledge graph (KG) tuples to provide world knowledge and improve performance on standard metrics. Interestingly, our study presents conflicting evidence for the role of the quality of KG tuples in generating implicit explanations. Consequently, simpler models incorporating external toxicity signals outperform KG-infused models. Compared to the KG-based setup, we observe a comparable performance for SBIC (LatentHatred) datasets with a performance variation of +0.44 (+0.49), +1.83 (-1.56), and -4.59 (+0.77) in BLEU, ROUGE-L, and BERTScore. Further human evaluation and error analysis reveal that our proposed setup produces more precise explanations than zero-shot GPT-3.5, highlighting the intricate nature of the task.","Yadav, Neemesh, Masud, Sarah, Goyal, Vikram, Akhtar, Md Shad, Chakraborty, Tanmoy",,,Tox-{BART}: Leveraging Toxicity Attributes for Explanation Generation of Implicit Hate Speech,,,10.18653/v1/2024.findings-acl.831 , ,,"Employing language models to generate explanations for an incoming implicit hate post is an active area of research. The explanation is intended to make explicit the underlying stereotype and aid content moderators. The training often combines top-k relevant knowledge graph (KG) tuples to provide world knowledge and improve performance on standard metrics. Interestingly, our study presents conflicting evidence for the role of the quality of KG tuples in generating implicit explanations. Consequently, simpler models incorporating external toxicity signals outperform KG-infused models. Compared to the KG-based setup, we observe a comparable performance for SBIC (LatentHatred) datasets with a performance variation of +0.44 (+0.49), +1.83 (-1.56), and -4.59 (+0.77) in BLEU, ROUGE-L, and BERTScore. Further human evaluation and error analysis reveal that our proposed setup produces more precise explanations than zero-shot GPT-3.5, highlighting the intricate nature of the task.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,detection,
3473,"**Title**From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models

**Abstract**To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it`s crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages. Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field.","Ermis, Beyza, Pozzobon, Luiza, Hooker, Sara, Lewis, Patrick",,,From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models,,,10.18653/v1/2024.findings-acl.893 , ,,"To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it`s crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages. Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,detox,
3474,"**Title**From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards

**Abstract**Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations.Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases. Using the case of Llama 2 as an example, we illustrate how LLMs' safety responses can still encode harmful assumptions. To do so, we create a set of non-toxic prompts, which we then use to evaluate Llama models. Through our new taxonomy of LLMs responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to different kinds of harms such as quality-of-service harms for marginalized populations.","Chehbouni, Khaoula, Roshan, Megha, Ma, Emmanuel, Wei, Futian, Taik, Afaf, Cheung, Jackie, Farnadi, Golnoosh",,,From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards,,,10.18653/v1/2024.findings-acl.927 , ,,"Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations.Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases. Using the case of Llama 2 as an example, we illustrate how LLMs' safety responses can still encode harmful assumptions. To do so, we create a set of non-toxic prompts, which we then use to evaluate Llama models. Through our new taxonomy of LLMs responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to different kinds of harms such as quality-of-service harms for marginalized populations.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,Gen_dataset,
3475,"**Title**Evaluating Psychological Safety of Large Language Models

**Abstract**In this work, we designed unbiased prompts to systematically evaluate the psychological safety of large language models (LLMs). First, we tested five different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT models. Following these observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model. Based on the findings, we recommended the application of systematic and comprehensive psychological metrics to further evaluate and improve the safety of LLMs.","Li, Xingxuan, Li, Yutong, Qiu, Lin, Joty, Shafiq, Bing, Lidong",,,Evaluating Psychological Safety of Large Language Models,,,10.18653/v1/2024.emnlp-main.108 , ,,"In this work, we designed unbiased prompts to systematically evaluate the psychological safety of large language models (LLMs). First, we tested five different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT models. Following these observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model. Based on the findings, we recommended the application of systematic and comprehensive psychological metrics to further evaluate and improve the safety of LLMs.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3477,"**Title**{LLM} See, {LLM} Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives

**Abstract**The widespread adoption of synthetic data raises new questions about how models generating the data can influence other large language models (LLMs). To start, our work exhaustively characterizes the impact of passive inheritance of model properties by systematically studying how the source of synthetic data shapes models' internal biases, calibration and preferences, and their generations' textual attributes, providing one of the most comprehensive studies to-date. We find that models are surprisingly sensitive towards certain attributes even when the synthetic data prompts appear {\textquotedblleft}neutral{\textquotedblright} which invites the question: can we explicitly steer the distilled data towards desired properties? We demonstrate how such active inheritance can steer the generation profiles of models towards desirable non-differentiable attributes in both directions, e.g. increasing lexical diversity or reducing toxicity. Overall, our study broadens the understanding of the implicit biases inherited by LLMs and explores how we can leverage them to positive effect.","Shimabucoro, Lu{\'i}sa, Ruder, Sebastian, Kreutzer, Julia, Fadaee, Marzieh, Hooker, Sara",,,"{LLM} See, {LLM} Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives",,,10.18653/v1/2024.emnlp-main.521 , ,,"The widespread adoption of synthetic data raises new questions about how models generating the data can influence other large language models (LLMs). To start, our work exhaustively characterizes the impact of passive inheritance of model properties by systematically studying how the source of synthetic data shapes models' internal biases, calibration and preferences, and their generations' textual attributes, providing one of the most comprehensive studies to-date. We find that models are surprisingly sensitive towards certain attributes even when the synthetic data prompts appear {\textquotedblleft}neutral{\textquotedblright} which invites the question: can we explicitly steer the distilled data towards desired properties? We demonstrate how such active inheritance can steer the generation profiles of models towards desirable non-differentiable attributes in both directions, e.g. increasing lexical diversity or reducing toxicity. Overall, our study broadens the understanding of the implicit biases inherited by LLMs and explores how we can leverage them to positive effect.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3479,"**Title**Style-Specific Neurons for Steering {LLM}s in Text Style Transfer

**Abstract**Text style transfer (TST) aims to modify the style of a text without altering its original meaning. Large language models (LLMs) demonstrate superior performance across multiple tasks, including TST. However, in zero-shot setups, they tend to directly copy a significant portion of the input text to the output without effectively changing its style. To enhance the stylistic variety and fluency of the text, we present sNeuron-TST, a novel approach for steering LLMs using style-specific neurons in TST. Specifically, we identify neurons associated with the source and target styles and deactivate source-style-only neurons to give target-style words a higher probability, aiming to enhance the stylistic diversity of the generated text. However, we find that this deactivation negatively impacts the fluency of the generated text, which we address by proposing an improved contrastive decoding method that accounts for rapid token probability shifts across layers caused by deactivated source-style neurons. Empirical experiments demonstrate the effectiveness of the proposed method on six benchmarks, encompassing formality, toxicity, politics, politeness, authorship, and sentiment.","Lai, Wen, Hangya, Viktor, Fraser, Alexander",,,Style-Specific Neurons for Steering {LLM}s in Text Style Transfer,,,10.18653/v1/2024.emnlp-main.745 , ,,"Text style transfer (TST) aims to modify the style of a text without altering its original meaning. Large language models (LLMs) demonstrate superior performance across multiple tasks, including TST. However, in zero-shot setups, they tend to directly copy a significant portion of the input text to the output without effectively changing its style. To enhance the stylistic variety and fluency of the text, we present sNeuron-TST, a novel approach for steering LLMs using style-specific neurons in TST. Specifically, we identify neurons associated with the source and target styles and deactivate source-style-only neurons to give target-style words a higher probability, aiming to enhance the stylistic diversity of the generated text. However, we find that this deactivation negatively impacts the fluency of the generated text, which we address by proposing an improved contrastive decoding method that accounts for rapid token probability shifts across layers caused by deactivated source-style neurons. Empirical experiments demonstrate the effectiveness of the proposed method on six benchmarks, encompassing formality, toxicity, politics, politeness, authorship, and sentiment.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3480,"**Title**The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis

**Abstract**Understanding in-context learning (ICL) capability that enables large language models (LLMs) to excel in proficiency through demonstration examples is of utmost importance. This importance stems not only from the better utilization of this capability across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns regarding truthfulness, bias, and toxicity, that may arise alongside the capability. In this paper, we present a thorough survey on the interpretation and analysis of in-context learning. First, we provide a concise introduction to the background and definition of in-context learning. Then, we give an overview of advancements from two perspectives: 1) a theoretical perspective, emphasizing studies on mechanistic interpretability and delving into the mathematical foundations behind ICL; and 2) an empirical perspective, concerning studies that empirically analyze factors associated with ICL. We conclude by discussing open questions and the challenges encountered, and suggesting potential avenues for future research. We believe that our work establishes the basis for further exploration into the interpretation of in-context learning. To aid this effort, we have created a repository containing resources that will be continually updated.","Zhou, Yuxiang, Li, Jiazheng, Xiang, Yanzheng, Yan, Hanqi, Gui, Lin, He, Yulan",,,The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis,,,10.18653/v1/2024.emnlp-main.795 , ,,"Understanding in-context learning (ICL) capability that enables large language models (LLMs) to excel in proficiency through demonstration examples is of utmost importance. This importance stems not only from the better utilization of this capability across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns regarding truthfulness, bias, and toxicity, that may arise alongside the capability. In this paper, we present a thorough survey on the interpretation and analysis of in-context learning. First, we provide a concise introduction to the background and definition of in-context learning. Then, we give an overview of advancements from two perspectives: 1) a theoretical perspective, emphasizing studies on mechanistic interpretability and delving into the mathematical foundations behind ICL; and 2) an empirical perspective, concerning studies that empirically analyze factors associated with ICL. We conclude by discussing open questions and the challenges encountered, and suggesting potential avenues for future research. We believe that our work establishes the basis for further exploration into the interpretation of in-context learning. To aid this effort, we have created a repository containing resources that will be continually updated.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detection,
3481,"**Title**Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis

**Abstract**Large Language Models (LLMs) are capable of producing content that perpetuates stereotypes, discrimination, and toxicity.The recently proposed \textit{moral self-correction} is a computationally efficient method for reducing harmful content in the responses of LLMs. However, the process of how injecting self-correction instructions can modify the behavior of LLMs remains under-explored. In this paper, we explore the effectiveness of moral self-correction by answering three research questions: (1) In what scenarios does moral self-correction work? (2) What are the internal mechanisms of LLMs, e.g., hidden states, that are influenced by moral self-correction instructions? (3) Is intrinsic moral self-correction actually superficial in terms of reduced immorality in hidden states? We argue that self-correction can help LLMs find a shortcut to more morally correct output, rather than truly reducing the immorality stored in hidden states.Through empirical investigation with tasks of language generation and multi-choice question answering, we conclude: (i) LLMs exhibit good performance across both tasks, and self-correction instructions are particularly beneficial when the correct answer is already top-ranked; (ii) The morality levels in intermediate hidden states are strong indicators as to whether one instruction would be more effective than another; (iii) Based on our analysis of intermediate hidden states and task case studies of self-correction behaviors, we are first to propose the hypothesis that intrinsic moral self-correction is in fact superficial.","Liu, Guangliang, Mao, Haitao, Tang, Jiliang, Johnson, Kristen",,,Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis,,,10.18653/v1/2024.emnlp-main.918 , ,,"Large Language Models (LLMs) are capable of producing content that perpetuates stereotypes, discrimination, and toxicity.The recently proposed \textit{moral self-correction} is a computationally efficient method for reducing harmful content in the responses of LLMs. However, the process of how injecting self-correction instructions can modify the behavior of LLMs remains under-explored. In this paper, we explore the effectiveness of moral self-correction by answering three research questions: (1) In what scenarios does moral self-correction work? (2) What are the internal mechanisms of LLMs, e.g., hidden states, that are influenced by moral self-correction instructions? (3) Is intrinsic moral self-correction actually superficial in terms of reduced immorality in hidden states? We argue that self-correction can help LLMs find a shortcut to more morally correct output, rather than truly reducing the immorality stored in hidden states.Through empirical investigation with tasks of language generation and multi-choice question answering, we conclude: (i) LLMs exhibit good performance across both tasks, and self-correction instructions are particularly beneficial when the correct answer is already top-ranked; (ii) The morality levels in intermediate hidden states are strong indicators as to whether one instruction would be more effective than another; (iii) Based on our analysis of intermediate hidden states and task case studies of self-correction behaviors, we are first to propose the hypothesis that intrinsic moral self-correction is in fact superficial.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3482,"**Title**Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree

**Abstract**When annotators disagree, predicting the labels given by individual annotators can capture nuances overlooked by traditional label aggregation. We introduce three approaches to predict individual annotator ratings on the toxicity of text by incorporating individual annotator-specific information: a neural collaborative filtering (NCF) approach, an in-context learning (ICL) approach, and an intermediate embedding-based architecture. We also study the utility of demographic information for rating prediction. NCF showed limited utility; however, integrating annotator history, demographics, and survey information permits both the embedding-based architecture and ICL to substantially improve prediction accuracy, with the embedding-based architecture outperforming the other methods. We also find that, if demographics are predicted from survey information, using these imputed demographics as features performs comparably to using true demographic data. This suggests that demographics may not provide substantial information for modeling ratings beyond what is captured in survey responses. Our findings raise considerations about the relative utility of different types of annotator information and provide new approaches for modeling annotators in subjective NLP tasks.","Jaggi, Harbani, Coimbatore Murali, Kashyap, Fleisig, Eve, Biyik, Erdem",,,Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree,,,10.18653/v1/2024.emnlp-main.1221 , ,,"When annotators disagree, predicting the labels given by individual annotators can capture nuances overlooked by traditional label aggregation. We introduce three approaches to predict individual annotator ratings on the toxicity of text by incorporating individual annotator-specific information: a neural collaborative filtering (NCF) approach, an in-context learning (ICL) approach, and an intermediate embedding-based architecture. We also study the utility of demographic information for rating prediction. NCF showed limited utility; however, integrating annotator history, demographics, and survey information permits both the embedding-based architecture and ICL to substantially improve prediction accuracy, with the embedding-based architecture outperforming the other methods. We also find that, if demographics are predicted from survey information, using these imputed demographics as features performs comparably to using true demographic data. This suggests that demographics may not provide substantial information for modeling ratings beyond what is captured in survey responses. Our findings raise considerations about the relative utility of different types of annotator information and provide new approaches for modeling annotators in subjective NLP tasks.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detection,
3483,"**Title**{R}e{S}e{TOX}: Re-learning attention weights for toxicity mitigation in machine translation

**Abstract**Our proposed method, RESETOX (REdoSEarch if TOXic), addresses the issue ofNeural Machine Translation (NMT) gener-ating translation outputs that contain toxicwords not present in the input. The ob-jective is to mitigate the introduction oftoxic language without the need for re-training. In the case of identified addedtoxicity during the inference process, RE-SETOX dynamically adjusts the key-valueself-attention weights and re-evaluates thebeam search hypotheses. Experimental re-sults demonstrate that RESETOX achievesa remarkable 57{\%} reduction in added tox-icity while maintaining an average trans-lation quality of 99.5{\%} across 164 lan-guages. Our code is available at: https://github.com","Garc{\'i}a Gilabert, Javier, Escolano, Carlos, Costa-juss{\`a}, Marta",,,{R}e{S}e{TOX}: Re-learning attention weights for toxicity mitigation in machine translation,,, , ,,"Our proposed method, RESETOX (REdoSEarch if TOXic), addresses the issue ofNeural Machine Translation (NMT) gener-ating translation outputs that contain toxicwords not present in the input. The ob-jective is to mitigate the introduction oftoxic language without the need for re-training. In the case of identified addedtoxicity during the inference process, RE-SETOX dynamically adjusts the key-valueself-attention weights and re-evaluates thebeam search hypotheses. Experimental re-sults demonstrate that RESETOX achievesa remarkable 57{\%} reduction in added tox-icity while maintaining an average trans-lation quality of 99.5{\%} across 164 lan-guages. Our code is available at: https://github.com",,,,, ,  Proceedings of the 25th Annual Conference of the European Association for Machine Translation (Volume 1),,detox,
3486,"**Title**{G}rad{S}afe: Detecting Jailbreak Prompts for {LLM}s via Safety-Critical Gradient Analysis

**Abstract**Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for detecting jailbreak prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects jailbreak prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our method is grounded in a pivotal observation: the gradients of an LLM`s loss for jailbreak prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect jailbreak prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard{---}despite its extensive finetuning with a large dataset{---}in detecting jailbreak prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe.","Xie, Yueqi, Fang, Minghong, Pi, Renjie, Gong, Neil",,,{G}rad{S}afe: Detecting Jailbreak Prompts for {LLM}s via Safety-Critical Gradient Analysis,,,10.18653/v1/2024.acl-long.30 , ,,"Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for detecting jailbreak prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects jailbreak prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our method is grounded in a pivotal observation: the gradients of an LLM`s loss for jailbreak prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect jailbreak prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard{---}despite its extensive finetuning with a large dataset{---}in detecting jailbreak prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe.",,,,, ,  Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,detection,
3489,"**Title**Chat Vector: A Simple Approach to Equip {LLM}s with Instruction Following and Model Alignment in New Languages

**Abstract**Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of $\textit{chat vector}$ to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model`s weights, we can endow the model with chat capabilities in new languages without the need for further training.Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our approach, we extend our experiments to encompass various languages, base models, and chat vectors. The results underscore the chat vector`s simplicity, effectiveness, and wide applicability, making it a compelling solution for efficiently enabling conversational capabilities in pre-trained language models. Our code is available at https://github.com/aqweteddy/ChatVector.","Huang, Shih-Cheng, Li, Pin-Zu, Hsu, Yu-chi, Chen, Kuang-Ming, Lin, Yu Tung, Hsiao, Shih-Kai, Tsai, Richard, Lee, Hung-yi",,,Chat Vector: A Simple Approach to Equip {LLM}s with Instruction Following and Model Alignment in New Languages,,,10.18653/v1/2024.acl-long.590 , ,,"Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of $\textit{chat vector}$ to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model`s weights, we can endow the model with chat capabilities in new languages without the need for further training.Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our approach, we extend our experiments to encompass various languages, base models, and chat vectors. The results underscore the chat vector`s simplicity, effectiveness, and wide applicability, making it a compelling solution for efficiently enabling conversational capabilities in pre-trained language models. Our code is available at https://github.com/aqweteddy/ChatVector.",,,,, ,  Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,detox,
3491,"**Title**Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech

**Abstract**Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.","Plaza-del-arco, Flor Miriam, Nozza, Debora, Hovy, Dirk",,,Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech,,,10.18653/v1/2023.woah-1.6 , ,,"Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.",,,,, ,  The 7th Workshop on Online Abuse and Harms (WOAH),,detection,
3492,"**Title**Aporophobia: An Overlooked Type of Toxic Language Targeting the Poor

**Abstract**While many types of hate speech and online toxicity have been the focus of extensive research in NLP, toxic language stigmatizing poor people has been mostly disregarded. Yet, aporophobia, a social bias against the poor, is a common phenomenon online, which can be psychologically damaging as well as hindering poverty reduction policy measures. We demonstrate that aporophobic attitudes are indeed present in social media and argue that the existing NLP datasets and models are inadequate to effectively address this problem. Efforts toward designing specialized resources and novel socio-technical mechanisms for confronting aporophobia are needed.","Kiritchenko, Svetlana, Curto Rex, Georgina, Nejadgholi, Isar, Fraser, Kathleen C.",,,Aporophobia: An Overlooked Type of Toxic Language Targeting the Poor,,,10.18653/v1/2023.woah-1.12 , ,,"While many types of hate speech and online toxicity have been the focus of extensive research in NLP, toxic language stigmatizing poor people has been mostly disregarded. Yet, aporophobia, a social bias against the poor, is a common phenomenon online, which can be psychologically damaging as well as hindering poverty reduction policy measures. We demonstrate that aporophobic attitudes are indeed present in social media and argue that the existing NLP datasets and models are inadequate to effectively address this problem. Efforts toward designing specialized resources and novel socio-technical mechanisms for confronting aporophobia are needed.",,,,, ,  The 7th Workshop on Online Abuse and Harms (WOAH),,detection,
3495,"**Title**Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation

**Abstract**Natural language generation has witnessed significant advancements due to the training of large language models on vast internet-scale datasets. Despite these advancements, there exists a critical challenge: These models can inadvertently generate content that is toxic, inaccurate, and unhelpful, and existing automatic evaluation metrics often fall short of identifying these shortcomings. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of recent research that has leveraged human feedback to improve natural language generation. First, we introduce a taxonomy distilled from existing research to categorize and organize the varied forms of feedback. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which uses large language models to make judgments based on a set of principles and minimize the need for human intervention. We also release a website of this survey at feedback-gap-survey.info.","Fernandes, Patrick, Madaan, Aman, Liu, Emmy, Farinhas, Ant{\'o}nio, Martins, Pedro Henrique, Bertsch, Amanda, de Souza, Jos{\'e} G. C., Zhou, Shuyan, Wu, Tongshuang, Neubig, Graham, Martins, Andr{\'e} F. T.",,,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,,,10.1162/tacl_a_00626 , ,,"Natural language generation has witnessed significant advancements due to the training of large language models on vast internet-scale datasets. Despite these advancements, there exists a critical challenge: These models can inadvertently generate content that is toxic, inaccurate, and unhelpful, and existing automatic evaluation metrics often fall short of identifying these shortcomings. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of recent research that has leveraged human feedback to improve natural language generation. First, we introduce a taxonomy distilled from existing research to categorize and organize the varied forms of feedback. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which uses large language models to make judgments based on a set of principles and minimize the need for human intervention. We also release a website of this survey at feedback-gap-survey.info.",,,,, ,  ,,detox,
3496,"**Title**Detoxifying Online Discourse: A Guided Response Generation Approach for Reducing Toxicity in User-Generated Text

**Abstract**The expression of opinions, stances, and moral foundations on social media often coincide with toxic, divisive, or inflammatory language that can make constructive discourse across communities difficult. Natural language generation methods could provide a means to reframe or reword such expressions in a way that fosters more civil discourse, yet current Large Language Model (LLM) methods tend towards language that is too generic or formal to seem authentic for social media discussions. We present preliminary work on training LLMs to maintain authenticity while presenting a community`s ideas and values in a constructive, non-toxic manner.","Bose, Ritwik, Perera, Ian, Dorr, Bonnie",,,Detoxifying Online Discourse: A Guided Response Generation Approach for Reducing Toxicity in User-Generated Text,,,10.18653/v1/2023.sicon-1.2 , ,,"The expression of opinions, stances, and moral foundations on social media often coincide with toxic, divisive, or inflammatory language that can make constructive discourse across communities difficult. Natural language generation methods could provide a means to reframe or reword such expressions in a way that fosters more civil discourse, yet current Large Language Model (LLM) methods tend towards language that is too generic or formal to seem authentic for social media discussions. We present preliminary work on training LLMs to maintain authenticity while presenting a community`s ideas and values in a constructive, non-toxic manner.",,,,, ,  Proceedings of the First Workshop on Social Influence in Conversations (SICon 2023),,detox,
3497,"**Title**Probing {LLM}s for hate speech detection: strengths and vulnerabilities

**Abstract**Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). We select two large language models (GPT-3.5 and text-davinci) and three datasets - HateXplain, implicit hate and ToxicSpans. We find that on average including the target information in the pipeline improves the model performance substantially ($\sim20-30\%$) over the baseline across the datasets. There is also a considerable effect of adding the rationales/explanations into the pipeline ($\sim10-20\%$) over the baseline across the datasets. In addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute {\textquoteleft}jailbreak' prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.","Roy, Sarthak, Harshvardhan, Ashish, Mukherjee, Animesh, Saha, Punyajoy",,,Probing {LLM}s for hate speech detection: strengths and vulnerabilities,,,10.18653/v1/2023.findings-emnlp.407 , ,,"Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). We select two large language models (GPT-3.5 and text-davinci) and three datasets - HateXplain, implicit hate and ToxicSpans. We find that on average including the target information in the pipeline improves the model performance substantially ($\sim20-30\%$) over the baseline across the datasets. There is also a considerable effect of adding the rationales/explanations into the pipeline ($\sim10-20\%$) over the baseline across the datasets. In addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute {\textquoteleft}jailbreak' prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,detection,
3499,"**Title**Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems

**Abstract**Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. We define generic personas to represent demographic groups, such as {\textquotedblleft}an Asian person{\textquotedblright}, whereas specific personas may take the form of specific popular Asian names like {\textquotedblleft}Yumi{\textquotedblright}. While the adoption of personas enriches user experiences by making dialogue systems more engaging and approachable, it also casts a shadow of potential risk by exacerbating social biases within model responses, thereby causing societal harm through interactions with users. In this paper, we systematically study {\textquotedblleft}persona biases{\textquotedblright}, which we define to be the sensitivity of dialogue models' harmful behaviors contingent upon the personas they adopt. We categorize persona biases into biases in harmful expression and harmful agreement, and establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to investigate persona biases by experimenting with UNIVERSALPERSONA, a systematically constructed persona dataset encompassing various types of both generic and specific model personas. Through benchmarking on four different models- including Blender, ChatGPT, Alpaca, and Vicuna- our study uncovers significant persona biases in dialogue systems. Our findings also underscore the pressing need to revisit the use of personas in dialogue agents to ensure safe application.","Wan, Yixin, Zhao, Jieyu, Chadha, Aman, Peng, Nanyun, Chang, Kai-Wei",,,Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems,,,10.18653/v1/2023.findings-emnlp.648 , ,,"Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. We define generic personas to represent demographic groups, such as {\textquotedblleft}an Asian person{\textquotedblright}, whereas specific personas may take the form of specific popular Asian names like {\textquotedblleft}Yumi{\textquotedblright}. While the adoption of personas enriches user experiences by making dialogue systems more engaging and approachable, it also casts a shadow of potential risk by exacerbating social biases within model responses, thereby causing societal harm through interactions with users. In this paper, we systematically study {\textquotedblleft}persona biases{\textquotedblright}, which we define to be the sensitivity of dialogue models' harmful behaviors contingent upon the personas they adopt. We categorize persona biases into biases in harmful expression and harmful agreement, and establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to investigate persona biases by experimenting with UNIVERSALPERSONA, a systematically constructed persona dataset encompassing various types of both generic and specific model personas. Through benchmarking on four different models- including Blender, ChatGPT, Alpaca, and Vicuna- our study uncovers significant persona biases in dialogue systems. Our findings also underscore the pressing need to revisit the use of personas in dialogue agents to ensure safe application.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,evaluation,
3500,"**Title**{S}teer{LM}: Attribute Conditioned {SFT} as an (User-Steerable) Alternative to {RLHF}

**Abstract**Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B","Dong, Yi, Wang, Zhilin, Sreedhar, Makesh, Wu, Xianchao, Kuchaiev, Oleksii",,,{S}teer{LM}: Attribute Conditioned {SFT} as an (User-Steerable) Alternative to {RLHF},,,10.18653/v1/2023.findings-emnlp.754 , ,,"Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,detox,
3502,"**Title**{ROBBIE}: Robust Bias Evaluation of Large Generative Language Models

**Abstract**As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs.","Esiobu, David, Tan, Xiaoqing, Hosseini, Saghar, Ung, Megan, Zhang, Yuchen, Fernandes, Jude, Dwivedi-Yu, Jane, Presani, Eleonora, Williams, Adina, Smith, Eric",,,{ROBBIE}: Robust Bias Evaluation of Large Generative Language Models,,,10.18653/v1/2023.emnlp-main.230 , ,,"As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs.",,,,, ,  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,,evaluation,
3503,"**Title**{B}ias{X}: {\textquotedblleft}Thinking Slow{\textquotedblright} in Toxic Content Moderation with Explanations of Implied Social Biases

**Abstract**Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4{\%} on hard toxic examples) help less compared to expert-written human explanations (+7.2{\%}). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.","Zhang, Yiming, Nanduri, Sravani, Jiang, Liwei, Wu, Tongshuang, Sap, Maarten",,,{B}ias{X}: {\textquotedblleft}Thinking Slow{\textquotedblright} in Toxic Content Moderation with Explanations of Implied Social Biases,,,10.18653/v1/2023.emnlp-main.300 , ,,"Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4{\%} on hard toxic examples) help less compared to expert-written human explanations (+7.2{\%}). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.",,,,, ,  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,,detection#methodology,
3505,"**Title**On the Challenges of Using Black-Box {API}s for Toxicity Evaluation in Research

**Abstract**Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and call for a more structured approach to evaluating toxicity over time.","Pozzobon, Luiza, Ermis, Beyza, Lewis, Patrick, Hooker, Sara",,,On the Challenges of Using Black-Box {API}s for Toxicity Evaluation in Research,,,10.18653/v1/2023.emnlp-main.472 , ,,"Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and call for a more structured approach to evaluating toxicity over time.",,,,, ,  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,,detection,
3506,"**Title**Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model

**Abstract**While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models while incurring a minimal computational overhead.","Deng, Haikang, Raffel, Colin",,,Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model,,,10.18653/v1/2023.emnlp-main.721 , ,,"While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models while incurring a minimal computational overhead.",,,,, ,  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,,detox,
3507,"**Title**Toward Disambiguating the Definitions of Abusive, Offensive, Toxic, and Uncivil Comments

**Abstract**The definitions of abusive, offensive, toxic and uncivil comments used for annotating corpora for automated content moderation are highly intersected and researchers call for their disambiguation. We summarize the definitions of these terms as they appear in 23 papers across different fields. We compare examples given for uncivil, offensive, and toxic comments, attempting to foster more unified scientific resources. Additionally, we stress that the term incivility that frequently appears in social science literature has hardly been mentioned in the literature we analyzed that focuses on computational linguistics and natural language processing.","Pachinger, Pia, Hanbury, Allan, Neidhardt, Julia, Planitzer, Anna",,,"Toward Disambiguating the Definitions of Abusive, Offensive, Toxic, and Uncivil Comments",,,10.18653/v1/2023.c3nlp-1.11 , ,,"The definitions of abusive, offensive, toxic and uncivil comments used for annotating corpora for automated content moderation are highly intersected and researchers call for their disambiguation. We summarize the definitions of these terms as they appear in 23 papers across different fields. We compare examples given for uncivil, offensive, and toxic comments, attempting to foster more unified scientific resources. Additionally, we stress that the term incivility that frequently appears in social science literature has hardly been mentioned in the literature we analyzed that focuses on computational linguistics and natural language processing.",,,,, ,  Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP),,detection#methodology,
3509,"**Title**The uncivil empathy: Investigating the relation between empathy and toxicity in online mental health support forums

**Abstract**We explore the relationship between empathy and toxicity in the context of online mental health forums. Despite the common assumption of a negative correlation between these concepts, it has not been empirically examined. We augment the EPITOME mental health empathy dataset with toxicity labels using two widely employed toxic/harmful content detection APIs: Perspective API and OpenAI moderation API. We find a notable presence of toxic/harmful content (17.77{\%}) within empathetic responses, and only a very weak negative correlation between the two variables. Qualitative analysis revealed contributions labeled as empathetic often contain harmful content such as promotion of suicidal ideas. Our results highlight the need for reevaluating empathy independently from toxicity in future research and encourage a reconsideration of empathy`s role in natural language generation and evaluation.","Chen, Ming-Bin, Lau, Jey Han, Frermann, Lea",,,The uncivil empathy: Investigating the relation between empathy and toxicity in online mental health support forums,,, , ,,"We explore the relationship between empathy and toxicity in the context of online mental health forums. Despite the common assumption of a negative correlation between these concepts, it has not been empirically examined. We augment the EPITOME mental health empathy dataset with toxicity labels using two widely employed toxic/harmful content detection APIs: Perspective API and OpenAI moderation API. We find a notable presence of toxic/harmful content (17.77{\%}) within empathetic responses, and only a very weak negative correlation between the two variables. Qualitative analysis revealed contributions labeled as empathetic often contain harmful content such as promotion of suicidal ideas. Our results highlight the need for reevaluating empathy independently from toxicity in future research and encourage a reconsideration of empathy`s role in natural language generation and evaluation.",,,,, ,  Proceedings of the 21st Annual Workshop of the Australasian Language Technology Association,,detection,
3510,"**Title**On Second Thought, Let`s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning

**Abstract**Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model`s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.","Shaikh, Omar, Zhang, Hongxin, Held, William, Bernstein, Michael, Yang, Diyi",,,"On Second Thought, Let`s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",,,10.18653/v1/2023.acl-long.244 , ,,"Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model`s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.",,,,, ,  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,evaluation,
3513,"**Title**Towards Building a Robust Toxicity Predictor

**Abstract**Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, {\textbackslash}texttt{\{}ToxicTrap{\}}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. {\textbackslash}texttt{\{}ToxicTrap{\}} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal function designs allow {\textbackslash}texttt{\{}ToxicTrap{\}} to identify weaknesses in both multiclass and multilabel toxic language detectors. Our empirical results show that SOTA toxicity text classifiers are indeed vulnerable to the proposed attacks, attaining over 98{\textbackslash}{\%} attack success rates in multilabel cases. We also show how a vanilla adversarial training and its improved version can help increase robustness of a toxicity detector even against unseen attacks.","Bespalov, Dmitriy, Bhabesh, Sourav, Xiang, Yi, Zhou, Liutong, Qi, Yanjun",,,Towards Building a Robust Toxicity Predictor,,,10.18653/v1/2023.acl-industry.56 , ,,"Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, {\textbackslash}texttt{\{}ToxicTrap{\}}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. {\textbackslash}texttt{\{}ToxicTrap{\}} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal function designs allow {\textbackslash}texttt{\{}ToxicTrap{\}} to identify weaknesses in both multiclass and multilabel toxic language detectors. Our empirical results show that SOTA toxicity text classifiers are indeed vulnerable to the proposed attacks, attaining over 98{\textbackslash}{\%} attack success rates in multilabel cases. We also show how a vanilla adversarial training and its improved version can help increase robustness of a toxicity detector even against unseen attacks.",,,,, ,  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track),,detection,
3514,"**Title**The subtle language of exclusion: Identifying the Toxic Speech of Trans-exclusionary Radical Feminists

**Abstract**Toxic language can take many forms, from explicit hate speech to more subtle microaggressions. Within this space, models identifying transphobic language have largely focused on overt forms. However, a more pernicious and subtle source of transphobic comments comes in the form of statements made by Trans-exclusionary Radical Feminists (TERFs); these statements often appear seemingly-positive and promote women`s causes and issues, while simultaneously denying the inclusion of transgender women as women. Here, we introduce two models to mitigate this antisocial behavior. The first model identifies TERF users in social media, recognizing that these users are a main source of transphobic material that enters mainstream discussion and whom other users may not desire to engage with in good faith. The second model tackles the harder task of recognizing the masked rhetoric of TERF messages and introduces a new dataset to support this task. Finally, we discuss the ethics of deploying these models to mitigate the harm of this language, arguing for a balanced approach that allows for restorative interactions.","Lu, Christina, Jurgens, David",,,The subtle language of exclusion: Identifying the Toxic Speech of Trans-exclusionary Radical Feminists,,,10.18653/v1/2022.woah-1.8 , ,,"Toxic language can take many forms, from explicit hate speech to more subtle microaggressions. Within this space, models identifying transphobic language have largely focused on overt forms. However, a more pernicious and subtle source of transphobic comments comes in the form of statements made by Trans-exclusionary Radical Feminists (TERFs); these statements often appear seemingly-positive and promote women`s causes and issues, while simultaneously denying the inclusion of transgender women as women. Here, we introduce two models to mitigate this antisocial behavior. The first model identifies TERF users in social media, recognizing that these users are a main source of transphobic material that enters mainstream discussion and whom other users may not desire to engage with in good faith. The second model tackles the harder task of recognizing the masked rhetoric of TERF messages and introduces a new dataset to support this task. Finally, we discuss the ethics of deploying these models to mitigate the harm of this language, arguing for a balanced approach that allows for restorative interactions.",,,,, ,  Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH),,detection,
3515,"**Title**Lost in Distillation: A Case Study in Toxicity Modeling

**Abstract**In an era of increasingly large pre-trained language models, knowledge distillation is a powerful tool for transferring information from a large model to a smaller one. In particular, distillation is of tremendous benefit when it comes to real-world constraints such as serving latency or serving at scale. However, a loss of robustness in language understanding may be hidden in the process and not immediately revealed when looking at high-level evaluation metrics. In this work, we investigate the hidden costs: what is {\textquotedblleft}lost in distillation{\textquotedblright}, especially in regards to identity-based model bias using the case study of toxicity modeling. With reproducible models using open source training sets, we investigate models distilled from a BERT teacher baseline. Using both open source and proprietary big data models, we investigate these hidden performance costs.","Chvasta, Alyssa, Lees, Alyssa, Sorensen, Jeffrey, Vasserman, Lucy, Goyal, Nitesh",,,Lost in Distillation: A Case Study in Toxicity Modeling,,,10.18653/v1/2022.woah-1.9 , ,,"In an era of increasingly large pre-trained language models, knowledge distillation is a powerful tool for transferring information from a large model to a smaller one. In particular, distillation is of tremendous benefit when it comes to real-world constraints such as serving latency or serving at scale. However, a loss of robustness in language understanding may be hidden in the process and not immediately revealed when looking at high-level evaluation metrics. In this work, we investigate the hidden costs: what is {\textquotedblleft}lost in distillation{\textquotedblright}, especially in regards to identity-based model bias using the case study of toxicity modeling. With reproducible models using open source training sets, we investigate models distilled from a BERT teacher baseline. Using both open source and proprietary big data models, we investigate these hidden performance costs.",,,,, ,  Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH),,detection,
3516,"**Title**Flexible text generation for counterfactual fairness probing

**Abstract**A common approach for testing fairness issues in text-based classifiers is through the use of counterfactuals: does the classifier output change if a sensitive attribute in the input is changed? Existing counterfactual generation methods typically rely on wordlists or templates, producing simple counterfactuals that fail to take into account grammar, context, or subtle sensitive attribute references, and could miss issues that the wordlist creators had not considered. In this paper, we introduce a task for generating counterfactuals that overcomes these shortcomings, and demonstrate how large language models (LLMs) can be leveraged to accomplish this task. We show that this LLM-based method can produce complex counterfactuals that existing methods cannot, comparing the performance of various counterfactual generation methods on the Civil Comments dataset and showing their value in evaluating a toxicity classifier.","Fryer, Zee, Axelrod, Vera, Packer, Ben, Beutel, Alex, Chen, Jilin, Webster, Kellie",,,Flexible text generation for counterfactual fairness probing,,,10.18653/v1/2022.woah-1.20 , ,,"A common approach for testing fairness issues in text-based classifiers is through the use of counterfactuals: does the classifier output change if a sensitive attribute in the input is changed? Existing counterfactual generation methods typically rely on wordlists or templates, producing simple counterfactuals that fail to take into account grammar, context, or subtle sensitive attribute references, and could miss issues that the wordlist creators had not considered. In this paper, we introduce a task for generating counterfactuals that overcomes these shortcomings, and demonstrate how large language models (LLMs) can be leveraged to accomplish this task. We show that this LLM-based method can produce complex counterfactuals that existing methods cannot, comparing the performance of various counterfactual generation methods on the Civil Comments dataset and showing their value in evaluating a toxicity classifier.",,,,, ,  Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH),,detox,
3517,"**Title**Annotating Targets of Toxic Language at the Span Level

**Abstract**In this paper, we discuss an interpretable framework to integrate toxic language annotations. Most data sets address only one aspect of the complex relationship in toxic communication and are inconsistent with each other. Enriching annotations with more details and information is however of great importance in order to develop high-performing and comprehensive explainable language models. Such systems should recognize and interpret both expressions that are toxic as well as expressions that make reference to specific targets to combat toxic language. We therefore created a crowd-annotation task to mark the spans of words that refer to target communities as an extension of the HateXplain data set. We present a quantitative and qualitative analysis of the annotations. We also fine-tuned RoBERTa-base on our data and experimented with different data thresholds to measure their effect on the classification. The F1-score of our best model on the test set is 79{\%}. The annotations are freely available and can be combined with the existing HateXplain annotation to build richer and more complete models.","Barbarestani, Baran, Maks, Isa, Vossen, Piek",,,Annotating Targets of Toxic Language at the Span Level,,, , ,,"In this paper, we discuss an interpretable framework to integrate toxic language annotations. Most data sets address only one aspect of the complex relationship in toxic communication and are inconsistent with each other. Enriching annotations with more details and information is however of great importance in order to develop high-performing and comprehensive explainable language models. Such systems should recognize and interpret both expressions that are toxic as well as expressions that make reference to specific targets to combat toxic language. We therefore created a crowd-annotation task to mark the spans of words that refer to target communities as an extension of the HateXplain data set. We present a quantitative and qualitative analysis of the annotations. We also fine-tuned RoBERTa-base on our data and experimented with different data thresholds to measure their effect on the classification. The F1-score of our best model on the test set is 79{\%}. The annotations are freely available and can be combined with the existing HateXplain annotation to build richer and more complete models.",,,,, ,"  Proceedings of the Third Workshop on Threat, Aggression and Cyberbullying (TRAC 2022)",,detection,
3518,"**Title**Towards Toxic Positivity Detection

**Abstract**Over the past few years, there has been a growing concern around toxic positivity on social media which is a phenomenon where positivity is used to minimize one`s emotional experience. In this paper, we create a dataset for toxic positivity classification from Twitter and an inspirational quote website. We then perform benchmarking experiments using various text classification models and show the suitability of these models for the task. We achieved a macro F1 score of 0.71 and a weighted F1 score of 0.85 by using an ensemble model. To the best of our knowledge, our dataset is the first such dataset created.","Upadhyay, Ishan Sanjeev, Srivatsa, KV Aditya, Mamidi, Radhika",,,Towards Toxic Positivity Detection,,,10.18653/v1/2022.socialnlp-1.7 , ,,"Over the past few years, there has been a growing concern around toxic positivity on social media which is a phenomenon where positivity is used to minimize one`s emotional experience. In this paper, we create a dataset for toxic positivity classification from Twitter and an inspirational quote website. We then perform benchmarking experiments using various text classification models and show the suitability of these models for the task. We achieved a macro F1 score of 0.71 and a weighted F1 score of 0.85 by using an ensemble model. To the best of our knowledge, our dataset is the first such dataset created.",,,,, ,  Proceedings of the Tenth International Workshop on Natural Language Processing for Social Media,,Gen_dataset#detection,
3519,"**Title**Explaining Toxic Text via Knowledge Enhanced Text Generation

**Abstract**Warning: This paper contains content that is offensive and may be upsetting. Biased or toxic speech can be harmful to various demographic groups. Therefore, it is not only important for models to detect these speech, but to also output explanations of why a given text is toxic. Previous literature has mostly focused on classifying and detecting toxic speech, and existing efforts on explaining stereotypes in toxic speech mainly use standard text generation approaches, resulting in generic and repetitive explanations. Building on these prior works, we introduce a novel knowledge-informed encoder-decoder framework to utilize multiple knowledge sources to generate implications of biased text. Experiments show that our knowledge informed models outperform prior state-of-the-art models significantly, and can generate detailed explanations of stereotypes in toxic speech compared to baselines, both quantitatively and qualitatively.","Sridhar, Rohit, Yang, Diyi",,,Explaining Toxic Text via Knowledge Enhanced Text Generation,,,10.18653/v1/2022.naacl-main.59 , ,,"Warning: This paper contains content that is offensive and may be upsetting. Biased or toxic speech can be harmful to various demographic groups. Therefore, it is not only important for models to detect these speech, but to also output explanations of why a given text is toxic. Previous literature has mostly focused on classifying and detecting toxic speech, and existing efforts on explaining stereotypes in toxic speech mainly use standard text generation approaches, resulting in generic and repetitive explanations. Building on these prior works, we introduce a novel knowledge-informed encoder-decoder framework to utilize multiple knowledge sources to generate implications of biased text. Experiments show that our knowledge informed models outperform prior state-of-the-art models significantly, and can generate detailed explanations of stereotypes in toxic speech compared to baselines, both quantitatively and qualitatively.",,,,, ,  Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,,detection,
3521,"**Title**Data Integration for Toxic Comment Classification: Making More Than 40 Datasets Easily Accessible in One Unified Format

**Abstract**With the rise of research on toxic comment classification, more and more annotated datasets have been released. The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings. Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset. They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects. To overcome these issues, we present a collection of more than thirty datasets in the form of a software tool that automatizes downloading and processing of the data and presents them in a unified data format that also offers a mapping of compatible class labels. Another advantage of that tool is that it gives an overview of properties of available datasets, such as different languages, platforms, and class labels to make it easier to select suitable training and test data.","Risch, Julian, Schmidt, Philipp, Krestel, Ralf",,,Data Integration for Toxic Comment Classification: Making More Than 40 Datasets Easily Accessible in One Unified Format,,,10.18653/v1/2021.woah-1.17 , ,,"With the rise of research on toxic comment classification, more and more annotated datasets have been released. The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings. Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset. They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects. To overcome these issues, we present a collection of more than thirty datasets in the form of a software tool that automatizes downloading and processing of the data and presents them in a unified data format that also offers a mapping of compatible class labels. Another advantage of that tool is that it gives an overview of properties of available datasets, such as different languages, platforms, and class labels to make it easier to select suitable training and test data.",,,,, ,  Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),,detection,
3523,"**Title**{S}em{E}val-2021 Task 5: Toxic Spans Detection

**Abstract**The Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts. The task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers. It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions.","Pavlopoulos, John, Sorensen, Jeffrey, Laugier, L{\'e}o, Androutsopoulos, Ion",,,{S}em{E}val-2021 Task 5: Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.6 , ,,"The Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts. The task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers. It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3524,"**Title**{IITK}@Detox at {S}em{E}val-2021 Task 5: Semi-Supervised Learning and Dice Loss for Toxic Spans Detection

**Abstract**In this work, we present our approach and findings for SemEval-2021 Task 5 - Toxic Spans Detection. The task`s main aim was to identify spans to which a given text`s toxicity could be attributed. The task is challenging mainly due to two constraints: the small training dataset and imbalanced class distribution. Our paper investigates two techniques, semi-supervised learning and learning with Self-Adjusting Dice Loss, for tackling these challenges. Our submitted system (ranked ninth on the leader board) consisted of an ensemble of various pre-trained Transformer Language Models trained using either of the above-proposed techniques.","Bansal, Archit, Kaushik, Abhay, Modi, Ashutosh",,,{IITK}@Detox at {S}em{E}val-2021 Task 5: Semi-Supervised Learning and Dice Loss for Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.24 , ,,"In this work, we present our approach and findings for SemEval-2021 Task 5 - Toxic Spans Detection. The task`s main aim was to identify spans to which a given text`s toxicity could be attributed. The task is challenging mainly due to two constraints: the small training dataset and imbalanced class distribution. Our paper investigates two techniques, semi-supervised learning and learning with Self-Adjusting Dice Loss, for tackling these challenges. Our submitted system (ranked ninth on the leader board) consisted of an ensemble of various pre-trained Transformer Language Models trained using either of the above-proposed techniques.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3525,"**Title**{U}ni{P}arma at {S}em{E}val-2021 Task 5: Toxic Spans Detection Using {C}haracter{BERT} and Bag-of-Words Model

**Abstract**With the ever-increasing availability of digital information, toxic content is also on the rise. Therefore, the detection of this type of language is of paramount importance. We tackle this problem utilizing a combination of a state-of-the-art pre-trained language model (CharacterBERT) and a traditional bag-of-words technique. Since the content is full of toxic words that have not been written according to their dictionary spelling, attendance to individual characters is crucial. Therefore, we use CharacterBERT to extract features based on the word characters. It consists of a CharacterCNN module that learns character embeddings from the context. These are, then, fed into the well-known BERT architecture. The bag-of-words method, on the other hand, further improves upon that by making sure that some frequently used toxic words get labeled accordingly. With a {\ensuremath{\sim}}4 percent difference from the first team, our system ranked 36 th in the competition. The code is available for further research and reproduction of the results.","Karimi, Akbar, Rossi, Leonardo, Prati, Andrea",,,{U}ni{P}arma at {S}em{E}val-2021 Task 5: Toxic Spans Detection Using {C}haracter{BERT} and Bag-of-Words Model,,,10.18653/v1/2021.semeval-1.25 , ,,"With the ever-increasing availability of digital information, toxic content is also on the rise. Therefore, the detection of this type of language is of paramount importance. We tackle this problem utilizing a combination of a state-of-the-art pre-trained language model (CharacterBERT) and a traditional bag-of-words technique. Since the content is full of toxic words that have not been written according to their dictionary spelling, attendance to individual characters is crucial. Therefore, we use CharacterBERT to extract features based on the word characters. It consists of a CharacterCNN module that learns character embeddings from the context. These are, then, fed into the well-known BERT architecture. The bag-of-words method, on the other hand, further improves upon that by making sure that some frequently used toxic words get labeled accordingly. With a {\ensuremath{\sim}}4 percent difference from the first team, our system ranked 36 th in the competition. The code is available for further research and reproduction of the results.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3526,"**Title**{U}o{B} at {S}em{E}val-2021 Task 5: Extending Pre-Trained Language Models to Include Task and Domain-Specific Information for Toxic Span Prediction

**Abstract**Toxicity is pervasive in social media and poses a major threat to the health of online communities. The recent introduction of pre-trained language models, which have achieved state-of-the-art results in many NLP tasks, has transformed the way in which we approach natural language processing. However, the inherent nature of pre-training means that they are unlikely to capture task-specific statistical information or learn domain-specific knowledge. Additionally, most implementations of these models typically do not employ conditional random fields, a method for simultaneous token classification. We show that these modifications can improve model performance on the Toxic Spans Detection task at SemEval-2021 to achieve a score within 4 percentage points of the top performing team.","Yan, Erik, Tayyar Madabushi, Harish",,,{U}o{B} at {S}em{E}val-2021 Task 5: Extending Pre-Trained Language Models to Include Task and Domain-Specific Information for Toxic Span Prediction,,,10.18653/v1/2021.semeval-1.28 , ,,"Toxicity is pervasive in social media and poses a major threat to the health of online communities. The recent introduction of pre-trained language models, which have achieved state-of-the-art results in many NLP tasks, has transformed the way in which we approach natural language processing. However, the inherent nature of pre-training means that they are unlikely to capture task-specific statistical information or learn domain-specific knowledge. Additionally, most implementations of these models typically do not employ conditional random fields, a method for simultaneous token classification. We show that these modifications can improve model performance on the Toxic Spans Detection task at SemEval-2021 to achieve a score within 4 percentage points of the top performing team.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3527,"**Title**{M}ed{AI} at {S}em{E}val-2021 Task 5: Start-to-end Tagging Framework for Toxic Spans Detection

**Abstract**This paper describes the system submitted to SemEval 2021 Task 5: Toxic Spans Detection. The task concerns evaluating systems that detect the spans that make a text toxic when detecting such spans are possible. To address the possibly multi-span detection problem, we develop a start-to-end tagging framework on top of RoBERTa based language model. Besides, we design a custom loss function that takes distance into account. In comparison to other participating teams, our system has achieved 69.03{\%} F1 score, which is slightly lower (-1.8 and -1.73) than the top 1(70.83{\%}) and top 2 (70.77{\%}), respectively.","Wang, Zhen, Fan, Hongjie, Liu, Junfei",,,{M}ed{AI} at {S}em{E}val-2021 Task 5: Start-to-end Tagging Framework for Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.30 , ,,"This paper describes the system submitted to SemEval 2021 Task 5: Toxic Spans Detection. The task concerns evaluating systems that detect the spans that make a text toxic when detecting such spans are possible. To address the possibly multi-span detection problem, we develop a start-to-end tagging framework on top of RoBERTa based language model. Besides, we design a custom loss function that takes distance into account. In comparison to other participating teams, our system has achieved 69.03{\%} F1 score, which is slightly lower (-1.8 and -1.73) than the top 1(70.83{\%}) and top 2 (70.77{\%}), respectively.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3528,"**Title**{H}amilton{D}inggg at {S}em{E}val-2021 Task 5: Investigating Toxic Span Detection using {R}o{BERT}a Pre-training

**Abstract**This paper presents our system submission to task 5: Toxic Spans Detection of the SemEval-2021 competition. The competition aims at detecting the spans that make a toxic span toxic. In this paper, we demonstrate our system for detecting toxic spans, which includes expanding the toxic training set with Local Interpretable Model-Agnostic Explanations (LIME), fine-tuning RoBERTa model for detection, and error analysis. We found that feeding the model with an expanded training set using Reddit comments of polarized-toxicity and labeling with LIME on top of logistic regression classification could help RoBERTa more accurately learn to recognize toxic spans. We achieved a span-level F1 score of 0.6715 on the testing phase. Our quantitative and qualitative results show that the predictions from our system could be a good supplement to the gold training set`s annotations.","Ding, Huiyang, Jurgens, David",,,{H}amilton{D}inggg at {S}em{E}val-2021 Task 5: Investigating Toxic Span Detection using {R}o{BERT}a Pre-training,,,10.18653/v1/2021.semeval-1.31 , ,,"This paper presents our system submission to task 5: Toxic Spans Detection of the SemEval-2021 competition. The competition aims at detecting the spans that make a toxic span toxic. In this paper, we demonstrate our system for detecting toxic spans, which includes expanding the toxic training set with Local Interpretable Model-Agnostic Explanations (LIME), fine-tuning RoBERTa model for detection, and error analysis. We found that feeding the model with an expanded training set using Reddit comments of polarized-toxicity and labeling with LIME on top of logistic regression classification could help RoBERTa more accurately learn to recognize toxic spans. We achieved a span-level F1 score of 0.6715 on the testing phase. Our quantitative and qualitative results show that the predictions from our system could be a good supplement to the gold training set`s annotations.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3529,"**Title**{HITSZ}-{HLT} at {S}em{E}val-2021 Task 5: Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection

**Abstract**This paper presents the winning system that participated in SemEval-2021 Task 5: Toxic Spans Detection. This task aims to locate those spans that attribute to the text`s toxicity within a text, which is crucial for semi-automated moderation in online discussions. We formalize this task as the Sequence Labeling (SL) problem and the Span Boundary Detection (SBD) problem separately and employ three state-of-the-art models. Next, we integrate predictions of these models to produce a more credible and complement result. Our system achieves a char-level score of 70.83{\%}, ranking 1/91. In addition, we also explore the lexicon-based method, which is strongly interpretable and flexible in practice.","Zhu, Qinglin, Lin, Zijie, Zhang, Yice, Sun, Jingyi, Li, Xiang, Lin, Qihui, Dang, Yixue, Xu, Ruifeng",,,{HITSZ}-{HLT} at {S}em{E}val-2021 Task 5: Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection,,,10.18653/v1/2021.semeval-1.63 , ,,"This paper presents the winning system that participated in SemEval-2021 Task 5: Toxic Spans Detection. This task aims to locate those spans that attribute to the text`s toxicity within a text, which is crucial for semi-automated moderation in online discussions. We formalize this task as the Sequence Labeling (SL) problem and the Span Boundary Detection (SBD) problem separately and employ three state-of-the-art models. Next, we integrate predictions of these models to produce a more credible and complement result. Our system achieves a char-level score of 70.83{\%}, ranking 1/91. In addition, we also explore the lexicon-based method, which is strongly interpretable and flexible in practice.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3530,"**Title**{YNU}-{HPCC} at {S}em{E}val-2021 Task 5: Using a Transformer-based Model with Auxiliary Information for Toxic Span Detection

**Abstract**Toxic span detection requires the detection of spans that make a text toxic instead of simply classifying the text. In this paper, a transformer-based model with auxiliary information is proposed for SemEval-2021 Task 5. The proposed model was implemented based on the BERT-CRF architecture. It consists of three parts: a transformer-based model that can obtain the token representation, an auxiliary information module that combines features from different layers, and an output layer used for the classification. Various BERT-based models, such as BERT, ALBERT, RoBERTa, and XLNET, were used to learn contextual representations. The predictions of these models were assembled to improve the sequence labeling tasks by using a voting strategy. Experimental results showed that the introduced auxiliary information can improve the performance of toxic spans detection. The proposed model ranked 5th of 91 in the competition. The code of this study is available at \url{https://github.com/Chenrj233/semeval2021_task5}","Chen, Ruijun, Wang, Jin, Zhang, Xuejie",,,{YNU}-{HPCC} at {S}em{E}val-2021 Task 5: Using a Transformer-based Model with Auxiliary Information for Toxic Span Detection,,,10.18653/v1/2021.semeval-1.112 , ,,"Toxic span detection requires the detection of spans that make a text toxic instead of simply classifying the text. In this paper, a transformer-based model with auxiliary information is proposed for SemEval-2021 Task 5. The proposed model was implemented based on the BERT-CRF architecture. It consists of three parts: a transformer-based model that can obtain the token representation, an auxiliary information module that combines features from different layers, and an output layer used for the classification. Various BERT-based models, such as BERT, ALBERT, RoBERTa, and XLNET, were used to learn contextual representations. The predictions of these models were assembled to improve the sequence labeling tasks by using a voting strategy. Experimental results showed that the introduced auxiliary information can improve the performance of toxic spans detection. The proposed model ranked 5th of 91 in the competition. The code of this study is available at \url{https://github.com/Chenrj233/semeval2021_task5}",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3531,"**Title**{UIT}-{ISE}-{NLP} at {S}em{E}val-2021 Task 5: Toxic Spans Detection with {B}i{LSTM}-{CRF} and {T}oxic{BERT} Comment Classification

**Abstract**We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This task aims to build a model for identifying toxic words in whole posts. We use the BiLSTM-CRF model combining with ToxicBERT Classification to train the detection model for identifying toxic words in posts. Our model achieves 62.23{\%} by F1-score on the Toxic Spans Detection task.","Luu, Son T., Nguyen, Ngan",,,{UIT}-{ISE}-{NLP} at {S}em{E}val-2021 Task 5: Toxic Spans Detection with {B}i{LSTM}-{CRF} and {T}oxic{BERT} Comment Classification,,,10.18653/v1/2021.semeval-1.113 , ,,We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This task aims to build a model for identifying toxic words in whole posts. We use the BiLSTM-CRF model combining with ToxicBERT Classification to train the detection model for identifying toxic words in posts. Our model achieves 62.23{\%} by F1-score on the Toxic Spans Detection task.,,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3532,"**Title**{G}olden{W}ind at {S}em{E}val-2021 Task 5: Orthrus - An Ensemble Approach to Identify Toxicity

**Abstract**Many new developments to detect and mitigate toxicity are currently being evaluated. We are particularly interested in the correlation between toxicity and the emotions expressed in online posts. While toxicity may be disguised by amending the wording of posts, emotions will not. Therefore, we describe here an ensemble method to identify toxicity and classify the emotions expressed on a corpus of annotated posts published by Task 5 of SemEval 2021{--}our analysis shows that the majority of such posts express anger, sadness and fear. Our method to identify toxicity combines a lexicon-based approach, which on its own achieves an F1 score of 61.07{\%}, with a supervised learning approach, which on its own achieves an F1 score of 60{\%}. When both methods are combined, the ensemble achieves an F1 score of 66.37{\%}.","Palomino, Marco, Grad, Dawid, Bedwell, James",,,{G}olden{W}ind at {S}em{E}val-2021 Task 5: Orthrus - An Ensemble Approach to Identify Toxicity,,,10.18653/v1/2021.semeval-1.115 , ,,"Many new developments to detect and mitigate toxicity are currently being evaluated. We are particularly interested in the correlation between toxicity and the emotions expressed in online posts. While toxicity may be disguised by amending the wording of posts, emotions will not. Therefore, we describe here an ensemble method to identify toxicity and classify the emotions expressed on a corpus of annotated posts published by Task 5 of SemEval 2021{--}our analysis shows that the majority of such posts express anger, sadness and fear. Our method to identify toxicity combines a lexicon-based approach, which on its own achieves an F1 score of 61.07{\%}, with a supervised learning approach, which on its own achieves an F1 score of 60{\%}. When both methods are combined, the ensemble achieves an F1 score of 66.37{\%}.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3533,"**Title**{LISAC} {FSDM} {USMBA} at {S}em{E}val-2021 Task 5: Tackling Toxic Spans Detection Challenge with Supervised {S}pan{BERT}-based Model and Unsupervised {LIME}-based Model

**Abstract**Toxic spans detection is an emerging challenge that aims to find toxic spans within a toxic text. In this paper, we describe our solutions to tackle toxic spans detection. The first solution, which follows a supervised approach, is based on SpanBERT model. This latter is intended to better embed and predict spans of text. The second solution, which adopts an unsupervised approach, combines linear support vector machine with the Local Interpretable Model-Agnostic Explanations (LIME). This last is used to interpret predictions of learning-based models. Our supervised model outperformed the unsupervised model and achieved the f-score of 67,84{\%} (ranked 22/85) in Task 5 at SemEval-2021: Toxic Spans Detection.","Benlahbib, Abdessamad, Alami, Ahmed, Alami, Hamza",,,{LISAC} {FSDM} {USMBA} at {S}em{E}val-2021 Task 5: Tackling Toxic Spans Detection Challenge with Supervised {S}pan{BERT}-based Model and Unsupervised {LIME}-based Model,,,10.18653/v1/2021.semeval-1.116 , ,,"Toxic spans detection is an emerging challenge that aims to find toxic spans within a toxic text. In this paper, we describe our solutions to tackle toxic spans detection. The first solution, which follows a supervised approach, is based on SpanBERT model. This latter is intended to better embed and predict spans of text. The second solution, which adopts an unsupervised approach, combines linear support vector machine with the Local Interpretable Model-Agnostic Explanations (LIME). This last is used to interpret predictions of learning-based models. Our supervised model outperformed the unsupervised model and achieved the f-score of 67,84{\%} (ranked 22/85) in Task 5 at SemEval-2021: Toxic Spans Detection.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3534,"**Title**{HITMI}{\&}{T} at {S}em{E}val-2021 Task 5: Integrating Transformer and {CRF} for Toxic Spans Detection

**Abstract**This paper introduces our system at SemEval-2021 Task 5: Toxic Spans Detection. The task aims to accurately locate toxic spans within a text. Using BIO tagging scheme, we model the task as a token-level sequence labeling task. Our system uses a single model built on the model of multi-layer bidirectional transformer encoder. And we introduce conditional random field (CRF) to make the model learn the constraints between tags. We use ERNIE as pre-trained model, which is more suitable for the task accroding to our experiments. In addition, we use adversarial training with the fast gradient method (FGM) to improve the robustness of the system. Our system obtains 69.85{\%} F1 score, ranking 3rd for the official evaluation.","Wang, Chenyi, Liu, Tianshu, Zhao, Tiejun",,,{HITMI}{\&}{T} at {S}em{E}val-2021 Task 5: Integrating Transformer and {CRF} for Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.117 , ,,"This paper introduces our system at SemEval-2021 Task 5: Toxic Spans Detection. The task aims to accurately locate toxic spans within a text. Using BIO tagging scheme, we model the task as a token-level sequence labeling task. Our system uses a single model built on the model of multi-layer bidirectional transformer encoder. And we introduce conditional random field (CRF) to make the model learn the constraints between tags. We use ERNIE as pre-trained model, which is more suitable for the task accroding to our experiments. In addition, we use adversarial training with the fast gradient method (FGM) to improve the robustness of the system. Our system obtains 69.85{\%} F1 score, ranking 3rd for the official evaluation.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3535,"**Title**{AS}tar{T}wice at {S}em{E}val-2021 Task 5: Toxic Span Detection Using {R}o{BERT}a-{CRF}, Domain Specific Pre-Training and Self-Training

**Abstract**This paper describes our contribution to SemEval-2021 Task 5: Toxic Spans Detection. Our solution is built upon RoBERTa language model and Conditional Random Fields (CRF). We pre-trained RoBERTa on Civil Comments dataset, enabling it to create better contextual representation for this task. We also employed the semi-supervised learning technique of self-training, which allowed us to extend our training dataset. In addition to these, we also identified some pre-processing steps that significantly improved our F1 score. Our proposed system achieved a rank of 41 with an F1 score of 66.16{\%}.","Suman, Thakur Ashutosh, Jain, Abhinav",,,"{AS}tar{T}wice at {S}em{E}val-2021 Task 5: Toxic Span Detection Using {R}o{BERT}a-{CRF}, Domain Specific Pre-Training and Self-Training",,,10.18653/v1/2021.semeval-1.118 , ,,"This paper describes our contribution to SemEval-2021 Task 5: Toxic Spans Detection. Our solution is built upon RoBERTa language model and Conditional Random Fields (CRF). We pre-trained RoBERTa on Civil Comments dataset, enabling it to create better contextual representation for this task. We also employed the semi-supervised learning technique of self-training, which allowed us to extend our training dataset. In addition to these, we also identified some pre-processing steps that significantly improved our F1 score. Our proposed system achieved a rank of 41 with an F1 score of 66.16{\%}.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3536,"**Title**{NLP}{\_}{UIOWA} at {S}emeval-2021 Task 5: Transferring Toxic Sets to Tag Toxic Spans

**Abstract**We leverage a BLSTM with attention to identify toxic spans in texts. We explore different dimensions which affect the model`s performance. The first dimension explored is the toxic set the model is trained on. Besides the provided dataset, we explore the transferability of 5 different toxic related sets, including offensive, toxic, abusive, and hate sets. We find that the solely offensive set shows the highest promise of transferability. The second dimension we explore is methodology, including leveraging attention, employing a greedy remove method, using a frequency ratio, and examining hybrid combinations of multiple methods. We conduct an error analysis to examine which types of toxic spans were missed and which were wrongly inferred as toxic along with the main reasons why they occurred. Finally, we extend our method via ensembles, which achieves our highest F1 score of 55.1.","Rusert, Jonathan",,,{NLP}{\_}{UIOWA} at {S}emeval-2021 Task 5: Transferring Toxic Sets to Tag Toxic Spans,,,10.18653/v1/2021.semeval-1.119 , ,,"We leverage a BLSTM with attention to identify toxic spans in texts. We explore different dimensions which affect the model`s performance. The first dimension explored is the toxic set the model is trained on. Besides the provided dataset, we explore the transferability of 5 different toxic related sets, including offensive, toxic, abusive, and hate sets. We find that the solely offensive set shows the highest promise of transferability. The second dimension we explore is methodology, including leveraging attention, employing a greedy remove method, using a frequency ratio, and examining hybrid combinations of multiple methods. We conduct an error analysis to examine which types of toxic spans were missed and which were wrongly inferred as toxic along with the main reasons why they occurred. Finally, we extend our method via ensembles, which achieves our highest F1 score of 55.1.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3537,"**Title**{UA}ntwerp at {S}em{E}val-2021 Task 5: Spans are Spans, stacking a binary word level approach to toxic span detection

**Abstract**This paper describes the system developed by the Antwerp Centre for Digital humanities and literary Criticism [UAntwerp] for toxic span detection. We used a stacked generalisation ensemble of five component models, with two distinct interpretations of the task. Two models attempted to predict binary word toxicity based on ngram sequences, whilst 3 categorical span based models were trained to predict toxic token labels based on complete sequence tokens. The five models' predictions were ensembled within an LSTM model. As well as describing the system, we perform error analysis to explore model performance in relation to textual features. The system described in this paper scored 0.6755 and ranked 26th.","Burtenshaw, Ben, Kestemont, Mike",,,"{UA}ntwerp at {S}em{E}val-2021 Task 5: Spans are Spans, stacking a binary word level approach to toxic span detection",,,10.18653/v1/2021.semeval-1.121 , ,,"This paper describes the system developed by the Antwerp Centre for Digital humanities and literary Criticism [UAntwerp] for toxic span detection. We used a stacked generalisation ensemble of five component models, with two distinct interpretations of the task. Two models attempted to predict binary word toxicity based on ngram sequences, whilst 3 categorical span based models were trained to predict toxic token labels based on complete sequence tokens. The five models' predictions were ensembled within an LSTM model. As well as describing the system, we perform error analysis to explore model performance in relation to textual features. The system described in this paper scored 0.6755 and ranked 26th.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3538,"**Title**hub at {S}em{E}val-2021 Task 5: Toxic Span Detection Based on Word-Level Classification

**Abstract**This article introduces the system description of the hub team, which explains the related work and experimental results of our team`s participation in SemEval 2021 Task 5: Toxic Spans Detection. The data for this shared task comes from some posts on the Internet. The task goal is to identify the toxic content contained in these text data. We need to find the span of the toxic text in the text data as accurately as possible. In the same post, the toxic text may be one paragraph or multiple paragraphs. Our team uses a classification scheme based on word-level to accomplish this task. The system we used to submit the results is ALBERT+BILSTM+CRF. The result evaluation index of the task submission is the F1 score, and the final score of the prediction result of the test set submitted by our team is 0.6640226029.","Huang, Bo, Bai, Yang, Zhou, Xiaobing",,,hub at {S}em{E}val-2021 Task 5: Toxic Span Detection Based on Word-Level Classification,,,10.18653/v1/2021.semeval-1.122 , ,,"This article introduces the system description of the hub team, which explains the related work and experimental results of our team`s participation in SemEval 2021 Task 5: Toxic Spans Detection. The data for this shared task comes from some posts on the Internet. The task goal is to identify the toxic content contained in these text data. We need to find the span of the toxic text in the text data as accurately as possible. In the same post, the toxic text may be one paragraph or multiple paragraphs. Our team uses a classification scheme based on word-level to accomplish this task. The system we used to submit the results is ALBERT+BILSTM+CRF. The result evaluation index of the task submission is the F1 score, and the final score of the prediction result of the test set submitted by our team is 0.6640226029.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3539,"**Title**{MIPT}-{NSU}-{UTMN} at {S}em{E}val-2021 Task 5: Ensembling Learning with Pre-trained Language Models for Toxic Spans Detection

**Abstract**This paper describes our system for SemEval-2021 Task 5 on Toxic Spans Detection. We developed ensemble models using BERT-based neural architectures and post-processing to combine tokens into spans. We evaluated several pre-trained language models using various ensemble techniques for toxic span identification and achieved sizable improvements over our baseline fine-tuned BERT models. Finally, our system obtained a F1-score of 67.55{\%} on test data.","Kotyushev, Mikhail, Glazkova, Anna, Morozov, Dmitry",,,{MIPT}-{NSU}-{UTMN} at {S}em{E}val-2021 Task 5: Ensembling Learning with Pre-trained Language Models for Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.124 , ,,"This paper describes our system for SemEval-2021 Task 5 on Toxic Spans Detection. We developed ensemble models using BERT-based neural architectures and post-processing to combine tokens into spans. We evaluated several pre-trained language models using various ensemble techniques for toxic span identification and achieved sizable improvements over our baseline fine-tuned BERT models. Finally, our system obtained a F1-score of 67.55{\%} on test data.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3540,"**Title**{UIT}-E10dot3 at {S}em{E}val-2021 Task 5: Toxic Spans Detection with Named Entity Recognition and Question-Answering Approaches

**Abstract**The increment of toxic comments on online space is causing tremendous effects on other vulnerable users. For this reason, considerable efforts are made to deal with this, and SemEval-2021 Task 5: Toxic Spans Detection is one of those. This task asks competitors to extract spans that have toxicity from the given texts, and we have done several analyses to understand its structure before doing experiments. We solve this task by two approaches, Named Entity Recognition with spaCy`s library and Question-Answering with RoBERTa combining with ToxicBERT, and the former gains the highest F1-score of 66.99{\%}.","Gia Hoang, Phu, Thanh Nguyen, Luan, Nguyen, Kiet",,,{UIT}-E10dot3 at {S}em{E}val-2021 Task 5: Toxic Spans Detection with Named Entity Recognition and Question-Answering Approaches,,,10.18653/v1/2021.semeval-1.125 , ,,"The increment of toxic comments on online space is causing tremendous effects on other vulnerable users. For this reason, considerable efforts are made to deal with this, and SemEval-2021 Task 5: Toxic Spans Detection is one of those. This task asks competitors to extract spans that have toxicity from the given texts, and we have done several analyses to understand its structure before doing experiments. We solve this task by two approaches, Named Entity Recognition with spaCy`s library and Question-Answering with RoBERTa combining with ToxicBERT, and the former gains the highest F1-score of 66.99{\%}.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3541,"**Title**{S}koltech{NLP} at {S}em{E}val-2021 Task 5: Leveraging Sentence-level Pre-training for Toxic Span Detection

**Abstract**This work describes the participation of the Skoltech NLP group team (Sk) in the Toxic Spans Detection task at SemEval-2021. The goal of the task is to identify the most toxic fragments of a given sentence, which is a binary sequence tagging problem. We show that fine-tuning a RoBERTa model for this problem is a strong baseline. This baseline can be further improved by pre-training the RoBERTa model on a large dataset labeled for toxicity at the sentence level. While our solution scored among the top 20{\%} participating models, it is only 2 points below the best result. This suggests the viability of our approach.","Dale, David, Markov, Igor, Logacheva, Varvara, Kozlova, Olga, Semenov, Nikita, Panchenko, Alexander",,,{S}koltech{NLP} at {S}em{E}val-2021 Task 5: Leveraging Sentence-level Pre-training for Toxic Span Detection,,,10.18653/v1/2021.semeval-1.126 , ,,"This work describes the participation of the Skoltech NLP group team (Sk) in the Toxic Spans Detection task at SemEval-2021. The goal of the task is to identify the most toxic fragments of a given sentence, which is a binary sequence tagging problem. We show that fine-tuning a RoBERTa model for this problem is a strong baseline. This baseline can be further improved by pre-training the RoBERTa model on a large dataset labeled for toxicity at the sentence level. While our solution scored among the top 20{\%} participating models, it is only 2 points below the best result. This suggests the viability of our approach.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3542,"**Title**Entity at {S}em{E}val-2021 Task 5: Weakly Supervised Token Labelling for Toxic Spans Detection

**Abstract**Detection of toxic spans - detecting toxicity of contents in the granularity of tokens - is crucial for effective moderation of online discussions. The baseline approach for this problem using the transformer model is to add a token classification head to the language model and fine-tune the layers with the token labeled dataset. One of the limitations of such a baseline approach is the scarcity of labeled data. To improve the results, We studied leveraging existing public datasets for a related but different task of entire comment/sentence classification. We propose two approaches: the first approach fine-tunes transformer models that are pre-trained on sentence classification samples. In the second approach, we perform weak supervision with soft attention to learn token level labels from sentence labels. Our experiments show improvements in the F1 score over the baseline approach. The implementation has been released publicly.","Jain, Vaibhav, Naghshnejad, Mina",,,Entity at {S}em{E}val-2021 Task 5: Weakly Supervised Token Labelling for Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.127 , ,,"Detection of toxic spans - detecting toxicity of contents in the granularity of tokens - is crucial for effective moderation of online discussions. The baseline approach for this problem using the transformer model is to add a token classification head to the language model and fine-tune the layers with the token labeled dataset. One of the limitations of such a baseline approach is the scarcity of labeled data. To improve the results, We studied leveraging existing public datasets for a related but different task of entire comment/sentence classification. We propose two approaches: the first approach fine-tunes transformer models that are pre-trained on sentence classification samples. In the second approach, we perform weak supervision with soft attention to learn token level labels from sentence labels. Our experiments show improvements in the F1 score over the baseline approach. The implementation has been released publicly.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3543,"**Title**{B}ennett{NLP} at {S}em{E}val-2021 Task 5: Toxic Spans Detection using Stacked Embedding Powered Toxic Entity Recognizer

**Abstract**With the rapid growth in technology, social media activity has seen a boom across all age groups. It is humanly impossible to check all the tweets, comments and status manually whether they follow proper community guidelines. A lot of toxicity is regularly posted on these social media platforms. This research aims to find toxic words in a sentence so that a healthy social community is built across the globe and the users receive censored content with specific warnings and facts. To solve this challenging problem, authors have combined concepts of Linked List for pre-processing and then used the idea of stacked embeddings like BERT Embeddings, Flair Embeddings and Word2Vec on the flairNLP framework to get the desired results. F1 metric was used to evaluate the model. The authors were able to produce a 0.74 F1 score on their test set.","Kataria, Harsh, Gupta, Ambuje, Mishra, Vipul",,,{B}ennett{NLP} at {S}em{E}val-2021 Task 5: Toxic Spans Detection using Stacked Embedding Powered Toxic Entity Recognizer,,,10.18653/v1/2021.semeval-1.128 , ,,"With the rapid growth in technology, social media activity has seen a boom across all age groups. It is humanly impossible to check all the tweets, comments and status manually whether they follow proper community guidelines. A lot of toxicity is regularly posted on these social media platforms. This research aims to find toxic words in a sentence so that a healthy social community is built across the globe and the users receive censored content with specific warnings and facts. To solve this challenging problem, authors have combined concepts of Linked List for pre-processing and then used the idea of stacked embeddings like BERT Embeddings, Flair Embeddings and Word2Vec on the flairNLP framework to get the desired results. F1 metric was used to evaluate the model. The authors were able to produce a 0.74 F1 score on their test set.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3544,"**Title**{U}o{T}-{UWF}-{P}art{AI} at {S}em{E}val-2021 Task 5: Self Attention Based {B}i-{GRU} with Multi-Embedding Representation for Toxicity Highlighter

**Abstract**Toxic Spans Detection(TSD) task is defined as highlighting spans that make a text toxic. Many works have been done to classify a given comment or document as toxic or non-toxic. However, none of those proposed models work at the token level. In this paper, we propose a self-attention-based bidirectional gated recurrent unit(BiGRU) with a multi-embedding representation of the tokens. Our proposed model enriches the representation by a combination of GPT-2, GloVe, and RoBERTa embeddings, which led to promising results. Experimental results show that our proposed approach is very effective in detecting span tokens.","Babaei Giglou, Hamed, Rahgooy, Taher, Rahgouy, Mostafa, Razmara, Jafar",,,{U}o{T}-{UWF}-{P}art{AI} at {S}em{E}val-2021 Task 5: Self Attention Based {B}i-{GRU} with Multi-Embedding Representation for Toxicity Highlighter,,,10.18653/v1/2021.semeval-1.129 , ,,"Toxic Spans Detection(TSD) task is defined as highlighting spans that make a text toxic. Many works have been done to classify a given comment or document as toxic or non-toxic. However, none of those proposed models work at the token level. In this paper, we propose a self-attention-based bidirectional gated recurrent unit(BiGRU) with a multi-embedding representation of the tokens. Our proposed model enriches the representation by a combination of GPT-2, GloVe, and RoBERTa embeddings, which led to promising results. Experimental results show that our proposed approach is very effective in detecting span tokens.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3545,"**Title**{Y}oung{S}heldon at {S}em{E}val-2021 Task 5: Fine-tuning Pre-trained Language Models for Toxic Spans Detection using Token classification Objective

**Abstract**In this paper, we describe our system used for SemEval 2021 Task 5: Toxic Spans Detection. Our proposed system approaches the problem as a token classification task. We trained our model to find toxic words and concatenate their spans to predict the toxic spans within a sentence. We fine-tuned Pre-trained Language Models (PLMs) for identifying the toxic words. For fine-tuning, we stacked the classification layer on top of the PLM features of each word to classify if it is toxic or not. PLMs are pre-trained using different objectives and their performance may differ on downstream tasks. We, therefore, compare the performance of BERT, ELECTRA, RoBERTa, XLM-RoBERTa, T5, XLNet, and MPNet for identifying toxic spans within a sentence. Our best performing system used RoBERTa. It performed well, achieving an F1 score of 0.6841 and secured a rank of 16 on the official leaderboard.","Sharma, Mayukh, Kandasamy, Ilanthenral, Vasantha, W.b.",,,{Y}oung{S}heldon at {S}em{E}val-2021 Task 5: Fine-tuning Pre-trained Language Models for Toxic Spans Detection using Token classification Objective,,,10.18653/v1/2021.semeval-1.130 , ,,"In this paper, we describe our system used for SemEval 2021 Task 5: Toxic Spans Detection. Our proposed system approaches the problem as a token classification task. We trained our model to find toxic words and concatenate their spans to predict the toxic spans within a sentence. We fine-tuned Pre-trained Language Models (PLMs) for identifying the toxic words. For fine-tuning, we stacked the classification layer on top of the PLM features of each word to classify if it is toxic or not. PLMs are pre-trained using different objectives and their performance may differ on downstream tasks. We, therefore, compare the performance of BERT, ELECTRA, RoBERTa, XLM-RoBERTa, T5, XLNet, and MPNet for identifying toxic spans within a sentence. Our best performing system used RoBERTa. It performed well, achieving an F1 score of 0.6841 and secured a rank of 16 on the official leaderboard.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3546,"**Title**{HLE}-{UPC} at {S}em{E}val-2021 Task 5: Multi-Depth {D}istil{BERT} for Toxic Spans Detection

**Abstract**This paper presents our submission to SemEval-2021 Task 5: Toxic Spans Detection. The purpose of this task is to detect the spans that make a text toxic, which is a complex labour for several reasons. Firstly, because of the intrinsic subjectivity of toxicity, and secondly, due to toxicity not always coming from single words like insults or offends, but sometimes from whole expressions formed by words that may not be toxic individually. Following this idea of focusing on both single words and multi-word expressions, we study the impact of using a multi-depth DistilBERT model, which uses embeddings from different layers to estimate the final per-token toxicity. Our quantitative results show that using information from multiple depths boosts the performance of the model. Finally, we also analyze our best model qualitatively.","Palliser-Sans, Rafel, Rial-Farr{\`a}s, Albert",,,{HLE}-{UPC} at {S}em{E}val-2021 Task 5: Multi-Depth {D}istil{BERT} for Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.131 , ,,"This paper presents our submission to SemEval-2021 Task 5: Toxic Spans Detection. The purpose of this task is to detect the spans that make a text toxic, which is a complex labour for several reasons. Firstly, because of the intrinsic subjectivity of toxicity, and secondly, due to toxicity not always coming from single words like insults or offends, but sometimes from whole expressions formed by words that may not be toxic individually. Following this idea of focusing on both single words and multi-word expressions, we study the impact of using a multi-depth DistilBERT model, which uses embeddings from different layers to estimate the final per-token toxicity. Our quantitative results show that using information from multiple depths boosts the performance of the model. Finally, we also analyze our best model qualitatively.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3547,"**Title**{SRPOL} {DIALOGUE} {SYSTEMS} at {S}em{E}val-2021 Task 5: Automatic Generation of Training Data for Toxic Spans Detection

**Abstract**This paper presents a system used for SemEval-2021 Task 5: Toxic Spans Detection. Our system is an ensemble of BERT-based models for binary word classification, trained on a dataset extended by toxic comments modified and generated by two language models. For the toxic word classification, the prediction threshold value was optimized separately for every comment, in order to maximize the expected F1 value.","Sat{\l}awa, Micha{\l}, Zam{\l}y{\'n}ska, Katarzyna, Piersa, Jaros{\l}aw, Kolis, Joanna, Firl{\k{a}}g, Klaudia, Beksa, Katarzyna, Bordzicka, Zuzanna, Goltz, Christian, Bujnowski, Pawe{\l}, Andruszkiewicz, Piotr",,,{SRPOL} {DIALOGUE} {SYSTEMS} at {S}em{E}val-2021 Task 5: Automatic Generation of Training Data for Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.133 , ,,"This paper presents a system used for SemEval-2021 Task 5: Toxic Spans Detection. Our system is an ensemble of BERT-based models for binary word classification, trained on a dataset extended by toxic comments modified and generated by two language models. For the toxic word classification, the prediction threshold value was optimized separately for every comment, in order to maximize the expected F1 value.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3548,"**Title**{SINAI} at {S}em{E}val-2021 Task 5: Combining Embeddings in a {B}i{LSTM}-{CRF} model for Toxic Spans Detection

**Abstract**This paper describes the participation of SINAI team at Task 5: Toxic Spans Detection which consists of identifying spans that make a text toxic. Although several resources and systems have been developed so far in the context of offensive language, both annotation and tasks have mainly focused on classifying whether a text is offensive or not. However, detecting toxic spans is crucial to identify why a text is toxic and can assist human moderators to locate this type of content on social media. In order to accomplish the task, we follow a deep learning-based approach using a Bidirectional variant of a Long Short Term Memory network along with a stacked Conditional Random Field decoding layer (BiLSTM-CRF). Specifically, we test the performance of the combination of different pre-trained word embeddings for recognizing toxic entities in text. The results show that the combination of word embeddings helps in detecting offensive content. Our team ranks 29th out of 91 participants.","Plaza-del-Arco, Flor Miriam, L{\'o}pez-{\'U}beda, Pilar, Ure{\~n}a-L{\'o}pez, L. Alfonso, Mart{\'i}n-Valdivia, M. Teresa",,,{SINAI} at {S}em{E}val-2021 Task 5: Combining Embeddings in a {B}i{LSTM}-{CRF} model for Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.134 , ,,"This paper describes the participation of SINAI team at Task 5: Toxic Spans Detection which consists of identifying spans that make a text toxic. Although several resources and systems have been developed so far in the context of offensive language, both annotation and tasks have mainly focused on classifying whether a text is offensive or not. However, detecting toxic spans is crucial to identify why a text is toxic and can assist human moderators to locate this type of content on social media. In order to accomplish the task, we follow a deep learning-based approach using a Bidirectional variant of a Long Short Term Memory network along with a stacked Conditional Random Field decoding layer (BiLSTM-CRF). Specifically, we test the performance of the combination of different pre-trained word embeddings for recognizing toxic entities in text. The results show that the combination of word embeddings helps in detecting offensive content. Our team ranks 29th out of 91 participants.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3549,"**Title**{CSECU}-{DSG} at {S}em{E}val-2021 Task 5: Leveraging Ensemble of Sequence Tagging Models for Toxic Spans Detection

**Abstract**The upsurge of prolific blogging and microblogging platforms enabled the abusers to spread negativity and threats greater than ever. Detecting the toxic portions substantially aids to moderate or exclude the abusive parts for maintaining sound online platforms. This paper describes our participation in the SemEval 2021 toxic span detection task. The task requires detecting spans that convey toxic remarks from the given text. We explore an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model to identify the toxic spans. Finally, a majority voting ensemble method is used to determine the unified toxic spans. Experimental results depict the competitive performance of our model among the participants.","Hossain, Tashin, Naim, Jannatun, Tasneem, Fareen, Tasnia, Radiathun, Chy, Abu Nowshed",,,{CSECU}-{DSG} at {S}em{E}val-2021 Task 5: Leveraging Ensemble of Sequence Tagging Models for Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.135 , ,,"The upsurge of prolific blogging and microblogging platforms enabled the abusers to spread negativity and threats greater than ever. Detecting the toxic portions substantially aids to moderate or exclude the abusive parts for maintaining sound online platforms. This paper describes our participation in the SemEval 2021 toxic span detection task. The task requires detecting spans that convey toxic remarks from the given text. We explore an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model to identify the toxic spans. Finally, a majority voting ensemble method is used to determine the unified toxic spans. Experimental results depict the competitive performance of our model among the participants.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3550,"**Title**macech at {S}em{E}val-2021 Task 5: Toxic Spans Detection

**Abstract**Toxic language is often present in online forums, especially when politics and other polarizing topics arise, and can lead to people becoming discouraged from joining or continuing conversations. In this paper, we use data consisting of comments with the indices of toxic text labelled to train an RNN to deter-mine which parts of the comments make them toxic, which could aid online moderators. We compare results using both the original dataset and an augmented set, as well as GRU versus LSTM RNN models.","Cech, Maggie",,,macech at {S}em{E}val-2021 Task 5: Toxic Spans Detection,,,10.18653/v1/2021.semeval-1.137 , ,,"Toxic language is often present in online forums, especially when politics and other polarizing topics arise, and can lead to people becoming discouraged from joining or continuing conversations. In this paper, we use data consisting of comments with the indices of toxic text labelled to train an RNN to deter-mine which parts of the comments make them toxic, which could aid online moderators. We compare results using both the original dataset and an augmented set, as well as GRU versus LSTM RNN models.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3551,"**Title**{LZ}1904 at {S}em{E}val-2021 Task 5: {B}i-{LSTM}-{CRF} for Toxic Span Detection using Pretrained Word Embedding

**Abstract**Recurrent Neural Networks (RNN) have been widely used in various Natural Language Processing (NLP) tasks such as text classification, sequence tagging, and machine translation. Long Short Term Memory (LSTM), a special unit of RNN, has the benefit of memorizing past and even future information in a sentence (especially for bidirectional LSTM). In the shared task of detecting spans which make texts toxic, we first apply pretrained word embedding (GloVe) to generate the word vectors after tokenization. And then we construct Bidirectional Long Short Term Memory-Conditional Random Field (Bi-LSTM-CRF) model by Baidu research to predict whether each word in the sentence is toxic or not. We tune hyperparameters of dropout rate, number of LSTM units, embedding size with 10 epochs and choose the best epoch with validation recall. Our model achieves an F1 score of 66.99 percent in test dataset.","Zou, Liang, Li, Wen",,,{LZ}1904 at {S}em{E}val-2021 Task 5: {B}i-{LSTM}-{CRF} for Toxic Span Detection using Pretrained Word Embedding,,,10.18653/v1/2021.semeval-1.138 , ,,"Recurrent Neural Networks (RNN) have been widely used in various Natural Language Processing (NLP) tasks such as text classification, sequence tagging, and machine translation. Long Short Term Memory (LSTM), a special unit of RNN, has the benefit of memorizing past and even future information in a sentence (especially for bidirectional LSTM). In the shared task of detecting spans which make texts toxic, we first apply pretrained word embedding (GloVe) to generate the word vectors after tokenization. And then we construct Bidirectional Long Short Term Memory-Conditional Random Field (Bi-LSTM-CRF) model by Baidu research to predict whether each word in the sentence is toxic or not. We tune hyperparameters of dropout rate, number of LSTM units, embedding size with 10 epochs and choose the best epoch with validation recall. Our model achieves an F1 score of 66.99 percent in test dataset.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3552,"**Title**Capturing Covertly Toxic Speech via Crowdsourcing

**Abstract**We study the task of labeling covert or veiled toxicity in online conversations. Prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions. Our investigations further underscore the difficulty in parsing such labels reliably from raters via crowdsourcing. We introduce an initial dataset, COVERTTOXICITY, which aims to identify and categorize such comments from a refined rater template. Finally, we fine-tune a comment-domain BERT model to classify covertly offensive comments and compare against existing baselines.","Lees, Alyssa, Borkan, Daniel, Kivlichan, Ian, Nario, Jorge, Goyal, Tesh",,,Capturing Covertly Toxic Speech via Crowdsourcing,,, , ,,"We study the task of labeling covert or veiled toxicity in online conversations. Prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions. Our investigations further underscore the difficulty in parsing such labels reliably from raters via crowdsourcing. We introduce an initial dataset, COVERTTOXICITY, which aims to identify and categorize such comments from a refined rater template. Finally, we fine-tune a comment-domain BERT model to classify covertly offensive comments and compare against existing baselines.",,,,, ,  Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing,,detection,
3562,"**Title**Say {\textquoteleft}{YES}' to Positivity: Detecting Toxic Language in Workplace Communications

**Abstract**Workplace communication (e.g. email, chat, etc.) is a central part of enterprise productivity. Healthy conversations are crucial for creating an inclusive environment and maintaining harmony in an organization. Toxic communications at the workplace can negatively impact overall job satisfaction and are often subtle, hidden, or demonstrate human biases. The linguistic subtlety of mild yet hurtful conversations has made it difficult for researchers to quantify and extract toxic conversations automatically. While offensive language or hate speech has been extensively studied in social communities, there has been little work studying toxic communication in emails. Specifically, the lack of corpus, sparsity of toxicity in enterprise emails, and well-defined criteria for annotating toxic conversations have prevented researchers from addressing the problem at scale. We take the first step towards studying toxicity in workplace emails by providing (1) a general and computationally viable taxonomy to study toxic language at the workplace (2) a dataset to study toxic language at the workplace based on the taxonomy and (3) analysis on why offensive language and hate-speech datasets are not suitable to detect workplace toxicity.","Bhat, Meghana Moorthy, Hosseini, Saghar, Awadallah, Ahmed Hassan, Bennett, Paul, Li, Weisheng",,,Say {\textquoteleft}{YES}' to Positivity: Detecting Toxic Language in Workplace Communications,,,10.18653/v1/2021.findings-emnlp.173 , ,,"Workplace communication (e.g. email, chat, etc.) is a central part of enterprise productivity. Healthy conversations are crucial for creating an inclusive environment and maintaining harmony in an organization. Toxic communications at the workplace can negatively impact overall job satisfaction and are often subtle, hidden, or demonstrate human biases. The linguistic subtlety of mild yet hurtful conversations has made it difficult for researchers to quantify and extract toxic conversations automatically. While offensive language or hate speech has been extensively studied in social communities, there has been little work studying toxic communication in emails. Specifically, the lack of corpus, sparsity of toxicity in enterprise emails, and well-defined criteria for annotating toxic conversations have prevented researchers from addressing the problem at scale. We take the first step towards studying toxicity in workplace emails by providing (1) a general and computationally viable taxonomy to study toxic language at the workplace (2) a dataset to study toxic language at the workplace based on the taxonomy and (3) analysis on why offensive language and hate-speech datasets are not suitable to detect workplace toxicity.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2021,,detection,
3563,"**Title**{CONDA}: a {CON}textual Dual-Annotated dataset for in-game toxicity understanding and detection

**Abstract**Traditional toxicity detection models have fo-
    cused on the single utterance level without
    deeper understanding of context. We introduce
   CONDA, a new dataset for in-game toxic lan-
    guage detection enabling joint intent classi-
    cation and slot lling analysis, which is the
    core task of Natural Language Understanding
   (NLU). The dataset consists of 45K utterances
    from 12K conversations from the chat logs of
   1.9K completed Dota 2 matches. We propose a
    robust dual semantic-level toxicity framework,
   which handles utterance and token-level pat-
     terns, and rich contextual chatting history. Ac-
    companying the dataset is a thorough in-game
     toxicity analysis, which provides comprehen-
    sive understanding of context at utterance, to-
    ken, and dual levels.  Inspired by NLU, we
    also apply its metrics to the toxicity detection
    tasks for assessing toxicity and game-specic
    aspects. We evaluate strong NLU models on
   CONDA, providing ne-grained results for dif-
    ferent intent classes and slot classes. Further-
    more, we examine the coverage of toxicity na-
    ture in our dataset by comparing it with other
     toxicity datasets.1

1  Introduction

As the popularity of multi-player online games has
grown, the phenomenon of in-game toxic behav-
ior has taken root within them. Toxic behavior
is strongly present in recent online games and is
problematic to the gaming industry (Adinolf and
Turkay, 2018). For instance, 74% of US players of
such games report harassment with 65% experienc-
ing severe harassment. (ADL, 2019).
   In the past few years, Natural Language Process-
ing (NLP) researchers have proposed several on-
line game/community toxicity analysis frameworks

    Corresponding author (caren.han@sydney.edu.au)
   1The  dataset and  lexicons  are  available  at  https://
github.com/usydnlp.Figure 1: An example intent/slot annotation from the
CONDA (CONtextual Dual-Annotated) dataset.



(Kwak et al., 2015; Murnion et al., 2018; Wang
et al., 2020) and datasets (Martens et al., 2015;
Stoop et al., 2019). However, existing datasets
(1) focus only on the single utterance level with-
out deeper understanding of context in the whole
conversation/chat, and (2) do not explicitly use se-
mantic clues from the words within the utterance.
  The chat in online games and communities is
similar in nature to spoken language, an area stud-
ied by Natural Language Understanding (NLU).
NLU research aims to best represent human com-
munication by extracting semantic structure in the
form of intent and slot analysis. Intent detection is
the classication of the desired outcome of an utter-
ance (or sentence), and slot lling is the labeling of
each token (or word) in the utterance with the type
of semantic information it carries. In recent litera-
ture, these two tasks are trained jointly to capture
synergies between them, and these jointly trained
models give better results (Zhang et al., 2019b).
Furthermore, researchers have made available joint
task datasets that contain the context of a multi-turn
conversation (Budzianowski et al., 2018; Schuster
et al., 2019)
  Inspired by this NLU research progress, we2406","Weld, Henry, Huang, Guanghao, Lee, Jean, Zhang, Tongshu, Wang, Kunze, Guo, Xinghong, Long, Siqu, Poon, Josiah, Han, Caren",,,{CONDA}: a {CON}textual Dual-Annotated dataset for in-game toxicity understanding and detection,,,10.18653/v1/2021.findings-acl.213 , ,,"Traditional toxicity detection models have fo-
    cused on the single utterance level without
    deeper understanding of context. We introduce
   CONDA, a new dataset for in-game toxic lan-
    guage detection enabling joint intent classi-
    cation and slot lling analysis, which is the
    core task of Natural Language Understanding
   (NLU). The dataset consists of 45K utterances
    from 12K conversations from the chat logs of
   1.9K completed Dota 2 matches. We propose a
    robust dual semantic-level toxicity framework,
   which handles utterance and token-level pat-
     terns, and rich contextual chatting history. Ac-
    companying the dataset is a thorough in-game
     toxicity analysis, which provides comprehen-
    sive understanding of context at utterance, to-
    ken, and dual levels.  Inspired by NLU, we
    also apply its metrics to the toxicity detection
    tasks for assessing toxicity and game-specic
    aspects. We evaluate strong NLU models on
   CONDA, providing ne-grained results for dif-
    ferent intent classes and slot classes. Further-
    more, we examine the coverage of toxicity na-
    ture in our dataset by comparing it with other
     toxicity datasets.1

1  Introduction

As the popularity of multi-player online games has
grown, the phenomenon of in-game toxic behav-
ior has taken root within them. Toxic behavior
is strongly present in recent online games and is
problematic to the gaming industry (Adinolf and
Turkay, 2018). For instance, 74% of US players of
such games report harassment with 65% experienc-
ing severe harassment. (ADL, 2019).
   In the past few years, Natural Language Process-
ing (NLP) researchers have proposed several on-
line game/community toxicity analysis frameworks

    Corresponding author (caren.han@sydney.edu.au)
   1The  dataset and  lexicons  are  available  at  https://
github.com/usydnlp.Figure 1: An example intent/slot annotation from the
CONDA (CONtextual Dual-Annotated) dataset.



(Kwak et al., 2015; Murnion et al., 2018; Wang
et al., 2020) and datasets (Martens et al., 2015;
Stoop et al., 2019). However, existing datasets
(1) focus only on the single utterance level with-
out deeper understanding of context in the whole
conversation/chat, and (2) do not explicitly use se-
mantic clues from the words within the utterance.
  The chat in online games and communities is
similar in nature to spoken language, an area stud-
ied by Natural Language Understanding (NLU).
NLU research aims to best represent human com-
munication by extracting semantic structure in the
form of intent and slot analysis. Intent detection is
the classication of the desired outcome of an utter-
ance (or sentence), and slot lling is the labeling of
each token (or word) in the utterance with the type
of semantic information it carries. In recent litera-
ture, these two tasks are trained jointly to capture
synergies between them, and these jointly trained
models give better results (Zhang et al., 2019b).
Furthermore, researchers have made available joint
task datasets that contain the context of a multi-turn
conversation (Budzianowski et al., 2018; Schuster
et al., 2019)
  Inspired by this NLU research progress, we2406",,,,, ,  Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,,detection,
3564,"**Title**From Toxicity in Online Comments to Incivility in {A}merican News: Proceed with Caution

**Abstract**The ability to quantify incivility online, in news and in congressional debates is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility.","Hede, Anushree, Agarwal, Oshin, Lu, Linda, Mutz, Diana C., Nenkova, Ani",,,From Toxicity in Online Comments to Incivility in {A}merican News: Proceed with Caution,,,10.18653/v1/2021.eacl-main.225 , ,,"The ability to quantify incivility online, in news and in congressional debates is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility.",,,,, ,  Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,,detection,
3565,"**Title**Towards Non-Toxic Landscapes: Automatic Toxic Comment Detection Using {DNN}

**Abstract**The spectacular expansion of the Internet has led to the development of a new research problem in the field of natural language processing: automatic toxic comment detection, since many countries prohibit hate speech in public media. There is no clear and formal definition of hate, offensive, toxic and abusive speeches. In this article, we put all these terms under the umbrella of {\textquotedblleft}toxic speech{\textquotedblright}. The contribution of this paper is the design of binary classification and regression-based approaches aiming to predict whether a comment is toxic or not. We compare different unsupervised word representations and different DNN based classifiers. Moreover, we study the robustness of the proposed approaches to adversarial attacks by adding one (healthy or toxic) word. We evaluate the proposed methodology on the English Wikipedia Detox corpus. Our experiments show that using BERT fine-tuning outperforms feature-based BERT, Mikolov`s and fastText representations with different DNN classifiers.","D{'}Sa, Ashwin Geet, Illina, Irina, Fohr, Dominique",,,Towards Non-Toxic Landscapes: Automatic Toxic Comment Detection Using {DNN},,, , ,,"The spectacular expansion of the Internet has led to the development of a new research problem in the field of natural language processing: automatic toxic comment detection, since many countries prohibit hate speech in public media. There is no clear and formal definition of hate, offensive, toxic and abusive speeches. In this article, we put all these terms under the umbrella of {\textquotedblleft}toxic speech{\textquotedblright}. The contribution of this paper is the design of binary classification and regression-based approaches aiming to predict whether a comment is toxic or not. We compare different unsupervised word representations and different DNN based classifiers. Moreover, we study the robustness of the proposed approaches to adversarial attacks by adding one (healthy or toxic) word. We evaluate the proposed methodology on the English Wikipedia Detox corpus. Our experiments show that using BERT fine-tuning outperforms feature-based BERT, Mikolov`s and fastText representations with different DNN classifiers.",,,,, ,"  Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying",,detection,
3567,"**Title**Hate and Toxic Speech Detection in the Context of Covid-19 Pandemic using {XAI}: Ongoing Applied Research

**Abstract**As social distancing, self-quarantines, and travel restrictions have shifted a lot of pandemic conversations to social media so does the spread of hate speech. While recent machine learning solutions for automated hate and offensive speech identification are available on Twitter, there are issues with their interpretability. We propose a novel use of learned feature importance which improves upon the performance of prior state-of-the-art text classification techniques, while producing more easily interpretable decisions. We also discuss both technical and practical challenges that remain for this task.","Hardage, David, Najafirad, Peyman",,,Hate and Toxic Speech Detection in the Context of Covid-19 Pandemic using {XAI}: Ongoing Applied Research,,,10.18653/v1/2020.nlpcovid19-2.36 , ,,"As social distancing, self-quarantines, and travel restrictions have shifted a lot of pandemic conversations to social media so does the spread of hate speech. While recent machine learning solutions for automated hate and offensive speech identification are available on Twitter, there are issues with their interpretability. We propose a novel use of learned feature importance which improves upon the performance of prior state-of-the-art text classification techniques, while producing more easily interpretable decisions. We also discuss both technical and practical challenges that remain for this task.",,,,, ,  Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,,detection,
3569,"**Title**Preemptive Toxic Language Detection in {W}ikipedia Comments Using Thread-Level Context

**Abstract**We address the task of automatically detecting toxic content in user generated texts. We fo cus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment. Moreover, we perform preliminary investigation of whether a model that jointly considers all comments in a conversation thread outperforms a model that considers only individual comments. Using an existing dataset of conversations among Wikipedia contributors as a starting point, we compile a new large-scale dataset for this task consisting of labeled comments and comments from their conversation threads.","Karan, Vanja Mladen, {\v{S}}najder, Jan",,,Preemptive Toxic Language Detection in {W}ikipedia Comments Using Thread-Level Context,,,10.18653/v1/W19-3514 , ,,"We address the task of automatically detecting toxic content in user generated texts. We fo cus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment. Moreover, we perform preliminary investigation of whether a model that jointly considers all comments in a conversation thread outperforms a model that considers only individual comments. Using an existing dataset of conversations among Wikipedia contributors as a starting point, we compile a new large-scale dataset for this task consisting of labeled comments and comments from their conversation threads.",,,,, ,  Proceedings of the Third Workshop on Abusive Language Online,,detection,
3572,"**Title**A Review of Standard Text Classification Practices for Multi-label Toxicity Identification of Online Content

**Abstract**Language toxicity identification presents a gray area in the ethical debate surrounding freedom of speech and censorship. Today`s social media landscape is littered with unfiltered content that can be anywhere from slightly abusive to hate inducing. In response, we focused on training a multi-label classifier to detect both the type and level of toxicity in online content. This content is typically colloquial and conversational in style. Its classification therefore requires huge amounts of annotated data due to its variability and inconsistency. We compare standard methods of text classification in this task. A conventional one-vs-rest SVM classifier with character and word level frequency-based representation of text reaches 0.9763 ROC AUC score. We demonstrated that leveraging more advanced technologies such as word embeddings, recurrent neural networks, attention mechanism, stacking of classifiers and semi-supervised training can improve the ROC AUC score of classification to 0.9862. We suggest that in order to choose the right model one has to consider the accuracy of models as well as inference complexity based on the application.","Gunasekara, Isuru, Nejadgholi, Isar",,,A Review of Standard Text Classification Practices for Multi-label Toxicity Identification of Online Content,,,10.18653/v1/W18-5103 , ,,"Language toxicity identification presents a gray area in the ethical debate surrounding freedom of speech and censorship. Today`s social media landscape is littered with unfiltered content that can be anywhere from slightly abusive to hate inducing. In response, we focused on training a multi-label classifier to detect both the type and level of toxicity in online content. This content is typically colloquial and conversational in style. Its classification therefore requires huge amounts of annotated data due to its variability and inconsistency. We compare standard methods of text classification in this task. A conventional one-vs-rest SVM classifier with character and word level frequency-based representation of text reaches 0.9763 ROC AUC score. We demonstrated that leveraging more advanced technologies such as word embeddings, recurrent neural networks, attention mechanism, stacking of classifiers and semi-supervised training can improve the ROC AUC score of classification to 0.9862. We suggest that in order to choose the right model one has to consider the accuracy of models as well as inference complexity based on the application.",,,,, ,  Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2),,detection,
3573,"**Title**Challenges for Toxic Comment Classification: An In-Depth Error Analysis

**Abstract**Toxic comment classification has become an active research field with many recently proposed approaches. However, while these approaches address some of the task`s challenges others still remain unsolved and directions for further research are needed. To this end, we compare different deep learning and shallow approaches on a new, large comment dataset and propose an ensemble that outperforms all individual models. Further, we validate our findings on a second dataset. The results of the ensemble enable us to perform an extensive error analysis, which reveals open challenges for state-of-the-art methods and directions towards pending future research. These challenges include missing paradigmatic context and inconsistent dataset labels.","van Aken, Betty, Risch, Julian, Krestel, Ralf, L{\""o}ser, Alexander",,,Challenges for Toxic Comment Classification: An In-Depth Error Analysis,,,10.18653/v1/W18-5105 , ,,"Toxic comment classification has become an active research field with many recently proposed approaches. However, while these approaches address some of the task`s challenges others still remain unsolved and directions for further research are needed. To this end, we compare different deep learning and shallow approaches on a new, large comment dataset and propose an ensemble that outperforms all individual models. Further, we validate our findings on a second dataset. The results of the ensemble enable us to perform an extensive error analysis, which reveals open challenges for state-of-the-art methods and directions towards pending future research. These challenges include missing paradigmatic context and inconsistent dataset labels.",,,,, ,  Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2),,detection,
3574,"**Title**Identifying Aggression and Toxicity in Comments using Capsule Network

**Abstract**Aggression and related activities like trolling, hate speech etc. involve toxic comments in various forms. These are common scenarios in today`s time and websites react by shutting down their comment sections. To tackle this, an algorithmic solution is preferred to human moderation which is slow and expensive. In this paper, we propose a single model capsule network with focal loss to achieve this task which is suitable for production environment. Our model achieves competitive results over other strong baseline methods, which show its effectiveness and that focal loss exhibits significant improvement in such cases where class imbalance is a regular issue. Additionally, we show that the problem of extensive data preprocessing, data augmentation can be tackled by capsule networks implicitly. We achieve an overall ROC AUC of 98.46 on Kaggle-toxic comment dataset and show that it beats other architectures by a good margin. As comments tend to be written in more than one language, and transliteration is a common problem, we further show that our model handles this effectively by applying our model on TRAC shared task dataset which contains comments in code-mixed Hindi-English.","Srivastava, Saurabh, Khurana, Prerna, Tewari, Vartika",,,Identifying Aggression and Toxicity in Comments using Capsule Network,,, , ,,"Aggression and related activities like trolling, hate speech etc. involve toxic comments in various forms. These are common scenarios in today`s time and websites react by shutting down their comment sections. To tackle this, an algorithmic solution is preferred to human moderation which is slow and expensive. In this paper, we propose a single model capsule network with focal loss to achieve this task which is suitable for production environment. Our model achieves competitive results over other strong baseline methods, which show its effectiveness and that focal loss exhibits significant improvement in such cases where class imbalance is a regular issue. Additionally, we show that the problem of extensive data preprocessing, data augmentation can be tackled by capsule networks implicitly. We achieve an overall ROC AUC of 98.46 on Kaggle-toxic comment dataset and show that it beats other architectures by a good margin. As comments tend to be written in more than one language, and transliteration is a common problem, we further show that our model handles this effectively by applying our model on TRAC shared task dataset which contains comments in code-mixed Hindi-English.",,,,, ,"  Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying ({TRAC}-2018)",,detection,
3576,"**Title**Continual Reinforcement Learning for Controlled Text Generation

**Abstract**Controlled Text Generation (CTG) steers the generation of continuations of a given context (prompt) by a Large Language Model (LLM) towards texts possessing a given attribute (e.g., topic, sentiment). In this paper we view CTG as a Continual Learning problem: how to learn at every step to steer next-word generation, without having to wait for end-of-sentence. This continual view is useful for online applications such as CTG for speech, where end-of-sentence is often uncertain. We depart from an existing model, the Plug-and-Play language models (PPLM), which perturbs the context at each step to better predict next-words that posses the desired attribute. While PPLM is intricate and has many hyper-parameters, we provide a proof that the PPLM objective function can be reduced to a Continual Reinforcement Learning (CRL) reward function, thereby simplifying PPLM and endowing it with a better understood learning framework. Subsequently, we present, the first of its kind, CTG algorithm that is fully based on CRL and exhibit promising empirical results.","Shulev, Velizar, Sima{'}an, Khalil",,,Continual Reinforcement Learning for Controlled Text Generation,,, , ,,"Controlled Text Generation (CTG) steers the generation of continuations of a given context (prompt) by a Large Language Model (LLM) towards texts possessing a given attribute (e.g., topic, sentiment). In this paper we view CTG as a Continual Learning problem: how to learn at every step to steer next-word generation, without having to wait for end-of-sentence. This continual view is useful for online applications such as CTG for speech, where end-of-sentence is often uncertain. We depart from an existing model, the Plug-and-Play language models (PPLM), which perturbs the context at each step to better predict next-words that posses the desired attribute. While PPLM is intricate and has many hyper-parameters, we provide a proof that the PPLM objective function can be reduced to a Continual Reinforcement Learning (CRL) reward function, thereby simplifying PPLM and endowing it with a better understood learning framework. Subsequently, we present, the first of its kind, CTG algorithm that is fully based on CRL and exhibit promising empirical results.",,,,, ,"  Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",,detox,
3583,"**Title**Plug-and-Play Recipe Generation with Content Planning

**Abstract**Recent pre-trained language models have shown promising capability to generate fluent and realistic natural text. However, generating multi-sentence text with global content planning has been a long-existing research question. The current controlled text generation models cannot directly address this issue, as they usually condition on single known control attribute. We propose a low-cost yet effective framework that explicitly models content plans and optimizes the joint distribution of the natural sequence and the content plans in a plug-and-play post-processing manner. We evaluate our model with extensive automatic metrics and human evaluations and show that it achieves the state-of-the-art performance on the recipe generation task on Recipe1M+ dataset.","Liu, Yinhong, Su, Yixuan, Shareghi, Ehsan, Collier, Nigel",,,Plug-and-Play Recipe Generation with Content Planning,,,10.18653/v1/2022.gem-1.19 , ,,"Recent pre-trained language models have shown promising capability to generate fluent and realistic natural text. However, generating multi-sentence text with global content planning has been a long-existing research question. The current controlled text generation models cannot directly address this issue, as they usually condition on single known control attribute. We propose a low-cost yet effective framework that explicitly models content plans and optimizes the joint distribution of the natural sequence and the content plans in a plug-and-play post-processing manner. We evaluate our model with extensive automatic metrics and human evaluations and show that it achieves the state-of-the-art performance on the recipe generation task on Recipe1M+ dataset.",,,,, ,"  Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",,detox,
3584,"**Title**Control Prefixes for Parameter-Efficient Text Generation

**Abstract**Prefix-tuning is a parameter-efficient and powerful technique for adapting a pre-trained language model to a downstream application. However, it uses the same dataset-level tuned set of parameters for all examples in the dataset. We extend the framework with a dynamic method, Control Prefixes, which allows for the effective inclusion of input-dependent information, thereby demonstrating how prefix-tuning can be used for controlled text generation tasks. The method incorporates attribute-level learnable representations into different layers of a pre-trained Transformer, enabling the generated text to be guided in a particular direction. We provide a systematic evaluation of the technique and apply it to five datasets from the GEM benchmark for natural language generation (NLG). Using only 0.1{--}2{\%} additional trainable parameters, we show Control Prefixes can even outperform full fine-tuning methods, and present state-of-the-art results on several data-to-text datasets, including WebNLG. We also examine the common case where input-dependent information is unavailable at test time and show Control Prefixes can excel in this setting also.","Clive, Jordan, Cao, Kris, Rei, Marek",,,Control Prefixes for Parameter-Efficient Text Generation,,,10.18653/v1/2022.gem-1.31 , ,,"Prefix-tuning is a parameter-efficient and powerful technique for adapting a pre-trained language model to a downstream application. However, it uses the same dataset-level tuned set of parameters for all examples in the dataset. We extend the framework with a dynamic method, Control Prefixes, which allows for the effective inclusion of input-dependent information, thereby demonstrating how prefix-tuning can be used for controlled text generation tasks. The method incorporates attribute-level learnable representations into different layers of a pre-trained Transformer, enabling the generated text to be guided in a particular direction. We provide a systematic evaluation of the technique and apply it to five datasets from the GEM benchmark for natural language generation (NLG). Using only 0.1{--}2{\%} additional trainable parameters, we show Control Prefixes can even outperform full fine-tuning methods, and present state-of-the-art results on several data-to-text datasets, including WebNLG. We also examine the common case where input-dependent information is unavailable at test time and show Control Prefixes can excel in this setting also.",,,,, ,"  Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",,detox,
3586,"**Title**{C}a{M}-{G}en: {C}ausally Aware Metric-Guided Text Generation

**Abstract**Content is created for a well-defined purpose, often described by a metric or signal represented in the form of structured information. The relationship between the goal (metrics) of target content and the content itself is non-trivial. While large-scale language models show promising text generation capabilities, guiding the generated text with external metrics is challenging. These metrics and content tend to have inherent relationships and not all of them may be of consequence. We introduce CaM-Gen: Causally aware Generative Networks guided by user-defined target metrics incorporating the causal relationships between the metric and content features. We leverage causal inference techniques to identify causally significant aspects of a text that lead to the target metric and then explicitly guide generative models towards these by a feedback mechanism. We propose this mechanism for variational autoencoder and Transformer-based generative models. The proposed models beat baselines in terms of the target metric control while maintaining fluency and language quality of the generated text. To the best of our knowledge, this is one of the early attempts at controlled generation incorporating a metric guide using causal inference.","Goyal, Navita, Paneri, Roodram, Agarwal, Ayush, Kalani, Udit, Sancheti, Abhilasha, Chhaya, Niyati",,,{C}a{M}-{G}en: {C}ausally Aware Metric-Guided Text Generation,,,10.18653/v1/2022.findings-acl.162 , ,,"Content is created for a well-defined purpose, often described by a metric or signal represented in the form of structured information. The relationship between the goal (metrics) of target content and the content itself is non-trivial. While large-scale language models show promising text generation capabilities, guiding the generated text with external metrics is challenging. These metrics and content tend to have inherent relationships and not all of them may be of consequence. We introduce CaM-Gen: Causally aware Generative Networks guided by user-defined target metrics incorporating the causal relationships between the metric and content features. We leverage causal inference techniques to identify causally significant aspects of a text that lead to the target metric and then explicitly guide generative models towards these by a feedback mechanism. We propose this mechanism for variational autoencoder and Transformer-based generative models. The proposed models beat baselines in terms of the target metric control while maintaining fluency and language quality of the generated text. To the best of our knowledge, this is one of the early attempts at controlled generation incorporating a metric guide using causal inference.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2022,,detox,
3595,"**Title**Ethos: Rectifying Language Models in Orthogonal Parameter Space

**Abstract**Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos separates the principal components that encode general from those associated with undesired knowledge. Ethos performs forgetting or unlearning by only negating the task vector with undesired knowledge, thereby minimizing collateral damage on general model utility. We demonstrate the efficacy of our approach on three different tasks: bias, toxicity, and memorization unlearning. Evaluations show Ethos is more effective in removing undesired knowledge while maintaining the overall model performance compared to current task arithmetic methods.","Gao, Lei, Niu, Yue, Tang, Tingting, Avestimehr, Salman, Annavaram, Murali",,,Ethos: Rectifying Language Models in Orthogonal Parameter Space,,,10.18653/v1/2024.findings-naacl.132 , ,,"Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos separates the principal components that encode general from those associated with undesired knowledge. Ethos performs forgetting or unlearning by only negating the task vector with undesired knowledge, thereby minimizing collateral damage on general model utility. We demonstrate the efficacy of our approach on three different tasks: bias, toxicity, and memorization unlearning. Evaluations show Ethos is more effective in removing undesired knowledge while maintaining the overall model performance compared to current task arithmetic methods.",,,,, ,  Findings of the Association for Computational Linguistics: NAACL 2024,,detox,
3596,"**Title**Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models

**Abstract**Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? We used such considerations to devise three criteria for our new protocol: Specification Polarity, Specification Importance, and Domain Transferability. As a case study, we apply our protocol to a popular debiasing method, Self-Debiasing, and compare it to one we propose, called Instructive Debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. We show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked.","Morabito, Robert, Kabbara, Jad, Emami, Ali",,,Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models,,,10.18653/v1/2023.findings-acl.280 , ,,"Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? We used such considerations to devise three criteria for our new protocol: Specification Polarity, Specification Importance, and Domain Transferability. As a case study, we apply our protocol to a popular debiasing method, Self-Debiasing, and compare it to one we propose, called Instructive Debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. We show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2023,,detox,
3597,"**Title**{T}oxi{G}en: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection

**Abstract**Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5{\%} of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.","Hartvigsen, Thomas, Gabriel, Saadia, Palangi, Hamid, Sap, Maarten, Ray, Dipankar, Kamar, Ece",,,{T}oxi{G}en: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection,,,10.18653/v1/2022.acl-long.234 , ,,"Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5{\%} of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.",,,,, ,  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,Gen_dataset#detection,
3598,"**Title**Demoting Racial Bias in Hate Speech Detection

**Abstract**In the task of hate speech detection, there exists a high correlation between African American English (AAE) and annotators' perceptions of toxicity in current datasets. This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech (high false positive rate) by current hate speech classifiers. Here, we use adversarial training to mitigate this bias. Experimental results on one hate speech dataset and one AAE dataset suggest that our method is able to reduce the false positive rate for AAE text with only a minimal compromise on the performance of hate speech classification.","Xia, Mengzhou, Field, Anjalie, Tsvetkov, Yulia",,,Demoting Racial Bias in Hate Speech Detection,,,10.18653/v1/2020.socialnlp-1.2 , ,,"In the task of hate speech detection, there exists a high correlation between African American English (AAE) and annotators' perceptions of toxicity in current datasets. This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech (high false positive rate) by current hate speech classifiers. Here, we use adversarial training to mitigate this bias. Experimental results on one hate speech dataset and one AAE dataset suggest that our method is able to reduce the false positive rate for AAE text with only a minimal compromise on the performance of hate speech classification.",,,,, ,  Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media,,detection#methodology,
3599,"**Title**Debiasing by obfuscating with 007-classifiers promotes fairness in multi-community settings

**Abstract**While there has been considerable amount of research on bias mitigation algorithms, two properties: multi-community perspective and fairness to *all* communities have not been given sufficient attention. Focusing on these, we propose an obfuscation based data augmentation debiasing approach. In it we add to the training data *obfuscated* versions of *all* false positive instances irrespective of source community. We test our approach by debiasing toxicity classifiers built using 5 neural models (multi layer perceptron model and masked language models) and 3 datasets in a 4 communities setting. We also explore 4 different obfuscators for debiasing. Results demonstrate the merits of our approach: bias is reduced for almost all of our runs without sacrificing false positive rates or F1 scores for minority or majority communities. In contrast, the 4 state of the art baselines typically make performance sacrifices (often large) while reducing bias. Crucially, we demonstrate that it is possible to debias while maintaining standards for both minority and majority communities.","Shrestha, Ingroj, Srinivasan, Padmini",,,Debiasing by obfuscating with 007-classifiers promotes fairness in multi-community settings,,, , ,,"While there has been considerable amount of research on bias mitigation algorithms, two properties: multi-community perspective and fairness to *all* communities have not been given sufficient attention. Focusing on these, we propose an obfuscation based data augmentation debiasing approach. In it we add to the training data *obfuscated* versions of *all* false positive instances irrespective of source community. We test our approach by debiasing toxicity classifiers built using 5 neural models (multi layer perceptron model and masked language models) and 3 datasets in a 4 communities setting. We also explore 4 different obfuscators for debiasing. Results demonstrate the merits of our approach: bias is reduced for almost all of our runs without sacrificing false positive rates or F1 scores for minority or majority communities. In contrast, the 4 state of the art baselines typically make performance sacrifices (often large) while reducing bias. Crucially, we demonstrate that it is possible to debias while maintaining standards for both minority and majority communities.",,,,, ,  Proceedings of the 31st International Conference on Computational Linguistics,,detection,
3602,"**Title**Flatness-Aware Gradient Descent for Safe Conversational {AI}

**Abstract**As generative dialog models become ubiquitous in real-world applications, it is paramount to ensure a harmless generation. There are two major challenges when enforcing safety to open-domain chatbots. Firstly, it is impractical to provide training data reflecting the desired response to all emerging forms of toxicity (generalisation challenge). Secondly, implementing safety features may compromise the quality of the conversation (trade-off challenge). To tackle the challenges, this paper introduces a regularized fine-tuning approach called FlatGD. By employing a safety-tailored loss, we translate better optimization to more safety. To ensure better optimization, FlatGD penalizes sharp trajectories of loss curve, encouraging flatness of the converged local minima. Experimental results on datasets of {\textquotedblleft}BAD{\textquotedblright} and {\textquotedblleft}prosocial dialog{\textquotedblright} demonstrate that our model outperforms the current baselines in reducing toxicity while preserving the conversation quality. Moreover, compared to other baselines, FlatGD can better generalize to unseen toxic data.","Khalatbari, Leila, Hosseini, Saeid, Sameti, Hossein, Fung, Pascale",,,Flatness-Aware Gradient Descent for Safe Conversational {AI},,,10.18653/v1/2024.trustnlp-1.15 , ,,"As generative dialog models become ubiquitous in real-world applications, it is paramount to ensure a harmless generation. There are two major challenges when enforcing safety to open-domain chatbots. Firstly, it is impractical to provide training data reflecting the desired response to all emerging forms of toxicity (generalisation challenge). Secondly, implementing safety features may compromise the quality of the conversation (trade-off challenge). To tackle the challenges, this paper introduces a regularized fine-tuning approach called FlatGD. By employing a safety-tailored loss, we translate better optimization to more safety. To ensure better optimization, FlatGD penalizes sharp trajectories of loss curve, encouraging flatness of the converged local minima. Experimental results on datasets of {\textquotedblleft}BAD{\textquotedblright} and {\textquotedblleft}prosocial dialog{\textquotedblright} demonstrate that our model outperforms the current baselines in reducing toxicity while preserving the conversation quality. Moreover, compared to other baselines, FlatGD can better generalize to unseen toxic data.",,,,, ,  Proceedings of the 4th Workshop on Trustworthy Natural Language Processing (TrustNLP 2024),,detox,
3604,"**Title**Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks

**Abstract**Supervised classification heavily depends on datasets annotated by humans. However, in subjective tasks such as toxicity classification, these annotations often exhibit low agreement among raters. Annotations have commonly been aggregated by employing methods like majority voting to determine a single ground truth label. In subjective tasks, aggregating labels will result in biased labeling and, consequently, biased models that can overlook minority opinions. Previous studies have shed light on the pitfalls of label aggregation and have introduced a handful of practical approaches to tackle this issue. Recently proposed multi-annotator models, which predict labels individually per annotator, are vulnerable to under-determination for annotators with few samples. This problem is exacerbated in crowdsourced datasets. In this work, we propose Annotator Aware Representations for Texts (AART) for subjective classification tasks. Our approach involves learning representations of annotators, allowing for exploration of annotation behaviors. We show the improvement of our method on metrics that assess the performance on capturing individual annotators' perspectives. Additionally, we demonstrate fairness metrics to evaluate our model`s equability of performance for marginalized annotators compared to others.","Mokhberian, Negar, Marmarelis, Myrl, Hopp, Frederic, Basile, Valerio, Morstatter, Fred, Lerman, Kristina",,,Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks,,,10.18653/v1/2024.naacl-long.407 , ,,"Supervised classification heavily depends on datasets annotated by humans. However, in subjective tasks such as toxicity classification, these annotations often exhibit low agreement among raters. Annotations have commonly been aggregated by employing methods like majority voting to determine a single ground truth label. In subjective tasks, aggregating labels will result in biased labeling and, consequently, biased models that can overlook minority opinions. Previous studies have shed light on the pitfalls of label aggregation and have introduced a handful of practical approaches to tackle this issue. Recently proposed multi-annotator models, which predict labels individually per annotator, are vulnerable to under-determination for annotators with few samples. This problem is exacerbated in crowdsourced datasets. In this work, we propose Annotator Aware Representations for Texts (AART) for subjective classification tasks. Our approach involves learning representations of annotators, allowing for exploration of annotation behaviors. We show the improvement of our method on metrics that assess the performance on capturing individual annotators' perspectives. Additionally, we demonstrate fairness metrics to evaluate our model`s equability of performance for marginalized annotators compared to others.",,,,, ,  Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,detection,
3605,"**Title**Humans Need Context, What about Machines? Investigating Conversational Context in Abusive Language Detection

**Abstract**A crucial aspect in abusive language on social media platforms (toxicity, hate speech, harmful stereotypes, etc.) is its inherent contextual nature. In this paper, we focus on the role of conversational context in abusive language detection, one of the most {\textquotedblleft}direct{\textquotedblright} forms of context in this domain, as given by the conversation threads (e.g., directly preceding message, original post). The incorporation of surrounding messages has proven vital for the accurate human annotation of harmful content. However, many prior works have either ignored this aspect, collecting and processing messages in isolation, or have obtained inconsistent results when attempting to embed such contextual information into traditional classification methods. The reasons behind these findings have not yet been properly addressed. To this end, we propose an analysis of the impact of conversational context in abusive language detection, through: (1) an analysis of prior works and the limitations of the most common concatenation-based approach, which we attempt to address with two alternative architectures; (2) an evaluation of these methods on existing datasets in English, and a new dataset of French tweets annotated for hate speech and stereotypes; and (3) a qualitative analysis showcasing the necessity for context-awareness in ALD, but also its difficulties.","Bourgeade, Tom, Li, Zongmin, Benamara, Farah, Moriceau, V{\'e}ronique, Su, Jian, Sun, Aixin",,,"Humans Need Context, What about Machines? Investigating Conversational Context in Abusive Language Detection",,, , ,,"A crucial aspect in abusive language on social media platforms (toxicity, hate speech, harmful stereotypes, etc.) is its inherent contextual nature. In this paper, we focus on the role of conversational context in abusive language detection, one of the most {\textquotedblleft}direct{\textquotedblright} forms of context in this domain, as given by the conversation threads (e.g., directly preceding message, original post). The incorporation of surrounding messages has proven vital for the accurate human annotation of harmful content. However, many prior works have either ignored this aspect, collecting and processing messages in isolation, or have obtained inconsistent results when attempting to embed such contextual information into traditional classification methods. The reasons behind these findings have not yet been properly addressed. To this end, we propose an analysis of the impact of conversational context in abusive language detection, through: (1) an analysis of prior works and the limitations of the most common concatenation-based approach, which we attempt to address with two alternative architectures; (2) an evaluation of these methods on existing datasets in English, and a new dataset of French tweets annotated for hate speech and stereotypes; and (3) a qualitative analysis showcasing the necessity for context-awareness in ALD, but also its difficulties.",,,,, ,"  Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",,detection,
3607,"**Title**{R}epro{H}um {\#}0927-3: Reproducing The Human Evaluation Of The {DE}xperts Controlled Text Generation Method

**Abstract**This paper presents a reproduction study aimed at reproducing and validating a human NLP evaluation performed for the DExperts text generation method. The original study introduces DExperts, a controlled text generation method, evaluated using non-toxic prompts from the RealToxicityPrompts dataset. Our reproduction study aims to reproduce the human evaluation of the continuations generated by DExperts in comparison with four baseline methods, in terms of toxicity, topicality, and fluency. We first describe the agreed approach for reproduction within the ReproHum project and detail the configuration of the original evaluation, including necessary adaptations for reproduction. Then, we make a comparison of our reproduction results with those reported in the reproduced paper. Interestingly, we observe how the human evaluators in our experiment appreciate higher quality in the texts generated by DExperts in terms of less toxicity and better fluency. All in all, new scores are higher, also for the baseline methods. This study contributes to ongoing efforts in ensuring the reproducibility and reliability of findings in NLP evaluation and emphasizes the critical role of robust methodologies in advancing the field.","Gonz{\'a}lez Corbelle, Javier, Vivel Couso, Ainhoa, Alonso-Moral, Jose Maria, Bugar{\'i}n-Diz, Alberto",,,{R}epro{H}um {\#}0927-3: Reproducing The Human Evaluation Of The {DE}xperts Controlled Text Generation Method,,, , ,,"This paper presents a reproduction study aimed at reproducing and validating a human NLP evaluation performed for the DExperts text generation method. The original study introduces DExperts, a controlled text generation method, evaluated using non-toxic prompts from the RealToxicityPrompts dataset. Our reproduction study aims to reproduce the human evaluation of the continuations generated by DExperts in comparison with four baseline methods, in terms of toxicity, topicality, and fluency. We first describe the agreed approach for reproduction within the ReproHum project and detail the configuration of the original evaluation, including necessary adaptations for reproduction. Then, we make a comparison of our reproduction results with those reported in the reproduced paper. Interestingly, we observe how the human evaluators in our experiment appreciate higher quality in the texts generated by DExperts in terms of less toxicity and better fluency. All in all, new scores are higher, also for the baseline methods. This study contributes to ongoing efforts in ensuring the reproducibility and reliability of findings in NLP evaluation and emphasizes the critical role of robust methodologies in advancing the field.",,,,, ,  Proceedings of the Fourth Workshop on Human Evaluation of NLP Systems (HumEval) @ LREC-COLING 2024,,detox,
3609,"**Title**Data, Data Everywhere: A Guide for Pretraining Dataset Construction

**Abstract**The impressive capabilities of recent language models can be largely attributed to the multi-trillion token pretraining datasets that they are trained on. However, model developers fail to disclose their construction methodology which has lead to a lack of open information on how to develop effective pretraining sets. To address this issue, we perform the first systematic study across the entire pipeline of pretraining set construction. First, we run ablations on existing techniques for pretraining set development to identify which methods translate to the largest gains in model accuracy on downstream evaluations. Then, we categorize the most widely used data source, web crawl snapshots, across the attributes of toxicity, quality, type of speech, and domain. Finally, we show how such attribute information can be used to further refine and improve the quality of a pretraining set. These findings constitute an actionable set of steps that practitioners can use to develop high quality pretraining sets.","Parmar, Jupinder, Prabhumoye, Shrimai, Jennings, Joseph, Liu, Bo, Jhunjhunwala, Aastha, Wang, Zhilin, Patwary, Mostofa, Shoeybi, Mohammad, Catanzaro, Bryan",,,"Data, Data Everywhere: A Guide for Pretraining Dataset Construction",,,10.18653/v1/2024.emnlp-main.596 , ,,"The impressive capabilities of recent language models can be largely attributed to the multi-trillion token pretraining datasets that they are trained on. However, model developers fail to disclose their construction methodology which has lead to a lack of open information on how to develop effective pretraining sets. To address this issue, we perform the first systematic study across the entire pipeline of pretraining set construction. First, we run ablations on existing techniques for pretraining set development to identify which methods translate to the largest gains in model accuracy on downstream evaluations. Then, we categorize the most widely used data source, web crawl snapshots, across the attributes of toxicity, quality, type of speech, and domain. Finally, we show how such attribute information can be used to further refine and improve the quality of a pretraining set. These findings constitute an actionable set of steps that practitioners can use to develop high quality pretraining sets.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detection,
3611,"**Title**Harmful Language Datasets: An Assessment of Robustness

**Abstract**The automated detection of harmful language has been of great importance for the online world, especially with the growing importance of social media and, consequently, polarisation. There are many open challenges to high quality detection of harmful text, from dataset creation to generalisable application, thus calling for more systematic studies. In this paper, we explore re-annotation as a means of examining the robustness of already existing labelled datasets, showing that, despite using alternative definitions, the inter-annotator agreement remains very inconsistent, highlighting the intrinsically subjective and variable nature of the task. In addition, we build automatic toxicity detectors using the existing datasets, with their original labels, and we evaluate them on our multi-definition and multi-source datasets. Surprisingly, while other studies show that hate speech detection models perform better on data that are derived from the same distribution as the training set, our analysis demonstrates this is not necessarily true.","Korre, Katerina, Pavlopoulos, John, Sorensen, Jeffrey, Laugier, L{\'e}o, Androutsopoulos, Ion, Dixon, Lucas, Barr{\'o}n-cede{\~n}o, Alberto",,,Harmful Language Datasets: An Assessment of Robustness,,,10.18653/v1/2023.woah-1.24 , ,,"The automated detection of harmful language has been of great importance for the online world, especially with the growing importance of social media and, consequently, polarisation. There are many open challenges to high quality detection of harmful text, from dataset creation to generalisable application, thus calling for more systematic studies. In this paper, we explore re-annotation as a means of examining the robustness of already existing labelled datasets, showing that, despite using alternative definitions, the inter-annotator agreement remains very inconsistent, highlighting the intrinsically subjective and variable nature of the task. In addition, we build automatic toxicity detectors using the existing datasets, with their original labels, and we evaluate them on our multi-definition and multi-source datasets. Surprisingly, while other studies show that hate speech detection models perform better on data that are derived from the same distribution as the training set, our analysis demonstrates this is not necessarily true.",,,,, ,  The 7th Workshop on Online Abuse and Harms (WOAH),,detection,
3612,"**Title**{PALS}: Personalized Active Learning for Subjective Tasks in {NLP}

**Abstract**For subjective NLP problems, such as classification of hate speech, aggression, or emotions, personalized solutions can be exploited. Then, the learned models infer about the perception of the content independently for each reader. To acquire training data, texts are commonly randomly assigned to users for annotation, which is expensive and highly inefficient. Therefore, for the first time, we suggest applying an active learning paradigm in a personalized context to better learn individual preferences. It aims to alleviate the labeling effort by selecting more relevant training samples. In this paper, we present novel Personalized Active Learning techniques for Subjective NLP tasks (PALS) to either reduce the cost of the annotation process or to boost the learning effect. Our five new measures allow us to determine the relevance of a text in the context of learning users personal preferences. We validated them on three datasets: Wiki discussion texts individually labeled with aggression and toxicity, and on Unhealthy Conversations dataset. Our PALS techniques outperform random selection even by more than 30{\%}. They can also be used to reduce the number of necessary annotations while maintaining a given quality level. Personalized annotation assignments based on our controversy measure decrease the amount of data needed to just 25{\%}-40{\%} of the initial size.","Kanclerz, Kamil, Karanowski, Konrad, Bielaniewicz, Julita, Gruza, Marcin, Mi{\l}kowski, Piotr, Kocon, Jan, Kazienko, Przemyslaw",,,{PALS}: Personalized Active Learning for Subjective Tasks in {NLP},,,10.18653/v1/2023.emnlp-main.823 , ,,"For subjective NLP problems, such as classification of hate speech, aggression, or emotions, personalized solutions can be exploited. Then, the learned models infer about the perception of the content independently for each reader. To acquire training data, texts are commonly randomly assigned to users for annotation, which is expensive and highly inefficient. Therefore, for the first time, we suggest applying an active learning paradigm in a personalized context to better learn individual preferences. It aims to alleviate the labeling effort by selecting more relevant training samples. In this paper, we present novel Personalized Active Learning techniques for Subjective NLP tasks (PALS) to either reduce the cost of the annotation process or to boost the learning effect. Our five new measures allow us to determine the relevance of a text in the context of learning users personal preferences. We validated them on three datasets: Wiki discussion texts individually labeled with aggression and toxicity, and on Unhealthy Conversations dataset. Our PALS techniques outperform random selection even by more than 30{\%}. They can also be used to reduce the number of necessary annotations while maintaining a given quality level. Personalized annotation assignments based on our controversy measure decrease the amount of data needed to just 25{\%}-40{\%} of the initial size.",,,,, ,  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,,detection,
3617,"**Title**Conditional Supervised Contrastive Learning for Fair Text Classification

**Abstract**Contrastive representation learning has gained much attention due to its superior performance in learning representations from both image and sequential data. However, the learned representations could potentially lead to performance disparities in downstream tasks, such as increased silencing of underrepresented groups in toxicity comment classification. In light of this challenge, in this work, we study learning fair representations that satisfy a notion of fairness known as equalized odds for text classification via contrastive learning. Specifically, we first theoretically analyze the connections between learning representations with a fairness constraint and conditional supervised contrastive objectives, and then propose to use conditional supervised contrastive objectives to learn fair representations for text classification. We conduct experiments on two text datasets to demonstrate the effectiveness of our approaches in balancing the trade-offs between task performance and bias mitigation among existing baselines for text classification. Furthermore, we also show that the proposed methods are stable in different hyperparameter settings.","Chi, Jianfeng, Shand, William, Yu, Yaodong, Chang, Kai-Wei, Zhao, Han, Tian, Yuan",,,Conditional Supervised Contrastive Learning for Fair Text Classification,,,10.18653/v1/2022.findings-emnlp.199 , ,,"Contrastive representation learning has gained much attention due to its superior performance in learning representations from both image and sequential data. However, the learned representations could potentially lead to performance disparities in downstream tasks, such as increased silencing of underrepresented groups in toxicity comment classification. In light of this challenge, in this work, we study learning fair representations that satisfy a notion of fairness known as equalized odds for text classification via contrastive learning. Specifically, we first theoretically analyze the connections between learning representations with a fairness constraint and conditional supervised contrastive objectives, and then propose to use conditional supervised contrastive objectives to learn fair representations for text classification. We conduct experiments on two text datasets to demonstrate the effectiveness of our approaches in balancing the trade-offs between task performance and bias mitigation among existing baselines for text classification. Furthermore, we also show that the proposed methods are stable in different hyperparameter settings.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2022,,detection,
3618,"**Title**Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation

**Abstract**Large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. In order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers or automatic generation to construct adversarial contexts that are likely to induce toxic generations. However, what type of context is more likely to induce unsafe responses is still under-explored. In this paper, we identify that context toxicity and context category (e.g., profanity, insult, drugs, etc.) are two important factors to cause safety issues in response generation. Hence, we propose a method called reverse generation to construct adversarial contexts conditioned on a given response, with the flexibility to control category, toxicity level and inductivity of the generated contexts. Via reverse generation, we augment the existing BAD dataset and construct a new dataset BAD+ which contains more than 120K diverse and highly inductive contexts in 12 categories. We test three popular pretrained dialogue models (Blender, DialoGPT and Plato2) and find that BAD+ can largely expose their safety problems. Furthermore, we show that BAD+ can greatly enhance the safety of generation, and we reveal the key factors of safety improvement. Our code and dataset is available at \url{https://github.com/thu-coai/Reverse_Generation}.","Zhang, Zhexin, Cheng, Jiale, Sun, Hao, Deng, Jiawen, Mi, Fei, Wang, Yasheng, Shang, Lifeng, Huang, Minlie",,,Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation,,,10.18653/v1/2022.findings-emnlp.270 , ,,"Large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. In order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers or automatic generation to construct adversarial contexts that are likely to induce toxic generations. However, what type of context is more likely to induce unsafe responses is still under-explored. In this paper, we identify that context toxicity and context category (e.g., profanity, insult, drugs, etc.) are two important factors to cause safety issues in response generation. Hence, we propose a method called reverse generation to construct adversarial contexts conditioned on a given response, with the flexibility to control category, toxicity level and inductivity of the generated contexts. Via reverse generation, we augment the existing BAD dataset and construct a new dataset BAD+ which contains more than 120K diverse and highly inductive contexts in 12 categories. We test three popular pretrained dialogue models (Blender, DialoGPT and Plato2) and find that BAD+ can largely expose their safety problems. Furthermore, we show that BAD+ can greatly enhance the safety of generation, and we reveal the key factors of safety improvement. Our code and dataset is available at \url{https://github.com/thu-coai/Reverse_Generation}.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2022,,detox,
3620,"**Title**Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company`s Reputation

**Abstract**Not all topics are equally {\textquotedblleft}flammable{\textquotedblright} in terms of toxicity: a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or sexual minorities. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness. While toxicity in user-generated data is well-studied, we aim at defining a more fine-grained notion of inappropriateness. The core of inappropriateness is that it can harm the reputation of a speaker. This is different from toxicity in two respects: (i) inappropriateness is topic-related, and (ii) inappropriate message is not toxic but still unacceptable. We collect and release two datasets for Russian: a topic-labelled dataset and an appropriateness-labelled dataset. We also release pre-trained classification models trained on this data.","Babakov, Nikolay, Logacheva, Varvara, Kozlova, Olga, Semenov, Nikita, Panchenko, Alexander",,,Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company`s Reputation,,, , ,,"Not all topics are equally {\textquotedblleft}flammable{\textquotedblright} in terms of toxicity: a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or sexual minorities. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the methodology of collecting and labelling a dataset for appropriateness. While toxicity in user-generated data is well-studied, we aim at defining a more fine-grained notion of inappropriateness. The core of inappropriateness is that it can harm the reputation of a speaker. This is different from toxicity in two respects: (i) inappropriateness is topic-related, and (ii) inappropriate message is not toxic but still unacceptable. We collect and release two datasets for Russian: a topic-labelled dataset and an appropriateness-labelled dataset. We also release pre-trained classification models trained on this data.",,,,, ,  Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,,detection,
3621,"**Title**{A}lex{U}-{B}ack{T}ranslation-{TL} at {S}em{E}val-2020 Task 12: Improving Offensive Language Detection Using Data Augmentation and Transfer Learning

**Abstract**Social media platforms, online news commenting spaces, and many other public forums have become widely known for issues of abusive behavior such as cyber-bullying and personal attacks. In this paper, we use the annotated tweets of the Offensive Language Identification Dataset (OLID) to train three levels of deep learning classifiers to solve the three sub-tasks associated with the dataset. Sub-task A is to determine if the tweet is toxic or not. Then, for offensive tweets, sub-task B requires determining whether the toxicity is targeted. Finally, for sub-task C, we predict the target of the offense; i.e. a group, individual, or other entity. In our solution, we tackle the problem of class imbalance in the dataset by using back translation for data augmentation and utilizing the fine-tuned BERT model in an ensemble of deep learning classifiers. We used this solution to participate in the three English sub-tasks of SemEval-2020 task 12. The proposed solution achieved 0.91393, 0.6300, and 0.57607 macro F1-average in sub-tasks A, B, and C respectively. We achieved the 9th, 14th, and 22nd places for sub-tasks A, B and C respectively.","Ibrahim, Mai, Torki, Marwan, El-Makky, Nagwa",,,{A}lex{U}-{B}ack{T}ranslation-{TL} at {S}em{E}val-2020 Task 12: Improving Offensive Language Detection Using Data Augmentation and Transfer Learning,,,10.18653/v1/2020.semeval-1.248 , ,,"Social media platforms, online news commenting spaces, and many other public forums have become widely known for issues of abusive behavior such as cyber-bullying and personal attacks. In this paper, we use the annotated tweets of the Offensive Language Identification Dataset (OLID) to train three levels of deep learning classifiers to solve the three sub-tasks associated with the dataset. Sub-task A is to determine if the tweet is toxic or not. Then, for offensive tweets, sub-task B requires determining whether the toxicity is targeted. Finally, for sub-task C, we predict the target of the offense; i.e. a group, individual, or other entity. In our solution, we tackle the problem of class imbalance in the dataset by using back translation for data augmentation and utilizing the fine-tuned BERT model in an ensemble of deep learning classifiers. We used this solution to participate in the three English sub-tasks of SemEval-2020 task 12. The proposed solution achieved 0.91393, 0.6300, and 0.57607 macro F1-average in sub-tasks A, B, and C respectively. We achieved the 9th, 14th, and 22nd places for sub-tasks A, B and C respectively.",,,,, ,  Proceedings of the Fourteenth Workshop on Semantic Evaluation,,detection,
3622,"**Title**Merging Datasets for Aggressive Text Identification

**Abstract**This paper presents the approach of the team {\textquotedblleft}groutar{\textquotedblright} to the shared task on Aggression Identification, considering the test sets in English, both from Facebook and general Social Media. This experiment aims to test the effect of merging new datasets in the performance of classification models. We followed a standard machine learning approach with training, validation, and testing phases, and considered features such as part-of-speech, frequencies of insults, punctuation, sentiment, and capitalization. In terms of algorithms, we experimented with Boosted Logistic Regression, Multi-Layer Perceptron, Parallel Random Forest and eXtreme Gradient Boosting. One question appearing was how to merge datasets using different classification systems (e.g. aggression vs. toxicity). Other issue concerns the possibility to generalize models and apply them to data from different social networks. Regarding these, we merged two datasets, and the results showed that training with similar data is an advantage in the classification of social networks data. However, adding data from different platforms, allowed slightly better results in both Facebook and Social Media, indicating that more generalized models can be an advantage.","Fortuna, Paula, Ferreira, Jos{\'e}, Pires, Luiz, Routar, Guilherme, Nunes, S{\'e}rgio",,,Merging Datasets for Aggressive Text Identification,,, , ,,"This paper presents the approach of the team {\textquotedblleft}groutar{\textquotedblright} to the shared task on Aggression Identification, considering the test sets in English, both from Facebook and general Social Media. This experiment aims to test the effect of merging new datasets in the performance of classification models. We followed a standard machine learning approach with training, validation, and testing phases, and considered features such as part-of-speech, frequencies of insults, punctuation, sentiment, and capitalization. In terms of algorithms, we experimented with Boosted Logistic Regression, Multi-Layer Perceptron, Parallel Random Forest and eXtreme Gradient Boosting. One question appearing was how to merge datasets using different classification systems (e.g. aggression vs. toxicity). Other issue concerns the possibility to generalize models and apply them to data from different social networks. Regarding these, we merged two datasets, and the results showed that training with similar data is an advantage in the classification of social networks data. However, adding data from different platforms, allowed slightly better results in both Facebook and Social Media, indicating that more generalized models can be an advantage.",,,,, ,"  Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying ({TRAC}-2018)",,detection,
3623,"**Title**{LUCE}: A Dynamic Framework and Interactive Dashboard for Opinionated Text Analysis

**Abstract**We introduce LUCE, an advanced dynamic framework with an interactive dashboard for analysing opinionated text aiming to understand people-centred communication. The framework features computational modules of text classification and extraction explicitly designed for analysing different elements of opinions, e.g., sentiment/emotion, suggestion, figurative language, hate/toxic speech, and topics. We designed the framework using a modular architecture, allowing scalability and extensibility with the aim of supporting other NLP tasks in subsequent versions. LUCE comprises trained models, python-based APIs, and a user-friendly dashboard, ensuring an intuitive user experience. LUCE has been validated in a relevant environment, and its capabilities and performance have been demonstrated through initial prototypes and pilot studies.","Zayed, Omnia, Negi, Gaurav, Manjunath, Sampritha Hassan, Pillai, Devishree, Buitelaar, Paul",,,{LUCE}: A Dynamic Framework and Interactive Dashboard for Opinionated Text Analysis,,, , ,,"We introduce LUCE, an advanced dynamic framework with an interactive dashboard for analysing opinionated text aiming to understand people-centred communication. The framework features computational modules of text classification and extraction explicitly designed for analysing different elements of opinions, e.g., sentiment/emotion, suggestion, figurative language, hate/toxic speech, and topics. We designed the framework using a modular architecture, allowing scalability and extensibility with the aim of supporting other NLP tasks in subsequent versions. LUCE comprises trained models, python-based APIs, and a user-friendly dashboard, ensuring an intuitive user experience. LUCE has been validated in a relevant environment, and its capabilities and performance have been demonstrated through initial prototypes and pilot studies.",,,,, ,  Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations,,detection,
3625,"**Title**Improving Dialog Safety using Socially Aware Contrastive Learning

**Abstract**State-of-the-art conversational AI systems raise concerns due to their potential risks of generating unsafe, toxic, unethical, or dangerous content. Previous works have developed datasets to teach conversational agents the appropriate social paradigms to respond effectively to specifically designed hazardous content. However, models trained on these adversarial datasets still struggle to recognize subtle unsafe situations that appear naturally in conversations or introduce an inappropriate response in a casual context. To understand the extent of this problem, we study prosociality in both adversarial and casual dialog contexts and audit the response quality of general-purpose language models in terms of propensity to produce unsafe content. We propose a dual-step fine-tuning process to address these issues using a socially aware n-pair contrastive loss. Subsequently, we train a base model that integrates prosocial behavior by leveraging datasets like Moral Integrity Corpus (MIC) and ProsocialDialog. Experimental results on several dialog datasets demonstrate the effectiveness of our approach in generating socially appropriate responses.","Das, Souvik, Srihari, Rohini K.",,,Improving Dialog Safety using Socially Aware Contrastive Learning,,, , ,,"State-of-the-art conversational AI systems raise concerns due to their potential risks of generating unsafe, toxic, unethical, or dangerous content. Previous works have developed datasets to teach conversational agents the appropriate social paradigms to respond effectively to specifically designed hazardous content. However, models trained on these adversarial datasets still struggle to recognize subtle unsafe situations that appear naturally in conversations or introduce an inappropriate response in a casual context. To understand the extent of this problem, we study prosociality in both adversarial and casual dialog contexts and audit the response quality of general-purpose language models in terms of propensity to produce unsafe content. We propose a dual-step fine-tuning process to address these issues using a socially aware n-pair contrastive loss. Subsequently, we train a base model that integrates prosocial behavior by leveraging datasets like Moral Integrity Corpus (MIC) and ProsocialDialog. Experimental results on several dialog datasets demonstrate the effectiveness of our approach in generating socially appropriate responses.",,,,, ,  Proceedings of the 1st Workshop on Simulating Conversational Intelligence in Chat (SCI-CHAT 2024),,detox,
3626,"**Title**Can Language Model Moderators Improve the Health of Online Discourse?

**Abstract**Conversational moderation of online communities is crucial to maintaining civility for a constructive environment, but it is challenging to scale and harmful to moderators. The inclusion of sophisticated natural language generation modules as a force multiplier to aid human moderators is a tantalizing prospect, but adequate evaluation approaches have so far been elusive. In this paper, we establish a systematic definition of conversational moderation effectiveness grounded on moderation literature and establish design criteria for conducting realistic yet safe evaluation. We then propose a comprehensive evaluation framework to assess models' moderation capabilities independently of human intervention. With our framework, we conduct the first known study of language models as conversational moderators, finding that appropriately prompted models that incorporate insights from social science can provide specific and fair feedback on toxic behavior but struggle to influence users to increase their levels of respect and cooperation.","Cho, Hyundong, Liu, Shuai, Shi, Taiwei, Jain, Darpan, Rizk, Basem, Huang, Yuyang, Lu, Zixun, Wen, Nuan, Gratch, Jonathan, Ferrara, Emilio, May, Jonathan",,,Can Language Model Moderators Improve the Health of Online Discourse?,,,10.18653/v1/2024.naacl-long.415 , ,,"Conversational moderation of online communities is crucial to maintaining civility for a constructive environment, but it is challenging to scale and harmful to moderators. The inclusion of sophisticated natural language generation modules as a force multiplier to aid human moderators is a tantalizing prospect, but adequate evaluation approaches have so far been elusive. In this paper, we establish a systematic definition of conversational moderation effectiveness grounded on moderation literature and establish design criteria for conducting realistic yet safe evaluation. We then propose a comprehensive evaluation framework to assess models' moderation capabilities independently of human intervention. With our framework, we conduct the first known study of language models as conversational moderators, finding that appropriately prompted models that incorporate insights from social science can provide specific and fair feedback on toxic behavior but struggle to influence users to increase their levels of respect and cooperation.",,,,, ,  Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,detection,
3629,"**Title**Emojis Trash or Treasure: Utilizing Emoji to Aid Hate Speech Detection

**Abstract**In this study, we delve into the fascinating realm of emojis and their impact on identifying hate speech in both Bengali and English languages. Through extensive exploration of various techniques, particularly the integration of Multilingual BERT (MBert) and Emoji2Vec embeddings, we strive to shed light on the immense potential of emojis in this detection process. By meticulously comparing these advanced models with conventional approaches, we uncover the intricate contextual cues that emojis bring to the table. Ultimately, our discoveries underscore the invaluable role of emojis in hate speech detection, thereby providing valuable insights for the creation of resilient and context-aware systems to combat online toxicity. Our findings showcase the potential of emojis as valuable assets rather than mere embellishments in the realm of hate speech detection. By leveraging the combined strength of MBert and Emoji2Vec, our models exhibit enhanced capabilities in deciphering the emotional subtleties often intertwined with hate speech expressions.","Saikh, Tanik, Barman, Soham, Kumar, Harsh, Sahu, Saswat, Palit, Souvick",,,Emojis Trash or Treasure: Utilizing Emoji to Aid Hate Speech Detection,,, , ,,"In this study, we delve into the fascinating realm of emojis and their impact on identifying hate speech in both Bengali and English languages. Through extensive exploration of various techniques, particularly the integration of Multilingual BERT (MBert) and Emoji2Vec embeddings, we strive to shed light on the immense potential of emojis in this detection process. By meticulously comparing these advanced models with conventional approaches, we uncover the intricate contextual cues that emojis bring to the table. Ultimately, our discoveries underscore the invaluable role of emojis in hate speech detection, thereby providing valuable insights for the creation of resilient and context-aware systems to combat online toxicity. Our findings showcase the potential of emojis as valuable assets rather than mere embellishments in the realm of hate speech detection. By leveraging the combined strength of MBert and Emoji2Vec, our models exhibit enhanced capabilities in deciphering the emotional subtleties often intertwined with hate speech expressions.",,,,, ,  Proceedings of the 21st International Conference on Natural Language Processing (ICON),,detection,
3630,"**Title**Divine {LL}a{MA}s: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models

**Abstract**Emotions play important epistemological and cognitive roles in our lives, revealing our values and guiding our actions. Previous work has shown that LLMs display biases in emotion attribution along gender lines. However, unlike gender, which says little about our values, religion, as a socio-cultural system, prescribes a set of beliefs and values for its followers. Religions, therefore, cultivate certain emotions. Moreover, these rules are explicitly laid out and interpreted by religious leaders. Using emotion attribution, we explore how different religions are represented in LLMs. We find that:Major religions in the US and European countries are represented with more nuance, displaying a more shaded model of their beliefs.Eastern religions like Hinduism and Buddhism are strongly stereotyped.Judaism and Islam are stigmatized {--} the models' refusal skyrocket. We ascribe these to cultural bias in LLMs and the scarcity of NLP literature on religion. In the rare instances where religion is discussed, it is often in the context of toxic language, perpetuating the perception of these religions as inherently toxic. This finding underscores the urgent need to address and rectify these biases. Our research emphasizes the crucial role emotions play in shaping our lives and how our values influence them.","Plaza-del-Arco, Flor Miriam, Curry, Amanda Cercas, Paoli, Susanna, Cercas Curry, Alba, Hovy, Dirk",,,"Divine {LL}a{MA}s: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models",,,10.18653/v1/2024.findings-emnlp.251 , ,,"Emotions play important epistemological and cognitive roles in our lives, revealing our values and guiding our actions. Previous work has shown that LLMs display biases in emotion attribution along gender lines. However, unlike gender, which says little about our values, religion, as a socio-cultural system, prescribes a set of beliefs and values for its followers. Religions, therefore, cultivate certain emotions. Moreover, these rules are explicitly laid out and interpreted by religious leaders. Using emotion attribution, we explore how different religions are represented in LLMs. We find that:Major religions in the US and European countries are represented with more nuance, displaying a more shaded model of their beliefs.Eastern religions like Hinduism and Buddhism are strongly stereotyped.Judaism and Islam are stigmatized {--} the models' refusal skyrocket. We ascribe these to cultural bias in LLMs and the scarcity of NLP literature on religion. In the rare instances where religion is discussed, it is often in the context of toxic language, perpetuating the perception of these religions as inherently toxic. This finding underscores the urgent need to address and rectify these biases. Our research emphasizes the crucial role emotions play in shaping our lives and how our values influence them.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2024,,detection,
3632,"**Title**Diffusion Guided Language Modeling

**Abstract**Current language models demonstrate remarkable proficiency in text generation. However, for many applications it is desirable to control attributes, such as sentiment, or toxicity, of the generated language{---}ideally tailored towards each specific use case and target audience. For auto-regressive language models, existing guidance methods are prone to decoding errors that cascade during generation and degrade performance. In contrast, text diffusion models can easily be guided with, for example, a simple linear sentiment classifier{---}however they do suffer from significantly higher perplexity than auto-regressive alternatives. In this paper we use a guided diffusion model to produce a latent proposal that steers an auto-regressive language model to generate text with desired properties. Our model inherits the unmatched fluency of the auto-regressive approach and the plug-and-play flexibility of diffusion. We show that it outperforms previous plug-and-play guidance methods across a wide range of benchmark data sets. Further, controlling a new attribute in our framework is reduced to training a single logistic regression classifier.","Lovelace, Justin, Kishore, Varsha, Chen, Yiwei, Weinberger, Kilian",,,Diffusion Guided Language Modeling,,,10.18653/v1/2024.findings-acl.887 , ,,"Current language models demonstrate remarkable proficiency in text generation. However, for many applications it is desirable to control attributes, such as sentiment, or toxicity, of the generated language{---}ideally tailored towards each specific use case and target audience. For auto-regressive language models, existing guidance methods are prone to decoding errors that cascade during generation and degrade performance. In contrast, text diffusion models can easily be guided with, for example, a simple linear sentiment classifier{---}however they do suffer from significantly higher perplexity than auto-regressive alternatives. In this paper we use a guided diffusion model to produce a latent proposal that steers an auto-regressive language model to generate text with desired properties. Our model inherits the unmatched fluency of the auto-regressive approach and the plug-and-play flexibility of diffusion. We show that it outperforms previous plug-and-play guidance methods across a wide range of benchmark data sets. Further, controlling a new attribute in our framework is reduced to training a single logistic regression classifier.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2024,,detox,
3633,"**Title**Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation

**Abstract**Textual style expresses a diverse set of information, including interpersonal dynamics (e.g., formality) and the author`s emotions or attitudes (e.g., disgust). An open question is how language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. One approach to such controlled generation is multi-objective reinforcement learning (RL), but how to best combine multiple objectives in a reward function is an open question. In this paper, we investigate various formulations of multi-style reward formulations, including calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. We find that our proposed dynamic weighting outperforms static weighting approaches with respect style control while maintaining linguistic quality, and we explore its effectiveness in 2- and 3-style control.","De Langis, Karin, Koo, Ryan, Kang, Dongyeop",,,Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation,,,10.18653/v1/2024.emnlp-main.386 , ,,"Textual style expresses a diverse set of information, including interpersonal dynamics (e.g., formality) and the author`s emotions or attitudes (e.g., disgust). An open question is how language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. One approach to such controlled generation is multi-objective reinforcement learning (RL), but how to best combine multiple objectives in a reward function is an open question. In this paper, we investigate various formulations of multi-style reward formulations, including calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. We find that our proposed dynamic weighting outperforms static weighting approaches with respect style control while maintaining linguistic quality, and we explore its effectiveness in 2- and 3-style control.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3634,"**Title**A Closer Look at Multidimensional Online Political Incivility

**Abstract**Toxic online political discourse has become prevalent, where scholars debate about its impact to Democratic processes. This work presents a large-scale study of political incivility on Twitter. In line with theories of political communication, we differentiate between harsh {\textquoteleft}impolite' style and intolerant substance. We present a dataset of 13K political tweets in the U.S. context, which we collected and labeled by those categories using crowd sourcing. Our dataset and results shed light on hostile political discourse focused on partisan conflicts in the U.S. The evaluation of state-of-the-art classifiers illustrates the challenges involved in political incivility detection, which often requires high-level semantic and social understanding. Nevertheless, performing incivility detection at scale, we are able to characterise its distribution across individual users and geopolitical regions, where our findings align and extend existing theories of political communication. In particular, we find that roughly 80{\%} of the uncivil tweets are authored by 20{\%} of the users, where users who are politically engaged are more inclined to use uncivil language. We further find that political incivility exhibits network homophily, and that incivility is more prominent in highly competitive geopolitical regions. Our results apply to both uncivil style and substance.","Pendzel, Sagi, Lotan, Nir, Zoizner, Alon, Minkov, Einat",,,A Closer Look at Multidimensional Online Political Incivility,,,10.18653/v1/2024.emnlp-main.827 , ,,"Toxic online political discourse has become prevalent, where scholars debate about its impact to Democratic processes. This work presents a large-scale study of political incivility on Twitter. In line with theories of political communication, we differentiate between harsh {\textquoteleft}impolite' style and intolerant substance. We present a dataset of 13K political tweets in the U.S. context, which we collected and labeled by those categories using crowd sourcing. Our dataset and results shed light on hostile political discourse focused on partisan conflicts in the U.S. The evaluation of state-of-the-art classifiers illustrates the challenges involved in political incivility detection, which often requires high-level semantic and social understanding. Nevertheless, performing incivility detection at scale, we are able to characterise its distribution across individual users and geopolitical regions, where our findings align and extend existing theories of political communication. In particular, we find that roughly 80{\%} of the uncivil tweets are authored by 20{\%} of the users, where users who are politically engaged are more inclined to use uncivil language. We further find that political incivility exhibits network homophily, and that incivility is more prominent in highly competitive geopolitical regions. Our results apply to both uncivil style and substance.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,Gen_dataset#detection,
3635,"**Title**Comparing a {BERT} Classifier and a {GPT} classifier for Detecting Connective Language Across Multiple Social Media

**Abstract**This study presents an approach for detecting connective language{---}defined as language that facilitates engagement, understanding, and conversation{---}from social media discussions. We developed and evaluated two types of classifiers: BERT and GPT-3.5 turbo. Our results demonstrate that the BERT classifier significantly outperforms GPT-3.5 turbo in detecting connective language. Furthermore, our analysis confirms that connective language is distinct from related concepts measuring discourse qualities, such as politeness and toxicity. We also explore the potential of BERT-based classifiers for platform-agnostic tools. This research advances our understanding of the linguistic dimensions of online communication and proposes practical tools for detecting connective language across diverse digital environments.","Lukito, Josephine, Chen, Bin, Masullo, Gina M., Stroud, Natalie Jomini",,,Comparing a {BERT} Classifier and a {GPT} classifier for Detecting Connective Language Across Multiple Social Media,,,10.18653/v1/2024.emnlp-main.1067 , ,,"This study presents an approach for detecting connective language{---}defined as language that facilitates engagement, understanding, and conversation{---}from social media discussions. We developed and evaluated two types of classifiers: BERT and GPT-3.5 turbo. Our results demonstrate that the BERT classifier significantly outperforms GPT-3.5 turbo in detecting connective language. Furthermore, our analysis confirms that connective language is distinct from related concepts measuring discourse qualities, such as politeness and toxicity. We also explore the potential of BERT-based classifiers for platform-agnostic tools. This research advances our understanding of the linguistic dimensions of online communication and proposes practical tools for detecting connective language across diverse digital environments.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detection,
3636,"**Title**Towards Aligning Language Models with Textual Feedback

**Abstract**We present ALT (ALignment with Textual feedback), an approach that aligns language models with user preferences expressed in text. We argue that text offers greater expressiveness, enabling users to provide richer feedback than simple comparative preferences and this richer feedback can lead to more efficient and effective alignment. ALT aligns the model by conditioning its generation on the textual feedback. Our method relies solely on language modeling techniques and requires minimal hyper-parameter tuning, though it still presents the main benefit of RL-based algorithms and can effectively learn from textual feedback. We explore the efficacy and efficiency of textual feedback across different tasks such as toxicity reduction, summarization, and dialog response. We find that ALT outperforms PPO for the task of toxicity reduction while being able to match its performance on summarization with only 20{\%} of the samples. We also explore how ALT can be used with feedback provided by an existing LLM.","Lloret, Sa{\""u}c Abadal, Dhuliawala, Shehzaad, Murugesan, Keerthiram, Sachan, Mrinmaya",,,Towards Aligning Language Models with Textual Feedback,,,10.18653/v1/2024.emnlp-main.1129 , ,,"We present ALT (ALignment with Textual feedback), an approach that aligns language models with user preferences expressed in text. We argue that text offers greater expressiveness, enabling users to provide richer feedback than simple comparative preferences and this richer feedback can lead to more efficient and effective alignment. ALT aligns the model by conditioning its generation on the textual feedback. Our method relies solely on language modeling techniques and requires minimal hyper-parameter tuning, though it still presents the main benefit of RL-based algorithms and can effectively learn from textual feedback. We explore the efficacy and efficiency of textual feedback across different tasks such as toxicity reduction, summarization, and dialog response. We find that ALT outperforms PPO for the task of toxicity reduction while being able to match its performance on summarization with only 20{\%} of the samples. We also explore how ALT can be used with feedback provided by an existing LLM.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3637,"**Title**Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models

**Abstract**We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69{\%} with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction.","Long, Do Xuan, Yen, Duong Ngoc, Luu, Anh Tuan, Kawaguchi, Kenji, Kan, Min-Yen, Chen, Nancy F.",,,"Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models",,,10.18653/v1/2024.emnlp-main.1135 , ,,"We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69{\%} with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction.",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detox,
3638,"**Title**{B}ias{W}ipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability

**Abstract**Toxic content detection plays a vital role in addressing the misuse of social media platforms to harm people or groups due to their race, gender or ethnicity. However, due to the nature of the datasets, systems develop an unintended bias due to the over-generalization of the model to the training data. This compromises the fairness of the systems, which can impact certain groups due to their race, gender, etc.Existing methods mitigate bias using data augmentation, adversarial learning, etc., which require re-training and adding extra parameters to the model.In this work, we present a robust and generalizable technique \textit{BiasWipe} to mitigate unintended bias in language models. \textit{BiasWipe} utilizes model interpretability using Shapley values, which achieve fairness by pruning the neuron weights responsible for unintended bias. It first identifies the neuron weights responsible for unintended bias and then achieves fairness by pruning them without loss of original performance. It does not require re-training or adding extra parameters to the model. To show the effectiveness of our proposed technique for bias unlearning, we perform extensive experiments for Toxic content detection for BERT, RoBERTa, and GPT models. .","Mamta, Mamta, Chigrupaatii, Rishikant, Ekbal, Asif",,,{B}ias{W}ipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability,,,10.18653/v1/2024.emnlp-main.1172 , ,,"Toxic content detection plays a vital role in addressing the misuse of social media platforms to harm people or groups due to their race, gender or ethnicity. However, due to the nature of the datasets, systems develop an unintended bias due to the over-generalization of the model to the training data. This compromises the fairness of the systems, which can impact certain groups due to their race, gender, etc.Existing methods mitigate bias using data augmentation, adversarial learning, etc., which require re-training and adding extra parameters to the model.In this work, we present a robust and generalizable technique \textit{BiasWipe} to mitigate unintended bias in language models. \textit{BiasWipe} utilizes model interpretability using Shapley values, which achieve fairness by pruning the neuron weights responsible for unintended bias. It first identifies the neuron weights responsible for unintended bias and then achieves fairness by pruning them without loss of original performance. It does not require re-training or adding extra parameters to the model. To show the effectiveness of our proposed technique for bias unlearning, we perform extensive experiments for Toxic content detection for BERT, RoBERTa, and GPT models. .",,,,, ,  Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,detection,
3639,"**Title**Aligning Large Language Models via Fine-grained Supervision

**Abstract**Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations. Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process. However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences. To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision. Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content. The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model. Our experiment results demonstrate that this approach can improve LLM performance by up to 5.1{\%} in terms of win rate against the reference model, compared with the traditional PPO model.","Xu, Dehong, Qiu, Liang, Kim, Minseok, Ladhak, Faisal, Do, Jaeyoung",,,Aligning Large Language Models via Fine-grained Supervision,,,10.18653/v1/2024.acl-short.62 , ,,"Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations. Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process. However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences. To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision. Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content. The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model. Our experiment results demonstrate that this approach can improve LLM performance by up to 5.1{\%} in terms of win rate against the reference model, compared with the traditional PPO model.",,,,, ,  Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),,detox,
3640,"**Title**Unlearning Traces the Influential Training Data of Language Models

**Abstract**Identifying the training datasets that influence a language model`s outputs is essential for minimizing the generation of harmful content and enhancing its performance. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac: unlearning traces the influence of a training dataset on the model`s performance. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model`s predictions change after unlearning. Furthermore, we propose a more scalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the unlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being efficient for massive training datasets. In the experiments, we examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Our methods estimate their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple checkpoints.","Isonuma, Masaru, Titov, Ivan",,,Unlearning Traces the Influential Training Data of Language Models,,,10.18653/v1/2024.acl-long.343 , ,,"Identifying the training datasets that influence a language model`s outputs is essential for minimizing the generation of harmful content and enhancing its performance. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac: unlearning traces the influence of a training dataset on the model`s performance. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model`s predictions change after unlearning. Furthermore, we propose a more scalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the unlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being efficient for massive training datasets. In the experiments, we examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Our methods estimate their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple checkpoints.",,,,, ,  Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,evaluation,
3641,"**Title**{D}e{T}ex{D}: A Benchmark Dataset for Delicate Text Detection

**Abstract**Over the past few years, much research has been conducted to identify and regulate toxic language. However, few studies have addressed a broader range of sensitive texts that are not necessarily overtly toxic. In this paper, we introduce and define a new category of sensitive text called {\textquotedblleft}delicate text.{\textquotedblright} We provide the taxonomy of delicate text and present a detailed annotation scheme. We annotate DeTexD, the first benchmark dataset for delicate text detection. The significance of the difference in the definitions is highlighted by the relative performance deltas between models trained each definitions and corpora and evaluated on the other. We make publicly available the DeTexD Benchmark dataset, annotation guidelines, and baseline model for delicate text detection.","Yavnyi, Serhii, Sliusarenko, Oleksii, Razzaghi, Jade, Nahorna, Olena, Mo, Yichen, Hovakimyan, Knar, Chernodub, Artem",,,{D}e{T}ex{D}: A Benchmark Dataset for Delicate Text Detection,,,10.18653/v1/2023.woah-1.2 , ,,"Over the past few years, much research has been conducted to identify and regulate toxic language. However, few studies have addressed a broader range of sensitive texts that are not necessarily overtly toxic. In this paper, we introduce and define a new category of sensitive text called {\textquotedblleft}delicate text.{\textquotedblright} We provide the taxonomy of delicate text and present a detailed annotation scheme. We annotate DeTexD, the first benchmark dataset for delicate text detection. The significance of the difference in the definitions is highlighted by the relative performance deltas between models trained each definitions and corpora and evaluated on the other. We make publicly available the DeTexD Benchmark dataset, annotation guidelines, and baseline model for delicate text detection.",,,,, ,  The 7th Workshop on Online Abuse and Harms (WOAH),,detection,
3642,"**Title**An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models

**Abstract**Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from massive human-written data which contains latent societal biases and toxic contents. In this paper, we leverage the primary task of PTLMs, i.e., language modeling, and propose a new metric to quantify manifested implicit representational harms in PTLMs towards 13 marginalized demographics. Using this metric, we conducted an empirical analysis of 24 widely used PTLMs. Our analysis provides insights into the correlation between the proposed metric in this work and other related metrics for representational harm. We observe that our metric correlates with most of the gender-specific metrics in the literature. Through extensive experiments, we explore the connections between PTLMs architectures and representational harms across two dimensions: depth and width of the networks. We found that prioritizing depth over width, mitigates representational harms in some PTLMs. Our code and data can be found at [place holder].","Hosseini, Saghar, Palangi, Hamid, Awadallah, Ahmed Hassan",,,An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models,,,10.18653/v1/2023.trustnlp-1.11 , ,,"Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from massive human-written data which contains latent societal biases and toxic contents. In this paper, we leverage the primary task of PTLMs, i.e., language modeling, and propose a new metric to quantify manifested implicit representational harms in PTLMs towards 13 marginalized demographics. Using this metric, we conducted an empirical analysis of 24 widely used PTLMs. Our analysis provides insights into the correlation between the proposed metric in this work and other related metrics for representational harm. We observe that our metric correlates with most of the gender-specific metrics in the literature. Through extensive experiments, we explore the connections between PTLMs architectures and representational harms across two dimensions: depth and width of the networks. We found that prioritizing depth over width, mitigates representational harms in some PTLMs. Our code and data can be found at [place holder].",,,,, ,  Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023),,detection,
3643,"**Title**Style Locality for Controllable Generation with k{NN} Language Models

**Abstract**Recent language models have been improved by the addition of external memory. Nearest neighbor language models retrieve similar contexts to assist in word prediction. The addition of locality levels allows a model to learn how to weight neighbors based on their relative location to the current text in source documents, and have been shown to further improve model performance. Nearest neighbor models have been explored for controllable generation but have not examined the use of locality levels. We present a novel approach for this purpose and evaluate it using automatic and human evaluation on politeness, formality, supportiveness, and toxicity textual data. We find that our model is successfully able to control style and provides a better fluency-style trade-off than previous work","Nawezi, Gilles, Flek, Lucie, Welch, Charles",,,Style Locality for Controllable Generation with k{NN} Language Models,,, , ,,"Recent language models have been improved by the addition of external memory. Nearest neighbor language models retrieve similar contexts to assist in word prediction. The addition of locality levels allows a model to learn how to weight neighbors based on their relative location to the current text in source documents, and have been shown to further improve model performance. Nearest neighbor models have been explored for controllable generation but have not examined the use of locality levels. We present a novel approach for this purpose and evaluate it using automatic and human evaluation on politeness, formality, supportiveness, and toxicity textual data. We find that our model is successfully able to control style and provides a better fluency-style trade-off than previous work",,,,, ,  Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants!,,detox,
3644,"**Title**{QC}on at {S}em{E}val-2023 Task 10: Data Augmentation and Model Ensembling for Detection of Online Sexism

**Abstract**The web contains an abundance of user- generated content. While this content is useful for many applications, it poses many challenges due to the presence of offensive, biased, and overall toxic language. In this work, we present a system that identifies and classifies sexist content at different levels of granularity. Using transformer-based models, we explore the value of data augmentation, use of ensemble methods, and leverage in-context learning using foundation models to tackle the task. We evaluate the different components of our system both quantitatively and qualitatively. Our best systems achieve an F1 score of 0.84 for the binary classification task aiming to identify whether a given content is sexist or not and 0.64 and 0.47 for the two multi-class tasks that aim to identify the coarse and fine-grained types of sexism present in the given content respectively.","Feely, Weston, Gupta, Prabhakar, Mohanty, Manas Ranjan, Chon, Timothy, Kundu, Tuhin, Singh, Vijit, Atluri, Sandeep, Roosta, Tanya, Ghaderi, Viviane, Schulam, Peter",,,{QC}on at {S}em{E}val-2023 Task 10: Data Augmentation and Model Ensembling for Detection of Online Sexism,,,10.18653/v1/2023.semeval-1.175 , ,,"The web contains an abundance of user- generated content. While this content is useful for many applications, it poses many challenges due to the presence of offensive, biased, and overall toxic language. In this work, we present a system that identifies and classifies sexist content at different levels of granularity. Using transformer-based models, we explore the value of data augmentation, use of ensemble methods, and leverage in-context learning using foundation models to tackle the task. We evaluate the different components of our system both quantitatively and qualitatively. Our best systems achieve an F1 score of 0.84 for the binary classification task aiming to identify whether a given content is sexist or not and 0.64 and 0.47 for the two multi-class tasks that aim to identify the coarse and fine-grained types of sexism present in the given content respectively.",,,,, ,  Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023),,detection,
3646,"**Title**Team-{KEC}@{LT}-{EDI}: Detecting Signs of Depression from Social Media Text

**Abstract**The rise of social media has led to a drastic surge in the dissemination of hostile and toxic content, fostering an alarming proliferation of hate speech, inflammatory remarks, and abusive language. The exponential growth of social media has facilitated the widespread circulation of hostile and toxic content, giving rise to an unprecedented influx of hate speech, incendiary language, and abusive rhetoric. The study utilized different techniques to represent the text data in a numerical format. Word embedding techniques aim to capture the semantic and syntactic information of the text data, which is essential in text classification tasks. The study utilized various techniques such as CNN, BERT, and N-gram to classify social media posts into depression and non-depression categories. Text classification tasks often rely on deep learning techniques such as Convolutional Neural Networks (CNN), while the BERT model, which is pre-trained, has shown exceptional performance in a range of natural language processing tasks. To assess the effectiveness of the suggested approaches, the research employed multiple metrics, including accuracy, precision, recall, and F1-score. The outcomes of the investigation indicate that the suggested techniques can identify symptoms of depression with an average accuracy rate of 56{\%}.","S, Malliga, Shanmugavadivel, Kogilavani, S, Arunaa, R, Gokulkrishna, A, Chandramukhii",,,Team-{KEC}@{LT}-{EDI}: Detecting Signs of Depression from Social Media Text,,, , ,,"The rise of social media has led to a drastic surge in the dissemination of hostile and toxic content, fostering an alarming proliferation of hate speech, inflammatory remarks, and abusive language. The exponential growth of social media has facilitated the widespread circulation of hostile and toxic content, giving rise to an unprecedented influx of hate speech, incendiary language, and abusive rhetoric. The study utilized different techniques to represent the text data in a numerical format. Word embedding techniques aim to capture the semantic and syntactic information of the text data, which is essential in text classification tasks. The study utilized various techniques such as CNN, BERT, and N-gram to classify social media posts into depression and non-depression categories. Text classification tasks often rely on deep learning techniques such as Convolutional Neural Networks (CNN), while the BERT model, which is pre-trained, has shown exceptional performance in a range of natural language processing tasks. To assess the effectiveness of the suggested approaches, the research employed multiple metrics, including accuracy, precision, recall, and F1-score. The outcomes of the investigation indicate that the suggested techniques can identify symptoms of depression with an average accuracy rate of 56{\%}.",,,,, ,"  Proceedings of the Third Workshop on Language Technology for Equality, Diversity and Inclusion",,detection,
3647,"**Title**Attention-Enhancing Backdoor Attacks Against {BERT}-based Models

**Abstract**Recent studies have revealed that Backdoor Attacks can threaten the safety of natural language processing (NLP) models. Investigating the strategies of backdoor attacks will help to understand the model`s vulnerability. Most existing textual backdoor attacks focus on generating stealthy triggers or modifying model weights. In this paper, we directly target the interior structure of neural networks and the backdoor mechanism. We propose a novel Trojan Attention Loss (TAL), which enhances the Trojan behavior by directly manipulating the attention patterns. Our loss can be applied to different attacking methods to boost their attack efficacy in terms of attack successful rates and poisoning rates. It applies to not only traditional dirty-label attacks, but also the more challenging clean-label attacks. We validate our method on different backbone models (BERT, RoBERTa, and DistilBERT) and various tasks (Sentiment Analysis, Toxic Detection, and Topic Classification).","Lyu, Weimin, Zheng, Songzhu, Pang, Lu, Ling, Haibin, Chen, Chao",,,Attention-Enhancing Backdoor Attacks Against {BERT}-based Models,,,10.18653/v1/2023.findings-emnlp.716 , ,,"Recent studies have revealed that Backdoor Attacks can threaten the safety of natural language processing (NLP) models. Investigating the strategies of backdoor attacks will help to understand the model`s vulnerability. Most existing textual backdoor attacks focus on generating stealthy triggers or modifying model weights. In this paper, we directly target the interior structure of neural networks and the backdoor mechanism. We propose a novel Trojan Attention Loss (TAL), which enhances the Trojan behavior by directly manipulating the attention patterns. Our loss can be applied to different attacking methods to boost their attack efficacy in terms of attack successful rates and poisoning rates. It applies to not only traditional dirty-label attacks, but also the more challenging clean-label attacks. We validate our method on different backbone models (BERT, RoBERTa, and DistilBERT) and various tasks (Sentiment Analysis, Toxic Detection, and Topic Classification).",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2023,,detection,
3648,"**Title**{COBRA} Frames: Contextual Reasoning about Effects and Harms of Offensive Statements

**Abstract**Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance {\textquotedblleft}your English is very good{\textquotedblright} may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement`s offensiveness (29{\%} accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.","Zhou, Xuhui, Zhu, Hao, Yerukola, Akhila, Davidson, Thomas, Hwang, Jena D., Swayamdipta, Swabha, Sap, Maarten",,,{COBRA} Frames: Contextual Reasoning about Effects and Harms of Offensive Statements,,,10.18653/v1/2023.findings-acl.392 , ,,"Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance {\textquotedblleft}your English is very good{\textquotedblright} may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement`s offensiveness (29{\%} accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.",,,,, ,  Findings of the Association for Computational Linguistics: ACL 2023,,detection#methodology,
3650,"**Title**Analyzing Norm Violations in Live-Stream Chat

**Abstract**Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the informational context humans use in live-stream moderation, and train models leveraging context to identify norm violations. Our results show that appropriate contextual information can boost moderation performance by 35{\%}.","Moon, Jihyung, Lee, Dong-Ho, Cho, Hyundong, Jin, Woojeong, Park, Chan, Kim, Minwoo, May, Jonathan, Pujara, Jay, Park, Sungjoon",,,Analyzing Norm Violations in Live-Stream Chat,,,10.18653/v1/2023.emnlp-main.55 , ,,"Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the informational context humans use in live-stream moderation, and train models leveraging context to identify norm violations. Our results show that appropriate contextual information can boost moderation performance by 35{\%}.",,,,, ,  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,,detection,
3651,"**Title**Inference-Time Policy Adapters ({IPA}): Tailoring Extreme-Scale {LM}s without Fine-tuning

**Abstract**While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.","Lu, Ximing, Brahman, Faeze, West, Peter, Jung, Jaehun, Chandu, Khyathi, Ravichander, Abhilasha, Ammanabrolu, Prithviraj, Jiang, Liwei, Ramnath, Sahana, Dziri, Nouha, Fisher, Jillian, Lin, Bill, Hallinan, Skyler, Qin, Lianhui, Ren, Xiang, Welleck, Sean, Choi, Yejin",,,Inference-Time Policy Adapters ({IPA}): Tailoring Extreme-Scale {LM}s without Fine-tuning,,,10.18653/v1/2023.emnlp-main.424 , ,,"While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.",,,,, ,  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,,detox,
3656,"**Title**{S}afe{C}onv: Explaining and Correcting Conversational Unsafe Behavior

**Abstract**One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SafeConv for the research of conversational safety: (1) Besides the utterance-level safety labels, SafeConv also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SafeConv provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. By virtue of the comprehensive annotation of SafeConv, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at \url{https://github.com/mianzhang/SafeConv}.","Zhang, Mian, Jin, Lifeng, Song, Linfeng, Mi, Haitao, Chen, Wenliang, Yu, Dong",,,{S}afe{C}onv: Explaining and Correcting Conversational Unsafe Behavior,,,10.18653/v1/2023.acl-long.2 , ,,"One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SafeConv for the research of conversational safety: (1) Besides the utterance-level safety labels, SafeConv also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SafeConv provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. By virtue of the comprehensive annotation of SafeConv, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at \url{https://github.com/mianzhang/SafeConv}.",,,,, ,  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,Gen_dataset#detox,
3658,"**Title**Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks

**Abstract**The last few years have witnessed an exponential rise in the propagation of offensive text on social media. Identification of this text with high precision is crucial for the well-being of society. Most of the existing approaches tend to give high toxicity scores to innocuous statements (e.g., {\textquotedblleft}I am a gay man{\textquotedblright}). These false positives result from over-generalization on the training data where specific terms in the statement may have been used in a pejorative sense (e.g., {\textquotedblleft}gay{\textquotedblright}). Emphasis on such words alone can lead to discrimination against the classes these systems are designed to protect. In this paper, we address the problem of offensive language detection on Twitter, while also detecting the type and the target of the offense. We propose a novel approach called SyLSTM, which integrates syntactic features in the form of the dependency parse tree of a sentence and semantic features in the form of word embeddings into a deep learning architecture using a Graph Convolutional Network. Results show that the proposed approach significantly outperforms the state-of-the-art BERT model with orders of magnitude fewer number of parameters.","Goel, Divyam, Sharma, Raksha",,,Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks,,,10.18653/v1/2022.socialnlp-1.4 , ,,"The last few years have witnessed an exponential rise in the propagation of offensive text on social media. Identification of this text with high precision is crucial for the well-being of society. Most of the existing approaches tend to give high toxicity scores to innocuous statements (e.g., {\textquotedblleft}I am a gay man{\textquotedblright}). These false positives result from over-generalization on the training data where specific terms in the statement may have been used in a pejorative sense (e.g., {\textquotedblleft}gay{\textquotedblright}). Emphasis on such words alone can lead to discrimination against the classes these systems are designed to protect. In this paper, we address the problem of offensive language detection on Twitter, while also detecting the type and the target of the offense. We propose a novel approach called SyLSTM, which integrates syntactic features in the form of the dependency parse tree of a sentence and semantic features in the form of word embeddings into a deep learning architecture using a Graph Convolutional Network. Results show that the proposed approach significantly outperforms the state-of-the-art BERT model with orders of magnitude fewer number of parameters.",,,,, ,  Proceedings of the Tenth International Workshop on Natural Language Processing for Social Media,,detection,
3659,"**Title**Guiding the Release of Safer {E}2{E} Conversational {AI} through Value Sensitive Design

**Abstract**Over the last several years, end-to-end neural conversational agents have vastly improved their ability to carry unrestricted, open-domain conversations with humans. However, these models are often trained on large datasets from the Internet and, as a result, may learn undesirable behaviours from this data, such as toxic or otherwise harmful language. Thus, researchers must wrestle with how and when to release these models. In this paper, we survey recent and related work to highlight tensions between values, potential positive impact, and potential harms. We also provide a framework to support practitioners in deciding whether and how to release these models, following the tenets of value-sensitive design.","Bergman, A. Stevie, Abercrombie, Gavin, Spruit, Shannon, Hovy, Dirk, Dinan, Emily, Boureau, Y-Lan, Rieser, Verena",,,Guiding the Release of Safer {E}2{E} Conversational {AI} through Value Sensitive Design,,,10.18653/v1/2022.sigdial-1.4 , ,,"Over the last several years, end-to-end neural conversational agents have vastly improved their ability to carry unrestricted, open-domain conversations with humans. However, these models are often trained on large datasets from the Internet and, as a result, may learn undesirable behaviours from this data, such as toxic or otherwise harmful language. Thus, researchers must wrestle with how and when to release these models. In this paper, we survey recent and related work to highlight tensions between values, potential positive impact, and potential harms. We also provide a framework to support practitioners in deciding whether and how to release these models, following the tenets of value-sensitive design.",,,,, ,  Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue,,survey,
3660,"**Title**{GUTS} at {S}em{E}val-2022 Task 4: Adversarial Training and Balancing Methods for Patronizing and Condescending Language Detection

**Abstract**Patronizing and Condescending Language (PCL) towards vulnerable communities in general media has been shown to have potentially harmful effects. Due to its subtlety and the good intentions behind its use, the audience is not aware of the language`s toxicity. In this paper, we present our method for the SemEval-2022 Task4 titled {\textquotedblleft}Patronizing and Condescending Language Detection{\textquotedblright}. In Subtask A, a binary classification task, we introduce adversarial training based on Fast Gradient Method (FGM) and employ pre-trained model in a unified architecture. For Subtask B, framed as a multi-label classification problem, we utilize various improved multi-label cross-entropy loss functions and analyze the performance of our method. In the final evaluation, our system achieved official rankings of 17/79 and 16/49 on Subtask A and Subtask B, respectively. In addition, we explore the relationship between PCL and emotional polarity and intensity it contains.","Lu, Junyu, Zhang, Hao, Zhang, Tongyue, Wang, Hongbo, Zhu, Haohao, Xu, Bo, Lin, Hongfei",,,{GUTS} at {S}em{E}val-2022 Task 4: Adversarial Training and Balancing Methods for Patronizing and Condescending Language Detection,,,10.18653/v1/2022.semeval-1.58 , ,,"Patronizing and Condescending Language (PCL) towards vulnerable communities in general media has been shown to have potentially harmful effects. Due to its subtlety and the good intentions behind its use, the audience is not aware of the language`s toxicity. In this paper, we present our method for the SemEval-2022 Task4 titled {\textquotedblleft}Patronizing and Condescending Language Detection{\textquotedblright}. In Subtask A, a binary classification task, we introduce adversarial training based on Fast Gradient Method (FGM) and employ pre-trained model in a unified architecture. For Subtask B, framed as a multi-label classification problem, we utilize various improved multi-label cross-entropy loss functions and analyze the performance of our method. In the final evaluation, our system achieved official rankings of 17/79 and 16/49 on Subtask A and Subtask B, respectively. In addition, we explore the relationship between PCL and emotional polarity and intensity it contains.",,,,, ,  Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022),,detection,
3663,"**Title**Nearest Neighbor Language Models for Stylistic Controllable Generation

**Abstract**Recent language modeling performance has been greatly improved by the use of external memory. This memory encodes the context so that similar contexts can be recalled during decoding. This similarity depends on how the model learns to encode context, which can be altered to include other attributes, such as style. We construct and evaluate an architecture for this purpose, using corpora annotated for politeness, formality, and toxicity. Through extensive experiments and human evaluation we demonstrate the potential of our method to generate text while controlling style. We find that style-specific datastores improve generation performance, though results vary greatly across styles, and the effect of pretraining data and specific styles should be explored in future work.","Trotta, Severino, Flek, Lucie, Welch, Charles",,,Nearest Neighbor Language Models for Stylistic Controllable Generation,,,10.18653/v1/2022.gem-1.25 , ,,"Recent language modeling performance has been greatly improved by the use of external memory. This memory encodes the context so that similar contexts can be recalled during decoding. This similarity depends on how the model learns to encode context, which can be altered to include other attributes, such as style. We construct and evaluate an architecture for this purpose, using corpora annotated for politeness, formality, and toxicity. Through extensive experiments and human evaluation we demonstrate the potential of our method to generate text while controlling style. We find that style-specific datastores improve generation performance, though results vary greatly across styles, and the effect of pretraining data and specific styles should be explored in future work.",,,,, ,"  Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",,detox,
3665,"**Title**Towards Robust {NLG} Bias Evaluation with Syntactically-diverse Prompts

**Abstract**We present a robust methodology for evaluating biases in natural language generation(NLG) systems. Previous works use fixed hand-crafted prefix templates with mentions of various demographic groups to prompt models to generate continuations for bias analysis. These fixed prefix templates could themselves be specific in terms of styles or linguistic structures, which may lead to unreliable fairness conclusions that are not representative of the general trends from tone varying prompts. To study this problem, we paraphrase the prompts with different syntactic structures and use these to evaluate demographic bias in NLG systems. Our results suggest similar overall bias trends but some syntactic structures lead to contradictory conclusions compared to past works. We show that our methodology is more robust and that some syntactic structures prompt more toxic content while others could prompt less biased generation. This suggests the importance of not relying on a fixed syntactic structure and using tone-invariant prompts. Introducing syntactically-diverse prompts can achieve more robust NLG (bias) evaluation.","Aggarwal, Arshiya, Sun, Jiao, Peng, Nanyun",,,Towards Robust {NLG} Bias Evaluation with Syntactically-diverse Prompts,,,10.18653/v1/2022.findings-emnlp.445 , ,,"We present a robust methodology for evaluating biases in natural language generation(NLG) systems. Previous works use fixed hand-crafted prefix templates with mentions of various demographic groups to prompt models to generate continuations for bias analysis. These fixed prefix templates could themselves be specific in terms of styles or linguistic structures, which may lead to unreliable fairness conclusions that are not representative of the general trends from tone varying prompts. To study this problem, we paraphrase the prompts with different syntactic structures and use these to evaluate demographic bias in NLG systems. Our results suggest similar overall bias trends but some syntactic structures lead to contradictory conclusions compared to past works. We show that our methodology is more robust and that some syntactic structures prompt more toxic content while others could prompt less biased generation. This suggests the importance of not relying on a fixed syntactic structure and using tone-invariant prompts. Introducing syntactically-diverse prompts can achieve more robust NLG (bias) evaluation.",,,,, ,  Findings of the Association for Computational Linguistics: EMNLP 2022,,evaluation#methodology,
3667,"**Title**Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space

**Abstract**Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50{\%}, and for improving computation efficiency with a simple early exit rule, saving 20{\%} of computation on average.","Geva, Mor, Caciularu, Avi, Wang, Kevin, Goldberg, Yoav",,,Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space,,,10.18653/v1/2022.emnlp-main.3 , ,,"Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50{\%}, and for improving computation efficiency with a simple early exit rule, saving 20{\%} of computation on average.",,,,, ,  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,,detox,
3668,"**Title**Gradient-based Constrained Sampling from Language Models

**Abstract**Large pretrained language models are successful at generating fluent text but are notoriously hard to controllably sample from. In this work, we study constrained sampling from such language models, i.e., generating text that satisfies user-defined constraints, while maintaining fluency and model`s performance in a downstream task. We propose MuCoLa{---}a sampling procedure that combines the log-likelihood of the language model with arbitrary (differentiable) constraints in a single energy function, and then generates samples in a non-autoregressive manner. Specifically, it initializes the entire output sequence with noise and follows a Markov chain defined by Langevin Dynamics using the gradients of this energy. We evaluate MuCoLa on text generation with soft and hard constraints as well as their combinations, obtaining significant improvements over competitive baselines for toxicity avoidance, sentiment control, and keyword-guided generation.","Kumar, Sachin, Paria, Biswajit, Tsvetkov, Yulia",,,Gradient-based Constrained Sampling from Language Models,,,10.18653/v1/2022.emnlp-main.144 , ,,"Large pretrained language models are successful at generating fluent text but are notoriously hard to controllably sample from. In this work, we study constrained sampling from such language models, i.e., generating text that satisfies user-defined constraints, while maintaining fluency and model`s performance in a downstream task. We propose MuCoLa{---}a sampling procedure that combines the log-likelihood of the language model with arbitrary (differentiable) constraints in a single energy function, and then generates samples in a non-autoregressive manner. Specifically, it initializes the entire output sequence with noise and follows a Markov chain defined by Langevin Dynamics using the gradients of this energy. We evaluate MuCoLa on text generation with soft and hard constraints as well as their combinations, obtaining significant improvements over competitive baselines for toxicity avoidance, sentiment control, and keyword-guided generation.",,,,, ,  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,,detox,
3669,"**Title**{P}rosocial{D}ialog: A Prosocial Backbone for Conversational Agents

**Abstract**Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. To address this issue, we introduce ProsocialDialog, the first large-scale multi-turn dialogue dataset to teach conversational agents to respond to problematic content following social norms. Covering diverse unethical, problematic, biased, and toxic situations, ProsocialDialog contains responses that encourage prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb, RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue safety labels accompanied by free-form rationales.With this dataset, we introduce a dialogue safety detection module, Canary, capable of generating RoTs given conversational context, and a socially-informed dialogue agent, Prost. Empirical results show that Prost generates more socially acceptable dialogues compared to other state-of-the-art language and dialogue models in both in-domain and out-of-domain settings. Additionally, Canary effectively guides conversational agents and off-the-shelf language models to generate significantly more prosocial responses. Our work highlights the promise and importance of creating and steering conversational AI to be socially responsible.","Kim, Hyunwoo, Yu, Youngjae, Jiang, Liwei, Lu, Ximing, Khashabi, Daniel, Kim, Gunhee, Choi, Yejin, Sap, Maarten",,,{P}rosocial{D}ialog: A Prosocial Backbone for Conversational Agents,,,10.18653/v1/2022.emnlp-main.267 , ,,"Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. To address this issue, we introduce ProsocialDialog, the first large-scale multi-turn dialogue dataset to teach conversational agents to respond to problematic content following social norms. Covering diverse unethical, problematic, biased, and toxic situations, ProsocialDialog contains responses that encourage prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb, RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue safety labels accompanied by free-form rationales.With this dataset, we introduce a dialogue safety detection module, Canary, capable of generating RoTs given conversational context, and a socially-informed dialogue agent, Prost. Empirical results show that Prost generates more socially acceptable dialogues compared to other state-of-the-art language and dialogue models in both in-domain and out-of-domain settings. Additionally, Canary effectively guides conversational agents and off-the-shelf language models to generate significantly more prosocial responses. Our work highlights the promise and importance of creating and steering conversational AI to be socially responsible.",,,,, ,  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,,Gen_dataset#detox,
3671,"**Title**Quantifying Bias from Decoding Techniques in Natural Language Generation

**Abstract**Natural language generation (NLG) models can propagate social bias towards particular demography. Though several studies investigated bias from data and model, NLG task distinctively uses stochastic decoder that can positively or negatively impact the bias-sensitive tokens initially predicted by the model. To address this gap in research, we present an extensive analysis of bias from decoding techniques for open-domain language generation considering the entire decoding space. We analyze to what extent bias metrics like toxicity and sentiment are impacted by the individual components of decoder algorithms. To this extent, we also analyze the trade-off between bias scores and human-annotated generation quality throughout the decoder space. Together, these methods reveal the imperative of testing inference time bias and provide evidence on the usefulness of inspecting the entire decoding spectrum.","Das, Mayukh, Balke, Wolf Tilo",,,Quantifying Bias from Decoding Techniques in Natural Language Generation,,, , ,,"Natural language generation (NLG) models can propagate social bias towards particular demography. Though several studies investigated bias from data and model, NLG task distinctively uses stochastic decoder that can positively or negatively impact the bias-sensitive tokens initially predicted by the model. To address this gap in research, we present an extensive analysis of bias from decoding techniques for open-domain language generation considering the entire decoding space. We analyze to what extent bias metrics like toxicity and sentiment are impacted by the individual components of decoder algorithms. To this extent, we also analyze the trade-off between bias scores and human-annotated generation quality throughout the decoder space. Together, these methods reveal the imperative of testing inference time bias and provide evidence on the usefulness of inspecting the entire decoding spectrum.",,,,, ,  Proceedings of the 29th International Conference on Computational Linguistics,,detection,
3674,"**Title**Director: Generator-Classifiers For Supervised Language Modeling

**Abstract**Current language models achieve low perplexity but their resulting generations still suffer from toxic responses, repetitiveness, and contradictions. The standard language modeling setup fails to address these issues. In this paper, we introduce a new architecture, Director, that consists of a unified generator-classifier with both a language modeling and a classification head for each output token. Training is conducted jointly using both standard language modeling data, and data labeled with desirable and undesirable sequences. Experiments in several settings show that the model has competitive training and decoding speed compared to standard language models while yielding superior results, avoiding undesirable behaviors while maintaining generation quality. It also outperforms existing model guiding approaches in terms of both accuracy and efficiency. Our code is made publicly available.","Arora, Kushal, Shuster, Kurt, Sukhbaatar, Sainbayar, Weston, Jason",,,Director: Generator-Classifiers For Supervised Language Modeling,,,10.18653/v1/2022.aacl-main.39 , ,,"Current language models achieve low perplexity but their resulting generations still suffer from toxic responses, repetitiveness, and contradictions. The standard language modeling setup fails to address these issues. In this paper, we introduce a new architecture, Director, that consists of a unified generator-classifier with both a language modeling and a classification head for each output token. Training is conducted jointly using both standard language modeling data, and data labeled with desirable and undesirable sequences. Experiments in several settings show that the model has competitive training and decoding speed compared to standard language models while yielding superior results, avoiding undesirable behaviors while maintaining generation quality. It also outperforms existing model guiding approaches in terms of both accuracy and efficiency. Our code is made publicly available.",,,,, ,  Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),,detox,
3677,"**Title**Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in {NLP}

**Abstract**This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model`s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1","Schick, Timo, Udupa, Sahana, Sch{\""u}tze, Hinrich",,,Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in {NLP},,,10.1162/tacl_a_00434 , ,,"This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model`s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1",,,,, ,  ,,detection,
3678,"**Title**{S}-{NLP} at {S}em{E}val-2021 Task 5: An Analysis of Dual Networks for Sequence Tagging

**Abstract**The SemEval 2021 task 5: Toxic Spans Detection is a task of identifying considered-toxic spans in text, which provides a valuable, automatic tool for moderating online contents. This paper represents the second-place method for the task, an ensemble of two approaches. While one approach relies on combining different embedding methods to extract diverse semantic and syntactic representations of words in context; the other utilizes extra data with a slightly customized Self-training, a semi-supervised learning technique, for sequence tagging problems. Both of our architectures take advantage of a strong language model, which was fine-tuned on a toxic classification task. Although experimental evidence indicates higher effectiveness of the first approach than the second one, combining them leads to our best results of 70.77 F1-score on the test dataset.","Nguyen, Viet Anh, Nguyen, Tam Minh, Quang Dao, Huy, Huu Pham, Quang",,,{S}-{NLP} at {S}em{E}val-2021 Task 5: An Analysis of Dual Networks for Sequence Tagging,,,10.18653/v1/2021.semeval-1.120 , ,,"The SemEval 2021 task 5: Toxic Spans Detection is a task of identifying considered-toxic spans in text, which provides a valuable, automatic tool for moderating online contents. This paper represents the second-place method for the task, an ensemble of two approaches. While one approach relies on combining different embedding methods to extract diverse semantic and syntactic representations of words in context; the other utilizes extra data with a slightly customized Self-training, a semi-supervised learning technique, for sequence tagging problems. Both of our architectures take advantage of a strong language model, which was fine-tuned on a toxic classification task. Although experimental evidence indicates higher effectiveness of the first approach than the second one, combining them leads to our best results of 70.77 F1-score on the test dataset.",,,,, ,  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),,detection,
3680,"**Title**{RAP}: {R}obustness-{A}ware {P}erturbations for Defending against Backdoor Attacks on {NLP} Models

**Abstract**Backdoor attacks, which maliciously control a well-trained model`s outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods. Our code is available at \url{https://github.com/lancopku/RAP}.","Yang, Wenkai, Lin, Yankai, Li, Peng, Zhou, Jie, Sun, Xu",,,{RAP}: {R}obustness-{A}ware {P}erturbations for Defending against Backdoor Attacks on {NLP} Models,,,10.18653/v1/2021.emnlp-main.659 , ,,"Backdoor attacks, which maliciously control a well-trained model`s outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods. Our code is available at \url{https://github.com/lancopku/RAP}.",,,,, ,  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,,detection,
3681,"**Title**Rethinking Stealthiness of Backdoor Attack against {NLP} Models

**Abstract**Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at \url{https://github.com/lancopku/SOS}.","Yang, Wenkai, Lin, Yankai, Li, Peng, Zhou, Jie, Sun, Xu",,,Rethinking Stealthiness of Backdoor Attack against {NLP} Models,,,10.18653/v1/2021.acl-long.431 , ,,"Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at \url{https://github.com/lancopku/SOS}.",,,,, ,  Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),,detection,
3683,"**Title**{LOGAN}: Local Group Bias Detection by Clustering

**Abstract**Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.","Zhao, Jieyu, Chang, Kai-Wei",,,{LOGAN}: Local Group Bias Detection by Clustering,,,10.18653/v1/2020.emnlp-main.155 , ,,"Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.",,,,, ,  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),,detection,
3685,"**Title**Social Biases in {NLP} Models as Barriers for Persons with Disabilities

**Abstract**Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","Hutchinson, Ben, Prabhakaran, Vinodkumar, Denton, Emily, Webster, Kellie, Zhong, Yu, Denuyl, Stephen",,,Social Biases in {NLP} Models as Barriers for Persons with Disabilities,,,10.18653/v1/2020.acl-main.487 , ,,"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",,,,, ,  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,,detection,
3687,"**Title**Incivility Detection in Online Comments

**Abstract**Incivility in public discourse has been a major concern in recent times as it can affect the quality and tenacity of the discourse negatively. In this paper, we present neural models that can learn to detect name-calling and vulgarity from a newspaper comment section. We show that in contrast to prior work on detecting toxic language, fine-grained incivilities like namecalling cannot be accurately detected by simple models like logistic regression. We apply the models trained on the newspaper comments data to detect uncivil comments in a Russian troll dataset, and find that despite the change of domain, the model makes accurate predictions.","Sadeque, Farig, Rains, Stephen, Shmargad, Yotam, Kenski, Kate, Coe, Kevin, Bethard, Steven",,,Incivility Detection in Online Comments,,,10.18653/v1/S19-1031 , ,,"Incivility in public discourse has been a major concern in recent times as it can affect the quality and tenacity of the discourse negatively. In this paper, we present neural models that can learn to detect name-calling and vulgarity from a newspaper comment section. We show that in contrast to prior work on detecting toxic language, fine-grained incivilities like namecalling cannot be accurately detected by simple models like logistic regression. We apply the models trained on the newspaper comments data to detect uncivil comments in a Russian troll dataset, and find that despite the change of domain, the model makes accurate predictions.",,,,, ,  Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),,detection,
